{"config":{"lang":["fr","en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Documentation de Formation","text":"<p>Bienvenue sur la plateforme de documentation pour les formations en technologies DevOps et Data Engineering !</p>"},{"location":"#objectif","title":"\ud83c\udfaf Objectif","text":"<p>Cette documentation propose des ressources compl\u00e8tes pour apprendre et ma\u00eetriser les technologies essentielles du DevOps et de la Data Engineering.</p>"},{"location":"#sujets-disponibles","title":"\ud83d\udcda Sujets Disponibles","text":""},{"location":"#devops-infrastructure","title":"DevOps &amp; Infrastructure","text":"<ul> <li>\ud83d\udc33 Docker - Conteneurisation et gestion d'applications</li> <li>\u2638\ufe0f Kubernetes - Orchestration de conteneurs \u00e0 grande \u00e9chelle</li> <li>\ud83d\udd04 AirFlow - Orchestration de workflows et pipelines de donn\u00e9es</li> <li>\ud83c\udf3f Git - Gestion de versions et collaboration</li> </ul>"},{"location":"#bases-de-donnees","title":"Bases de Donn\u00e9es","text":"<ul> <li>\ud83d\udcca MongoDB - Base de donn\u00e9es NoSQL orient\u00e9e documents</li> <li>\u26a1 ClickHouse - Base de donn\u00e9es analytique pour le traitement OLAP</li> <li>\ud83d\udd0d Qdrant - Base de donn\u00e9es vectorielle pour la recherche s\u00e9mantique</li> <li>\ud83d\udd2c SQL Avanc\u00e9 - Optimisation et techniques avanc\u00e9es PostgreSQL</li> </ul>"},{"location":"#cloud-computing","title":"Cloud Computing","text":"<ul> <li>\u2601\ufe0f AWS - Amazon Web Services</li> <li>\u2601\ufe0f Azure - Microsoft Azure</li> </ul>"},{"location":"#langues-disponibles","title":"\ud83c\udf0d Langues Disponibles","text":"<p>Toute la documentation est disponible en trois langues :</p> <ul> <li>\ud83c\uddeb\ud83c\uddf7 Fran\u00e7ais</li> <li>\ud83c\uddec\ud83c\udde7 English</li> <li>\ud83c\uddf5\ud83c\uddf1 Polski</li> </ul>"},{"location":"#comment-utiliser-cette-documentation","title":"\ud83d\ude80 Comment Utiliser Cette Documentation","text":"<ol> <li>Choisissez votre sujet dans le menu de navigation</li> <li>S\u00e9lectionnez votre langue pr\u00e9f\u00e9r\u00e9e</li> <li>Suivez les modules dans l'ordre pour une progression logique</li> <li>Pratiquez avec les exercices et projets propos\u00e9s</li> </ol>"},{"location":"#structure-des-modules","title":"\ud83d\udca1 Structure des Modules","text":"<p>Chaque module de formation suit g\u00e9n\u00e9ralement cette structure :</p> <ol> <li>Introduction - Pr\u00e9sentation et concepts de base</li> <li>Installation/Configuration - Mise en place de l'environnement</li> <li>Concepts fondamentaux - Th\u00e9orie et principes</li> <li>Pratique - Commandes, op\u00e9rations et manipulations</li> <li>Concepts avanc\u00e9s - Techniques sophistiqu\u00e9es</li> <li>Bonnes pratiques - Standards et recommandations</li> <li>Projets - Mise en application r\u00e9elle</li> </ol>"},{"location":"#niveaux-de-difficulte","title":"\ud83c\udf93 Niveaux de Difficult\u00e9","text":"<ul> <li>\ud83d\udfe2 D\u00e9butant - Aucune connaissance pr\u00e9alable requise</li> <li>\ud83d\udfe1 Interm\u00e9diaire - Connaissances de base recommand\u00e9es</li> <li>\ud83d\udd34 Avanc\u00e9 - Exp\u00e9rience significative n\u00e9cessaire</li> </ul>"},{"location":"#contribuer","title":"\ud83d\udcdd Contribuer","text":"<p>Cette documentation est en constante \u00e9volution. N'h\u00e9sitez pas \u00e0 :</p> <ul> <li>Signaler des erreurs</li> <li>Proposer des am\u00e9liorations</li> <li>Ajouter des exemples</li> <li>Traduire du contenu</li> </ul>"},{"location":"#ressources-supplementaires","title":"\ud83d\udd17 Ressources Suppl\u00e9mentaires","text":"<p>Pour chaque technologie, vous trouverez :</p> <ul> <li>\ud83d\udcd6 Documentation officielle</li> <li>\ud83c\udfa5 Tutoriels vid\u00e9o</li> <li>\ud83d\udcbb Exemples de code</li> <li>\ud83c\udfcb\ufe0f Exercices pratiques</li> <li>\ud83d\ude80 Projets complets</li> </ul> <p>Bonne formation ! \ud83c\udf89</p>"},{"location":"AirFlow/EN/","title":"Apache Airflow Training for Data Analyst","text":""},{"location":"AirFlow/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through learning Apache Airflow as a Data Analyst. Airflow is an open-source platform for orchestrating and automating complex data workflows.</p>"},{"location":"AirFlow/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand Apache Airflow and its role in ETL orchestration</li> <li>Install and configure Airflow</li> <li>Create DAGs (Directed Acyclic Graphs)</li> <li>Use operators, sensors, and hooks</li> <li>Orchestrate complex data pipelines</li> <li>Integrate with databases and cloud services</li> <li>Create practical projects for your portfolio</li> </ul>"},{"location":"AirFlow/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 Apache Airflow : Open-source and free - \u2705 Python : Free programming language - \u2705 PostgreSQL/SQLite : Free databases - \u2705 Official Documentation : Complete free guides</p> <p>Total Budget: $0</p>"},{"location":"AirFlow/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"AirFlow/EN/#1-airflow-getting-started","title":"1. Airflow Getting Started","text":"<ul> <li>Install Airflow</li> <li>Basic configuration</li> <li>Airflow web interface</li> <li>First DAGs</li> </ul>"},{"location":"AirFlow/EN/#2-fundamental-concepts","title":"2. Fundamental Concepts","text":"<ul> <li>DAGs (Directed Acyclic Graphs)</li> <li>Tasks and dependencies</li> <li>Scheduling and triggers</li> <li>Variables and connections</li> </ul>"},{"location":"AirFlow/EN/#3-operators","title":"3. Operators","text":"<ul> <li>Python operators</li> <li>SQL operators</li> <li>Bash operators</li> <li>Custom operators</li> </ul>"},{"location":"AirFlow/EN/#4-sensors","title":"4. Sensors","text":"<ul> <li>FileSensor</li> <li>SqlSensor</li> <li>HttpSensor</li> <li>Custom sensors</li> </ul>"},{"location":"AirFlow/EN/#5-hooks","title":"5. Hooks","text":"<ul> <li>Database hooks</li> <li>Cloud hooks (AWS, Azure)</li> <li>HTTP hooks</li> <li>Create custom hooks</li> </ul>"},{"location":"AirFlow/EN/#6-variables-and-connections","title":"6. Variables and Connections","text":"<ul> <li>Manage variables</li> <li>Configure connections</li> <li>Security and best practices</li> <li>Dynamic variables</li> </ul>"},{"location":"AirFlow/EN/#7-best-practices","title":"7. Best Practices","text":"<ul> <li>DAG structure</li> <li>Error handling</li> <li>Performance and optimization</li> <li>Testing and debugging</li> </ul>"},{"location":"AirFlow/EN/#8-practical-projects","title":"8. Practical Projects","text":"<ul> <li>Complete ETL pipeline</li> <li>Workflow orchestration</li> <li>Database integration</li> <li>Portfolio projects</li> </ul>"},{"location":"AirFlow/EN/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"AirFlow/EN/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+ : Installed on your system</li> <li>pip : Python package manager</li> <li>PostgreSQL (optional) : For metadata database</li> </ul>"},{"location":"AirFlow/EN/#quick-installation","title":"Quick Installation","text":"<pre><code># Create a virtual environment\npython -m venv airflow-env\n\n# Activate the environment\n# Windows\nairflow-env\\Scripts\\activate\n# Linux/Mac\nsource airflow-env/bin/activate\n\n# Install Airflow\npip install apache-airflow\n\n# Initialize the database\nairflow db init\n\n# Create an admin user\nairflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com\n\n# Start the web server\nairflow webserver --port 8080\n\n# In another terminal, start the scheduler\nairflow scheduler\n</code></pre>"},{"location":"AirFlow/EN/#access-the-web-interface","title":"Access the Web Interface","text":"<ol> <li>Open a browser</li> <li>Go to: <code>http://localhost:8080</code></li> <li>Log in with the created credentials</li> </ol>"},{"location":"AirFlow/EN/#use-cases-for-data-analyst","title":"\ud83d\udcca Use Cases for Data Analyst","text":"<ul> <li>ETL Orchestration : Coordinate data pipelines</li> <li>Scheduling : Schedule recurring tasks</li> <li>Monitoring : Monitor workflow execution</li> <li>Error Handling : Automatic retry and alerts</li> <li>Integration : Connect multiple tools and services</li> </ul>"},{"location":"AirFlow/EN/#remote-installation","title":"\u26a0\ufe0f Remote Installation","text":"<p>If you install Airflow on machine A and want to access it from machine B, see the guide Remote Installation and Access.</p>"},{"location":"AirFlow/EN/#free-resources","title":"\ud83d\udcda Free Resources","text":""},{"location":"AirFlow/EN/#official-documentation","title":"Official Documentation","text":"<ul> <li>Apache Airflow : https://airflow.apache.org/docs/</li> <li>Complete guides</li> <li>Step-by-step tutorials</li> <li>Code examples</li> <li> <p>API Reference</p> </li> <li> <p>GitHub Airflow : https://github.com/apache/airflow</p> </li> <li>Source code</li> <li>DAG examples</li> <li>Contributions</li> </ul>"},{"location":"AirFlow/EN/#external-resources","title":"External Resources","text":"<ul> <li>YouTube : Airflow tutorials</li> <li>Medium : Articles and guides</li> <li>Stack Overflow : Questions and answers</li> </ul>"},{"location":"AirFlow/EN/#certifications-optional","title":"\ud83c\udf93 Certifications (Optional)","text":""},{"location":"AirFlow/EN/#apache-airflow-no-official-certification","title":"Apache Airflow (no official certification)","text":"<ul> <li>Training : Free documentation and tutorials</li> <li>Duration : 2-4 weeks</li> <li>Level : Intermediate to advanced</li> </ul>"},{"location":"AirFlow/EN/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>All examples use Python 3.8+</li> <li>DAGs are tested on Airflow 2.x</li> <li>Paths may vary by operating system</li> <li>Default ports can be modified</li> </ul>"},{"location":"AirFlow/EN/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>This training is designed to be evolving. Feel free to suggest improvements or additional use cases.</p>"},{"location":"AirFlow/EN/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Apache Airflow Documentation</li> <li>GitHub Apache Airflow</li> <li>Airflow Community</li> <li>Airflow Examples</li> </ul>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/","title":"Airflow Installation - Remote Access","text":""},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#objective","title":"\ud83c\udfaf Objective","text":"<p>This guide allows you to install Apache Airflow on Machine A (server) and access it from Machine B (client) via the local network.</p>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#prerequisites","title":"\ud83d\udccb Prerequisites","text":""},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#machine-a-server","title":"Machine A (Server)","text":"<ul> <li>Python 3.8+ installed</li> <li>Administrator access</li> <li>Active network connection</li> <li>Port 8080 available (or another port)</li> </ul>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#machine-b-client","title":"Machine B (Client)","text":"<ul> <li>Web browser</li> <li>Connection to the same local network as Machine A</li> </ul>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#installation-on-machine-a","title":"\ud83d\udd27 Installation on Machine A","text":""},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#step-1-install-python-and-dependencies","title":"Step 1: Install Python and Dependencies","text":"<p>Windows: <pre><code># Check Python\npython --version\n\n# Install pip if needed\npython -m ensurepip --upgrade\n</code></pre></p> <p>Linux: <pre><code># Install Python and pip\nsudo apt update\nsudo apt install python3 python3-pip python3-venv\n</code></pre></p>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#step-2-create-airflow-environment","title":"Step 2: Create Airflow Environment","text":"<pre><code># Create a directory for Airflow\nmkdir airflow-install\ncd airflow-install\n\n# Create a virtual environment\npython -m venv airflow-env\n\n# Activate the environment\n# Windows\nairflow-env\\Scripts\\activate\n# Linux\nsource airflow-env/bin/activate\n\n# Install Airflow\npip install apache-airflow\n\n# Install PostgreSQL provider (optional, for metadata database)\npip install apache-airflow-providers-postgres\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#step-3-configure-airflow","title":"Step 3: Configure Airflow","text":"<pre><code># Initialize the database\nairflow db init\n\n# Create an administrator user\nairflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password admin123\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#step-4-configure-network-access","title":"Step 4: Configure Network Access","text":"<p>Modify Airflow configuration:</p> <ol> <li>Find the <code>airflow.cfg</code> file:</li> <li>Windows: <code>%USERPROFILE%\\airflow\\airflow.cfg</code></li> <li> <p>Linux: <code>~/airflow/airflow.cfg</code></p> </li> <li> <p>Modify the following parameters:</p> </li> </ol> <pre><code>[webserver]\n# Allow access from all interfaces\nweb_server_host = 0.0.0.0\nweb_server_port = 8080\n\n# Disable basic authentication (optional, for development)\nauth_backend = airflow.api.auth.backend.basic_auth\n</code></pre> <p>Or create a custom configuration file:</p> <pre><code># Create a custom airflow.cfg file\nexport AIRFLOW_HOME=/path/to/airflow\nairflow config get-value webserver web_server_host\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#step-5-configure-firewall","title":"Step 5: Configure Firewall","text":"<p>Windows (Firewall):</p> <ol> <li>Open \"Windows Defender Firewall\"</li> <li>\"Advanced settings\"</li> <li>\"Inbound Rules\" \u2192 \"New Rule\"</li> <li>Type: Port</li> <li>Port: 8080 (TCP)</li> <li>Action: Allow the connection</li> <li>Name: \"Airflow Web Server\"</li> </ol> <p>Linux (UFW):</p> <pre><code># Allow port 8080\nsudo ufw allow 8080/tcp\nsudo ufw reload\n</code></pre> <p>Linux (firewalld):</p> <pre><code># Allow port 8080\nsudo firewall-cmd --permanent --add-port=8080/tcp\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#step-6-start-airflow","title":"Step 6: Start Airflow","text":"<p>Terminal 1 - Web Server:</p> <pre><code># Activate the virtual environment\nsource airflow-env/bin/activate  # Linux\n# or\nairflow-env\\Scripts\\activate  # Windows\n\n# Start the web server\nairflow webserver --port 8080 --host 0.0.0.0\n</code></pre> <p>Terminal 2 - Scheduler:</p> <pre><code># Activate the virtual environment\nsource airflow-env/bin/activate  # Linux\n# or\nairflow-env\\Scripts\\activate  # Windows\n\n# Start the scheduler\nairflow scheduler\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#step-7-get-machine-a-ip-address","title":"Step 7: Get Machine A IP Address","text":"<p>Windows: <pre><code>ipconfig\n# Look for \"IPv4 Address\" (e.g., 192.168.1.100)\n</code></pre></p> <p>Linux: <pre><code>ip addr show\n# or\nhostname -I\n# Look for the IP address (e.g., 192.168.1.100)\n</code></pre></p>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#access-from-machine-b","title":"\ud83c\udf10 Access from Machine B","text":""},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#step-1-verify-connectivity","title":"Step 1: Verify Connectivity","text":"<p>From Machine B:</p> <pre><code># Test connection\nping 192.168.1.100  # Replace with Machine A IP\n\n# Test port\ntelnet 192.168.1.100 8080\n# or\ncurl http://192.168.1.100:8080\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#step-2-access-web-interface","title":"Step 2: Access Web Interface","text":"<ol> <li>Open a browser on Machine B</li> <li>Go to: <code>http://192.168.1.100:8080</code></li> <li>Replace <code>192.168.1.100</code> with Machine A IP</li> <li>Log in with:</li> <li>Username: <code>admin</code></li> <li>Password: <code>admin123</code> (or the one you created)</li> </ol>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#security","title":"\ud83d\udd12 Security","text":""},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#recommendations","title":"Recommendations","text":"<ol> <li> <p>Change default password <pre><code>airflow users set-password admin\n</code></pre></p> </li> <li> <p>Use HTTPS (in production)</p> </li> <li>Configure a reverse proxy (nginx, Apache)</li> <li> <p>Use SSL certificates</p> </li> <li> <p>Limit network access</p> </li> <li>Use a VPN</li> <li> <p>Restrict allowed IPs in firewall</p> </li> <li> <p>Enhanced authentication</p> </li> <li>Use OAuth</li> <li>Integrate with LDAP/Active Directory</li> </ol>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#secure-configuration","title":"Secure Configuration","text":"<p>Modify <code>airflow.cfg</code>:</p> <pre><code>[webserver]\n# Enable authentication\nauth_backend = airflow.api.auth.backend.basic_auth\n\n# Limit allowed hosts (optional)\nhostname_callable = airflow.utils.net.get_hostname\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#issue-cannot-connect-from-machine-b","title":"Issue: Cannot connect from Machine B","text":"<p>Solutions:</p> <ol> <li> <p>Check firewall <pre><code># Windows\nnetsh advfirewall firewall show rule name=\"Airflow Web Server\"\n\n# Linux\nsudo ufw status\n</code></pre></p> </li> <li> <p>Verify Airflow listens on 0.0.0.0 <pre><code># Check open ports\nnetstat -an | grep 8080\n# Should show: 0.0.0.0:8080\n</code></pre></p> </li> <li> <p>Check network configuration</p> </li> <li>Both machines are on the same network</li> <li>No VPN blocking connection</li> <li>No proxy interfering</li> </ol>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#issue-connection-refused-error","title":"Issue: \"Connection refused\" error","text":"<p>Solutions:</p> <ol> <li>Verify web server is started</li> <li>Check the port (8080 by default)</li> <li>Check Airflow logs:    <pre><code># Webserver logs\ntail -f ~/airflow/logs/webserver.log\n</code></pre></li> </ol>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#issue-authentication-error","title":"Issue: Authentication error","text":"<p>Solutions:</p> <ol> <li>Verify credentials</li> <li>Recreate user if needed:    <pre><code>airflow users create \\\n    --username admin \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password new_password\n</code></pre></li> </ol>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#advanced-configuration","title":"\ud83d\udcdd Advanced Configuration","text":""},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#use-a-reverse-proxy-nginx","title":"Use a reverse proxy (nginx)","text":"<p>Install nginx:</p> <pre><code># Linux\nsudo apt install nginx\n\n# Configure nginx\nsudo nano /etc/nginx/sites-available/airflow\n</code></pre> <p>Nginx configuration:</p> <pre><code>server {\n    listen 80;\n    server_name airflow.local;\n\n    location / {\n        proxy_pass http://127.0.0.1:8080;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>Enable configuration:</p> <pre><code>sudo ln -s /etc/nginx/sites-available/airflow /etc/nginx/sites-enabled/\nsudo nginx -t\nsudo systemctl reload nginx\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#systemd-service-linux","title":"Systemd service (Linux)","text":"<p>Create a service for Airflow:</p> <pre><code>sudo nano /etc/systemd/system/airflow-webserver.service\n</code></pre> <p>Content:</p> <pre><code>[Unit]\nDescription=Airflow webserver daemon\nAfter=network.target\n\n[Service]\nUser=airflow\nGroup=airflow\nType=simple\nExecStart=/path/to/airflow-env/bin/airflow webserver\nRestart=on-failure\nRestartSec=5s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable service:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable airflow-webserver\nsudo systemctl start airflow-webserver\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#verification","title":"\ud83d\udcca Verification","text":""},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#connection-test","title":"Connection Test","text":"<p>From Machine B:</p> <pre><code># HTTP test\ncurl http://192.168.1.100:8080/health\n\n# Test with authentication\ncurl -u admin:admin123 http://192.168.1.100:8080/api/v1/dags\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#check-logs","title":"Check Logs","text":"<p>On Machine A:</p> <pre><code># Webserver logs\ntail -f ~/airflow/logs/webserver.log\n\n# Scheduler logs\ntail -f ~/airflow/logs/scheduler/*.log\n</code></pre>"},{"location":"AirFlow/EN/INSTALLATION_REMOTE/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>Airflow Documentation</li> <li>Airflow Configuration</li> <li>Airflow Security</li> </ul> <p>Note: This configuration is for a development environment. For production, use enhanced security practices (HTTPS, OAuth authentication, etc.).</p>"},{"location":"AirFlow/EN/01-getting-started/","title":"1. Airflow Getting Started","text":""},{"location":"AirFlow/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Apache Airflow</li> <li>Install Airflow locally</li> <li>Configure the environment</li> <li>Access the web interface</li> <li>Create your first DAG</li> </ul>"},{"location":"AirFlow/EN/01-getting-started/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Airflow</li> <li>Installation</li> <li>Basic Configuration</li> <li>Web Interface</li> <li>First DAG</li> </ol>"},{"location":"AirFlow/EN/01-getting-started/#introduction-to-airflow","title":"Introduction to Airflow","text":""},{"location":"AirFlow/EN/01-getting-started/#what-is-apache-airflow","title":"What is Apache Airflow?","text":"<p>Apache Airflow = Open-source workflow orchestration platform</p> <ul> <li>Workflows : Complex data pipelines</li> <li>Scheduling : Automatic scheduling</li> <li>Monitoring : Real-time monitoring</li> <li>Python : Defined in Python</li> <li>Scalable : From simple to very complex</li> </ul>"},{"location":"AirFlow/EN/01-getting-started/#why-airflow-for-data-analyst","title":"Why Airflow for Data Analyst?","text":"<ul> <li>ETL Orchestration : Coordinate multiple steps</li> <li>Scheduling : Automate recurring tasks</li> <li>Monitoring : See pipeline status</li> <li>Retry : Automatic retry on error</li> <li>Integration : With databases, APIs, cloud services</li> </ul>"},{"location":"AirFlow/EN/01-getting-started/#airflow-components","title":"Airflow Components","text":"<ol> <li>Web Server : Web interface (port 8080)</li> <li>Scheduler : Schedules and executes DAGs</li> <li>Metadata Database : Stores state and metadata</li> <li>Workers : Execute tasks (optional)</li> </ol>"},{"location":"AirFlow/EN/01-getting-started/#installation","title":"Installation","text":""},{"location":"AirFlow/EN/01-getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+ : Installed on your system</li> <li>pip : Python package manager</li> <li>7-8 GB RAM : Minimum recommended</li> </ul>"},{"location":"AirFlow/EN/01-getting-started/#installation-with-pip","title":"Installation with pip","text":"<p>Step 1: Create a virtual environment</p> <pre><code># Create a directory\nmkdir airflow-project\ncd airflow-project\n\n# Create a virtual environment\npython -m venv airflow-env\n\n# Activate the environment\n# Windows\nairflow-env\\Scripts\\activate\n# Linux/Mac\nsource airflow-env/bin/activate\n</code></pre> <p>Step 2: Install Airflow</p> <pre><code># Install Airflow\npip install apache-airflow\n\n# Install additional providers (optional)\npip install apache-airflow-providers-postgres\npip install apache-airflow-providers-http\n</code></pre> <p>Step 3: Initialize the database</p> <pre><code># Initialize SQLite database (default)\nairflow db init\n</code></pre> <p>Step 4: Create an admin user</p> <pre><code>airflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password admin123\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#installation-with-constraints-recommended","title":"Installation with constraints (recommended)","text":"<p>To avoid dependency conflicts:</p> <pre><code># Download constraints\nAIRFLOW_VERSION=2.7.0\nPYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n\n# Install with constraints\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#basic-configuration","title":"Basic Configuration","text":""},{"location":"AirFlow/EN/01-getting-started/#airflowcfg-file","title":"airflow.cfg file","text":"<p>Location: - Windows: <code>%USERPROFILE%\\airflow\\airflow.cfg</code> - Linux/Mac: <code>~/airflow/airflow.cfg</code></p> <p>Important parameters:</p> <pre><code>[core]\n# DAGs directory\ndags_folder = ~/airflow/dags\n\n# Logs directory\nbase_log_folder = ~/airflow/logs\n\n# Timezone\ndefault_timezone = UTC\n\n[webserver]\n# Web server port\nweb_server_port = 8080\n\n# Host (0.0.0.0 for network access)\nweb_server_host = 0.0.0.0\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#environment-variables","title":"Environment Variables","text":"<p>AIRFLOW_HOME:</p> <pre><code># Windows\nset AIRFLOW_HOME=C:\\airflow\n\n# Linux/Mac\nexport AIRFLOW_HOME=~/airflow\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#directory-structure","title":"Directory Structure","text":"<pre><code>airflow/\n\u251c\u2500\u2500 dags/          # Your DAGs\n\u251c\u2500\u2500 logs/          # Execution logs\n\u251c\u2500\u2500 plugins/       # Custom plugins\n\u2514\u2500\u2500 airflow.cfg   # Configuration\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#web-interface","title":"Web Interface","text":""},{"location":"AirFlow/EN/01-getting-started/#start-the-web-server","title":"Start the web server","text":"<pre><code># Activate the virtual environment\nsource airflow-env/bin/activate  # Linux/Mac\n# or\nairflow-env\\Scripts\\activate  # Windows\n\n# Start the web server\nairflow webserver --port 8080\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#start-the-scheduler","title":"Start the scheduler","text":"<p>In another terminal:</p> <pre><code># Activate the virtual environment\nsource airflow-env/bin/activate\n\n# Start the scheduler\nairflow scheduler\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#access-the-interface","title":"Access the interface","text":"<ol> <li>Open a browser</li> <li>Go to: <code>http://localhost:8080</code></li> <li>Log in with:</li> <li>Username: <code>admin</code></li> <li>Password: <code>admin123</code></li> </ol>"},{"location":"AirFlow/EN/01-getting-started/#navigation-in-the-interface","title":"Navigation in the interface","text":"<p>Main tabs: - DAGs: List of all DAGs - Graph: Graphical view of a DAG - Tree: Tree view of executions - Gantt: Gantt chart - Code: DAG source code - Logs: Execution logs</p>"},{"location":"AirFlow/EN/01-getting-started/#first-dag","title":"First DAG","text":""},{"location":"AirFlow/EN/01-getting-started/#create-a-simple-dag","title":"Create a simple DAG","text":"<p>Step 1: Create the DAG file</p> <pre><code># Create the dags directory\nmkdir -p ~/airflow/dags\n\n# Create a DAG file\nnano ~/airflow/dags/my_first_dag.py\n</code></pre> <p>Step 2: DAG code</p> <pre><code>from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\n# Define default arguments\ndefault_args = {\n    'owner': 'data_analyst',\n    'depends_on_past': False,\n    'email': ['admin@example.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'my_first_dag',\n    default_args=default_args,\n    description='My first Airflow DAG',\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['tutorial'],\n)\n\n# Task 1: Print date\nt1 = BashOperator(\n    task_id='print_date',\n    bash_command='date',\n    dag=dag,\n)\n\n# Task 2: Print a message\ndef print_hello():\n    print(\"Hello from Airflow!\")\n\nt2 = PythonOperator(\n    task_id='print_hello',\n    python_callable=print_hello,\n    dag=dag,\n)\n\n# Define dependencies\nt1 &gt;&gt; t2  # t1 runs before t2\n</code></pre> <p>Step 3: Verify the DAG</p> <pre><code># List DAGs\nairflow dags list\n\n# Check syntax\nairflow dags list-import-errors\n\n# Test the DAG\nairflow dags test my_first_dag 2024-01-01\n</code></pre> <p>Step 4: View in web interface</p> <ol> <li>Refresh the web page</li> <li>The DAG <code>my_first_dag</code> appears in the list</li> <li>Click \"Trigger DAG\" to execute it</li> </ol>"},{"location":"AirFlow/EN/01-getting-started/#practical-examples","title":"Practical Examples","text":""},{"location":"AirFlow/EN/01-getting-started/#example-1-dag-with-multiple-tasks","title":"Example 1: DAG with multiple tasks","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data_analyst',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='Example DAG with multiple tasks',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\n# Task 1\nextract = BashOperator(\n    task_id='extract_data',\n    bash_command='echo \"Extracting data...\"',\n    dag=dag,\n)\n\n# Task 2\ntransform = BashOperator(\n    task_id='transform_data',\n    bash_command='echo \"Transforming data...\"',\n    dag=dag,\n)\n\n# Task 3\nload = BashOperator(\n    task_id='load_data',\n    bash_command='echo \"Loading data...\"',\n    dag=dag,\n)\n\n# Define dependencies\nextract &gt;&gt; transform &gt;&gt; load\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#example-2-dag-with-branches","title":"Example 2: DAG with branches","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'branching_dag',\n    description='DAG with branches',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\ndef decide_path():\n    # Logic to decide path\n    return 'path_a'\n\ndecide = PythonOperator(\n    task_id='decide',\n    python_callable=decide_path,\n    dag=dag,\n)\n\npath_a = BashOperator(\n    task_id='path_a',\n    bash_command='echo \"Path A\"',\n    dag=dag,\n)\n\npath_b = BashOperator(\n    task_id='path_b',\n    bash_command='echo \"Path B\"',\n    dag=dag,\n)\n\n# Conditional branching\ndecide &gt;&gt; [path_a, path_b]\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#useful-commands","title":"Useful Commands","text":""},{"location":"AirFlow/EN/01-getting-started/#dag-management","title":"DAG Management","text":"<pre><code># List all DAGs\nairflow dags list\n\n# Check import errors\nairflow dags list-import-errors\n\n# Test a DAG\nairflow dags test my_first_dag 2024-01-01\n\n# Pause a DAG\nairflow dags pause my_first_dag\n\n# Unpause a DAG\nairflow dags unpause my_first_dag\n\n# Delete a DAG\nairflow dags delete my_first_dag\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#task-management","title":"Task Management","text":"<pre><code># Test a task\nairflow tasks test my_first_dag print_date 2024-01-01\n\n# Run a task\nairflow tasks run my_first_dag print_date 2024-01-01\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#database-management","title":"Database Management","text":"<pre><code># Initialize database\nairflow db init\n\n# Upgrade database\nairflow db upgrade\n\n# Reset database (WARNING: deletes everything)\nairflow db reset\n</code></pre>"},{"location":"AirFlow/EN/01-getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"AirFlow/EN/01-getting-started/#issue-dag-not-visible-in-interface","title":"Issue: DAG not visible in interface","text":"<p>Solutions: 1. Verify file is in <code>~/airflow/dags/</code> 2. Check Python syntax 3. Check errors: <code>airflow dags list-import-errors</code> 4. Restart scheduler</p>"},{"location":"AirFlow/EN/01-getting-started/#issue-import-error","title":"Issue: Import error","text":"<p>Solutions: 1. Verify all dependencies are installed 2. Check imports in DAG 3. Check Python paths</p>"},{"location":"AirFlow/EN/01-getting-started/#issue-scheduler-wont-start","title":"Issue: Scheduler won't start","text":"<p>Solutions: 1. Verify database is initialized 2. Check logs: <code>~/airflow/logs/scheduler/</code> 3. Check permissions</p>"},{"location":"AirFlow/EN/01-getting-started/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Airflow = Orchestration of Python workflows</li> <li>DAGs define workflows</li> <li>Tasks are individual steps</li> <li>Scheduler executes DAGs according to schedule</li> <li>Web interface allows monitoring and management</li> </ol>"},{"location":"AirFlow/EN/01-getting-started/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 2. Fundamental Concepts to deepen Airflow concepts.</p>"},{"location":"AirFlow/EN/02-concepts/","title":"2. Airflow Fundamental Concepts","text":""},{"location":"AirFlow/EN/02-concepts/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand DAGs (Directed Acyclic Graphs)</li> <li>Master tasks and dependencies</li> <li>Understand scheduling</li> <li>Use variables and connections</li> <li>Manage executions</li> </ul>"},{"location":"AirFlow/EN/02-concepts/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>DAGs (Directed Acyclic Graphs)</li> <li>Tasks and Dependencies</li> <li>Scheduling</li> <li>Variables and Connections</li> <li>Executions and States</li> </ol>"},{"location":"AirFlow/EN/02-concepts/#dags-directed-acyclic-graphs","title":"DAGs (Directed Acyclic Graphs)","text":""},{"location":"AirFlow/EN/02-concepts/#what-is-a-dag","title":"What is a DAG?","text":"<p>DAG = Directed Acyclic Graph</p> <ul> <li>Directed : Tasks have direction (dependencies)</li> <li>Acyclic : No loops (no circular dependencies)</li> <li>Graph : Visual representation of workflows</li> </ul>"},{"location":"AirFlow/EN/02-concepts/#dag-structure","title":"DAG Structure","text":"<pre><code>from airflow import DAG\nfrom datetime import datetime, timedelta\n\n# Default arguments\ndefault_args = {\n    'owner': 'data_analyst',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Create the DAG\ndag = DAG(\n    'my_dag',\n    default_args=default_args,\n    description='DAG description',\n    schedule_interval='@daily',  # Execution frequency\n    start_date=datetime(2024, 1, 1),\n    catchup=False,  # Don't execute past runs\n    tags=['example'],\n)\n</code></pre>"},{"location":"AirFlow/EN/02-concepts/#dag-properties","title":"DAG Properties","text":"<p>Unique ID: - Must be unique in Airflow installation - Used to identify the DAG</p> <p>Schedule interval: - <code>@daily</code> : Every day - <code>@hourly</code> : Every hour - <code>timedelta(days=1)</code> : Every day - <code>'0 2 * * *'</code> : Cron expression (every day at 2am) - <code>None</code> : Manual trigger only</p> <p>Start date: - Start date of scheduling - Format: <code>datetime(year, month, day)</code></p> <p>Catchup: - <code>True</code> : Executes missing runs since start_date - <code>False</code> : Executes only future runs</p>"},{"location":"AirFlow/EN/02-concepts/#tasks-and-dependencies","title":"Tasks and Dependencies","text":""},{"location":"AirFlow/EN/02-concepts/#what-is-a-task","title":"What is a Task?","text":"<p>Task = Individual step in a DAG</p> <ul> <li>Operator : Task type (Python, Bash, SQL, etc.)</li> <li>Unique ID : Identifier in the DAG</li> <li>Dependencies : Relationships with other tasks</li> </ul>"},{"location":"AirFlow/EN/02-concepts/#operator-types","title":"Operator Types","text":"<p>BashOperator: <pre><code>from airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='bash_task',\n    bash_command='echo \"Hello\"',\n    dag=dag,\n)\n</code></pre></p> <p>PythonOperator: <pre><code>from airflow.operators.python import PythonOperator\n\ndef my_function():\n    print(\"Hello from Python\")\n\ntask = PythonOperator(\n    task_id='python_task',\n    python_callable=my_function,\n    dag=dag,\n)\n</code></pre></p>"},{"location":"AirFlow/EN/02-concepts/#define-dependencies","title":"Define Dependencies","text":"<p>Method 1: Operator &gt;&gt;</p> <pre><code># t1 runs before t2\nt1 &gt;&gt; t2\n\n# Multiple dependencies\nt1 &gt;&gt; [t2, t3] &gt;&gt; t4\n</code></pre> <p>Method 2: set_upstream / set_downstream</p> <pre><code># t1 runs before t2\nt1.set_downstream(t2)\n# or\nt2.set_upstream(t1)\n</code></pre> <p>Method 3: bitshift</p> <pre><code># t1 &gt;&gt; t2 is equivalent to\nt1 &gt;&gt; t2\n</code></pre>"},{"location":"AirFlow/EN/02-concepts/#dependency-example","title":"Dependency Example","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime\n\ndag = DAG('dependencies_example', start_date=datetime(2024, 1, 1))\n\n# Tasks\nextract = BashOperator(task_id='extract', bash_command='echo extract', dag=dag)\ntransform = BashOperator(task_id='transform', bash_command='echo transform', dag=dag)\nload = BashOperator(task_id='load', bash_command='echo load', dag=dag)\nvalidate = BashOperator(task_id='validate', bash_command='echo validate', dag=dag)\n\n# Dependencies\nextract &gt;&gt; transform &gt;&gt; [load, validate]\n</code></pre>"},{"location":"AirFlow/EN/02-concepts/#scheduling","title":"Scheduling","text":""},{"location":"AirFlow/EN/02-concepts/#schedule-interval","title":"Schedule Interval","text":"<p>Common expressions:</p> <pre><code># Every day at midnight\nschedule_interval='@daily'\n# or\nschedule_interval=timedelta(days=1)\n\n# Every hour\nschedule_interval='@hourly'\n# or\nschedule_interval=timedelta(hours=1)\n\n# Every week\nschedule_interval='@weekly'\n\n# Cron expression\nschedule_interval='0 2 * * *'  # Every day at 2am\nschedule_interval='0 */6 * * *'  # Every 6 hours\nschedule_interval='0 0 * * MON'  # Every Monday at midnight\n</code></pre>"},{"location":"AirFlow/EN/02-concepts/#start-date-and-execution-date","title":"Start Date and Execution Date","text":"<p>Start Date: - Start date of scheduling - Format: <code>datetime(2024, 1, 1)</code></p> <p>Execution Date: - Logical execution date - Format: <code>YYYY-MM-DDTHH:MM:SS</code></p> <p>Example: <pre><code>dag = DAG(\n    'scheduled_dag',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n</code></pre></p>"},{"location":"AirFlow/EN/02-concepts/#catchup","title":"Catchup","text":"<p>Catchup = True: - Executes all missing runs since start_date - Can create many runs</p> <p>Catchup = False: - Executes only future runs - Recommended for most cases</p>"},{"location":"AirFlow/EN/02-concepts/#variables-and-connections","title":"Variables and Connections","text":""},{"location":"AirFlow/EN/02-concepts/#variables","title":"Variables","text":"<p>Variables = Global configuration</p> <p>Create a variable:</p> <pre><code># Via CLI\nairflow variables set my_key \"my_value\"\n\n# Via web interface\n# Admin \u2192 Variables \u2192 Add\n</code></pre> <p>Use a variable:</p> <pre><code>from airflow.models import Variable\n\n# Get a variable\nmy_value = Variable.get(\"my_key\")\nmy_value_default = Variable.get(\"my_key\", default_var=\"default\")\n\n# In a template\n# {{ var.value.my_key }}\n</code></pre> <p>Example:</p> <pre><code>from airflow import DAG\nfrom airflow.models import Variable\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('variables_example', start_date=datetime(2024, 1, 1))\n\ndef use_variable():\n    api_key = Variable.get(\"api_key\")\n    print(f\"API Key: {api_key}\")\n\ntask = PythonOperator(\n    task_id='use_variable',\n    python_callable=use_variable,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/02-concepts/#connections","title":"Connections","text":"<p>Connections = Connection information</p> <p>Create a connection:</p> <pre><code># Via CLI\nairflow connections add 'my_postgres' \\\n    --conn-type 'postgres' \\\n    --conn-host 'localhost' \\\n    --conn-login 'user' \\\n    --conn-password 'password' \\\n    --conn-port 5432 \\\n    --conn-schema 'mydb'\n</code></pre> <p>Use a connection:</p> <pre><code>from airflow.hooks.base import BaseHook\n\n# Get a connection\nconn = BaseHook.get_connection('my_postgres')\nprint(f\"Host: {conn.host}\")\nprint(f\"Login: {conn.login}\")\nprint(f\"Password: {conn.password}\")\n</code></pre>"},{"location":"AirFlow/EN/02-concepts/#executions-and-states","title":"Executions and States","text":""},{"location":"AirFlow/EN/02-concepts/#task-states","title":"Task States","text":"<ul> <li>None : Not yet executed</li> <li>Scheduled : Scheduled</li> <li>Queued : Waiting</li> <li>Running : Running</li> <li>Success : Succeeded</li> <li>Failed : Failed</li> <li>Skipped : Skipped</li> <li>Retry : Retrying</li> <li>Up for retry : Ready for retry</li> </ul>"},{"location":"AirFlow/EN/02-concepts/#dag-states","title":"DAG States","text":"<ul> <li>Running : Running</li> <li>Success : All tasks succeeded</li> <li>Failed : At least one task failed</li> </ul>"},{"location":"AirFlow/EN/02-concepts/#manage-executions","title":"Manage Executions","text":"<p>Via web interface: - View execution state - Rerun a task - Mark as success/failure - View logs</p> <p>Via CLI:</p> <pre><code># List runs\nairflow dags list-runs -d my_dag\n\n# Trigger a DAG\nairflow dags trigger my_dag\n\n# Mark a task as success\nairflow tasks clear my_dag task_id -s 2024-01-01\n</code></pre>"},{"location":"AirFlow/EN/02-concepts/#practical-examples","title":"Practical Examples","text":""},{"location":"AirFlow/EN/02-concepts/#example-1-dag-with-variables","title":"Example 1: DAG with variables","text":"<pre><code>from airflow import DAG\nfrom airflow.models import Variable\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('variables_dag', start_date=datetime(2024, 1, 1))\n\ndef process_data():\n    # Get variables\n    input_path = Variable.get(\"input_path\")\n    output_path = Variable.get(\"output_path\")\n\n    print(f\"Processing: {input_path} -&gt; {output_path}\")\n\ntask = PythonOperator(\n    task_id='process',\n    python_callable=process_data,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/02-concepts/#example-2-dag-with-connection","title":"Example 2: DAG with connection","text":"<pre><code>from airflow import DAG\nfrom airflow.hooks.postgres import PostgresHook\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('connection_dag', start_date=datetime(2024, 1, 1))\n\ndef query_database():\n    # Use a PostgreSQL connection\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    records = hook.get_records(\"SELECT * FROM users LIMIT 10\")\n    print(records)\n\ntask = PythonOperator(\n    task_id='query',\n    python_callable=query_database,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/02-concepts/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>DAGs define workflows</li> <li>Tasks are individual steps</li> <li>Dependencies define execution order</li> <li>Scheduling schedules executions</li> <li>Variables and Connections for configuration</li> </ol>"},{"location":"AirFlow/EN/02-concepts/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 3. Operators to learn how to use different Airflow operators.</p>"},{"location":"AirFlow/EN/03-operators/","title":"3. Airflow Operators","text":""},{"location":"AirFlow/EN/03-operators/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand different operator types</li> <li>Use Python, Bash, SQL operators</li> <li>Create custom operators</li> <li>Manage data between tasks</li> </ul>"},{"location":"AirFlow/EN/03-operators/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Operator Types</li> <li>PythonOperator</li> <li>BashOperator</li> <li>SQL Operators</li> <li>Custom Operators</li> </ol>"},{"location":"AirFlow/EN/03-operators/#operator-types","title":"Operator Types","text":""},{"location":"AirFlow/EN/03-operators/#basic-operators","title":"Basic Operators","text":"<ul> <li>PythonOperator : Executes Python code</li> <li>BashOperator : Executes bash commands</li> <li>SQLExecuteQueryOperator : Executes SQL queries</li> <li>EmailOperator : Sends emails</li> <li>HttpOperator : Makes HTTP requests</li> </ul>"},{"location":"AirFlow/EN/03-operators/#transfer-operators","title":"Transfer Operators","text":"<ul> <li>FileTransferOperator : Transfers files</li> <li>FTPOperator : FTP operations</li> <li>S3FileTransformOperator : Transforms S3 files</li> </ul>"},{"location":"AirFlow/EN/03-operators/#pythonoperator","title":"PythonOperator","text":""},{"location":"AirFlow/EN/03-operators/#basic-usage","title":"Basic Usage","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('python_operator', start_date=datetime(2024, 1, 1))\n\ndef my_function():\n    print(\"Hello from Python!\")\n    return \"Success\"\n\ntask = PythonOperator(\n    task_id='python_task',\n    python_callable=my_function,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/03-operators/#pass-arguments","title":"Pass Arguments","text":"<pre><code>def process_data(file_path, output_path):\n    print(f\"Processing {file_path} -&gt; {output_path}\")\n    # Processing...\n\ntask = PythonOperator(\n    task_id='process',\n    python_callable=process_data,\n    op_args=['/path/to/input.csv', '/path/to/output.csv'],\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/03-operators/#use-xcom-to-share-data","title":"Use XCom to Share Data","text":"<pre><code>def extract_data(**context):\n    data = {'key': 'value'}\n    return data  # Automatically returns via XCom\n\ndef process_data(**context):\n    # Get data from previous task\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    print(f\"Received: {data}\")\n\nextract = PythonOperator(\n    task_id='extract',\n    python_callable=extract_data,\n    dag=dag,\n)\n\nprocess = PythonOperator(\n    task_id='process',\n    python_callable=process_data,\n    dag=dag,\n)\n\nextract &gt;&gt; process\n</code></pre>"},{"location":"AirFlow/EN/03-operators/#bashoperator","title":"BashOperator","text":""},{"location":"AirFlow/EN/03-operators/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='bash_task',\n    bash_command='echo \"Hello from Bash\"',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/03-operators/#use-templates","title":"Use Templates","text":"<pre><code>task = BashOperator(\n    task_id='bash_template',\n    bash_command='echo \"Date: {{ ds }}\"',  # ds = execution date\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/03-operators/#available-template-variables","title":"Available Template Variables","text":"<ul> <li><code>{{ ds }}</code> : Execution date (YYYY-MM-DD)</li> <li><code>{{ ds_nodash }}</code> : Date without dashes (YYYYMMDD)</li> <li><code>{{ ts }}</code> : Execution timestamp</li> <li><code>{{ dag }}</code> : DAG object</li> <li><code>{{ task }}</code> : Task object</li> </ul>"},{"location":"AirFlow/EN/03-operators/#sql-operators","title":"SQL Operators","text":""},{"location":"AirFlow/EN/03-operators/#sqlexecutequeryoperator","title":"SQLExecuteQueryOperator","text":"<pre><code>from airflow.providers.postgres.operators.postgres import PostgresOperator\n\ntask = PostgresOperator(\n    task_id='sql_task',\n    postgres_conn_id='my_postgres',\n    sql='SELECT * FROM users LIMIT 10;',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/03-operators/#use-sql-templates","title":"Use SQL Templates","text":"<pre><code>task = PostgresOperator(\n    task_id='sql_template',\n    postgres_conn_id='my_postgres',\n    sql='''\n        SELECT * FROM users\n        WHERE created_at &gt;= '{{ ds }}'\n    ''',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/03-operators/#custom-operators","title":"Custom Operators","text":""},{"location":"AirFlow/EN/03-operators/#create-a-custom-operator","title":"Create a Custom Operator","text":"<pre><code>from airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass MyCustomOperator(BaseOperator):\n    @apply_defaults\n    def __init__(self, my_param, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.my_param = my_param\n\n    def execute(self, context):\n        print(f\"Executing with param: {self.my_param}\")\n        # Your logic here\n        return \"Success\"\n\n# Usage\ntask = MyCustomOperator(\n    task_id='custom_task',\n    my_param='value',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/03-operators/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>PythonOperator for Python logic</li> <li>BashOperator for shell commands</li> <li>SQL Operators for SQL queries</li> <li>XCom to share data</li> <li>Templates for dynamic values</li> </ol>"},{"location":"AirFlow/EN/03-operators/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 4. Sensors to learn how to use sensors.</p>"},{"location":"AirFlow/EN/04-sensors/","title":"4. Airflow Sensors","text":""},{"location":"AirFlow/EN/04-sensors/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand sensors</li> <li>Use FileSensor, SqlSensor, HttpSensor</li> <li>Create custom sensors</li> <li>Manage timeouts and retries</li> </ul>"},{"location":"AirFlow/EN/04-sensors/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Sensors</li> <li>FileSensor</li> <li>SqlSensor</li> <li>HttpSensor</li> <li>Custom Sensors</li> </ol>"},{"location":"AirFlow/EN/04-sensors/#introduction-to-sensors","title":"Introduction to Sensors","text":""},{"location":"AirFlow/EN/04-sensors/#what-is-a-sensor","title":"What is a Sensor?","text":"<p>Sensor = Task that waits for a condition</p> <ul> <li>Polling : Periodically checks a condition</li> <li>Timeout : Stops after a certain time</li> <li>Poke interval : Interval between checks</li> <li>Mode : Resume or poke</li> </ul>"},{"location":"AirFlow/EN/04-sensors/#sensor-types","title":"Sensor Types","text":"<ul> <li>FileSensor : Waits for a file</li> <li>SqlSensor : Waits for a SQL condition</li> <li>HttpSensor : Waits for an HTTP response</li> <li>S3KeySensor : Waits for an S3 key</li> </ul>"},{"location":"AirFlow/EN/04-sensors/#filesensor","title":"FileSensor","text":""},{"location":"AirFlow/EN/04-sensors/#basic-usage","title":"Basic Usage","text":"<pre><code>from airflow.sensors.filesystem import FileSensor\n\ntask = FileSensor(\n    task_id='wait_for_file',\n    filepath='/path/to/file.csv',\n    poke_interval=30,  # Check every 30 seconds\n    timeout=3600,  # Timeout after 1 hour\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/04-sensors/#filesensor-with-wildcards","title":"FileSensor with Wildcards","text":"<pre><code>task = FileSensor(\n    task_id='wait_for_files',\n    filepath='/path/to/data/*.csv',\n    poke_interval=60,\n    timeout=7200,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/04-sensors/#sqlsensor","title":"SqlSensor","text":""},{"location":"AirFlow/EN/04-sensors/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from airflow.sensors.sql import SqlSensor\n\ntask = SqlSensor(\n    task_id='wait_for_data',\n    conn_id='my_postgres',\n    sql=\"SELECT COUNT(*) FROM users WHERE status = 'active'\",\n    poke_interval=60,\n    timeout=3600,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/04-sensors/#sqlsensor-with-condition","title":"SqlSensor with Condition","text":"<pre><code>task = SqlSensor(\n    task_id='wait_for_count',\n    conn_id='my_postgres',\n    sql=\"SELECT COUNT(*) as count FROM orders WHERE date = '{{ ds }}'\",\n    poke_interval=30,\n    timeout=1800,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/04-sensors/#httpsensor","title":"HttpSensor","text":""},{"location":"AirFlow/EN/04-sensors/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from airflow.sensors.http import HttpSensor\n\ntask = HttpSensor(\n    task_id='wait_for_api',\n    http_conn_id='my_http',\n    endpoint='/api/status',\n    method='GET',\n    poke_interval=30,\n    timeout=3600,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/04-sensors/#httpsensor-with-response","title":"HttpSensor with Response","text":"<pre><code>def check_response(response):\n    return response.status_code == 200\n\ntask = HttpSensor(\n    task_id='wait_for_api_ready',\n    http_conn_id='my_http',\n    endpoint='/api/health',\n    response_check=check_response,\n    poke_interval=60,\n    timeout=7200,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/04-sensors/#custom-sensors","title":"Custom Sensors","text":""},{"location":"AirFlow/EN/04-sensors/#create-a-custom-sensor","title":"Create a Custom Sensor","text":"<pre><code>from airflow.sensors.base import BaseSensorOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass MyCustomSensor(BaseSensorOperator):\n    @apply_defaults\n    def __init__(self, my_param, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.my_param = my_param\n\n    def poke(self, context):\n        # Check condition\n        # Return True if condition met\n        # Return False to keep waiting\n        return self.check_condition()\n\n    def check_condition(self):\n        # Your verification logic\n        return True  # or False\n\n# Usage\ntask = MyCustomSensor(\n    task_id='custom_sensor',\n    my_param='value',\n    poke_interval=30,\n    timeout=3600,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/04-sensors/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Sensors wait for conditions</li> <li>Polling checks periodically</li> <li>Timeout limits wait time</li> <li>Poke interval defines frequency</li> <li>Mode controls behavior</li> </ol>"},{"location":"AirFlow/EN/04-sensors/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 5. Hooks to learn how to use hooks.</p>"},{"location":"AirFlow/EN/05-hooks/","title":"5. Airflow Hooks","text":""},{"location":"AirFlow/EN/05-hooks/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand hooks</li> <li>Use database hooks</li> <li>Use cloud hooks</li> <li>Create custom hooks</li> </ul>"},{"location":"AirFlow/EN/05-hooks/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Hooks</li> <li>Database Hooks</li> <li>Cloud Hooks</li> <li>HTTP Hooks</li> <li>Custom Hooks</li> </ol>"},{"location":"AirFlow/EN/05-hooks/#introduction-to-hooks","title":"Introduction to Hooks","text":""},{"location":"AirFlow/EN/05-hooks/#what-is-a-hook","title":"What is a Hook?","text":"<p>Hook = Interface to interact with external systems</p> <ul> <li>Reusable : Can be used in multiple tasks</li> <li>Manages connections : Uses Airflow connections</li> <li>Abstraction : Hides implementation details</li> </ul>"},{"location":"AirFlow/EN/05-hooks/#hook-types","title":"Hook Types","text":"<ul> <li>Database hooks : PostgreSQL, MySQL, SQLite</li> <li>Cloud hooks : AWS, Azure, GCP</li> <li>HTTP hooks : HTTP requests</li> <li>File hooks : File operations</li> </ul>"},{"location":"AirFlow/EN/05-hooks/#database-hooks","title":"Database Hooks","text":""},{"location":"AirFlow/EN/05-hooks/#postgreshook","title":"PostgresHook","text":"<pre><code>from airflow.hooks.postgres import PostgresHook\n\ndef query_database():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n\n    # Execute a query\n    records = hook.get_records(\"SELECT * FROM users LIMIT 10\")\n\n    # Execute a command\n    hook.run(\"INSERT INTO logs VALUES ('test')\")\n\n    # Get pandas DataFrame\n    df = hook.get_pandas_df(\"SELECT * FROM users\")\n\ntask = PythonOperator(\n    task_id='query_db',\n    python_callable=query_database,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/05-hooks/#mysqlhook","title":"MySqlHook","text":"<pre><code>from airflow.providers.mysql.hooks.mysql import MySqlHook\n\ndef query_mysql():\n    hook = MySqlHook(mysql_conn_id='my_mysql')\n    records = hook.get_records(\"SELECT * FROM orders\")\n\ntask = PythonOperator(\n    task_id='query_mysql',\n    python_callable=query_mysql,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/05-hooks/#cloud-hooks","title":"Cloud Hooks","text":""},{"location":"AirFlow/EN/05-hooks/#s3hook-aws","title":"S3Hook (AWS)","text":"<pre><code>from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\ndef upload_to_s3():\n    hook = S3Hook(aws_conn_id='my_aws')\n\n    # Upload a file\n    hook.load_file(\n        filename='/local/path/file.csv',\n        key='s3/path/file.csv',\n        bucket_name='my-bucket',\n    )\n\n    # Download a file\n    hook.download_file(\n        key='s3/path/file.csv',\n        bucket_name='my-bucket',\n        local_path='/local/path/file.csv',\n    )\n\ntask = PythonOperator(\n    task_id='s3_upload',\n    python_callable=upload_to_s3,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/05-hooks/#azure-blob-storage-hook","title":"Azure Blob Storage Hook","text":"<pre><code>from airflow.providers.microsoft.azure.hooks.wasb import WasbHook\n\ndef upload_to_azure():\n    hook = WasbHook(wasb_conn_id='my_azure')\n\n    # Upload a file\n    hook.upload(\n        container_name='my-container',\n        blob_name='file.csv',\n        file_path='/local/path/file.csv',\n    )\n\ntask = PythonOperator(\n    task_id='azure_upload',\n    python_callable=upload_to_azure,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/05-hooks/#http-hooks","title":"HTTP Hooks","text":""},{"location":"AirFlow/EN/05-hooks/#httphook","title":"HttpHook","text":"<pre><code>from airflow.providers.http.hooks.http import HttpHook\n\ndef call_api():\n    hook = HttpHook(http_conn_id='my_api', method='GET')\n\n    # Make a GET request\n    response = hook.run(endpoint='/api/data')\n    print(response.json())\n\n    # Make a POST request\n    hook = HttpHook(http_conn_id='my_api', method='POST')\n    response = hook.run(\n        endpoint='/api/data',\n        data={'key': 'value'},\n    )\n\ntask = PythonOperator(\n    task_id='call_api',\n    python_callable=call_api,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/05-hooks/#custom-hooks","title":"Custom Hooks","text":""},{"location":"AirFlow/EN/05-hooks/#create-a-custom-hook","title":"Create a Custom Hook","text":"<pre><code>from airflow.hooks.base import BaseHook\n\nclass MyCustomHook(BaseHook):\n    def __init__(self, my_conn_id):\n        super().__init__()\n        self.conn_id = my_conn_id\n        self.conn = self.get_connection(my_conn_id)\n\n    def do_something(self):\n        # Your logic\n        print(f\"Connecting to {self.conn.host}\")\n        return \"Success\"\n\n# Usage\ndef use_custom_hook():\n    hook = MyCustomHook(my_conn_id='my_connection')\n    result = hook.do_something()\n\ntask = PythonOperator(\n    task_id='use_hook',\n    python_callable=use_custom_hook,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/05-hooks/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Hooks abstract connections</li> <li>Reusable in multiple tasks</li> <li>Manage connections via Airflow</li> <li>Support databases, cloud, HTTP</li> <li>Extensible with custom hooks</li> </ol>"},{"location":"AirFlow/EN/05-hooks/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 6. Variables and Connections to learn how to manage configuration.</p>"},{"location":"AirFlow/EN/06-variables-connections/","title":"6. Variables and Connections","text":""},{"location":"AirFlow/EN/06-variables-connections/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Manage Airflow variables</li> <li>Configure connections</li> <li>Secure credentials</li> <li>Use dynamic variables</li> </ul>"},{"location":"AirFlow/EN/06-variables-connections/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Variables</li> <li>Connections</li> <li>Security</li> <li>Best Practices</li> </ol>"},{"location":"AirFlow/EN/06-variables-connections/#variables","title":"Variables","text":""},{"location":"AirFlow/EN/06-variables-connections/#create-variables","title":"Create Variables","text":"<p>Via CLI:</p> <pre><code># Create a variable\nairflow variables set my_key \"my_value\"\n\n# Create a JSON variable\nairflow variables set my_config '{\"key\": \"value\"}'\n\n# Delete a variable\nairflow variables delete my_key\n\n# List variables\nairflow variables list\n</code></pre> <p>Via web interface: 1. Admin \u2192 Variables 2. Click on \"+\" 3. Enter Key and Value 4. Save</p>"},{"location":"AirFlow/EN/06-variables-connections/#use-variables","title":"Use Variables","text":"<pre><code>from airflow.models import Variable\n\n# Get a variable\nmy_value = Variable.get(\"my_key\")\n\n# With default value\nmy_value = Variable.get(\"my_key\", default_var=\"default\")\n\n# JSON variable\nconfig = Variable.get(\"my_config\", deserialize_json=True)\nprint(config['key'])\n</code></pre>"},{"location":"AirFlow/EN/06-variables-connections/#variables-in-templates","title":"Variables in Templates","text":"<pre><code>from airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='use_var',\n    bash_command='echo \"Value: {{ var.value.my_key }}\"',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/06-variables-connections/#connections","title":"Connections","text":""},{"location":"AirFlow/EN/06-variables-connections/#create-a-connection","title":"Create a Connection","text":"<p>Via CLI:</p> <pre><code># PostgreSQL\nairflow connections add 'my_postgres' \\\n    --conn-type 'postgres' \\\n    --conn-host 'localhost' \\\n    --conn-login 'user' \\\n    --conn-password 'password' \\\n    --conn-port 5432 \\\n    --conn-schema 'mydb'\n\n# MySQL\nairflow connections add 'my_mysql' \\\n    --conn-type 'mysql' \\\n    --conn-host 'localhost' \\\n    --conn-login 'user' \\\n    --conn-password 'password' \\\n    --conn-port 3306\n\n# HTTP\nairflow connections add 'my_api' \\\n    --conn-type 'http' \\\n    --conn-host 'https://api.example.com'\n</code></pre> <p>Via web interface: 1. Admin \u2192 Connections 2. Click on \"+\" 3. Fill in fields 4. Save</p>"},{"location":"AirFlow/EN/06-variables-connections/#use-a-connection","title":"Use a Connection","text":"<pre><code>from airflow.hooks.base import BaseHook\n\n# Get a connection\nconn = BaseHook.get_connection('my_postgres')\nprint(f\"Host: {conn.host}\")\nprint(f\"Login: {conn.login}\")\nprint(f\"Password: {conn.password}\")\n</code></pre>"},{"location":"AirFlow/EN/06-variables-connections/#security","title":"Security","text":""},{"location":"AirFlow/EN/06-variables-connections/#hide-passwords","title":"Hide Passwords","text":"<p>Use connections: - Passwords are encrypted in database - Never hardcode credentials</p> <p>Use variables: - For sensitive secrets - Use secret management tools (Vault, etc.)</p>"},{"location":"AirFlow/EN/06-variables-connections/#best-practices","title":"Best Practices","text":"<ol> <li>Never commit credentials</li> <li>Use connections for access</li> <li>Use variables for configuration</li> <li>Encrypt sensitive data</li> <li>Limit access to connections</li> </ol>"},{"location":"AirFlow/EN/06-variables-connections/#best-practices_1","title":"Best Practices","text":""},{"location":"AirFlow/EN/06-variables-connections/#variable-organization","title":"Variable Organization","text":"<ul> <li>Prefixes : <code>project_name_key</code></li> <li>Groups : <code>db_</code>, <code>api_</code>, <code>s3_</code></li> <li>Documentation : Document usage</li> </ul>"},{"location":"AirFlow/EN/06-variables-connections/#connection-organization","title":"Connection Organization","text":"<ul> <li>Clear names : <code>postgres_prod</code>, <code>postgres_dev</code></li> <li>Correct types : Use correct connection type</li> <li>Testing : Test connections regularly</li> </ul>"},{"location":"AirFlow/EN/06-variables-connections/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Variables for configuration</li> <li>Connections for access</li> <li>Security : Never hardcode</li> <li>Organization : Prefixes and groups</li> <li>Documentation : Document usage</li> </ol>"},{"location":"AirFlow/EN/06-variables-connections/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 7. Best Practices to learn best practices.</p>"},{"location":"AirFlow/EN/07-best-practices/","title":"7. Airflow Best Practices","text":""},{"location":"AirFlow/EN/07-best-practices/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Structure DAGs efficiently</li> <li>Handle errors</li> <li>Optimize performance</li> <li>Test and debug</li> </ul>"},{"location":"AirFlow/EN/07-best-practices/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>DAG Structure</li> <li>Error Handling</li> <li>Performance</li> <li>Testing</li> <li>Debugging</li> </ol>"},{"location":"AirFlow/EN/07-best-practices/#dag-structure","title":"DAG Structure","text":""},{"location":"AirFlow/EN/07-best-practices/#file-organization","title":"File Organization","text":"<pre><code>airflow/\n\u251c\u2500\u2500 dags/\n\u2502   \u251c\u2500\u2500 etl/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 extract.py\n\u2502   \u2502   \u251c\u2500\u2500 transform.py\n\u2502   \u2502   \u2514\u2500\u2500 load.py\n\u2502   \u2514\u2500\u2500 analytics/\n\u2502       \u2514\u2500\u2500 reports.py\n\u251c\u2500\u2500 plugins/\n\u2502   \u2514\u2500\u2500 custom_operators.py\n\u2514\u2500\u2500 config/\n    \u2514\u2500\u2500 settings.py\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#code-best-practices","title":"Code Best Practices","text":"<p>1. Organized imports:</p> <pre><code># Standard library\nfrom datetime import datetime, timedelta\n\n# Third-party\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\n# Local\nfrom utils.helpers import process_data\n</code></pre> <p>2. Reusable functions:</p> <pre><code># utils/helpers.py\ndef extract_data(source):\n    # Extraction logic\n    return data\n\ndef transform_data(data):\n    # Transformation logic\n    return transformed_data\n\n# dags/etl_pipeline.py\nfrom utils.helpers import extract_data, transform_data\n\nextract_task = PythonOperator(\n    task_id='extract',\n    python_callable=extract_data,\n    op_args=['source'],\n    dag=dag,\n)\n</code></pre> <p>3. Centralized configuration:</p> <pre><code># config/settings.py\nDEFAULT_ARGS = {\n    'owner': 'data_team',\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# dags/my_dag.py\nfrom config.settings import DEFAULT_ARGS\n\ndag = DAG('my_dag', default_args=DEFAULT_ARGS)\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#error-handling","title":"Error Handling","text":""},{"location":"AirFlow/EN/07-best-practices/#retry-and-backoff","title":"Retry and Backoff","text":"<pre><code>default_args = {\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'retry_exponential_backoff': True,\n    'max_retry_delay': timedelta(hours=1),\n}\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#exception-handling","title":"Exception Handling","text":"<pre><code>def process_with_error_handling():\n    try:\n        # Code that may fail\n        result = risky_operation()\n        return result\n    except SpecificError as e:\n        # Handle specific error\n        logger.error(f\"Error: {e}\")\n        raise\n    except Exception as e:\n        # Handle other errors\n        logger.error(f\"Unexpected error: {e}\")\n        raise\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#callbacks","title":"Callbacks","text":"<pre><code>def on_failure_callback(context):\n    logger.error(\"Task failed!\")\n    # Send an alert\n    send_alert(context)\n\ndef on_success_callback(context):\n    logger.info(\"Task succeeded!\")\n\ntask = PythonOperator(\n    task_id='task',\n    python_callable=my_function,\n    on_failure_callback=on_failure_callback,\n    on_success_callback=on_success_callback,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#performance","title":"Performance","text":""},{"location":"AirFlow/EN/07-best-practices/#optimize-dags","title":"Optimize DAGs","text":"<p>1. Avoid unnecessary dependencies:</p> <pre><code># Bad: unnecessary sequential dependencies\ntask1 &gt;&gt; task2 &gt;&gt; task3 &gt;&gt; task4\n\n# Good: parallelism when possible\n[task1, task2] &gt;&gt; task3 &gt;&gt; task4\n</code></pre> <p>2. Use reschedule mode for sensors:</p> <pre><code>sensor = FileSensor(\n    task_id='wait_file',\n    filepath='/path/to/file',\n    mode='reschedule',  # Frees worker slot\n    poke_interval=60,\n    dag=dag,\n)\n</code></pre> <p>3. Limit parallelism:</p> <pre><code>dag = DAG(\n    'my_dag',\n    max_active_runs=1,  # Only one run at a time\n    max_active_tasks=10,  # Maximum 10 tasks in parallel\n)\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#testing","title":"Testing","text":""},{"location":"AirFlow/EN/07-best-practices/#unit-tests","title":"Unit Tests","text":"<pre><code># tests/test_dag.py\nimport pytest\nfrom airflow.models import DagBag\n\ndef test_dag_loaded():\n    dagbag = DagBag()\n    dag = dagbag.get_dag(dag_id='my_dag')\n    assert dag is not None\n    assert len(dag.tasks) == 3\n\ndef test_dag_structure():\n    dagbag = DagBag()\n    dag = dagbag.get_dag(dag_id='my_dag')\n    assert 'extract' in dag.task_ids\n    assert 'transform' in dag.task_ids\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#integration-tests","title":"Integration Tests","text":"<pre><code>from airflow.operators.python import PythonOperator\n\ndef test_task_execution():\n    task = PythonOperator(\n        task_id='test_task',\n        python_callable=lambda: \"success\",\n    )\n    result = task.execute({})\n    assert result == \"success\"\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#debugging","title":"Debugging","text":""},{"location":"AirFlow/EN/07-best-practices/#logs","title":"Logs","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef my_function():\n    logger.info(\"Starting process\")\n    logger.debug(\"Debug information\")\n    logger.error(\"Error occurred\")\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#check-logs","title":"Check Logs","text":"<pre><code># Task logs\nairflow tasks logs my_dag my_task 2024-01-01\n\n# Scheduler logs\ntail -f ~/airflow/logs/scheduler/*.log\n</code></pre>"},{"location":"AirFlow/EN/07-best-practices/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Structure : Organize code cleanly</li> <li>Errors : Handle with retry and callbacks</li> <li>Performance : Optimize parallelism</li> <li>Tests : Test DAGs</li> <li>Logs : Use logging effectively</li> </ol>"},{"location":"AirFlow/EN/07-best-practices/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 8. Practical Projects to create complete projects.</p>"},{"location":"AirFlow/EN/08-projets/","title":"8. Airflow Practical Projects","text":""},{"location":"AirFlow/EN/08-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create a complete ETL pipeline</li> <li>Orchestrate complex workflows</li> <li>Integrate with databases</li> <li>Create portfolio projects</li> </ul>"},{"location":"AirFlow/EN/08-projets/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Project 1: Simple ETL Pipeline</li> <li>Project 2: Workflow Orchestration</li> <li>Project 3: Database Integration</li> <li>Project 4: Complete Pipeline</li> </ol>"},{"location":"AirFlow/EN/08-projets/#project-1-simple-etl-pipeline","title":"Project 1: Simple ETL Pipeline","text":""},{"location":"AirFlow/EN/08-projets/#objective","title":"Objective","text":"<p>Create an ETL pipeline that extracts, transforms, and loads data.</p>"},{"location":"AirFlow/EN/08-projets/#architecture","title":"Architecture","text":"<pre><code>Extract (API) \u2192 Transform (Python) \u2192 Load (File)\n</code></pre>"},{"location":"AirFlow/EN/08-projets/#code","title":"Code","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\nimport requests\nimport pandas as pd\n\ndefault_args = {\n    'owner': 'data_analyst',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'etl_pipeline',\n    default_args=default_args,\n    description='Simple ETL pipeline',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\ndef extract_data(**context):\n    # Extract data from an API\n    response = requests.get('https://api.example.com/data')\n    data = response.json()\n\n    # Save temporarily\n    df = pd.DataFrame(data)\n    df.to_csv('/tmp/raw_data.csv', index=False)\n\n    return '/tmp/raw_data.csv'\n\ndef transform_data(**context):\n    # Get file from previous task\n    ti = context['ti']\n    input_file = ti.xcom_pull(task_ids='extract')\n\n    # Read and transform\n    df = pd.read_csv(input_file)\n    df['processed_at'] = datetime.now()\n    df = df.dropna()\n\n    # Save\n    output_file = '/tmp/transformed_data.csv'\n    df.to_csv(output_file, index=False)\n\n    return output_file\n\ndef load_data(**context):\n    # Get transformed file\n    ti = context['ti']\n    input_file = ti.xcom_pull(task_ids='transform')\n\n    # Load into destination (e.g., database)\n    df = pd.read_csv(input_file)\n    print(f\"Loaded {len(df)} rows\")\n\n    # Here, you could load into a database\n    # df.to_sql('table', con=engine, if_exists='append')\n\nextract = PythonOperator(\n    task_id='extract',\n    python_callable=extract_data,\n    dag=dag,\n)\n\ntransform = PythonOperator(\n    task_id='transform',\n    python_callable=transform_data,\n    dag=dag,\n)\n\nload = PythonOperator(\n    task_id='load',\n    python_callable=load_data,\n    dag=dag,\n)\n\nextract &gt;&gt; transform &gt;&gt; load\n</code></pre>"},{"location":"AirFlow/EN/08-projets/#project-2-workflow-orchestration","title":"Project 2: Workflow Orchestration","text":""},{"location":"AirFlow/EN/08-projets/#objective_1","title":"Objective","text":"<p>Orchestrate multiple workflows with complex dependencies.</p>"},{"location":"AirFlow/EN/08-projets/#architecture_1","title":"Architecture","text":"<pre><code>Data Collection \u2192 Validation \u2192 Processing \u2192 Reporting\n                      \u2193\n                  Alerting\n</code></pre>"},{"location":"AirFlow/EN/08-projets/#code_1","title":"Code","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.branch import BranchPythonOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'workflow_orchestration',\n    description='Workflow orchestration',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\ndef collect_data():\n    print(\"Collecting data...\")\n    return \"success\"\n\ndef validate_data(**context):\n    data = context['ti'].xcom_pull(task_ids='collect')\n    if data == \"success\":\n        return \"valid\"\n    return \"invalid\"\n\ndef process_valid():\n    print(\"Processing valid data...\")\n\ndef process_invalid():\n    print(\"Alerting: Invalid data!\")\n\ndef generate_report():\n    print(\"Generating report...\")\n\ncollect = PythonOperator(\n    task_id='collect',\n    python_callable=collect_data,\n    dag=dag,\n)\n\nvalidate = BranchPythonOperator(\n    task_id='validate',\n    python_callable=validate_data,\n    dag=dag,\n)\n\nprocess_valid_task = PythonOperator(\n    task_id='process_valid',\n    python_callable=process_valid,\n    dag=dag,\n)\n\nprocess_invalid_task = PythonOperator(\n    task_id='process_invalid',\n    python_callable=process_invalid,\n    dag=dag,\n)\n\nreport = PythonOperator(\n    task_id='report',\n    python_callable=generate_report,\n    dag=dag,\n)\n\ncollect &gt;&gt; validate &gt;&gt; [process_valid_task, process_invalid_task] &gt;&gt; report\n</code></pre>"},{"location":"AirFlow/EN/08-projets/#project-3-database-integration","title":"Project 3: Database Integration","text":""},{"location":"AirFlow/EN/08-projets/#objective_2","title":"Objective","text":"<p>Integrate Airflow with PostgreSQL for a data pipeline.</p>"},{"location":"AirFlow/EN/08-projets/#code_2","title":"Code","text":"<pre><code>from airflow import DAG\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'database_pipeline',\n    description='Pipeline with database',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\n# Create a table\ncreate_table = PostgresOperator(\n    task_id='create_table',\n    postgres_conn_id='my_postgres',\n    sql='''\n        CREATE TABLE IF NOT EXISTS users_processed (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100),\n            email VARCHAR(100),\n            created_at TIMESTAMP\n        );\n    ''',\n    dag=dag,\n)\n\n# Insert data\ndef insert_data():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    hook.run(\"\"\"\n        INSERT INTO users_processed (name, email, created_at)\n        VALUES ('John Doe', 'john@example.com', NOW());\n    \"\"\")\n\ninsert = PythonOperator(\n    task_id='insert',\n    python_callable=insert_data,\n    dag=dag,\n)\n\n# Analytical query\nanalytics_query = PostgresOperator(\n    task_id='analytics',\n    postgres_conn_id='my_postgres',\n    sql='''\n        SELECT \n            DATE(created_at) as date,\n            COUNT(*) as user_count\n        FROM users_processed\n        GROUP BY DATE(created_at);\n    ''',\n    dag=dag,\n)\n\ncreate_table &gt;&gt; insert &gt;&gt; analytics_query\n</code></pre>"},{"location":"AirFlow/EN/08-projets/#project-4-complete-pipeline","title":"Project 4: Complete Pipeline","text":""},{"location":"AirFlow/EN/08-projets/#objective_3","title":"Objective","text":"<p>Create a complete ETL pipeline with multiple sources and destinations.</p>"},{"location":"AirFlow/EN/08-projets/#architecture_2","title":"Architecture","text":"<pre><code>API \u2192 Validation \u2192 Transformation \u2192 Database \u2192 Reporting\n  \u2193\nFile \u2192 Validation \u2192 Transformation \u2192 Database\n</code></pre>"},{"location":"AirFlow/EN/08-projets/#code_3","title":"Code","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport requests\n\ndag = DAG(\n    'complete_pipeline',\n    description='Complete ETL pipeline',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    default_args={\n        'retries': 2,\n        'retry_delay': timedelta(minutes=5),\n    },\n)\n\n# Wait for a file\nwait_file = FileSensor(\n    task_id='wait_file',\n    filepath='/data/input/file.csv',\n    poke_interval=60,\n    timeout=3600,\n    dag=dag,\n)\n\n# Extract from API\ndef extract_api():\n    response = requests.get('https://api.example.com/data')\n    data = response.json()\n    df = pd.DataFrame(data)\n    df.to_csv('/tmp/api_data.csv', index=False)\n    return '/tmp/api_data.csv'\n\nextract_api_task = PythonOperator(\n    task_id='extract_api',\n    python_callable=extract_api,\n    dag=dag,\n)\n\n# Transform\ndef transform(**context):\n    # API file\n    api_file = context['ti'].xcom_pull(task_ids='extract_api')\n    df_api = pd.read_csv(api_file)\n\n    # Local file\n    df_file = pd.read_csv('/data/input/file.csv')\n\n    # Combine and transform\n    df_combined = pd.concat([df_api, df_file])\n    df_combined = df_combined.dropna()\n    df_combined['processed_at'] = datetime.now()\n\n    # Save\n    df_combined.to_csv('/tmp/transformed.csv', index=False)\n    return '/tmp/transformed.csv'\n\ntransform_task = PythonOperator(\n    task_id='transform',\n    python_callable=transform,\n    dag=dag,\n)\n\n# Load into database\nload_db = PostgresOperator(\n    task_id='load_db',\n    postgres_conn_id='my_postgres',\n    sql='''\n        COPY users FROM '/tmp/transformed.csv' \n        WITH (FORMAT csv, HEADER true);\n    ''',\n    dag=dag,\n)\n\n# Generate a report\ndef generate_report():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    df = hook.get_pandas_df(\"SELECT * FROM users\")\n    report = df.describe()\n    report.to_csv('/tmp/report.csv')\n    print(\"Report generated!\")\n\nreport_task = PythonOperator(\n    task_id='report',\n    python_callable=generate_report,\n    dag=dag,\n)\n\n# Dependencies\n[wait_file, extract_api_task] &gt;&gt; transform_task &gt;&gt; load_db &gt;&gt; report_task\n</code></pre>"},{"location":"AirFlow/EN/08-projets/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>ETL : Extract, Transform, Load</li> <li>Orchestration : Coordinate multiple workflows</li> <li>Integration : Databases, APIs, files</li> <li>Monitoring : Monitor executions</li> <li>Portfolio : Create demonstrable projects</li> </ol>"},{"location":"AirFlow/EN/08-projets/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>Airflow Examples</li> <li>Airflow Documentation</li> </ul> <p>Congratulations! You have completed the Airflow training. You can now create complex data pipelines with Airflow.</p>"},{"location":"AirFlow/FR/","title":"Formation Apache Airflow pour Data Analyst","text":""},{"location":"AirFlow/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'apprentissage d'Apache Airflow en tant que Data Analyst. Airflow est une plateforme open-source pour orchestrer et automatiser des workflows de donn\u00e9es complexes.</p>"},{"location":"AirFlow/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre Apache Airflow et son r\u00f4le dans l'orchestration ETL</li> <li>Installer et configurer Airflow</li> <li>Cr\u00e9er des DAGs (Directed Acyclic Graphs)</li> <li>Utiliser les op\u00e9rateurs, sensors et hooks</li> <li>Orchestrer des pipelines de donn\u00e9es complexes</li> <li>Int\u00e9grer avec des bases de donn\u00e9es et services cloud</li> <li>Cr\u00e9er des projets pratiques pour votre portfolio</li> </ul>"},{"location":"AirFlow/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 Apache Airflow : Open-source et gratuit - \u2705 Python : Langage de programmation gratuit - \u2705 PostgreSQL/SQLite : Bases de donn\u00e9es gratuites - \u2705 Documentation officielle : Guides complets gratuits</p> <p>Budget total : 0\u20ac</p>"},{"location":"AirFlow/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"AirFlow/FR/#1-prise-en-main-airflow","title":"1. Prise en main Airflow","text":"<ul> <li>Installer Airflow</li> <li>Configuration de base</li> <li>Interface web Airflow</li> <li>Premiers DAGs</li> </ul>"},{"location":"AirFlow/FR/#2-concepts-fondamentaux","title":"2. Concepts fondamentaux","text":"<ul> <li>DAGs (Directed Acyclic Graphs)</li> <li>Tasks et d\u00e9pendances</li> <li>Scheduling et triggers</li> <li>Variables et connexions</li> </ul>"},{"location":"AirFlow/FR/#3-operateurs","title":"3. Op\u00e9rateurs","text":"<ul> <li>Op\u00e9rateurs Python</li> <li>Op\u00e9rateurs SQL</li> <li>Op\u00e9rateurs Bash</li> <li>Op\u00e9rateurs personnalis\u00e9s</li> </ul>"},{"location":"AirFlow/FR/#4-sensors","title":"4. Sensors","text":"<ul> <li>FileSensor</li> <li>SqlSensor</li> <li>HttpSensor</li> <li>Sensors personnalis\u00e9s</li> </ul>"},{"location":"AirFlow/FR/#5-hooks","title":"5. Hooks","text":"<ul> <li>Hooks de base de donn\u00e9es</li> <li>Hooks cloud (AWS, Azure)</li> <li>Hooks HTTP</li> <li>Cr\u00e9er des hooks personnalis\u00e9s</li> </ul>"},{"location":"AirFlow/FR/#6-variables-et-connexions","title":"6. Variables et Connexions","text":"<ul> <li>G\u00e9rer les variables</li> <li>Configurer les connexions</li> <li>S\u00e9curit\u00e9 et bonnes pratiques</li> <li>Variables dynamiques</li> </ul>"},{"location":"AirFlow/FR/#7-bonnes-pratiques","title":"7. Bonnes pratiques","text":"<ul> <li>Structure des DAGs</li> <li>Gestion des erreurs</li> <li>Performance et optimisation</li> <li>Tests et d\u00e9bogage</li> </ul>"},{"location":"AirFlow/FR/#8-projets-pratiques","title":"8. Projets pratiques","text":"<ul> <li>Pipeline ETL complet</li> <li>Orchestration de workflows</li> <li>Int\u00e9gration avec bases de donn\u00e9es</li> <li>Projets pour portfolio</li> </ul>"},{"location":"AirFlow/FR/#demarrage-rapide","title":"\ud83d\ude80 D\u00e9marrage rapide","text":""},{"location":"AirFlow/FR/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Python 3.8+ : Install\u00e9 sur votre syst\u00e8me</li> <li>pip : Gestionnaire de paquets Python</li> <li>PostgreSQL (optionnel) : Pour la base de m\u00e9tadonn\u00e9es</li> </ul>"},{"location":"AirFlow/FR/#installation-rapide","title":"Installation rapide","text":"<pre><code># Cr\u00e9er un environnement virtuel\npython -m venv airflow-env\n\n# Activer l'environnement\n# Windows\nairflow-env\\Scripts\\activate\n# Linux/Mac\nsource airflow-env/bin/activate\n\n# Installer Airflow\npip install apache-airflow\n\n# Initialiser la base de donn\u00e9es\nairflow db init\n\n# Cr\u00e9er un utilisateur admin\nairflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com\n\n# D\u00e9marrer le serveur web\nairflow webserver --port 8080\n\n# Dans un autre terminal, d\u00e9marrer le scheduler\nairflow scheduler\n</code></pre>"},{"location":"AirFlow/FR/#acceder-a-linterface-web","title":"Acc\u00e9der \u00e0 l'interface web","text":"<ol> <li>Ouvrir un navigateur</li> <li>Aller sur : <code>http://localhost:8080</code></li> <li>Se connecter avec les credentials cr\u00e9\u00e9s</li> </ol>"},{"location":"AirFlow/FR/#cas-dusage-pour-data-analyst","title":"\ud83d\udcca Cas d'usage pour Data Analyst","text":"<ul> <li>Orchestration ETL : Coordonner des pipelines de donn\u00e9es</li> <li>Scheduling : Planifier des t\u00e2ches r\u00e9currentes</li> <li>Monitoring : Surveiller l'ex\u00e9cution des workflows</li> <li>Gestion d'erreurs : Retry automatique et alertes</li> <li>Int\u00e9gration : Connecter plusieurs outils et services</li> </ul>"},{"location":"AirFlow/FR/#installation-sur-machine-distante","title":"\u26a0\ufe0f Installation sur machine distante","text":"<p>Si vous installez Airflow sur une machine A et souhaitez y acc\u00e9der depuis une machine B, consultez le guide Installation et acc\u00e8s distant.</p>"},{"location":"AirFlow/FR/#ressources-gratuites","title":"\ud83d\udcda Ressources gratuites","text":""},{"location":"AirFlow/FR/#documentation-officielle","title":"Documentation officielle","text":"<ul> <li>Apache Airflow : https://airflow.apache.org/docs/</li> <li>Guides complets</li> <li>Tutoriels pas \u00e0 pas</li> <li>Exemples de code</li> <li> <p>API Reference</p> </li> <li> <p>GitHub Airflow : https://github.com/apache/airflow</p> </li> <li>Code source</li> <li>Exemples de DAGs</li> <li>Contributions</li> </ul>"},{"location":"AirFlow/FR/#ressources-externes","title":"Ressources externes","text":"<ul> <li>YouTube : Tutoriels Airflow</li> <li>Medium : Articles et guides</li> <li>Stack Overflow : Questions et r\u00e9ponses</li> </ul>"},{"location":"AirFlow/FR/#certifications-optionnel","title":"\ud83c\udf93 Certifications (optionnel)","text":""},{"location":"AirFlow/FR/#apache-airflow-pas-de-certification-officielle","title":"Apache Airflow (pas de certification officielle)","text":"<ul> <li>Formation : Documentation et tutoriels gratuits</li> <li>Dur\u00e9e : 2-4 semaines</li> <li>Niveau : Interm\u00e9diaire \u00e0 avanc\u00e9</li> </ul>"},{"location":"AirFlow/FR/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>Tous les exemples utilisent Python 3.8+</li> <li>Les DAGs sont test\u00e9s sur Airflow 2.x</li> <li>Les chemins peuvent varier selon le syst\u00e8me d'exploitation</li> <li>Les ports par d\u00e9faut peuvent \u00eatre modifi\u00e9s</li> </ul>"},{"location":"AirFlow/FR/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>Cette formation est con\u00e7ue pour \u00eatre \u00e9volutive. N'h\u00e9sitez pas \u00e0 proposer des am\u00e9liorations ou des cas d'usage suppl\u00e9mentaires.</p>"},{"location":"AirFlow/FR/#ressources-complementaires","title":"\ud83d\udcda Ressources compl\u00e9mentaires","text":"<ul> <li>Documentation Apache Airflow</li> <li>GitHub Apache Airflow</li> <li>Airflow Community</li> <li>Airflow Examples</li> </ul>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/","title":"Installation Airflow - Acc\u00e8s distant","text":""},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#objectif","title":"\ud83c\udfaf Objectif","text":"<p>Ce guide vous permet d'installer Apache Airflow sur une machine A (serveur) et d'y acc\u00e9der depuis une machine B (client) via le r\u00e9seau local.</p>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#prerequis","title":"\ud83d\udccb Pr\u00e9requis","text":""},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#machine-a-serveur","title":"Machine A (Serveur)","text":"<ul> <li>Python 3.8+ install\u00e9</li> <li>Acc\u00e8s administrateur</li> <li>Connexion r\u00e9seau active</li> <li>Port 8080 disponible (ou autre port)</li> </ul>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#machine-b-client","title":"Machine B (Client)","text":"<ul> <li>Navigateur web</li> <li>Connexion au m\u00eame r\u00e9seau local que la machine A</li> </ul>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#installation-sur-machine-a","title":"\ud83d\udd27 Installation sur Machine A","text":""},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#etape-1-installer-python-et-dependances","title":"\u00c9tape 1 : Installer Python et d\u00e9pendances","text":"<p>Windows : <pre><code># V\u00e9rifier Python\npython --version\n\n# Installer pip si n\u00e9cessaire\npython -m ensurepip --upgrade\n</code></pre></p> <p>Linux : <pre><code># Installer Python et pip\nsudo apt update\nsudo apt install python3 python3-pip python3-venv\n</code></pre></p>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#etape-2-creer-lenvironnement-airflow","title":"\u00c9tape 2 : Cr\u00e9er l'environnement Airflow","text":"<pre><code># Cr\u00e9er un r\u00e9pertoire pour Airflow\nmkdir airflow-install\ncd airflow-install\n\n# Cr\u00e9er un environnement virtuel\npython -m venv airflow-env\n\n# Activer l'environnement\n# Windows\nairflow-env\\Scripts\\activate\n# Linux\nsource airflow-env/bin/activate\n\n# Installer Airflow\npip install apache-airflow\n\n# Installer le provider PostgreSQL (optionnel, pour base de m\u00e9tadonn\u00e9es)\npip install apache-airflow-providers-postgres\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#etape-3-configurer-airflow","title":"\u00c9tape 3 : Configurer Airflow","text":"<pre><code># Initialiser la base de donn\u00e9es\nairflow db init\n\n# Cr\u00e9er un utilisateur administrateur\nairflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password admin123\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#etape-4-configurer-lacces-reseau","title":"\u00c9tape 4 : Configurer l'acc\u00e8s r\u00e9seau","text":"<p>Modifier la configuration Airflow :</p> <ol> <li>Trouver le fichier <code>airflow.cfg</code> :</li> <li>Windows : <code>%USERPROFILE%\\airflow\\airflow.cfg</code></li> <li> <p>Linux : <code>~/airflow/airflow.cfg</code></p> </li> <li> <p>Modifier les param\u00e8tres suivants :</p> </li> </ol> <pre><code>[webserver]\n# Permettre l'acc\u00e8s depuis toutes les interfaces\nweb_server_host = 0.0.0.0\nweb_server_port = 8080\n\n# D\u00e9sactiver l'authentification basique (optionnel, pour d\u00e9veloppement)\nauth_backend = airflow.api.auth.backend.basic_auth\n</code></pre> <p>Ou cr\u00e9er un fichier de configuration personnalis\u00e9 :</p> <pre><code># Cr\u00e9er un fichier airflow.cfg personnalis\u00e9\nexport AIRFLOW_HOME=/path/to/airflow\nairflow config get-value webserver web_server_host\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#etape-5-configurer-le-pare-feu","title":"\u00c9tape 5 : Configurer le pare-feu","text":"<p>Windows (Firewall) :</p> <ol> <li>Ouvrir \"Pare-feu Windows Defender\"</li> <li>\"Param\u00e8tres avanc\u00e9s\"</li> <li>\"R\u00e8gles de trafic entrant\" \u2192 \"Nouvelle r\u00e8gle\"</li> <li>Type : Port</li> <li>Port : 8080 (TCP)</li> <li>Action : Autoriser la connexion</li> <li>Nom : \"Airflow Web Server\"</li> </ol> <p>Linux (UFW) :</p> <pre><code># Autoriser le port 8080\nsudo ufw allow 8080/tcp\nsudo ufw reload\n</code></pre> <p>Linux (firewalld) :</p> <pre><code># Autoriser le port 8080\nsudo firewall-cmd --permanent --add-port=8080/tcp\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#etape-6-demarrer-airflow","title":"\u00c9tape 6 : D\u00e9marrer Airflow","text":"<p>Terminal 1 - Web Server :</p> <pre><code># Activer l'environnement virtuel\nsource airflow-env/bin/activate  # Linux\n# ou\nairflow-env\\Scripts\\activate  # Windows\n\n# D\u00e9marrer le serveur web\nairflow webserver --port 8080 --host 0.0.0.0\n</code></pre> <p>Terminal 2 - Scheduler :</p> <pre><code># Activer l'environnement virtuel\nsource airflow-env/bin/activate  # Linux\n# ou\nairflow-env\\Scripts\\activate  # Windows\n\n# D\u00e9marrer le scheduler\nairflow scheduler\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#etape-7-obtenir-ladresse-ip-de-la-machine-a","title":"\u00c9tape 7 : Obtenir l'adresse IP de la Machine A","text":"<p>Windows : <pre><code>ipconfig\n# Chercher \"Adresse IPv4\" (ex: 192.168.1.100)\n</code></pre></p> <p>Linux : <pre><code>ip addr show\n# ou\nhostname -I\n# Chercher l'adresse IP (ex: 192.168.1.100)\n</code></pre></p>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#acces-depuis-machine-b","title":"\ud83c\udf10 Acc\u00e8s depuis Machine B","text":""},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#etape-1-verifier-la-connectivite","title":"\u00c9tape 1 : V\u00e9rifier la connectivit\u00e9","text":"<p>Depuis Machine B :</p> <pre><code># Tester la connexion\nping 192.168.1.100  # Remplacer par l'IP de la Machine A\n\n# Tester le port\ntelnet 192.168.1.100 8080\n# ou\ncurl http://192.168.1.100:8080\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#etape-2-acceder-a-linterface-web","title":"\u00c9tape 2 : Acc\u00e9der \u00e0 l'interface web","text":"<ol> <li>Ouvrir un navigateur sur Machine B</li> <li>Aller sur : <code>http://192.168.1.100:8080</code></li> <li>Remplacer <code>192.168.1.100</code> par l'IP de la Machine A</li> <li>Se connecter avec :</li> <li>Username : <code>admin</code></li> <li>Password : <code>admin123</code> (ou celui que vous avez cr\u00e9\u00e9)</li> </ol>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#securite","title":"\ud83d\udd12 S\u00e9curit\u00e9","text":""},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#recommandations","title":"Recommandations","text":"<ol> <li> <p>Changer le mot de passe par d\u00e9faut <pre><code>airflow users set-password admin\n</code></pre></p> </li> <li> <p>Utiliser HTTPS (en production)</p> </li> <li>Configurer un reverse proxy (nginx, Apache)</li> <li> <p>Utiliser des certificats SSL</p> </li> <li> <p>Limiter l'acc\u00e8s r\u00e9seau</p> </li> <li>Utiliser un VPN</li> <li> <p>Restreindre les IPs autoris\u00e9es dans le firewall</p> </li> <li> <p>Authentification renforc\u00e9e</p> </li> <li>Utiliser OAuth</li> <li>Int\u00e9grer avec LDAP/Active Directory</li> </ol>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#configuration-securisee","title":"Configuration s\u00e9curis\u00e9e","text":"<p>Modifier <code>airflow.cfg</code> :</p> <pre><code>[webserver]\n# Activer l'authentification\nauth_backend = airflow.api.auth.backend.basic_auth\n\n# Limiter les h\u00f4tes autoris\u00e9s (optionnel)\nhostname_callable = airflow.utils.net.get_hostname\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#depannage","title":"\ud83d\udc1b D\u00e9pannage","text":""},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#probleme-impossible-de-se-connecter-depuis-machine-b","title":"Probl\u00e8me : Impossible de se connecter depuis Machine B","text":"<p>Solutions :</p> <ol> <li> <p>V\u00e9rifier le pare-feu <pre><code># Windows\nnetsh advfirewall firewall show rule name=\"Airflow Web Server\"\n\n# Linux\nsudo ufw status\n</code></pre></p> </li> <li> <p>V\u00e9rifier que Airflow \u00e9coute sur 0.0.0.0 <pre><code># V\u00e9rifier les ports ouverts\nnetstat -an | grep 8080\n# Doit afficher : 0.0.0.0:8080\n</code></pre></p> </li> <li> <p>V\u00e9rifier la configuration r\u00e9seau</p> </li> <li>Les deux machines sont sur le m\u00eame r\u00e9seau</li> <li>Pas de VPN qui bloque la connexion</li> <li>Pas de proxy qui interf\u00e8re</li> </ol>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#probleme-erreur-connection-refused","title":"Probl\u00e8me : Erreur \"Connection refused\"","text":"<p>Solutions :</p> <ol> <li>V\u00e9rifier que le serveur web est d\u00e9marr\u00e9</li> <li>V\u00e9rifier le port (8080 par d\u00e9faut)</li> <li>V\u00e9rifier les logs Airflow :    <pre><code># Logs du webserver\ntail -f ~/airflow/logs/webserver.log\n</code></pre></li> </ol>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#probleme-erreur-dauthentification","title":"Probl\u00e8me : Erreur d'authentification","text":"<p>Solutions :</p> <ol> <li>V\u00e9rifier les credentials</li> <li>Recr\u00e9er l'utilisateur si n\u00e9cessaire :    <pre><code>airflow users create \\\n    --username admin \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password nouveau_mot_de_passe\n</code></pre></li> </ol>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#configuration-avancee","title":"\ud83d\udcdd Configuration avanc\u00e9e","text":""},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#utiliser-un-reverse-proxy-nginx","title":"Utiliser un reverse proxy (nginx)","text":"<p>Installation nginx :</p> <pre><code># Linux\nsudo apt install nginx\n\n# Configuration nginx\nsudo nano /etc/nginx/sites-available/airflow\n</code></pre> <p>Configuration nginx :</p> <pre><code>server {\n    listen 80;\n    server_name airflow.local;\n\n    location / {\n        proxy_pass http://127.0.0.1:8080;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>Activer la configuration :</p> <pre><code>sudo ln -s /etc/nginx/sites-available/airflow /etc/nginx/sites-enabled/\nsudo nginx -t\nsudo systemctl reload nginx\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#service-systemd-linux","title":"Service systemd (Linux)","text":"<p>Cr\u00e9er un service pour Airflow :</p> <pre><code>sudo nano /etc/systemd/system/airflow-webserver.service\n</code></pre> <p>Contenu :</p> <pre><code>[Unit]\nDescription=Airflow webserver daemon\nAfter=network.target\n\n[Service]\nUser=airflow\nGroup=airflow\nType=simple\nExecStart=/path/to/airflow-env/bin/airflow webserver\nRestart=on-failure\nRestartSec=5s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Activer le service :</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable airflow-webserver\nsudo systemctl start airflow-webserver\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#verification","title":"\ud83d\udcca V\u00e9rification","text":""},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#test-de-connexion","title":"Test de connexion","text":"<p>Depuis Machine B :</p> <pre><code># Test HTTP\ncurl http://192.168.1.100:8080/health\n\n# Test avec authentification\ncurl -u admin:admin123 http://192.168.1.100:8080/api/v1/dags\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#verifier-les-logs","title":"V\u00e9rifier les logs","text":"<p>Sur Machine A :</p> <pre><code># Logs du webserver\ntail -f ~/airflow/logs/webserver.log\n\n# Logs du scheduler\ntail -f ~/airflow/logs/scheduler/*.log\n</code></pre>"},{"location":"AirFlow/FR/INSTALLATION_REMOTE/#ressources","title":"\ud83d\udd17 Ressources","text":"<ul> <li>Documentation Airflow</li> <li>Configuration Airflow</li> <li>S\u00e9curit\u00e9 Airflow</li> </ul> <p>Note : Cette configuration est pour un environnement de d\u00e9veloppement. Pour la production, utilisez des pratiques de s\u00e9curit\u00e9 renforc\u00e9es (HTTPS, authentification OAuth, etc.).</p>"},{"location":"AirFlow/FR/01-getting-started/","title":"1. Prise en main Airflow","text":""},{"location":"AirFlow/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Apache Airflow</li> <li>Installer Airflow localement</li> <li>Configurer l'environnement</li> <li>Acc\u00e9der \u00e0 l'interface web</li> <li>Cr\u00e9er votre premier DAG</li> </ul>"},{"location":"AirFlow/FR/01-getting-started/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Airflow</li> <li>Installation</li> <li>Configuration de base</li> <li>Interface web</li> <li>Premier DAG</li> </ol>"},{"location":"AirFlow/FR/01-getting-started/#introduction-a-airflow","title":"Introduction \u00e0 Airflow","text":""},{"location":"AirFlow/FR/01-getting-started/#quest-ce-quapache-airflow","title":"Qu'est-ce qu'Apache Airflow ?","text":"<p>Apache Airflow = Plateforme open-source d'orchestration de workflows</p> <ul> <li>Workflows : Pipelines de donn\u00e9es complexes</li> <li>Scheduling : Planification automatique</li> <li>Monitoring : Surveillance en temps r\u00e9el</li> <li>Python : D\u00e9fini en Python</li> <li>Scalable : De simple \u00e0 tr\u00e8s complexe</li> </ul>"},{"location":"AirFlow/FR/01-getting-started/#pourquoi-airflow-pour-data-analyst","title":"Pourquoi Airflow pour Data Analyst ?","text":"<ul> <li>Orchestration ETL : Coordonner plusieurs \u00e9tapes</li> <li>Scheduling : Automatiser les t\u00e2ches r\u00e9currentes</li> <li>Monitoring : Voir l'\u00e9tat des pipelines</li> <li>Retry : Nouvelle tentative automatique en cas d'erreur</li> <li>Int\u00e9gration : Avec bases de donn\u00e9es, APIs, services cloud</li> </ul>"},{"location":"AirFlow/FR/01-getting-started/#composants-airflow","title":"Composants Airflow","text":"<ol> <li>Web Server : Interface web (port 8080)</li> <li>Scheduler : Planifie et ex\u00e9cute les DAGs</li> <li>Metadata Database : Stocke l'\u00e9tat et les m\u00e9tadonn\u00e9es</li> <li>Workers : Ex\u00e9cutent les t\u00e2ches (optionnel)</li> </ol>"},{"location":"AirFlow/FR/01-getting-started/#installation","title":"Installation","text":""},{"location":"AirFlow/FR/01-getting-started/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Python 3.8+ : Install\u00e9 sur votre syst\u00e8me</li> <li>pip : Gestionnaire de paquets Python</li> <li>7-8 Go RAM : Minimum recommand\u00e9</li> </ul>"},{"location":"AirFlow/FR/01-getting-started/#installation-avec-pip","title":"Installation avec pip","text":"<p>\u00c9tape 1 : Cr\u00e9er un environnement virtuel</p> <pre><code># Cr\u00e9er un r\u00e9pertoire\nmkdir airflow-project\ncd airflow-project\n\n# Cr\u00e9er un environnement virtuel\npython -m venv airflow-env\n\n# Activer l'environnement\n# Windows\nairflow-env\\Scripts\\activate\n# Linux/Mac\nsource airflow-env/bin/activate\n</code></pre> <p>\u00c9tape 2 : Installer Airflow</p> <pre><code># Installer Airflow\npip install apache-airflow\n\n# Installer des providers suppl\u00e9mentaires (optionnel)\npip install apache-airflow-providers-postgres\npip install apache-airflow-providers-http\n</code></pre> <p>\u00c9tape 3 : Initialiser la base de donn\u00e9es</p> <pre><code># Initialiser la base de donn\u00e9es SQLite (par d\u00e9faut)\nairflow db init\n</code></pre> <p>\u00c9tape 4 : Cr\u00e9er un utilisateur admin</p> <pre><code>airflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password admin123\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#installation-avec-constraints-recommande","title":"Installation avec constraints (recommand\u00e9)","text":"<p>Pour \u00e9viter les conflits de d\u00e9pendances :</p> <pre><code># T\u00e9l\u00e9charger les constraints\nAIRFLOW_VERSION=2.7.0\nPYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n\n# Installer avec constraints\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#configuration-de-base","title":"Configuration de base","text":""},{"location":"AirFlow/FR/01-getting-started/#fichier-airflowcfg","title":"Fichier airflow.cfg","text":"<p>Localisation : - Windows : <code>%USERPROFILE%\\airflow\\airflow.cfg</code> - Linux/Mac : <code>~/airflow/airflow.cfg</code></p> <p>Param\u00e8tres importants :</p> <pre><code>[core]\n# R\u00e9pertoire des DAGs\ndags_folder = ~/airflow/dags\n\n# R\u00e9pertoire des logs\nbase_log_folder = ~/airflow/logs\n\n# Fuseau horaire\ndefault_timezone = Europe/Paris\n\n[webserver]\n# Port du serveur web\nweb_server_port = 8080\n\n# Host (0.0.0.0 pour acc\u00e8s r\u00e9seau)\nweb_server_host = 0.0.0.0\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#variables-denvironnement","title":"Variables d'environnement","text":"<p>AIRFLOW_HOME :</p> <pre><code># Windows\nset AIRFLOW_HOME=C:\\airflow\n\n# Linux/Mac\nexport AIRFLOW_HOME=~/airflow\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#structure-des-repertoires","title":"Structure des r\u00e9pertoires","text":"<pre><code>airflow/\n\u251c\u2500\u2500 dags/          # Vos DAGs\n\u251c\u2500\u2500 logs/          # Logs d'ex\u00e9cution\n\u251c\u2500\u2500 plugins/       # Plugins personnalis\u00e9s\n\u2514\u2500\u2500 airflow.cfg   # Configuration\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#interface-web","title":"Interface web","text":""},{"location":"AirFlow/FR/01-getting-started/#demarrer-le-serveur-web","title":"D\u00e9marrer le serveur web","text":"<pre><code># Activer l'environnement virtuel\nsource airflow-env/bin/activate  # Linux/Mac\n# ou\nairflow-env\\Scripts\\activate  # Windows\n\n# D\u00e9marrer le serveur web\nairflow webserver --port 8080\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#demarrer-le-scheduler","title":"D\u00e9marrer le scheduler","text":"<p>Dans un autre terminal :</p> <pre><code># Activer l'environnement virtuel\nsource airflow-env/bin/activate\n\n# D\u00e9marrer le scheduler\nairflow scheduler\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#acceder-a-linterface","title":"Acc\u00e9der \u00e0 l'interface","text":"<ol> <li>Ouvrir un navigateur</li> <li>Aller sur : <code>http://localhost:8080</code></li> <li>Se connecter avec :</li> <li>Username : <code>admin</code></li> <li>Password : <code>admin123</code></li> </ol>"},{"location":"AirFlow/FR/01-getting-started/#navigation-dans-linterface","title":"Navigation dans l'interface","text":"<p>Onglets principaux : - DAGs : Liste de tous les DAGs - Graph : Vue graphique d'un DAG - Tree : Vue arborescente des ex\u00e9cutions - Gantt : Diagramme de Gantt - Code : Code source du DAG - Logs : Logs d'ex\u00e9cution</p>"},{"location":"AirFlow/FR/01-getting-started/#premier-dag","title":"Premier DAG","text":""},{"location":"AirFlow/FR/01-getting-started/#creer-un-dag-simple","title":"Cr\u00e9er un DAG simple","text":"<p>\u00c9tape 1 : Cr\u00e9er le fichier DAG</p> <pre><code># Cr\u00e9er le r\u00e9pertoire dags\nmkdir -p ~/airflow/dags\n\n# Cr\u00e9er un fichier DAG\nnano ~/airflow/dags/my_first_dag.py\n</code></pre> <p>\u00c9tape 2 : Code du DAG</p> <pre><code>from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\n# D\u00e9finir les arguments par d\u00e9faut\ndefault_args = {\n    'owner': 'data_analyst',\n    'depends_on_past': False,\n    'email': ['admin@example.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Cr\u00e9er le DAG\ndag = DAG(\n    'my_first_dag',\n    default_args=default_args,\n    description='Mon premier DAG Airflow',\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['tutorial'],\n)\n\n# T\u00e2che 1 : Afficher la date\nt1 = BashOperator(\n    task_id='print_date',\n    bash_command='date',\n    dag=dag,\n)\n\n# T\u00e2che 2 : Afficher un message\ndef print_hello():\n    print(\"Hello from Airflow!\")\n\nt2 = PythonOperator(\n    task_id='print_hello',\n    python_callable=print_hello,\n    dag=dag,\n)\n\n# D\u00e9finir les d\u00e9pendances\nt1 &gt;&gt; t2  # t1 s'ex\u00e9cute avant t2\n</code></pre> <p>\u00c9tape 3 : V\u00e9rifier le DAG</p> <pre><code># Lister les DAGs\nairflow dags list\n\n# V\u00e9rifier la syntaxe\nairflow dags list-import-errors\n\n# Tester le DAG\nairflow dags test my_first_dag 2024-01-01\n</code></pre> <p>\u00c9tape 4 : Voir dans l'interface web</p> <ol> <li>Rafra\u00eechir la page web</li> <li>Le DAG <code>my_first_dag</code> appara\u00eet dans la liste</li> <li>Cliquer sur \"Trigger DAG\" pour l'ex\u00e9cuter</li> </ol>"},{"location":"AirFlow/FR/01-getting-started/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"AirFlow/FR/01-getting-started/#exemple-1-dag-avec-plusieurs-taches","title":"Exemple 1 : DAG avec plusieurs t\u00e2ches","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data_analyst',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='Exemple de DAG avec plusieurs t\u00e2ches',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\n# T\u00e2che 1\nextract = BashOperator(\n    task_id='extract_data',\n    bash_command='echo \"Extracting data...\"',\n    dag=dag,\n)\n\n# T\u00e2che 2\ntransform = BashOperator(\n    task_id='transform_data',\n    bash_command='echo \"Transforming data...\"',\n    dag=dag,\n)\n\n# T\u00e2che 3\nload = BashOperator(\n    task_id='load_data',\n    bash_command='echo \"Loading data...\"',\n    dag=dag,\n)\n\n# D\u00e9finir les d\u00e9pendances\nextract &gt;&gt; transform &gt;&gt; load\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#exemple-2-dag-avec-branches","title":"Exemple 2 : DAG avec branches","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'branching_dag',\n    description='DAG avec branches',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\ndef decide_path():\n    # Logique pour d\u00e9cider du chemin\n    return 'path_a'\n\ndecide = PythonOperator(\n    task_id='decide',\n    python_callable=decide_path,\n    dag=dag,\n)\n\npath_a = BashOperator(\n    task_id='path_a',\n    bash_command='echo \"Path A\"',\n    dag=dag,\n)\n\npath_b = BashOperator(\n    task_id='path_b',\n    bash_command='echo \"Path B\"',\n    dag=dag,\n)\n\n# Branchement conditionnel\ndecide &gt;&gt; [path_a, path_b]\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#commandes-utiles","title":"Commandes utiles","text":""},{"location":"AirFlow/FR/01-getting-started/#gestion-des-dags","title":"Gestion des DAGs","text":"<pre><code># Lister tous les DAGs\nairflow dags list\n\n# V\u00e9rifier les erreurs d'import\nairflow dags list-import-errors\n\n# Tester un DAG\nairflow dags test my_first_dag 2024-01-01\n\n# Pauser un DAG\nairflow dags pause my_first_dag\n\n# Reprendre un DAG\nairflow dags unpause my_first_dag\n\n# Supprimer un DAG\nairflow dags delete my_first_dag\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#gestion-des-taches","title":"Gestion des t\u00e2ches","text":"<pre><code># Tester une t\u00e2che\nairflow tasks test my_first_dag print_date 2024-01-01\n\n# Ex\u00e9cuter une t\u00e2che\nairflow tasks run my_first_dag print_date 2024-01-01\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#gestion-de-la-base-de-donnees","title":"Gestion de la base de donn\u00e9es","text":"<pre><code># Initialiser la base\nairflow db init\n\n# Mettre \u00e0 jour la base\nairflow db upgrade\n\n# R\u00e9initialiser la base (ATTENTION : supprime tout)\nairflow db reset\n</code></pre>"},{"location":"AirFlow/FR/01-getting-started/#depannage","title":"D\u00e9pannage","text":""},{"location":"AirFlow/FR/01-getting-started/#probleme-dag-non-visible-dans-linterface","title":"Probl\u00e8me : DAG non visible dans l'interface","text":"<p>Solutions : 1. V\u00e9rifier que le fichier est dans <code>~/airflow/dags/</code> 2. V\u00e9rifier la syntaxe Python 3. V\u00e9rifier les erreurs : <code>airflow dags list-import-errors</code> 4. Red\u00e9marrer le scheduler</p>"},{"location":"AirFlow/FR/01-getting-started/#probleme-erreur-dimport","title":"Probl\u00e8me : Erreur d'import","text":"<p>Solutions : 1. V\u00e9rifier que toutes les d\u00e9pendances sont install\u00e9es 2. V\u00e9rifier les imports dans le DAG 3. V\u00e9rifier les chemins Python</p>"},{"location":"AirFlow/FR/01-getting-started/#probleme-scheduler-ne-demarre-pas","title":"Probl\u00e8me : Scheduler ne d\u00e9marre pas","text":"<p>Solutions : 1. V\u00e9rifier que la base de donn\u00e9es est initialis\u00e9e 2. V\u00e9rifier les logs : <code>~/airflow/logs/scheduler/</code> 3. V\u00e9rifier les permissions</p>"},{"location":"AirFlow/FR/01-getting-started/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Airflow = Orchestration de workflows Python</li> <li>DAGs d\u00e9finissent les workflows</li> <li>Tasks sont les \u00e9tapes individuelles</li> <li>Scheduler ex\u00e9cute les DAGs selon le planning</li> <li>Interface web permet de monitorer et g\u00e9rer</li> </ol>"},{"location":"AirFlow/FR/01-getting-started/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 2. Concepts fondamentaux pour approfondir les concepts d'Airflow.</p>"},{"location":"AirFlow/FR/02-concepts/","title":"2. Concepts fondamentaux Airflow","text":""},{"location":"AirFlow/FR/02-concepts/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les DAGs (Directed Acyclic Graphs)</li> <li>Ma\u00eetriser les t\u00e2ches et d\u00e9pendances</li> <li>Comprendre le scheduling</li> <li>Utiliser les variables et connexions</li> <li>G\u00e9rer les ex\u00e9cutions</li> </ul>"},{"location":"AirFlow/FR/02-concepts/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>DAGs (Directed Acyclic Graphs)</li> <li>Tasks et d\u00e9pendances</li> <li>Scheduling</li> <li>Variables et Connexions</li> <li>Ex\u00e9cutions et \u00e9tats</li> </ol>"},{"location":"AirFlow/FR/02-concepts/#dags-directed-acyclic-graphs","title":"DAGs (Directed Acyclic Graphs)","text":""},{"location":"AirFlow/FR/02-concepts/#quest-ce-quun-dag","title":"Qu'est-ce qu'un DAG ?","text":"<p>DAG = Graphe orient\u00e9 acyclique</p> <ul> <li>Oriented : Les t\u00e2ches ont un sens (d\u00e9pendances)</li> <li>Acyclic : Pas de boucles (pas de d\u00e9pendances circulaires)</li> <li>Graph : Repr\u00e9sentation visuelle des workflows</li> </ul>"},{"location":"AirFlow/FR/02-concepts/#structure-dun-dag","title":"Structure d'un DAG","text":"<pre><code>from airflow import DAG\nfrom datetime import datetime, timedelta\n\n# Arguments par d\u00e9faut\ndefault_args = {\n    'owner': 'data_analyst',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Cr\u00e9er le DAG\ndag = DAG(\n    'my_dag',\n    default_args=default_args,\n    description='Description du DAG',\n    schedule_interval='@daily',  # Fr\u00e9quence d'ex\u00e9cution\n    start_date=datetime(2024, 1, 1),\n    catchup=False,  # Ne pas ex\u00e9cuter les runs pass\u00e9s\n    tags=['example'],\n)\n</code></pre>"},{"location":"AirFlow/FR/02-concepts/#proprietes-dun-dag","title":"Propri\u00e9t\u00e9s d'un DAG","text":"<p>ID unique : - Doit \u00eatre unique dans l'installation Airflow - Utilis\u00e9 pour identifier le DAG</p> <p>Schedule interval : - <code>@daily</code> : Tous les jours - <code>@hourly</code> : Toutes les heures - <code>timedelta(days=1)</code> : Tous les jours - <code>'0 2 * * *'</code> : Cron expression (tous les jours \u00e0 2h) - <code>None</code> : D\u00e9clenchement manuel uniquement</p> <p>Start date : - Date de d\u00e9but du scheduling - Format : <code>datetime(ann\u00e9e, mois, jour)</code></p> <p>Catchup : - <code>True</code> : Ex\u00e9cute les runs manqu\u00e9s depuis start_date - <code>False</code> : N'ex\u00e9cute que les runs futurs</p>"},{"location":"AirFlow/FR/02-concepts/#tasks-et-dependances","title":"Tasks et d\u00e9pendances","text":""},{"location":"AirFlow/FR/02-concepts/#quest-ce-quune-task","title":"Qu'est-ce qu'une Task ?","text":"<p>Task = \u00c9tape individuelle dans un DAG</p> <ul> <li>Op\u00e9rateur : Type de t\u00e2che (Python, Bash, SQL, etc.)</li> <li>ID unique : Identifiant dans le DAG</li> <li>D\u00e9pendances : Relations avec d'autres t\u00e2ches</li> </ul>"},{"location":"AirFlow/FR/02-concepts/#types-doperateurs","title":"Types d'op\u00e9rateurs","text":"<p>BashOperator : <pre><code>from airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='bash_task',\n    bash_command='echo \"Hello\"',\n    dag=dag,\n)\n</code></pre></p> <p>PythonOperator : <pre><code>from airflow.operators.python import PythonOperator\n\ndef my_function():\n    print(\"Hello from Python\")\n\ntask = PythonOperator(\n    task_id='python_task',\n    python_callable=my_function,\n    dag=dag,\n)\n</code></pre></p>"},{"location":"AirFlow/FR/02-concepts/#definir-les-dependances","title":"D\u00e9finir les d\u00e9pendances","text":"<p>M\u00e9thode 1 : Op\u00e9rateur &gt;&gt;</p> <pre><code># t1 s'ex\u00e9cute avant t2\nt1 &gt;&gt; t2\n\n# Plusieurs d\u00e9pendances\nt1 &gt;&gt; [t2, t3] &gt;&gt; t4\n</code></pre> <p>M\u00e9thode 2 : set_upstream / set_downstream</p> <pre><code># t1 s'ex\u00e9cute avant t2\nt1.set_downstream(t2)\n# ou\nt2.set_upstream(t1)\n</code></pre> <p>M\u00e9thode 3 : bitshift</p> <pre><code># t1 &gt;&gt; t2 \u00e9quivaut \u00e0\nt1 &gt;&gt; t2\n</code></pre>"},{"location":"AirFlow/FR/02-concepts/#exemple-de-dependances","title":"Exemple de d\u00e9pendances","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime\n\ndag = DAG('dependencies_example', start_date=datetime(2024, 1, 1))\n\n# T\u00e2ches\nextract = BashOperator(task_id='extract', bash_command='echo extract', dag=dag)\ntransform = BashOperator(task_id='transform', bash_command='echo transform', dag=dag)\nload = BashOperator(task_id='load', bash_command='echo load', dag=dag)\nvalidate = BashOperator(task_id='validate', bash_command='echo validate', dag=dag)\n\n# D\u00e9pendances\nextract &gt;&gt; transform &gt;&gt; [load, validate]\n</code></pre>"},{"location":"AirFlow/FR/02-concepts/#scheduling","title":"Scheduling","text":""},{"location":"AirFlow/FR/02-concepts/#schedule-interval","title":"Schedule Interval","text":"<p>Expressions courantes :</p> <pre><code># Tous les jours \u00e0 minuit\nschedule_interval='@daily'\n# ou\nschedule_interval=timedelta(days=1)\n\n# Toutes les heures\nschedule_interval='@hourly'\n# ou\nschedule_interval=timedelta(hours=1)\n\n# Toutes les semaines\nschedule_interval='@weekly'\n\n# Expression cron\nschedule_interval='0 2 * * *'  # Tous les jours \u00e0 2h\nschedule_interval='0 */6 * * *'  # Toutes les 6 heures\nschedule_interval='0 0 * * MON'  # Tous les lundis \u00e0 minuit\n</code></pre>"},{"location":"AirFlow/FR/02-concepts/#start-date-et-execution-date","title":"Start Date et Execution Date","text":"<p>Start Date : - Date de d\u00e9but du scheduling - Format : <code>datetime(2024, 1, 1)</code></p> <p>Execution Date : - Date logique d'ex\u00e9cution - Format : <code>YYYY-MM-DDTHH:MM:SS</code></p> <p>Exemple : <pre><code>dag = DAG(\n    'scheduled_dag',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n</code></pre></p>"},{"location":"AirFlow/FR/02-concepts/#catchup","title":"Catchup","text":"<p>Catchup = True : - Ex\u00e9cute tous les runs manqu\u00e9s depuis start_date - Peut cr\u00e9er beaucoup de runs</p> <p>Catchup = False : - N'ex\u00e9cute que les runs futurs - Recommand\u00e9 pour la plupart des cas</p>"},{"location":"AirFlow/FR/02-concepts/#variables-et-connexions","title":"Variables et Connexions","text":""},{"location":"AirFlow/FR/02-concepts/#variables","title":"Variables","text":"<p>Variables = Configuration globale</p> <p>Cr\u00e9er une variable :</p> <pre><code># Via CLI\nairflow variables set my_key \"my_value\"\n\n# Via interface web\n# Admin \u2192 Variables \u2192 Add\n</code></pre> <p>Utiliser une variable :</p> <pre><code>from airflow.models import Variable\n\n# R\u00e9cup\u00e9rer une variable\nmy_value = Variable.get(\"my_key\")\nmy_value_default = Variable.get(\"my_key\", default_var=\"default\")\n\n# Dans un template\n# {{ var.value.my_key }}\n</code></pre> <p>Exemple :</p> <pre><code>from airflow import DAG\nfrom airflow.models import Variable\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('variables_example', start_date=datetime(2024, 1, 1))\n\ndef use_variable():\n    api_key = Variable.get(\"api_key\")\n    print(f\"API Key: {api_key}\")\n\ntask = PythonOperator(\n    task_id='use_variable',\n    python_callable=use_variable,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/02-concepts/#connexions","title":"Connexions","text":"<p>Connexions = Informations de connexion</p> <p>Cr\u00e9er une connexion :</p> <pre><code># Via CLI\nairflow connections add 'my_postgres' \\\n    --conn-type 'postgres' \\\n    --conn-host 'localhost' \\\n    --conn-login 'user' \\\n    --conn-password 'password' \\\n    --conn-port 5432 \\\n    --conn-schema 'mydb'\n</code></pre> <p>Utiliser une connexion :</p> <pre><code>from airflow.hooks.base import BaseHook\n\n# R\u00e9cup\u00e9rer une connexion\nconn = BaseHook.get_connection('my_postgres')\nprint(f\"Host: {conn.host}\")\nprint(f\"Login: {conn.login}\")\nprint(f\"Password: {conn.password}\")\n</code></pre>"},{"location":"AirFlow/FR/02-concepts/#executions-et-etats","title":"Ex\u00e9cutions et \u00e9tats","text":""},{"location":"AirFlow/FR/02-concepts/#etats-des-taches","title":"\u00c9tats des t\u00e2ches","text":"<ul> <li>None : Pas encore ex\u00e9cut\u00e9e</li> <li>Scheduled : Planifi\u00e9e</li> <li>Queued : En attente</li> <li>Running : En cours d'ex\u00e9cution</li> <li>Success : R\u00e9ussie</li> <li>Failed : \u00c9chou\u00e9e</li> <li>Skipped : Ignor\u00e9e</li> <li>Retry : Nouvelle tentative</li> <li>Up for retry : Pr\u00eate pour retry</li> </ul>"},{"location":"AirFlow/FR/02-concepts/#etats-des-dags","title":"\u00c9tats des DAGs","text":"<ul> <li>Running : En cours d'ex\u00e9cution</li> <li>Success : Toutes les t\u00e2ches r\u00e9ussies</li> <li>Failed : Au moins une t\u00e2che \u00e9chou\u00e9e</li> </ul>"},{"location":"AirFlow/FR/02-concepts/#gerer-les-executions","title":"G\u00e9rer les ex\u00e9cutions","text":"<p>Via l'interface web : - Voir l'\u00e9tat des ex\u00e9cutions - Relancer une t\u00e2che - Marquer comme succ\u00e8s/\u00e9chec - Voir les logs</p> <p>Via CLI :</p> <pre><code># Lister les runs\nairflow dags list-runs -d my_dag\n\n# D\u00e9clencher un DAG\nairflow dags trigger my_dag\n\n# Marquer une t\u00e2che comme succ\u00e8s\nairflow tasks clear my_dag task_id -s 2024-01-01\n</code></pre>"},{"location":"AirFlow/FR/02-concepts/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"AirFlow/FR/02-concepts/#exemple-1-dag-avec-variables","title":"Exemple 1 : DAG avec variables","text":"<pre><code>from airflow import DAG\nfrom airflow.models import Variable\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('variables_dag', start_date=datetime(2024, 1, 1))\n\ndef process_data():\n    # R\u00e9cup\u00e9rer des variables\n    input_path = Variable.get(\"input_path\")\n    output_path = Variable.get(\"output_path\")\n\n    print(f\"Processing: {input_path} -&gt; {output_path}\")\n\ntask = PythonOperator(\n    task_id='process',\n    python_callable=process_data,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/02-concepts/#exemple-2-dag-avec-connexion","title":"Exemple 2 : DAG avec connexion","text":"<pre><code>from airflow import DAG\nfrom airflow.hooks.postgres import PostgresHook\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('connection_dag', start_date=datetime(2024, 1, 1))\n\ndef query_database():\n    # Utiliser une connexion PostgreSQL\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    records = hook.get_records(\"SELECT * FROM users LIMIT 10\")\n    print(records)\n\ntask = PythonOperator(\n    task_id='query',\n    python_callable=query_database,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/02-concepts/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>DAGs d\u00e9finissent les workflows</li> <li>Tasks sont les \u00e9tapes individuelles</li> <li>D\u00e9pendances d\u00e9finissent l'ordre d'ex\u00e9cution</li> <li>Scheduling planifie les ex\u00e9cutions</li> <li>Variables et Connexions pour la configuration</li> </ol>"},{"location":"AirFlow/FR/02-concepts/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 3. Op\u00e9rateurs pour apprendre \u00e0 utiliser les diff\u00e9rents op\u00e9rateurs Airflow.</p>"},{"location":"AirFlow/FR/03-operators/","title":"3. Op\u00e9rateurs Airflow","text":""},{"location":"AirFlow/FR/03-operators/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les diff\u00e9rents types d'op\u00e9rateurs</li> <li>Utiliser les op\u00e9rateurs Python, Bash, SQL</li> <li>Cr\u00e9er des op\u00e9rateurs personnalis\u00e9s</li> <li>G\u00e9rer les donn\u00e9es entre t\u00e2ches</li> </ul>"},{"location":"AirFlow/FR/03-operators/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Types d'op\u00e9rateurs</li> <li>PythonOperator</li> <li>BashOperator</li> <li>SQL Operators</li> <li>Op\u00e9rateurs personnalis\u00e9s</li> </ol>"},{"location":"AirFlow/FR/03-operators/#types-doperateurs","title":"Types d'op\u00e9rateurs","text":""},{"location":"AirFlow/FR/03-operators/#operateurs-de-base","title":"Op\u00e9rateurs de base","text":"<ul> <li>PythonOperator : Ex\u00e9cute du code Python</li> <li>BashOperator : Ex\u00e9cute des commandes bash</li> <li>SQLExecuteQueryOperator : Ex\u00e9cute des requ\u00eates SQL</li> <li>EmailOperator : Envoie des emails</li> <li>HttpOperator : Fait des requ\u00eates HTTP</li> </ul>"},{"location":"AirFlow/FR/03-operators/#operateurs-de-transfert","title":"Op\u00e9rateurs de transfert","text":"<ul> <li>FileTransferOperator : Transf\u00e8re des fichiers</li> <li>FTPOperator : Op\u00e9rations FTP</li> <li>S3FileTransformOperator : Transforme des fichiers S3</li> </ul>"},{"location":"AirFlow/FR/03-operators/#pythonoperator","title":"PythonOperator","text":""},{"location":"AirFlow/FR/03-operators/#utilisation-de-base","title":"Utilisation de base","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('python_operator', start_date=datetime(2024, 1, 1))\n\ndef my_function():\n    print(\"Hello from Python!\")\n    return \"Success\"\n\ntask = PythonOperator(\n    task_id='python_task',\n    python_callable=my_function,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/03-operators/#passer-des-arguments","title":"Passer des arguments","text":"<pre><code>def process_data(file_path, output_path):\n    print(f\"Processing {file_path} -&gt; {output_path}\")\n    # Traitement...\n\ntask = PythonOperator(\n    task_id='process',\n    python_callable=process_data,\n    op_args=['/path/to/input.csv', '/path/to/output.csv'],\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/03-operators/#utiliser-xcom-pour-partager-des-donnees","title":"Utiliser XCom pour partager des donn\u00e9es","text":"<pre><code>def extract_data(**context):\n    data = {'key': 'value'}\n    return data  # Retourne automatiquement via XCom\n\ndef process_data(**context):\n    # R\u00e9cup\u00e9rer les donn\u00e9es de la t\u00e2che pr\u00e9c\u00e9dente\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    print(f\"Received: {data}\")\n\nextract = PythonOperator(\n    task_id='extract',\n    python_callable=extract_data,\n    dag=dag,\n)\n\nprocess = PythonOperator(\n    task_id='process',\n    python_callable=process_data,\n    dag=dag,\n)\n\nextract &gt;&gt; process\n</code></pre>"},{"location":"AirFlow/FR/03-operators/#bashoperator","title":"BashOperator","text":""},{"location":"AirFlow/FR/03-operators/#utilisation-de-base_1","title":"Utilisation de base","text":"<pre><code>from airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='bash_task',\n    bash_command='echo \"Hello from Bash\"',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/03-operators/#utiliser-des-templates","title":"Utiliser des templates","text":"<pre><code>task = BashOperator(\n    task_id='bash_template',\n    bash_command='echo \"Date: {{ ds }}\"',  # ds = execution date\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/03-operators/#variables-de-template-disponibles","title":"Variables de template disponibles","text":"<ul> <li><code>{{ ds }}</code> : Date d'ex\u00e9cution (YYYY-MM-DD)</li> <li><code>{{ ds_nodash }}</code> : Date sans tirets (YYYYMMDD)</li> <li><code>{{ ts }}</code> : Timestamp d'ex\u00e9cution</li> <li><code>{{ dag }}</code> : Objet DAG</li> <li><code>{{ task }}</code> : Objet Task</li> </ul>"},{"location":"AirFlow/FR/03-operators/#sql-operators","title":"SQL Operators","text":""},{"location":"AirFlow/FR/03-operators/#sqlexecutequeryoperator","title":"SQLExecuteQueryOperator","text":"<pre><code>from airflow.providers.postgres.operators.postgres import PostgresOperator\n\ntask = PostgresOperator(\n    task_id='sql_task',\n    postgres_conn_id='my_postgres',\n    sql='SELECT * FROM users LIMIT 10;',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/03-operators/#utiliser-des-templates-sql","title":"Utiliser des templates SQL","text":"<pre><code>task = PostgresOperator(\n    task_id='sql_template',\n    postgres_conn_id='my_postgres',\n    sql='''\n        SELECT * FROM users\n        WHERE created_at &gt;= '{{ ds }}'\n    ''',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/03-operators/#operateurs-personnalises","title":"Op\u00e9rateurs personnalis\u00e9s","text":""},{"location":"AirFlow/FR/03-operators/#creer-un-operateur-personnalise","title":"Cr\u00e9er un op\u00e9rateur personnalis\u00e9","text":"<pre><code>from airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass MyCustomOperator(BaseOperator):\n    @apply_defaults\n    def __init__(self, my_param, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.my_param = my_param\n\n    def execute(self, context):\n        print(f\"Executing with param: {self.my_param}\")\n        # Votre logique ici\n        return \"Success\"\n\n# Utilisation\ntask = MyCustomOperator(\n    task_id='custom_task',\n    my_param='value',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/03-operators/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>PythonOperator pour logique Python</li> <li>BashOperator pour commandes shell</li> <li>SQL Operators pour requ\u00eates SQL</li> <li>XCom pour partager des donn\u00e9es</li> <li>Templates pour valeurs dynamiques</li> </ol>"},{"location":"AirFlow/FR/03-operators/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 4. Sensors pour apprendre \u00e0 utiliser les sensors.</p>"},{"location":"AirFlow/FR/04-sensors/","title":"4. Sensors Airflow","text":""},{"location":"AirFlow/FR/04-sensors/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les sensors</li> <li>Utiliser FileSensor, SqlSensor, HttpSensor</li> <li>Cr\u00e9er des sensors personnalis\u00e9s</li> <li>G\u00e9rer les timeouts et retries</li> </ul>"},{"location":"AirFlow/FR/04-sensors/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction aux sensors</li> <li>FileSensor</li> <li>SqlSensor</li> <li>HttpSensor</li> <li>Sensors personnalis\u00e9s</li> </ol>"},{"location":"AirFlow/FR/04-sensors/#introduction-aux-sensors","title":"Introduction aux sensors","text":""},{"location":"AirFlow/FR/04-sensors/#quest-ce-quun-sensor","title":"Qu'est-ce qu'un Sensor ?","text":"<p>Sensor = T\u00e2che qui attend une condition</p> <ul> <li>Polling : V\u00e9rifie p\u00e9riodiquement une condition</li> <li>Timeout : Arr\u00eate apr\u00e8s un certain temps</li> <li>Poke interval : Intervalle entre les v\u00e9rifications</li> <li>Mode : Resume ou poke</li> </ul>"},{"location":"AirFlow/FR/04-sensors/#types-de-sensors","title":"Types de sensors","text":"<ul> <li>FileSensor : Attend un fichier</li> <li>SqlSensor : Attend une condition SQL</li> <li>HttpSensor : Attend une r\u00e9ponse HTTP</li> <li>S3KeySensor : Attend une cl\u00e9 S3</li> </ul>"},{"location":"AirFlow/FR/04-sensors/#filesensor","title":"FileSensor","text":""},{"location":"AirFlow/FR/04-sensors/#utilisation-de-base","title":"Utilisation de base","text":"<pre><code>from airflow.sensors.filesystem import FileSensor\n\ntask = FileSensor(\n    task_id='wait_for_file',\n    filepath='/path/to/file.csv',\n    poke_interval=30,  # V\u00e9rifier toutes les 30 secondes\n    timeout=3600,  # Timeout apr\u00e8s 1 heure\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/04-sensors/#filesensor-avec-wildcards","title":"FileSensor avec wildcards","text":"<pre><code>task = FileSensor(\n    task_id='wait_for_files',\n    filepath='/path/to/data/*.csv',\n    poke_interval=60,\n    timeout=7200,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/04-sensors/#sqlsensor","title":"SqlSensor","text":""},{"location":"AirFlow/FR/04-sensors/#utilisation-de-base_1","title":"Utilisation de base","text":"<pre><code>from airflow.sensors.sql import SqlSensor\n\ntask = SqlSensor(\n    task_id='wait_for_data',\n    conn_id='my_postgres',\n    sql=\"SELECT COUNT(*) FROM users WHERE status = 'active'\",\n    poke_interval=60,\n    timeout=3600,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/04-sensors/#sqlsensor-avec-condition","title":"SqlSensor avec condition","text":"<pre><code>task = SqlSensor(\n    task_id='wait_for_count',\n    conn_id='my_postgres',\n    sql=\"SELECT COUNT(*) as count FROM orders WHERE date = '{{ ds }}'\",\n    poke_interval=30,\n    timeout=1800,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/04-sensors/#httpsensor","title":"HttpSensor","text":""},{"location":"AirFlow/FR/04-sensors/#utilisation-de-base_2","title":"Utilisation de base","text":"<pre><code>from airflow.sensors.http import HttpSensor\n\ntask = HttpSensor(\n    task_id='wait_for_api',\n    http_conn_id='my_http',\n    endpoint='/api/status',\n    method='GET',\n    poke_interval=30,\n    timeout=3600,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/04-sensors/#httpsensor-avec-reponse","title":"HttpSensor avec r\u00e9ponse","text":"<pre><code>def check_response(response):\n    return response.status_code == 200\n\ntask = HttpSensor(\n    task_id='wait_for_api_ready',\n    http_conn_id='my_http',\n    endpoint='/api/health',\n    response_check=check_response,\n    poke_interval=60,\n    timeout=7200,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/04-sensors/#sensors-personnalises","title":"Sensors personnalis\u00e9s","text":""},{"location":"AirFlow/FR/04-sensors/#creer-un-sensor-personnalise","title":"Cr\u00e9er un sensor personnalis\u00e9","text":"<pre><code>from airflow.sensors.base import BaseSensorOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass MyCustomSensor(BaseSensorOperator):\n    @apply_defaults\n    def __init__(self, my_param, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.my_param = my_param\n\n    def poke(self, context):\n        # V\u00e9rifier la condition\n        # Retourner True si condition remplie\n        # Retourner False pour continuer \u00e0 attendre\n        return self.check_condition()\n\n    def check_condition(self):\n        # Votre logique de v\u00e9rification\n        return True  # ou False\n\n# Utilisation\ntask = MyCustomSensor(\n    task_id='custom_sensor',\n    my_param='value',\n    poke_interval=30,\n    timeout=3600,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/04-sensors/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Sensors attendent des conditions</li> <li>Polling v\u00e9rifie p\u00e9riodiquement</li> <li>Timeout limite le temps d'attente</li> <li>Poke interval d\u00e9finit la fr\u00e9quence</li> <li>Mode contr\u00f4le le comportement</li> </ol>"},{"location":"AirFlow/FR/04-sensors/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 5. Hooks pour apprendre \u00e0 utiliser les hooks.</p>"},{"location":"AirFlow/FR/05-hooks/","title":"5. Hooks Airflow","text":""},{"location":"AirFlow/FR/05-hooks/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les hooks</li> <li>Utiliser les hooks de base de donn\u00e9es</li> <li>Utiliser les hooks cloud</li> <li>Cr\u00e9er des hooks personnalis\u00e9s</li> </ul>"},{"location":"AirFlow/FR/05-hooks/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction aux hooks</li> <li>Hooks de base de donn\u00e9es</li> <li>Hooks cloud</li> <li>Hooks HTTP</li> <li>Hooks personnalis\u00e9s</li> </ol>"},{"location":"AirFlow/FR/05-hooks/#introduction-aux-hooks","title":"Introduction aux hooks","text":""},{"location":"AirFlow/FR/05-hooks/#quest-ce-quun-hook","title":"Qu'est-ce qu'un Hook ?","text":"<p>Hook = Interface pour interagir avec des syst\u00e8mes externes</p> <ul> <li>R\u00e9utilisable : Peut \u00eatre utilis\u00e9 dans plusieurs t\u00e2ches</li> <li>G\u00e8re les connexions : Utilise les connexions Airflow</li> <li>Abstraction : Cache les d\u00e9tails d'impl\u00e9mentation</li> </ul>"},{"location":"AirFlow/FR/05-hooks/#types-de-hooks","title":"Types de hooks","text":"<ul> <li>Database hooks : PostgreSQL, MySQL, SQLite</li> <li>Cloud hooks : AWS, Azure, GCP</li> <li>HTTP hooks : Requ\u00eates HTTP</li> <li>File hooks : Op\u00e9rations sur fichiers</li> </ul>"},{"location":"AirFlow/FR/05-hooks/#hooks-de-base-de-donnees","title":"Hooks de base de donn\u00e9es","text":""},{"location":"AirFlow/FR/05-hooks/#postgreshook","title":"PostgresHook","text":"<pre><code>from airflow.hooks.postgres import PostgresHook\n\ndef query_database():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n\n    # Ex\u00e9cuter une requ\u00eate\n    records = hook.get_records(\"SELECT * FROM users LIMIT 10\")\n\n    # Ex\u00e9cuter une commande\n    hook.run(\"INSERT INTO logs VALUES ('test')\")\n\n    # Obtenir pandas DataFrame\n    df = hook.get_pandas_df(\"SELECT * FROM users\")\n\ntask = PythonOperator(\n    task_id='query_db',\n    python_callable=query_database,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/05-hooks/#mysqlhook","title":"MySqlHook","text":"<pre><code>from airflow.providers.mysql.hooks.mysql import MySqlHook\n\ndef query_mysql():\n    hook = MySqlHook(mysql_conn_id='my_mysql')\n    records = hook.get_records(\"SELECT * FROM orders\")\n\ntask = PythonOperator(\n    task_id='query_mysql',\n    python_callable=query_mysql,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/05-hooks/#hooks-cloud","title":"Hooks cloud","text":""},{"location":"AirFlow/FR/05-hooks/#s3hook-aws","title":"S3Hook (AWS)","text":"<pre><code>from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\ndef upload_to_s3():\n    hook = S3Hook(aws_conn_id='my_aws')\n\n    # Uploader un fichier\n    hook.load_file(\n        filename='/local/path/file.csv',\n        key='s3/path/file.csv',\n        bucket_name='my-bucket',\n    )\n\n    # T\u00e9l\u00e9charger un fichier\n    hook.download_file(\n        key='s3/path/file.csv',\n        bucket_name='my-bucket',\n        local_path='/local/path/file.csv',\n    )\n\ntask = PythonOperator(\n    task_id='s3_upload',\n    python_callable=upload_to_s3,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/05-hooks/#azure-blob-storage-hook","title":"Azure Blob Storage Hook","text":"<pre><code>from airflow.providers.microsoft.azure.hooks.wasb import WasbHook\n\ndef upload_to_azure():\n    hook = WasbHook(wasb_conn_id='my_azure')\n\n    # Uploader un fichier\n    hook.upload(\n        container_name='my-container',\n        blob_name='file.csv',\n        file_path='/local/path/file.csv',\n    )\n\ntask = PythonOperator(\n    task_id='azure_upload',\n    python_callable=upload_to_azure,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/05-hooks/#hooks-http","title":"Hooks HTTP","text":""},{"location":"AirFlow/FR/05-hooks/#httphook","title":"HttpHook","text":"<pre><code>from airflow.providers.http.hooks.http import HttpHook\n\ndef call_api():\n    hook = HttpHook(http_conn_id='my_api', method='GET')\n\n    # Faire une requ\u00eate GET\n    response = hook.run(endpoint='/api/data')\n    print(response.json())\n\n    # Faire une requ\u00eate POST\n    hook = HttpHook(http_conn_id='my_api', method='POST')\n    response = hook.run(\n        endpoint='/api/data',\n        data={'key': 'value'},\n    )\n\ntask = PythonOperator(\n    task_id='call_api',\n    python_callable=call_api,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/05-hooks/#hooks-personnalises","title":"Hooks personnalis\u00e9s","text":""},{"location":"AirFlow/FR/05-hooks/#creer-un-hook-personnalise","title":"Cr\u00e9er un hook personnalis\u00e9","text":"<pre><code>from airflow.hooks.base import BaseHook\n\nclass MyCustomHook(BaseHook):\n    def __init__(self, my_conn_id):\n        super().__init__()\n        self.conn_id = my_conn_id\n        self.conn = self.get_connection(my_conn_id)\n\n    def do_something(self):\n        # Votre logique\n        print(f\"Connecting to {self.conn.host}\")\n        return \"Success\"\n\n# Utilisation\ndef use_custom_hook():\n    hook = MyCustomHook(my_conn_id='my_connection')\n    result = hook.do_something()\n\ntask = PythonOperator(\n    task_id='use_hook',\n    python_callable=use_custom_hook,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/05-hooks/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Hooks abstraient les connexions</li> <li>R\u00e9utilisables dans plusieurs t\u00e2ches</li> <li>G\u00e8rent les connexions via Airflow</li> <li>Supportent bases de donn\u00e9es, cloud, HTTP</li> <li>Extensibles avec hooks personnalis\u00e9s</li> </ol>"},{"location":"AirFlow/FR/05-hooks/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 6. Variables et Connexions pour apprendre \u00e0 g\u00e9rer la configuration.</p>"},{"location":"AirFlow/FR/06-variables-connections/","title":"6. Variables et Connexions","text":""},{"location":"AirFlow/FR/06-variables-connections/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>G\u00e9rer les variables Airflow</li> <li>Configurer les connexions</li> <li>S\u00e9curiser les credentials</li> <li>Utiliser des variables dynamiques</li> </ul>"},{"location":"AirFlow/FR/06-variables-connections/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Variables</li> <li>Connexions</li> <li>S\u00e9curit\u00e9</li> <li>Bonnes pratiques</li> </ol>"},{"location":"AirFlow/FR/06-variables-connections/#variables","title":"Variables","text":""},{"location":"AirFlow/FR/06-variables-connections/#creer-des-variables","title":"Cr\u00e9er des variables","text":"<p>Via CLI :</p> <pre><code># Cr\u00e9er une variable\nairflow variables set my_key \"my_value\"\n\n# Cr\u00e9er une variable JSON\nairflow variables set my_config '{\"key\": \"value\"}'\n\n# Supprimer une variable\nairflow variables delete my_key\n\n# Lister les variables\nairflow variables list\n</code></pre> <p>Via interface web : 1. Admin \u2192 Variables 2. Cliquer sur \"+\" 3. Entrer Key et Value 4. Sauvegarder</p>"},{"location":"AirFlow/FR/06-variables-connections/#utiliser-des-variables","title":"Utiliser des variables","text":"<pre><code>from airflow.models import Variable\n\n# R\u00e9cup\u00e9rer une variable\nmy_value = Variable.get(\"my_key\")\n\n# Avec valeur par d\u00e9faut\nmy_value = Variable.get(\"my_key\", default_var=\"default\")\n\n# Variable JSON\nconfig = Variable.get(\"my_config\", deserialize_json=True)\nprint(config['key'])\n</code></pre>"},{"location":"AirFlow/FR/06-variables-connections/#variables-dans-les-templates","title":"Variables dans les templates","text":"<pre><code>from airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='use_var',\n    bash_command='echo \"Value: {{ var.value.my_key }}\"',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/06-variables-connections/#connexions","title":"Connexions","text":""},{"location":"AirFlow/FR/06-variables-connections/#creer-une-connexion","title":"Cr\u00e9er une connexion","text":"<p>Via CLI :</p> <pre><code># PostgreSQL\nairflow connections add 'my_postgres' \\\n    --conn-type 'postgres' \\\n    --conn-host 'localhost' \\\n    --conn-login 'user' \\\n    --conn-password 'password' \\\n    --conn-port 5432 \\\n    --conn-schema 'mydb'\n\n# MySQL\nairflow connections add 'my_mysql' \\\n    --conn-type 'mysql' \\\n    --conn-host 'localhost' \\\n    --conn-login 'user' \\\n    --conn-password 'password' \\\n    --conn-port 3306\n\n# HTTP\nairflow connections add 'my_api' \\\n    --conn-type 'http' \\\n    --conn-host 'https://api.example.com'\n</code></pre> <p>Via interface web : 1. Admin \u2192 Connections 2. Cliquer sur \"+\" 3. Remplir les champs 4. Sauvegarder</p>"},{"location":"AirFlow/FR/06-variables-connections/#utiliser-une-connexion","title":"Utiliser une connexion","text":"<pre><code>from airflow.hooks.base import BaseHook\n\n# R\u00e9cup\u00e9rer une connexion\nconn = BaseHook.get_connection('my_postgres')\nprint(f\"Host: {conn.host}\")\nprint(f\"Login: {conn.login}\")\nprint(f\"Password: {conn.password}\")\n</code></pre>"},{"location":"AirFlow/FR/06-variables-connections/#securite","title":"S\u00e9curit\u00e9","text":""},{"location":"AirFlow/FR/06-variables-connections/#masquer-les-mots-de-passe","title":"Masquer les mots de passe","text":"<p>Utiliser des connexions : - Les mots de passe sont chiffr\u00e9s dans la base - Ne jamais hardcoder les credentials</p> <p>Utiliser des variables : - Pour les secrets sensibles - Utiliser des outils de gestion de secrets (Vault, etc.)</p>"},{"location":"AirFlow/FR/06-variables-connections/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ol> <li>Ne jamais commiter les credentials</li> <li>Utiliser des connexions pour les acc\u00e8s</li> <li>Utiliser des variables pour la configuration</li> <li>Chiffrer les donn\u00e9es sensibles</li> <li>Limiter les acc\u00e8s aux connexions</li> </ol>"},{"location":"AirFlow/FR/06-variables-connections/#bonnes-pratiques_1","title":"Bonnes pratiques","text":""},{"location":"AirFlow/FR/06-variables-connections/#organisation-des-variables","title":"Organisation des variables","text":"<ul> <li>Pr\u00e9fixes : <code>project_name_key</code></li> <li>Groupes : <code>db_</code>, <code>api_</code>, <code>s3_</code></li> <li>Documentation : Documenter l'usage</li> </ul>"},{"location":"AirFlow/FR/06-variables-connections/#organisation-des-connexions","title":"Organisation des connexions","text":"<ul> <li>Noms clairs : <code>postgres_prod</code>, <code>postgres_dev</code></li> <li>Types corrects : Utiliser le bon type de connexion</li> <li>Tests : Tester les connexions r\u00e9guli\u00e8rement</li> </ul>"},{"location":"AirFlow/FR/06-variables-connections/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Variables pour la configuration</li> <li>Connexions pour les acc\u00e8s</li> <li>S\u00e9curit\u00e9 : Ne jamais hardcoder</li> <li>Organisation : Pr\u00e9fixes et groupes</li> <li>Documentation : Documenter l'usage</li> </ol>"},{"location":"AirFlow/FR/06-variables-connections/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 7. Bonnes pratiques pour apprendre les meilleures pratiques.</p>"},{"location":"AirFlow/FR/07-best-practices/","title":"7. Bonnes pratiques Airflow","text":""},{"location":"AirFlow/FR/07-best-practices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Structurer les DAGs efficacement</li> <li>G\u00e9rer les erreurs</li> <li>Optimiser les performances</li> <li>Tester et d\u00e9boguer</li> </ul>"},{"location":"AirFlow/FR/07-best-practices/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Structure des DAGs</li> <li>Gestion des erreurs</li> <li>Performance</li> <li>Tests</li> <li>D\u00e9bogage</li> </ol>"},{"location":"AirFlow/FR/07-best-practices/#structure-des-dags","title":"Structure des DAGs","text":""},{"location":"AirFlow/FR/07-best-practices/#organisation-des-fichiers","title":"Organisation des fichiers","text":"<pre><code>airflow/\n\u251c\u2500\u2500 dags/\n\u2502   \u251c\u2500\u2500 etl/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 extract.py\n\u2502   \u2502   \u251c\u2500\u2500 transform.py\n\u2502   \u2502   \u2514\u2500\u2500 load.py\n\u2502   \u2514\u2500\u2500 analytics/\n\u2502       \u2514\u2500\u2500 reports.py\n\u251c\u2500\u2500 plugins/\n\u2502   \u2514\u2500\u2500 custom_operators.py\n\u2514\u2500\u2500 config/\n    \u2514\u2500\u2500 settings.py\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#bonnes-pratiques-de-code","title":"Bonnes pratiques de code","text":"<p>1. Imports organis\u00e9s :</p> <pre><code># Standard library\nfrom datetime import datetime, timedelta\n\n# Third-party\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\n# Local\nfrom utils.helpers import process_data\n</code></pre> <p>2. Fonctions r\u00e9utilisables :</p> <pre><code># utils/helpers.py\ndef extract_data(source):\n    # Logique d'extraction\n    return data\n\ndef transform_data(data):\n    # Logique de transformation\n    return transformed_data\n\n# dags/etl_pipeline.py\nfrom utils.helpers import extract_data, transform_data\n\nextract_task = PythonOperator(\n    task_id='extract',\n    python_callable=extract_data,\n    op_args=['source'],\n    dag=dag,\n)\n</code></pre> <p>3. Configuration centralis\u00e9e :</p> <pre><code># config/settings.py\nDEFAULT_ARGS = {\n    'owner': 'data_team',\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# dags/my_dag.py\nfrom config.settings import DEFAULT_ARGS\n\ndag = DAG('my_dag', default_args=DEFAULT_ARGS)\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#gestion-des-erreurs","title":"Gestion des erreurs","text":""},{"location":"AirFlow/FR/07-best-practices/#retry-et-backoff","title":"Retry et backoff","text":"<pre><code>default_args = {\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'retry_exponential_backoff': True,\n    'max_retry_delay': timedelta(hours=1),\n}\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#gestion-dexceptions","title":"Gestion d'exceptions","text":"<pre><code>def process_with_error_handling():\n    try:\n        # Code qui peut \u00e9chouer\n        result = risky_operation()\n        return result\n    except SpecificError as e:\n        # G\u00e9rer l'erreur sp\u00e9cifique\n        logger.error(f\"Error: {e}\")\n        raise\n    except Exception as e:\n        # G\u00e9rer les autres erreurs\n        logger.error(f\"Unexpected error: {e}\")\n        raise\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#callbacks","title":"Callbacks","text":"<pre><code>def on_failure_callback(context):\n    logger.error(\"Task failed!\")\n    # Envoyer une alerte\n    send_alert(context)\n\ndef on_success_callback(context):\n    logger.info(\"Task succeeded!\")\n\ntask = PythonOperator(\n    task_id='task',\n    python_callable=my_function,\n    on_failure_callback=on_failure_callback,\n    on_success_callback=on_success_callback,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#performance","title":"Performance","text":""},{"location":"AirFlow/FR/07-best-practices/#optimiser-les-dags","title":"Optimiser les DAGs","text":"<p>1. \u00c9viter les d\u00e9pendances inutiles :</p> <pre><code># Mauvais : d\u00e9pendances s\u00e9quentielles inutiles\ntask1 &gt;&gt; task2 &gt;&gt; task3 &gt;&gt; task4\n\n# Bon : parall\u00e9lisme quand possible\n[task1, task2] &gt;&gt; task3 &gt;&gt; task4\n</code></pre> <p>2. Utiliser le mode reschedule pour les sensors :</p> <pre><code>sensor = FileSensor(\n    task_id='wait_file',\n    filepath='/path/to/file',\n    mode='reschedule',  # Lib\u00e8re le slot worker\n    poke_interval=60,\n    dag=dag,\n)\n</code></pre> <p>3. Limiter le parall\u00e9lisme :</p> <pre><code>dag = DAG(\n    'my_dag',\n    max_active_runs=1,  # Un seul run \u00e0 la fois\n    max_active_tasks=10,  # Maximum 10 t\u00e2ches en parall\u00e8le\n)\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#tests","title":"Tests","text":""},{"location":"AirFlow/FR/07-best-practices/#tests-unitaires","title":"Tests unitaires","text":"<pre><code># tests/test_dag.py\nimport pytest\nfrom airflow.models import DagBag\n\ndef test_dag_loaded():\n    dagbag = DagBag()\n    dag = dagbag.get_dag(dag_id='my_dag')\n    assert dag is not None\n    assert len(dag.tasks) == 3\n\ndef test_dag_structure():\n    dagbag = DagBag()\n    dag = dagbag.get_dag(dag_id='my_dag')\n    assert 'extract' in dag.task_ids\n    assert 'transform' in dag.task_ids\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#tests-dintegration","title":"Tests d'int\u00e9gration","text":"<pre><code>from airflow.operators.python import PythonOperator\n\ndef test_task_execution():\n    task = PythonOperator(\n        task_id='test_task',\n        python_callable=lambda: \"success\",\n    )\n    result = task.execute({})\n    assert result == \"success\"\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#debogage","title":"D\u00e9bogage","text":""},{"location":"AirFlow/FR/07-best-practices/#logs","title":"Logs","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef my_function():\n    logger.info(\"Starting process\")\n    logger.debug(\"Debug information\")\n    logger.error(\"Error occurred\")\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#verifier-les-logs","title":"V\u00e9rifier les logs","text":"<pre><code># Logs d'une t\u00e2che\nairflow tasks logs my_dag my_task 2024-01-01\n\n# Logs du scheduler\ntail -f ~/airflow/logs/scheduler/*.log\n</code></pre>"},{"location":"AirFlow/FR/07-best-practices/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Structure : Organiser le code proprement</li> <li>Erreurs : G\u00e9rer avec retry et callbacks</li> <li>Performance : Optimiser le parall\u00e9lisme</li> <li>Tests : Tester les DAGs</li> <li>Logs : Utiliser le logging efficacement</li> </ol>"},{"location":"AirFlow/FR/07-best-practices/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 8. Projets pratiques pour cr\u00e9er des projets complets.</p>"},{"location":"AirFlow/FR/08-projets/","title":"8. Projets pratiques Airflow","text":""},{"location":"AirFlow/FR/08-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er un pipeline ETL complet</li> <li>Orchestrer des workflows complexes</li> <li>Int\u00e9grer avec bases de donn\u00e9es</li> <li>Cr\u00e9er des projets pour portfolio</li> </ul>"},{"location":"AirFlow/FR/08-projets/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Projet 1 : Pipeline ETL simple</li> <li>Projet 2 : Orchestration de workflows</li> <li>Projet 3 : Int\u00e9gration avec bases de donn\u00e9es</li> <li>Projet 4 : Pipeline complet</li> </ol>"},{"location":"AirFlow/FR/08-projets/#projet-1-pipeline-etl-simple","title":"Projet 1 : Pipeline ETL simple","text":""},{"location":"AirFlow/FR/08-projets/#objectif","title":"Objectif","text":"<p>Cr\u00e9er un pipeline ETL qui extrait, transforme et charge des donn\u00e9es.</p>"},{"location":"AirFlow/FR/08-projets/#architecture","title":"Architecture","text":"<pre><code>Extract (API) \u2192 Transform (Python) \u2192 Load (Fichier)\n</code></pre>"},{"location":"AirFlow/FR/08-projets/#code","title":"Code","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\nimport requests\nimport pandas as pd\n\ndefault_args = {\n    'owner': 'data_analyst',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'etl_pipeline',\n    default_args=default_args,\n    description='Pipeline ETL simple',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\ndef extract_data(**context):\n    # Extraire des donn\u00e9es depuis une API\n    response = requests.get('https://api.example.com/data')\n    data = response.json()\n\n    # Sauvegarder temporairement\n    df = pd.DataFrame(data)\n    df.to_csv('/tmp/raw_data.csv', index=False)\n\n    return '/tmp/raw_data.csv'\n\ndef transform_data(**context):\n    # R\u00e9cup\u00e9rer le fichier de la t\u00e2che pr\u00e9c\u00e9dente\n    ti = context['ti']\n    input_file = ti.xcom_pull(task_ids='extract')\n\n    # Lire et transformer\n    df = pd.read_csv(input_file)\n    df['processed_at'] = datetime.now()\n    df = df.dropna()\n\n    # Sauvegarder\n    output_file = '/tmp/transformed_data.csv'\n    df.to_csv(output_file, index=False)\n\n    return output_file\n\ndef load_data(**context):\n    # R\u00e9cup\u00e9rer le fichier transform\u00e9\n    ti = context['ti']\n    input_file = ti.xcom_pull(task_ids='transform')\n\n    # Charger dans destination (ex: base de donn\u00e9es)\n    df = pd.read_csv(input_file)\n    print(f\"Loaded {len(df)} rows\")\n\n    # Ici, vous pourriez charger dans une base de donn\u00e9es\n    # df.to_sql('table', con=engine, if_exists='append')\n\nextract = PythonOperator(\n    task_id='extract',\n    python_callable=extract_data,\n    dag=dag,\n)\n\ntransform = PythonOperator(\n    task_id='transform',\n    python_callable=transform_data,\n    dag=dag,\n)\n\nload = PythonOperator(\n    task_id='load',\n    python_callable=load_data,\n    dag=dag,\n)\n\nextract &gt;&gt; transform &gt;&gt; load\n</code></pre>"},{"location":"AirFlow/FR/08-projets/#projet-2-orchestration-de-workflows","title":"Projet 2 : Orchestration de workflows","text":""},{"location":"AirFlow/FR/08-projets/#objectif_1","title":"Objectif","text":"<p>Orchestrer plusieurs workflows avec d\u00e9pendances complexes.</p>"},{"location":"AirFlow/FR/08-projets/#architecture_1","title":"Architecture","text":"<pre><code>Data Collection \u2192 Validation \u2192 Processing \u2192 Reporting\n                      \u2193\n                  Alerting\n</code></pre>"},{"location":"AirFlow/FR/08-projets/#code_1","title":"Code","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.branch import BranchPythonOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'workflow_orchestration',\n    description='Orchestration de workflows',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\ndef collect_data():\n    print(\"Collecting data...\")\n    return \"success\"\n\ndef validate_data(**context):\n    data = context['ti'].xcom_pull(task_ids='collect')\n    if data == \"success\":\n        return \"valid\"\n    return \"invalid\"\n\ndef process_valid():\n    print(\"Processing valid data...\")\n\ndef process_invalid():\n    print(\"Alerting: Invalid data!\")\n\ndef generate_report():\n    print(\"Generating report...\")\n\ncollect = PythonOperator(\n    task_id='collect',\n    python_callable=collect_data,\n    dag=dag,\n)\n\nvalidate = BranchPythonOperator(\n    task_id='validate',\n    python_callable=validate_data,\n    dag=dag,\n)\n\nprocess_valid_task = PythonOperator(\n    task_id='process_valid',\n    python_callable=process_valid,\n    dag=dag,\n)\n\nprocess_invalid_task = PythonOperator(\n    task_id='process_invalid',\n    python_callable=process_invalid,\n    dag=dag,\n)\n\nreport = PythonOperator(\n    task_id='report',\n    python_callable=generate_report,\n    dag=dag,\n)\n\ncollect &gt;&gt; validate &gt;&gt; [process_valid_task, process_invalid_task] &gt;&gt; report\n</code></pre>"},{"location":"AirFlow/FR/08-projets/#projet-3-integration-avec-bases-de-donnees","title":"Projet 3 : Int\u00e9gration avec bases de donn\u00e9es","text":""},{"location":"AirFlow/FR/08-projets/#objectif_2","title":"Objectif","text":"<p>Int\u00e9grer Airflow avec PostgreSQL pour un pipeline de donn\u00e9es.</p>"},{"location":"AirFlow/FR/08-projets/#code_2","title":"Code","text":"<pre><code>from airflow import DAG\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'database_pipeline',\n    description='Pipeline avec base de donn\u00e9es',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\n# Cr\u00e9er une table\ncreate_table = PostgresOperator(\n    task_id='create_table',\n    postgres_conn_id='my_postgres',\n    sql='''\n        CREATE TABLE IF NOT EXISTS users_processed (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100),\n            email VARCHAR(100),\n            created_at TIMESTAMP\n        );\n    ''',\n    dag=dag,\n)\n\n# Ins\u00e9rer des donn\u00e9es\ndef insert_data():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    hook.run(\"\"\"\n        INSERT INTO users_processed (name, email, created_at)\n        VALUES ('John Doe', 'john@example.com', NOW());\n    \"\"\")\n\ninsert = PythonOperator(\n    task_id='insert',\n    python_callable=insert_data,\n    dag=dag,\n)\n\n# Requ\u00eate analytique\nanalytics_query = PostgresOperator(\n    task_id='analytics',\n    postgres_conn_id='my_postgres',\n    sql='''\n        SELECT \n            DATE(created_at) as date,\n            COUNT(*) as user_count\n        FROM users_processed\n        GROUP BY DATE(created_at);\n    ''',\n    dag=dag,\n)\n\ncreate_table &gt;&gt; insert &gt;&gt; analytics_query\n</code></pre>"},{"location":"AirFlow/FR/08-projets/#projet-4-pipeline-complet","title":"Projet 4 : Pipeline complet","text":""},{"location":"AirFlow/FR/08-projets/#objectif_3","title":"Objectif","text":"<p>Cr\u00e9er un pipeline ETL complet avec plusieurs sources et destinations.</p>"},{"location":"AirFlow/FR/08-projets/#architecture_2","title":"Architecture","text":"<pre><code>API \u2192 Validation \u2192 Transformation \u2192 Database \u2192 Reporting\n  \u2193\nFile \u2192 Validation \u2192 Transformation \u2192 Database\n</code></pre>"},{"location":"AirFlow/FR/08-projets/#code_3","title":"Code","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport requests\n\ndag = DAG(\n    'complete_pipeline',\n    description='Pipeline ETL complet',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    default_args={\n        'retries': 2,\n        'retry_delay': timedelta(minutes=5),\n    },\n)\n\n# Attendre un fichier\nwait_file = FileSensor(\n    task_id='wait_file',\n    filepath='/data/input/file.csv',\n    poke_interval=60,\n    timeout=3600,\n    dag=dag,\n)\n\n# Extraire depuis API\ndef extract_api():\n    response = requests.get('https://api.example.com/data')\n    data = response.json()\n    df = pd.DataFrame(data)\n    df.to_csv('/tmp/api_data.csv', index=False)\n    return '/tmp/api_data.csv'\n\nextract_api_task = PythonOperator(\n    task_id='extract_api',\n    python_callable=extract_api,\n    dag=dag,\n)\n\n# Transformer\ndef transform(**context):\n    # Fichier API\n    api_file = context['ti'].xcom_pull(task_ids='extract_api')\n    df_api = pd.read_csv(api_file)\n\n    # Fichier local\n    df_file = pd.read_csv('/data/input/file.csv')\n\n    # Combiner et transformer\n    df_combined = pd.concat([df_api, df_file])\n    df_combined = df_combined.dropna()\n    df_combined['processed_at'] = datetime.now()\n\n    # Sauvegarder\n    df_combined.to_csv('/tmp/transformed.csv', index=False)\n    return '/tmp/transformed.csv'\n\ntransform_task = PythonOperator(\n    task_id='transform',\n    python_callable=transform,\n    dag=dag,\n)\n\n# Charger dans base de donn\u00e9es\nload_db = PostgresOperator(\n    task_id='load_db',\n    postgres_conn_id='my_postgres',\n    sql='''\n        COPY users FROM '/tmp/transformed.csv' \n        WITH (FORMAT csv, HEADER true);\n    ''',\n    dag=dag,\n)\n\n# G\u00e9n\u00e9rer un rapport\ndef generate_report():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    df = hook.get_pandas_df(\"SELECT * FROM users\")\n    report = df.describe()\n    report.to_csv('/tmp/report.csv')\n    print(\"Report generated!\")\n\nreport_task = PythonOperator(\n    task_id='report',\n    python_callable=generate_report,\n    dag=dag,\n)\n\n# D\u00e9pendances\n[wait_file, extract_api_task] &gt;&gt; transform_task &gt;&gt; load_db &gt;&gt; report_task\n</code></pre>"},{"location":"AirFlow/FR/08-projets/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>ETL : Extract, Transform, Load</li> <li>Orchestration : Coordonner plusieurs workflows</li> <li>Int\u00e9gration : Bases de donn\u00e9es, APIs, fichiers</li> <li>Monitoring : Surveiller les ex\u00e9cutions</li> <li>Portfolio : Cr\u00e9er des projets d\u00e9montrables</li> </ol>"},{"location":"AirFlow/FR/08-projets/#ressources","title":"\ud83d\udd17 Ressources","text":"<ul> <li>Exemples Airflow</li> <li>Documentation Airflow</li> </ul> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation Airflow. Vous pouvez maintenant cr\u00e9er des pipelines de donn\u00e9es complexes avec Airflow.</p>"},{"location":"AirFlow/PL/","title":"Szkolenie Apache Airflow dla Data Analyst","text":""},{"location":"AirFlow/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>Ten kurs prowadzi Ci\u0119 przez nauk\u0119 Apache Airflow jako Data Analyst. Airflow to platforma open-source do orkiestracji i automatyzacji z\u0142o\u017conych przep\u0142yw\u00f3w pracy danych.</p>"},{"location":"AirFlow/PL/#cele-edukacyjne","title":"\ud83c\udfaf Cele edukacyjne","text":"<ul> <li>Zrozumie\u0107 Apache Airflow i jego rol\u0119 w orkiestracji ETL</li> <li>Zainstalowa\u0107 i skonfigurowa\u0107 Airflow</li> <li>Tworzy\u0107 DAGi (Directed Acyclic Graphs)</li> <li>U\u017cywa\u0107 operator\u00f3w, sensor\u00f3w i hook\u00f3w</li> <li>Orkiestrowa\u0107 z\u0142o\u017cone pipeline'y danych</li> <li>Integrowa\u0107 z bazami danych i us\u0142ugami chmurowymi</li> <li>Tworzy\u0107 praktyczne projekty do portfolio</li> </ul>"},{"location":"AirFlow/PL/#wszystko-jest-bezpatne","title":"\ud83d\udcb0 Wszystko jest bezp\u0142atne!","text":"<p>Ten kurs wykorzystuje wy\u0142\u0105cznie: - \u2705 Apache Airflow : Open-source i bezp\u0142atne - \u2705 Python : Bezp\u0142atny j\u0119zyk programowania - \u2705 PostgreSQL/SQLite : Bezp\u0142atne bazy danych - \u2705 Oficjalna dokumentacja : Kompletne bezp\u0142atne przewodniki</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"AirFlow/PL/#struktura-kursu","title":"\ud83d\udcd6 Struktura kursu","text":""},{"location":"AirFlow/PL/#1-rozpoczecie-z-airflow","title":"1. Rozpocz\u0119cie z Airflow","text":"<ul> <li>Zainstalowa\u0107 Airflow</li> <li>Podstawowa konfiguracja</li> <li>Interfejs web Airflow</li> <li>Pierwsze DAGi</li> </ul>"},{"location":"AirFlow/PL/#2-podstawowe-koncepcje","title":"2. Podstawowe koncepcje","text":"<ul> <li>DAGi (Directed Acyclic Graphs)</li> <li>Zadania i zale\u017cno\u015bci</li> <li>Harmonogramowanie i wyzwalacze</li> <li>Zmienne i po\u0142\u0105czenia</li> </ul>"},{"location":"AirFlow/PL/#3-operatory","title":"3. Operatory","text":"<ul> <li>Operatory Python</li> <li>Operatory SQL</li> <li>Operatory Bash</li> <li>Operatory niestandardowe</li> </ul>"},{"location":"AirFlow/PL/#4-sensory","title":"4. Sensory","text":"<ul> <li>FileSensor</li> <li>SqlSensor</li> <li>HttpSensor</li> <li>Sensory niestandardowe</li> </ul>"},{"location":"AirFlow/PL/#5-hooki","title":"5. Hooki","text":"<ul> <li>Hooki baz danych</li> <li>Hooki chmurowe (AWS, Azure)</li> <li>Hooki HTTP</li> <li>Tworzy\u0107 hooki niestandardowe</li> </ul>"},{"location":"AirFlow/PL/#6-zmienne-i-poaczenia","title":"6. Zmienne i Po\u0142\u0105czenia","text":"<ul> <li>Zarz\u0105dza\u0107 zmiennymi</li> <li>Konfigurowa\u0107 po\u0142\u0105czenia</li> <li>Bezpiecze\u0144stwo i dobre praktyki</li> <li>Zmienne dynamiczne</li> </ul>"},{"location":"AirFlow/PL/#7-dobre-praktyki","title":"7. Dobre praktyki","text":"<ul> <li>Struktura DAG\u00f3w</li> <li>Obs\u0142uga b\u0142\u0119d\u00f3w</li> <li>Wydajno\u015b\u0107 i optymalizacja</li> <li>Testy i debugowanie</li> </ul>"},{"location":"AirFlow/PL/#8-projekty-praktyczne","title":"8. Projekty praktyczne","text":"<ul> <li>Kompletny pipeline ETL</li> <li>Orkiestracja przep\u0142yw\u00f3w pracy</li> <li>Integracja z bazami danych</li> <li>Projekty do portfolio</li> </ul>"},{"location":"AirFlow/PL/#szybki-start","title":"\ud83d\ude80 Szybki start","text":""},{"location":"AirFlow/PL/#wymagania-wstepne","title":"Wymagania wst\u0119pne","text":"<ul> <li>Python 3.8+ : Zainstalowany w systemie</li> <li>pip : Mened\u017cer pakiet\u00f3w Python</li> <li>PostgreSQL (opcjonalne) : Dla bazy metadanych</li> </ul>"},{"location":"AirFlow/PL/#szybka-instalacja","title":"Szybka instalacja","text":"<pre><code># Utworzy\u0107 \u015brodowisko wirtualne\npython -m venv airflow-env\n\n# Aktywowa\u0107 \u015brodowisko\n# Windows\nairflow-env\\Scripts\\activate\n# Linux/Mac\nsource airflow-env/bin/activate\n\n# Zainstalowa\u0107 Airflow\npip install apache-airflow\n\n# Zainicjalizowa\u0107 baz\u0119 danych\nairflow db init\n\n# Utworzy\u0107 u\u017cytkownika admin\nairflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com\n\n# Uruchomi\u0107 serwer web\nairflow webserver --port 8080\n\n# W innym terminalu, uruchomi\u0107 scheduler\nairflow scheduler\n</code></pre>"},{"location":"AirFlow/PL/#dostep-do-interfejsu-web","title":"Dost\u0119p do interfejsu web","text":"<ol> <li>Otworzy\u0107 przegl\u0105dark\u0119</li> <li>Przej\u015b\u0107 do : <code>http://localhost:8080</code></li> <li>Zalogowa\u0107 si\u0119 z utworzonymi credentials</li> </ol>"},{"location":"AirFlow/PL/#przypadki-uzycia-dla-data-analyst","title":"\ud83d\udcca Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Orkiestracja ETL : Koordynowa\u0107 pipeline'y danych</li> <li>Harmonogramowanie : Planowa\u0107 zadania cykliczne</li> <li>Monitorowanie : Monitorowa\u0107 wykonanie przep\u0142yw\u00f3w pracy</li> <li>Obs\u0142uga b\u0142\u0119d\u00f3w : Automatyczne ponowienie i alerty</li> <li>Integracja : \u0141\u0105czy\u0107 wiele narz\u0119dzi i us\u0142ug</li> </ul>"},{"location":"AirFlow/PL/#instalacja-zdalna","title":"\u26a0\ufe0f Instalacja zdalna","text":"<p>Je\u015bli instalujesz Airflow na maszynie A i chcesz uzyska\u0107 do niej dost\u0119p z maszyny B, zobacz przewodnik Instalacja i dost\u0119p zdalny.</p>"},{"location":"AirFlow/PL/#bezpatne-zasoby","title":"\ud83d\udcda Bezp\u0142atne zasoby","text":""},{"location":"AirFlow/PL/#oficjalna-dokumentacja","title":"Oficjalna dokumentacja","text":"<ul> <li>Apache Airflow : https://airflow.apache.org/docs/</li> <li>Kompletne przewodniki</li> <li>Tutoriale krok po kroku</li> <li>Przyk\u0142ady kodu</li> <li> <p>Referencja API</p> </li> <li> <p>GitHub Airflow : https://github.com/apache/airflow</p> </li> <li>Kod \u017ar\u00f3d\u0142owy</li> <li>Przyk\u0142ady DAG\u00f3w</li> <li>Wk\u0142ady</li> </ul>"},{"location":"AirFlow/PL/#zasoby-zewnetrzne","title":"Zasoby zewn\u0119trzne","text":"<ul> <li>YouTube : Tutoriale Airflow</li> <li>Medium : Artyku\u0142y i przewodniki</li> <li>Stack Overflow : Pytania i odpowiedzi</li> </ul>"},{"location":"AirFlow/PL/#certyfikacje-opcjonalne","title":"\ud83c\udf93 Certyfikacje (opcjonalne)","text":""},{"location":"AirFlow/PL/#apache-airflow-brak-oficjalnej-certyfikacji","title":"Apache Airflow (brak oficjalnej certyfikacji)","text":"<ul> <li>Szkolenie : Bezp\u0142atna dokumentacja i tutoriale</li> <li>Czas trwania : 2-4 tygodnie</li> <li>Poziom : \u015aredniozaawansowany do zaawansowanego</li> </ul>"},{"location":"AirFlow/PL/#konwencje","title":"\ud83d\udcdd Konwencje","text":"<ul> <li>Wszystkie przyk\u0142ady u\u017cywaj\u0105 Python 3.8+</li> <li>DAGi s\u0105 testowane na Airflow 2.x</li> <li>\u015acie\u017cki mog\u0105 si\u0119 r\u00f3\u017cni\u0107 w zale\u017cno\u015bci od systemu operacyjnego</li> <li>Porty domy\u015blne mog\u0105 by\u0107 modyfikowane</li> </ul>"},{"location":"AirFlow/PL/#wkad","title":"\ud83e\udd1d Wk\u0142ad","text":"<p>Ten kurs jest zaprojektowany jako rozwijaj\u0105cy si\u0119. Nie wahaj si\u0119 proponowa\u0107 ulepsze\u0144 lub dodatkowych przypadk\u00f3w u\u017cycia.</p>"},{"location":"AirFlow/PL/#dodatkowe-zasoby","title":"\ud83d\udcda Dodatkowe zasoby","text":"<ul> <li>Dokumentacja Apache Airflow</li> <li>GitHub Apache Airflow</li> <li>Spo\u0142eczno\u015b\u0107 Airflow</li> <li>Przyk\u0142ady Airflow</li> </ul>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/","title":"Instalacja Airflow - Dost\u0119p zdalny","text":""},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#cel","title":"\ud83c\udfaf Cel","text":"<p>Ten przewodnik pozwala zainstalowa\u0107 Apache Airflow na maszynie A (serwer) i uzyska\u0107 do niej dost\u0119p z maszyny B (klient) przez sie\u0107 lokaln\u0105.</p>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#wymagania-wstepne","title":"\ud83d\udccb Wymagania wst\u0119pne","text":""},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#maszyna-a-serwer","title":"Maszyna A (Serwer)","text":"<ul> <li>Python 3.8+ zainstalowany</li> <li>Dost\u0119p administratora</li> <li>Aktywne po\u0142\u0105czenie sieciowe</li> <li>Port 8080 dost\u0119pny (lub inny port)</li> </ul>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#maszyna-b-klient","title":"Maszyna B (Klient)","text":"<ul> <li>Przegl\u0105darka web</li> <li>Po\u0142\u0105czenie z t\u0105 sam\u0105 sieci\u0105 lokaln\u0105 co maszyna A</li> </ul>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#instalacja-na-maszynie-a","title":"\ud83d\udd27 Instalacja na maszynie A","text":""},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#krok-1-zainstalowac-python-i-zaleznosci","title":"Krok 1 : Zainstalowa\u0107 Python i zale\u017cno\u015bci","text":"<p>Windows : <pre><code># Sprawdzi\u0107 Python\npython --version\n\n# Zainstalowa\u0107 pip je\u015bli potrzeba\npython -m ensurepip --upgrade\n</code></pre></p> <p>Linux : <pre><code># Zainstalowa\u0107 Python i pip\nsudo apt update\nsudo apt install python3 python3-pip python3-venv\n</code></pre></p>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#krok-2-utworzyc-srodowisko-airflow","title":"Krok 2 : Utworzy\u0107 \u015brodowisko Airflow","text":"<pre><code># Utworzy\u0107 katalog dla Airflow\nmkdir airflow-install\ncd airflow-install\n\n# Utworzy\u0107 \u015brodowisko wirtualne\npython -m venv airflow-env\n\n# Aktywowa\u0107 \u015brodowisko\n# Windows\nairflow-env\\Scripts\\activate\n# Linux\nsource airflow-env/bin/activate\n\n# Zainstalowa\u0107 Airflow\npip install apache-airflow\n\n# Zainstalowa\u0107 provider PostgreSQL (opcjonalne, dla bazy metadanych)\npip install apache-airflow-providers-postgres\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#krok-3-skonfigurowac-airflow","title":"Krok 3 : Skonfigurowa\u0107 Airflow","text":"<pre><code># Zainicjalizowa\u0107 baz\u0119 danych\nairflow db init\n\n# Utworzy\u0107 u\u017cytkownika administratora\nairflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password admin123\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#krok-4-skonfigurowac-dostep-sieciowy","title":"Krok 4 : Skonfigurowa\u0107 dost\u0119p sieciowy","text":"<p>Zmodyfikowa\u0107 konfiguracj\u0119 Airflow :</p> <ol> <li>Znale\u017a\u0107 plik <code>airflow.cfg</code> :</li> <li>Windows : <code>%USERPROFILE%\\airflow\\airflow.cfg</code></li> <li> <p>Linux : <code>~/airflow/airflow.cfg</code></p> </li> <li> <p>Zmodyfikowa\u0107 nast\u0119puj\u0105ce parametry :</p> </li> </ol> <pre><code>[webserver]\n# Zezwoli\u0107 dost\u0119p ze wszystkich interfejs\u00f3w\nweb_server_host = 0.0.0.0\nweb_server_port = 8080\n\n# Wy\u0142\u0105czy\u0107 uwierzytelnianie podstawowe (opcjonalne, dla rozwoju)\nauth_backend = airflow.api.auth.backend.basic_auth\n</code></pre> <p>Lub utworzy\u0107 plik konfiguracyjny niestandardowy :</p> <pre><code># Utworzy\u0107 niestandardowy plik airflow.cfg\nexport AIRFLOW_HOME=/path/to/airflow\nairflow config get-value webserver web_server_host\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#krok-5-skonfigurowac-firewall","title":"Krok 5 : Skonfigurowa\u0107 firewall","text":"<p>Windows (Firewall) :</p> <ol> <li>Otworzy\u0107 \"Zapora Windows Defender\"</li> <li>\"Ustawienia zaawansowane\"</li> <li>\"Regu\u0142y ruchu przychodz\u0105cego\" \u2192 \"Nowa regu\u0142a\"</li> <li>Typ : Port</li> <li>Port : 8080 (TCP)</li> <li>Akcja : Zezwoli\u0107 na po\u0142\u0105czenie</li> <li>Nazwa : \"Airflow Web Server\"</li> </ol> <p>Linux (UFW) :</p> <pre><code># Zezwoli\u0107 port 8080\nsudo ufw allow 8080/tcp\nsudo ufw reload\n</code></pre> <p>Linux (firewalld) :</p> <pre><code># Zezwoli\u0107 port 8080\nsudo firewall-cmd --permanent --add-port=8080/tcp\nsudo firewall-cmd --reload\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#krok-6-uruchomic-airflow","title":"Krok 6 : Uruchomi\u0107 Airflow","text":"<p>Terminal 1 - Web Server :</p> <pre><code># Aktywowa\u0107 \u015brodowisko wirtualne\nsource airflow-env/bin/activate  # Linux\n# lub\nairflow-env\\Scripts\\activate  # Windows\n\n# Uruchomi\u0107 serwer web\nairflow webserver --port 8080 --host 0.0.0.0\n</code></pre> <p>Terminal 2 - Scheduler :</p> <pre><code># Aktywowa\u0107 \u015brodowisko wirtualne\nsource airflow-env/bin/activate  # Linux\n# lub\nairflow-env\\Scripts\\activate  # Windows\n\n# Uruchomi\u0107 scheduler\nairflow scheduler\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#krok-7-uzyskac-adres-ip-maszyny-a","title":"Krok 7 : Uzyska\u0107 adres IP maszyny A","text":"<p>Windows : <pre><code>ipconfig\n# Szuka\u0107 \"Adres IPv4\" (np. 192.168.1.100)\n</code></pre></p> <p>Linux : <pre><code>ip addr show\n# lub\nhostname -I\n# Szuka\u0107 adresu IP (np. 192.168.1.100)\n</code></pre></p>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#dostep-z-maszyny-b","title":"\ud83c\udf10 Dost\u0119p z maszyny B","text":""},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#krok-1-sprawdzic-acznosc","title":"Krok 1 : Sprawdzi\u0107 \u0142\u0105czno\u015b\u0107","text":"<p>Z maszyny B :</p> <pre><code># Testowa\u0107 po\u0142\u0105czenie\nping 192.168.1.100  # Zast\u0105pi\u0107 IP maszyny A\n\n# Testowa\u0107 port\ntelnet 192.168.1.100 8080\n# lub\ncurl http://192.168.1.100:8080\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#krok-2-dostep-do-interfejsu-web","title":"Krok 2 : Dost\u0119p do interfejsu web","text":"<ol> <li>Otworzy\u0107 przegl\u0105dark\u0119 na maszynie B</li> <li>Przej\u015b\u0107 do : <code>http://192.168.1.100:8080</code></li> <li>Zast\u0105pi\u0107 <code>192.168.1.100</code> IP maszyny A</li> <li>Zalogowa\u0107 si\u0119 z :</li> <li>Username : <code>admin</code></li> <li>Password : <code>admin123</code> (lub ten kt\u00f3ry utworzy\u0142e\u015b)</li> </ol>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#bezpieczenstwo","title":"\ud83d\udd12 Bezpiecze\u0144stwo","text":""},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#zalecenia","title":"Zalecenia","text":"<ol> <li> <p>Zmieni\u0107 domy\u015blne has\u0142o <pre><code>airflow users set-password admin\n</code></pre></p> </li> <li> <p>U\u017cywa\u0107 HTTPS (w produkcji)</p> </li> <li>Skonfigurowa\u0107 reverse proxy (nginx, Apache)</li> <li> <p>U\u017cywa\u0107 certyfikat\u00f3w SSL</p> </li> <li> <p>Ogranicza\u0107 dost\u0119p sieciowy</p> </li> <li>U\u017cywa\u0107 VPN</li> <li> <p>Ogranicza\u0107 dozwolone IP w firewall</p> </li> <li> <p>Uwierzytelnianie wzmocnione</p> </li> <li>U\u017cywa\u0107 OAuth</li> <li>Integrowa\u0107 z LDAP/Active Directory</li> </ol>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#konfiguracja-bezpieczna","title":"Konfiguracja bezpieczna","text":"<p>Zmodyfikowa\u0107 <code>airflow.cfg</code> :</p> <pre><code>[webserver]\n# W\u0142\u0105czy\u0107 uwierzytelnianie\nauth_backend = airflow.api.auth.backend.basic_auth\n\n# Ograniczy\u0107 dozwolone hosty (opcjonalne)\nhostname_callable = airflow.utils.net.get_hostname\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#rozwiazywanie-problemow","title":"\ud83d\udc1b Rozwi\u0105zywanie problem\u00f3w","text":""},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#problem-niemozliwe-poaczenie-z-maszyny-b","title":"Problem : Niemo\u017cliwe po\u0142\u0105czenie z maszyny B","text":"<p>Rozwi\u0105zania :</p> <ol> <li> <p>Sprawdzi\u0107 firewall <pre><code># Windows\nnetsh advfirewall firewall show rule name=\"Airflow Web Server\"\n\n# Linux\nsudo ufw status\n</code></pre></p> </li> <li> <p>Sprawdzi\u0107 \u017ce Airflow nas\u0142uchuje na 0.0.0.0 <pre><code># Sprawdzi\u0107 otwarte porty\nnetstat -an | grep 8080\n# Powinno pokaza\u0107 : 0.0.0.0:8080\n</code></pre></p> </li> <li> <p>Sprawdzi\u0107 konfiguracj\u0119 sieci</p> </li> <li>Obie maszyny s\u0105 w tej samej sieci</li> <li>Brak VPN blokuj\u0105cego po\u0142\u0105czenie</li> <li>Brak proxy interferuj\u0105cego</li> </ol>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#problem-bad-connection-refused","title":"Problem : B\u0142\u0105d \"Connection refused\"","text":"<p>Rozwi\u0105zania :</p> <ol> <li>Sprawdzi\u0107 \u017ce serwer web jest uruchomiony</li> <li>Sprawdzi\u0107 port (8080 domy\u015blnie)</li> <li>Sprawdzi\u0107 logi Airflow :    <pre><code># Logi webserver\ntail -f ~/airflow/logs/webserver.log\n</code></pre></li> </ol>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#problem-bad-uwierzytelniania","title":"Problem : B\u0142\u0105d uwierzytelniania","text":"<p>Rozwi\u0105zania :</p> <ol> <li>Sprawdzi\u0107 credentials</li> <li>Utworzy\u0107 ponownie u\u017cytkownika je\u015bli potrzeba :    <pre><code>airflow users create \\\n    --username admin \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password nowe_haslo\n</code></pre></li> </ol>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#konfiguracja-zaawansowana","title":"\ud83d\udcdd Konfiguracja zaawansowana","text":""},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#uzywac-reverse-proxy-nginx","title":"U\u017cywa\u0107 reverse proxy (nginx)","text":"<p>Instalacja nginx :</p> <pre><code># Linux\nsudo apt install nginx\n\n# Konfiguracja nginx\nsudo nano /etc/nginx/sites-available/airflow\n</code></pre> <p>Konfiguracja nginx :</p> <pre><code>server {\n    listen 80;\n    server_name airflow.local;\n\n    location / {\n        proxy_pass http://127.0.0.1:8080;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>W\u0142\u0105czy\u0107 konfiguracj\u0119 :</p> <pre><code>sudo ln -s /etc/nginx/sites-available/airflow /etc/nginx/sites-enabled/\nsudo nginx -t\nsudo systemctl reload nginx\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#usuga-systemd-linux","title":"Us\u0142uga systemd (Linux)","text":"<p>Utworzy\u0107 us\u0142ug\u0119 dla Airflow :</p> <pre><code>sudo nano /etc/systemd/system/airflow-webserver.service\n</code></pre> <p>Zawarto\u015b\u0107 :</p> <pre><code>[Unit]\nDescription=Airflow webserver daemon\nAfter=network.target\n\n[Service]\nUser=airflow\nGroup=airflow\nType=simple\nExecStart=/path/to/airflow-env/bin/airflow webserver\nRestart=on-failure\nRestartSec=5s\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>W\u0142\u0105czy\u0107 us\u0142ug\u0119 :</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable airflow-webserver\nsudo systemctl start airflow-webserver\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#weryfikacja","title":"\ud83d\udcca Weryfikacja","text":""},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#test-poaczenia","title":"Test po\u0142\u0105czenia","text":"<p>Z maszyny B :</p> <pre><code># Test HTTP\ncurl http://192.168.1.100:8080/health\n\n# Test z uwierzytelnianiem\ncurl -u admin:admin123 http://192.168.1.100:8080/api/v1/dags\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#sprawdzic-logi","title":"Sprawdzi\u0107 logi","text":"<p>Na maszynie A :</p> <pre><code># Logi webserver\ntail -f ~/airflow/logs/webserver.log\n\n# Logi scheduler\ntail -f ~/airflow/logs/scheduler/*.log\n</code></pre>"},{"location":"AirFlow/PL/INSTALLATION_REMOTE/#zasoby","title":"\ud83d\udd17 Zasoby","text":"<ul> <li>Dokumentacja Airflow</li> <li>Konfiguracja Airflow</li> <li>Bezpiecze\u0144stwo Airflow</li> </ul> <p>Uwaga : Ta konfiguracja jest dla \u015brodowiska deweloperskiego. Dla produkcji, u\u017cywa\u0107 wzmocnionych praktyk bezpiecze\u0144stwa (HTTPS, uwierzytelnianie OAuth, itp.).</p>"},{"location":"AirFlow/PL/01-getting-started/","title":"1. Rozpocz\u0119cie z Airflow","text":""},{"location":"AirFlow/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Apache Airflow</li> <li>Zainstalowa\u0107 Airflow lokalnie</li> <li>Skonfigurowa\u0107 \u015brodowisko</li> <li>Uzyska\u0107 dost\u0119p do interfejsu web</li> <li>Utworzy\u0107 pierwszy DAG</li> </ul>"},{"location":"AirFlow/PL/01-getting-started/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Airflow</li> <li>Instalacja</li> <li>Podstawowa konfiguracja</li> <li>Interfejs web</li> <li>Pierwszy DAG</li> </ol>"},{"location":"AirFlow/PL/01-getting-started/#wprowadzenie-do-airflow","title":"Wprowadzenie do Airflow","text":""},{"location":"AirFlow/PL/01-getting-started/#czym-jest-apache-airflow","title":"Czym jest Apache Airflow?","text":"<p>Apache Airflow = Platforma open-source do orkiestracji przep\u0142yw\u00f3w pracy</p> <ul> <li>Workflows : Z\u0142o\u017cone pipeline'y danych</li> <li>Scheduling : Automatyczne planowanie</li> <li>Monitorowanie : Monitorowanie w czasie rzeczywistym</li> <li>Python : Zdefiniowane w Pythonie</li> <li>Skalowalne : Od prostych do bardzo z\u0142o\u017conych</li> </ul>"},{"location":"AirFlow/PL/01-getting-started/#dlaczego-airflow-dla-data-analyst","title":"Dlaczego Airflow dla Data Analyst?","text":"<ul> <li>Orkiestracja ETL : Koordynowa\u0107 wiele krok\u00f3w</li> <li>Harmonogramowanie : Automatyzowa\u0107 zadania cykliczne</li> <li>Monitorowanie : Widzie\u0107 status pipeline'\u00f3w</li> <li>Retry : Automatyczne ponowienie przy b\u0142\u0119dzie</li> <li>Integracja : Z bazami danych, API, us\u0142ugami chmurowymi</li> </ul>"},{"location":"AirFlow/PL/01-getting-started/#komponenty-airflow","title":"Komponenty Airflow","text":"<ol> <li>Web Server : Interfejs web (port 8080)</li> <li>Scheduler : Planuje i wykonuje DAGi</li> <li>Metadata Database : Przechowuje stan i metadane</li> <li>Workers : Wykonuj\u0105 zadania (opcjonalne)</li> </ol>"},{"location":"AirFlow/PL/01-getting-started/#instalacja","title":"Instalacja","text":""},{"location":"AirFlow/PL/01-getting-started/#wymagania-wstepne","title":"Wymagania wst\u0119pne","text":"<ul> <li>Python 3.8+ : Zainstalowany w systemie</li> <li>pip : Mened\u017cer pakiet\u00f3w Python</li> <li>7-8 GB RAM : Minimum zalecane</li> </ul>"},{"location":"AirFlow/PL/01-getting-started/#instalacja-z-pip","title":"Instalacja z pip","text":"<p>Krok 1 : Utworzy\u0107 \u015brodowisko wirtualne</p> <pre><code># Utworzy\u0107 katalog\nmkdir airflow-project\ncd airflow-project\n\n# Utworzy\u0107 \u015brodowisko wirtualne\npython -m venv airflow-env\n\n# Aktywowa\u0107 \u015brodowisko\n# Windows\nairflow-env\\Scripts\\activate\n# Linux/Mac\nsource airflow-env/bin/activate\n</code></pre> <p>Krok 2 : Zainstalowa\u0107 Airflow</p> <pre><code># Zainstalowa\u0107 Airflow\npip install apache-airflow\n\n# Zainstalowa\u0107 dodatkowe providers (opcjonalne)\npip install apache-airflow-providers-postgres\npip install apache-airflow-providers-http\n</code></pre> <p>Krok 3 : Zainicjalizowa\u0107 baz\u0119 danych</p> <pre><code># Zainicjalizowa\u0107 baz\u0119 danych SQLite (domy\u015blnie)\nairflow db init\n</code></pre> <p>Krok 4 : Utworzy\u0107 u\u017cytkownika admin</p> <pre><code>airflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password admin123\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#instalacja-z-constraints-zalecane","title":"Instalacja z constraints (zalecane)","text":"<p>Aby unikn\u0105\u0107 konflikt\u00f3w zale\u017cno\u015bci :</p> <pre><code># Pobra\u0107 constraints\nAIRFLOW_VERSION=2.7.0\nPYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n\n# Zainstalowa\u0107 z constraints\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#podstawowa-konfiguracja","title":"Podstawowa konfiguracja","text":""},{"location":"AirFlow/PL/01-getting-started/#plik-airflowcfg","title":"Plik airflow.cfg","text":"<p>Lokalizacja : - Windows : <code>%USERPROFILE%\\airflow\\airflow.cfg</code> - Linux/Mac : <code>~/airflow/airflow.cfg</code></p> <p>Wa\u017cne parametry :</p> <pre><code>[core]\n# Katalog DAG\u00f3w\ndags_folder = ~/airflow/dags\n\n# Katalog log\u00f3w\nbase_log_folder = ~/airflow/logs\n\n# Strefa czasowa\ndefault_timezone = Europe/Warsaw\n\n[webserver]\n# Port serwera web\nweb_server_port = 8080\n\n# Host (0.0.0.0 dla dost\u0119pu sieciowego)\nweb_server_host = 0.0.0.0\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#zmienne-srodowiskowe","title":"Zmienne \u015brodowiskowe","text":"<p>AIRFLOW_HOME :</p> <pre><code># Windows\nset AIRFLOW_HOME=C:\\airflow\n\n# Linux/Mac\nexport AIRFLOW_HOME=~/airflow\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#struktura-katalogow","title":"Struktura katalog\u00f3w","text":"<pre><code>airflow/\n\u251c\u2500\u2500 dags/          # Twoje DAGi\n\u251c\u2500\u2500 logs/          # Logi wykonania\n\u251c\u2500\u2500 plugins/       # Pluginy niestandardowe\n\u2514\u2500\u2500 airflow.cfg   # Konfiguracja\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#interfejs-web","title":"Interfejs web","text":""},{"location":"AirFlow/PL/01-getting-started/#uruchomic-serwer-web","title":"Uruchomi\u0107 serwer web","text":"<pre><code># Aktywowa\u0107 \u015brodowisko wirtualne\nsource airflow-env/bin/activate  # Linux/Mac\n# lub\nairflow-env\\Scripts\\activate  # Windows\n\n# Uruchomi\u0107 serwer web\nairflow webserver --port 8080\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#uruchomic-scheduler","title":"Uruchomi\u0107 scheduler","text":"<p>W innym terminalu :</p> <pre><code># Aktywowa\u0107 \u015brodowisko wirtualne\nsource airflow-env/bin/activate\n\n# Uruchomi\u0107 scheduler\nairflow scheduler\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#dostep-do-interfejsu","title":"Dost\u0119p do interfejsu","text":"<ol> <li>Otworzy\u0107 przegl\u0105dark\u0119</li> <li>Przej\u015b\u0107 do : <code>http://localhost:8080</code></li> <li>Zalogowa\u0107 si\u0119 z :</li> <li>Username : <code>admin</code></li> <li>Password : <code>admin123</code></li> </ol>"},{"location":"AirFlow/PL/01-getting-started/#nawigacja-w-interfejsie","title":"Nawigacja w interfejsie","text":"<p>G\u0142\u00f3wne zak\u0142adki : - DAGs : Lista wszystkich DAG\u00f3w - Graph : Widok graficzny DAGa - Tree : Widok drzewa wykonania - Gantt : Diagram Gantta - Code : Kod \u017ar\u00f3d\u0142owy DAGa - Logs : Logi wykonania</p>"},{"location":"AirFlow/PL/01-getting-started/#pierwszy-dag","title":"Pierwszy DAG","text":""},{"location":"AirFlow/PL/01-getting-started/#utworzyc-prosty-dag","title":"Utworzy\u0107 prosty DAG","text":"<p>Krok 1 : Utworzy\u0107 plik DAG</p> <pre><code># Utworzy\u0107 katalog dags\nmkdir -p ~/airflow/dags\n\n# Utworzy\u0107 plik DAG\nnano ~/airflow/dags/my_first_dag.py\n</code></pre> <p>Krok 2 : Kod DAGa</p> <pre><code>from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\n# Zdefiniowa\u0107 argumenty domy\u015blne\ndefault_args = {\n    'owner': 'data_analyst',\n    'depends_on_past': False,\n    'email': ['admin@example.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Utworzy\u0107 DAG\ndag = DAG(\n    'my_first_dag',\n    default_args=default_args,\n    description='M\u00f3j pierwszy DAG Airflow',\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['tutorial'],\n)\n\n# Zadanie 1 : Wy\u015bwietli\u0107 dat\u0119\nt1 = BashOperator(\n    task_id='print_date',\n    bash_command='date',\n    dag=dag,\n)\n\n# Zadanie 2 : Wy\u015bwietli\u0107 wiadomo\u015b\u0107\ndef print_hello():\n    print(\"Hello from Airflow!\")\n\nt2 = PythonOperator(\n    task_id='print_hello',\n    python_callable=print_hello,\n    dag=dag,\n)\n\n# Zdefiniowa\u0107 zale\u017cno\u015bci\nt1 &gt;&gt; t2  # t1 wykonuje si\u0119 przed t2\n</code></pre> <p>Krok 3 : Sprawdzi\u0107 DAG</p> <pre><code># Listowa\u0107 DAGi\nairflow dags list\n\n# Sprawdzi\u0107 sk\u0142adni\u0119\nairflow dags list-import-errors\n\n# Testowa\u0107 DAG\nairflow dags test my_first_dag 2024-01-01\n</code></pre> <p>Krok 4 : Zobaczy\u0107 w interfejsie web</p> <ol> <li>Od\u015bwie\u017cy\u0107 stron\u0119 web</li> <li>DAG <code>my_first_dag</code> pojawia si\u0119 na li\u015bcie</li> <li>Klikn\u0105\u0107 \"Trigger DAG\" aby go wykona\u0107</li> </ol>"},{"location":"AirFlow/PL/01-getting-started/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"AirFlow/PL/01-getting-started/#przykad-1-dag-z-wieloma-zadaniami","title":"Przyk\u0142ad 1 : DAG z wieloma zadaniami","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data_analyst',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    description='Przyk\u0142ad DAGa z wieloma zadaniami',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\n# Zadanie 1\nextract = BashOperator(\n    task_id='extract_data',\n    bash_command='echo \"Extracting data...\"',\n    dag=dag,\n)\n\n# Zadanie 2\ntransform = BashOperator(\n    task_id='transform_data',\n    bash_command='echo \"Transforming data...\"',\n    dag=dag,\n)\n\n# Zadanie 3\nload = BashOperator(\n    task_id='load_data',\n    bash_command='echo \"Loading data...\"',\n    dag=dag,\n)\n\n# Zdefiniowa\u0107 zale\u017cno\u015bci\nextract &gt;&gt; transform &gt;&gt; load\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#przykad-2-dag-z-gaeziami","title":"Przyk\u0142ad 2 : DAG z ga\u0142\u0119ziami","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'branching_dag',\n    description='DAG z ga\u0142\u0119ziami',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\ndef decide_path():\n    # Logika do decyzji \u015bcie\u017cki\n    return 'path_a'\n\ndecide = PythonOperator(\n    task_id='decide',\n    python_callable=decide_path,\n    dag=dag,\n)\n\npath_a = BashOperator(\n    task_id='path_a',\n    bash_command='echo \"Path A\"',\n    dag=dag,\n)\n\npath_b = BashOperator(\n    task_id='path_b',\n    bash_command='echo \"Path B\"',\n    dag=dag,\n)\n\n# Rozga\u0142\u0119zienie warunkowe\ndecide &gt;&gt; [path_a, path_b]\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#przydatne-polecenia","title":"Przydatne polecenia","text":""},{"location":"AirFlow/PL/01-getting-started/#zarzadzanie-dagami","title":"Zarz\u0105dzanie DAGami","text":"<pre><code># Listowa\u0107 wszystkie DAGi\nairflow dags list\n\n# Sprawdzi\u0107 b\u0142\u0119dy importu\nairflow dags list-import-errors\n\n# Testowa\u0107 DAG\nairflow dags test my_first_dag 2024-01-01\n\n# Wstrzyma\u0107 DAG\nairflow dags pause my_first_dag\n\n# Wznowi\u0107 DAG\nairflow dags unpause my_first_dag\n\n# Usun\u0105\u0107 DAG\nairflow dags delete my_first_dag\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#zarzadzanie-zadaniami","title":"Zarz\u0105dzanie zadaniami","text":"<pre><code># Testowa\u0107 zadanie\nairflow tasks test my_first_dag print_date 2024-01-01\n\n# Wykona\u0107 zadanie\nairflow tasks run my_first_dag print_date 2024-01-01\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#zarzadzanie-baza-danych","title":"Zarz\u0105dzanie baz\u0105 danych","text":"<pre><code># Zainicjalizowa\u0107 baz\u0119\nairflow db init\n\n# Zaktualizowa\u0107 baz\u0119\nairflow db upgrade\n\n# Zresetowa\u0107 baz\u0119 (UWAGA : usuwa wszystko)\nairflow db reset\n</code></pre>"},{"location":"AirFlow/PL/01-getting-started/#rozwiazywanie-problemow","title":"Rozwi\u0105zywanie problem\u00f3w","text":""},{"location":"AirFlow/PL/01-getting-started/#problem-dag-niewidoczny-w-interfejsie","title":"Problem : DAG niewidoczny w interfejsie","text":"<p>Rozwi\u0105zania : 1. Sprawdzi\u0107 \u017ce plik jest w <code>~/airflow/dags/</code> 2. Sprawdzi\u0107 sk\u0142adni\u0119 Pythona 3. Sprawdzi\u0107 b\u0142\u0119dy : <code>airflow dags list-import-errors</code> 4. Uruchomi\u0107 ponownie scheduler</p>"},{"location":"AirFlow/PL/01-getting-started/#problem-bad-importu","title":"Problem : B\u0142\u0105d importu","text":"<p>Rozwi\u0105zania : 1. Sprawdzi\u0107 \u017ce wszystkie zale\u017cno\u015bci s\u0105 zainstalowane 2. Sprawdzi\u0107 importy w DAGu 3. Sprawdzi\u0107 \u015bcie\u017cki Pythona</p>"},{"location":"AirFlow/PL/01-getting-started/#problem-scheduler-nie-uruchamia-sie","title":"Problem : Scheduler nie uruchamia si\u0119","text":"<p>Rozwi\u0105zania : 1. Sprawdzi\u0107 \u017ce baza danych jest zainicjalizowana 2. Sprawdzi\u0107 logi : <code>~/airflow/logs/scheduler/</code> 3. Sprawdzi\u0107 uprawnienia</p>"},{"location":"AirFlow/PL/01-getting-started/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Airflow = Orkiestracja przep\u0142yw\u00f3w pracy Python</li> <li>DAGi definiuj\u0105 workflows</li> <li>Zadania to pojedyncze kroki</li> <li>Scheduler wykonuje DAGi wed\u0142ug harmonogramu</li> <li>Interfejs web umo\u017cliwia monitorowanie i zarz\u0105dzanie</li> </ol>"},{"location":"AirFlow/PL/01-getting-started/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 2. Podstawowe koncepcje, aby pog\u0142\u0119bi\u0107 koncepcje Airflow.</p>"},{"location":"AirFlow/PL/02-concepts/","title":"2. Podstawowe koncepcje Airflow","text":""},{"location":"AirFlow/PL/02-concepts/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 DAGi (Directed Acyclic Graphs)</li> <li>Opanowa\u0107 zadania i zale\u017cno\u015bci</li> <li>Zrozumie\u0107 harmonogramowanie</li> <li>U\u017cywa\u0107 zmiennych i po\u0142\u0105cze\u0144</li> <li>Zarz\u0105dza\u0107 wykonaniami</li> </ul>"},{"location":"AirFlow/PL/02-concepts/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>DAGi (Directed Acyclic Graphs)</li> <li>Zadania i zale\u017cno\u015bci</li> <li>Harmonogramowanie</li> <li>Zmienne i Po\u0142\u0105czenia</li> <li>Wykonania i stany</li> </ol>"},{"location":"AirFlow/PL/02-concepts/#dagi-directed-acyclic-graphs","title":"DAGi (Directed Acyclic Graphs)","text":""},{"location":"AirFlow/PL/02-concepts/#czym-jest-dag","title":"Czym jest DAG?","text":"<p>DAG = Graf skierowany acykliczny</p> <ul> <li>Skierowany : Zadania maj\u0105 kierunek (zale\u017cno\u015bci)</li> <li>Acykliczny : Brak p\u0119tli (brak zale\u017cno\u015bci cyklicznych)</li> <li>Graf : Wizualna reprezentacja przep\u0142yw\u00f3w pracy</li> </ul>"},{"location":"AirFlow/PL/02-concepts/#struktura-daga","title":"Struktura DAGa","text":"<pre><code>from airflow import DAG\nfrom datetime import datetime, timedelta\n\n# Argumenty domy\u015blne\ndefault_args = {\n    'owner': 'data_analyst',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Utworzy\u0107 DAG\ndag = DAG(\n    'my_dag',\n    default_args=default_args,\n    description='Opis DAGa',\n    schedule_interval='@daily',  # Cz\u0119stotliwo\u015b\u0107 wykonania\n    start_date=datetime(2024, 1, 1),\n    catchup=False,  # Nie wykonywa\u0107 przesz\u0142ych run\u00f3w\n    tags=['example'],\n)\n</code></pre>"},{"location":"AirFlow/PL/02-concepts/#wasciwosci-daga","title":"W\u0142a\u015bciwo\u015bci DAGa","text":"<p>Unikalne ID : - Musi by\u0107 unikalne w instalacji Airflow - U\u017cywane do identyfikacji DAGa</p> <p>Schedule interval : - <code>@daily</code> : Codziennie - <code>@hourly</code> : Co godzin\u0119 - <code>timedelta(days=1)</code> : Codziennie - <code>'0 2 * * *'</code> : Wyra\u017cenie cron (codziennie o 2h) - <code>None</code> : Tylko r\u0119czne wyzwalanie</p> <p>Start date : - Data rozpocz\u0119cia harmonogramowania - Format : <code>datetime(rok, miesi\u0105c, dzie\u0144)</code></p> <p>Catchup : - <code>True</code> : Wykonuje brakuj\u0105ce runy od start_date - <code>False</code> : Wykonuje tylko przysz\u0142e runy</p>"},{"location":"AirFlow/PL/02-concepts/#zadania-i-zaleznosci","title":"Zadania i zale\u017cno\u015bci","text":""},{"location":"AirFlow/PL/02-concepts/#czym-jest-zadanie","title":"Czym jest Zadanie?","text":"<p>Zadanie = Pojedynczy krok w DAGu</p> <ul> <li>Operator : Typ zadania (Python, Bash, SQL, itp.)</li> <li>Unikalne ID : Identyfikator w DAGu</li> <li>Zale\u017cno\u015bci : Relacje z innymi zadaniami</li> </ul>"},{"location":"AirFlow/PL/02-concepts/#typy-operatorow","title":"Typy operator\u00f3w","text":"<p>BashOperator : <pre><code>from airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='bash_task',\n    bash_command='echo \"Hello\"',\n    dag=dag,\n)\n</code></pre></p> <p>PythonOperator : <pre><code>from airflow.operators.python import PythonOperator\n\ndef my_function():\n    print(\"Hello from Python\")\n\ntask = PythonOperator(\n    task_id='python_task',\n    python_callable=my_function,\n    dag=dag,\n)\n</code></pre></p>"},{"location":"AirFlow/PL/02-concepts/#zdefiniowac-zaleznosci","title":"Zdefiniowa\u0107 zale\u017cno\u015bci","text":"<p>Metoda 1 : Operator &gt;&gt;</p> <pre><code># t1 wykonuje si\u0119 przed t2\nt1 &gt;&gt; t2\n\n# Wiele zale\u017cno\u015bci\nt1 &gt;&gt; [t2, t3] &gt;&gt; t4\n</code></pre> <p>Metoda 2 : set_upstream / set_downstream</p> <pre><code># t1 wykonuje si\u0119 przed t2\nt1.set_downstream(t2)\n# lub\nt2.set_upstream(t1)\n</code></pre> <p>Metoda 3 : bitshift</p> <pre><code># t1 &gt;&gt; t2 jest r\u00f3wnowa\u017cne\nt1 &gt;&gt; t2\n</code></pre>"},{"location":"AirFlow/PL/02-concepts/#przykad-zaleznosci","title":"Przyk\u0142ad zale\u017cno\u015bci","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime\n\ndag = DAG('dependencies_example', start_date=datetime(2024, 1, 1))\n\n# Zadania\nextract = BashOperator(task_id='extract', bash_command='echo extract', dag=dag)\ntransform = BashOperator(task_id='transform', bash_command='echo transform', dag=dag)\nload = BashOperator(task_id='load', bash_command='echo load', dag=dag)\nvalidate = BashOperator(task_id='validate', bash_command='echo validate', dag=dag)\n\n# Zale\u017cno\u015bci\nextract &gt;&gt; transform &gt;&gt; [load, validate]\n</code></pre>"},{"location":"AirFlow/PL/02-concepts/#harmonogramowanie","title":"Harmonogramowanie","text":""},{"location":"AirFlow/PL/02-concepts/#schedule-interval","title":"Schedule Interval","text":"<p>Cz\u0119ste wyra\u017cenia :</p> <pre><code># Codziennie o p\u00f3\u0142nocy\nschedule_interval='@daily'\n# lub\nschedule_interval=timedelta(days=1)\n\n# Co godzin\u0119\nschedule_interval='@hourly'\n# lub\nschedule_interval=timedelta(hours=1)\n\n# Co tydzie\u0144\nschedule_interval='@weekly'\n\n# Wyra\u017cenie cron\nschedule_interval='0 2 * * *'  # Codziennie o 2h\nschedule_interval='0 */6 * * *'  # Co 6 godzin\nschedule_interval='0 0 * * MON'  # W ka\u017cdy poniedzia\u0142ek o p\u00f3\u0142nocy\n</code></pre>"},{"location":"AirFlow/PL/02-concepts/#start-date-i-execution-date","title":"Start Date i Execution Date","text":"<p>Start Date : - Data rozpocz\u0119cia harmonogramowania - Format : <code>datetime(2024, 1, 1)</code></p> <p>Execution Date : - Logiczna data wykonania - Format : <code>YYYY-MM-DDTHH:MM:SS</code></p> <p>Przyk\u0142ad : <pre><code>dag = DAG(\n    'scheduled_dag',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n</code></pre></p>"},{"location":"AirFlow/PL/02-concepts/#catchup","title":"Catchup","text":"<p>Catchup = True : - Wykonuje wszystkie brakuj\u0105ce runy od start_date - Mo\u017ce utworzy\u0107 wiele run\u00f3w</p> <p>Catchup = False : - Wykonuje tylko przysz\u0142e runy - Zalecane dla wi\u0119kszo\u015bci przypadk\u00f3w</p>"},{"location":"AirFlow/PL/02-concepts/#zmienne-i-poaczenia","title":"Zmienne i Po\u0142\u0105czenia","text":""},{"location":"AirFlow/PL/02-concepts/#zmienne","title":"Zmienne","text":"<p>Zmienne = Konfiguracja globalna</p> <p>Utworzy\u0107 zmienn\u0105 :</p> <pre><code># Przez CLI\nairflow variables set my_key \"my_value\"\n\n# Przez interfejs web\n# Admin \u2192 Variables \u2192 Add\n</code></pre> <p>U\u017cywa\u0107 zmiennej :</p> <pre><code>from airflow.models import Variable\n\n# Pobra\u0107 zmienn\u0105\nmy_value = Variable.get(\"my_key\")\nmy_value_default = Variable.get(\"my_key\", default_var=\"default\")\n\n# W szablonie\n# {{ var.value.my_key }}\n</code></pre> <p>Przyk\u0142ad :</p> <pre><code>from airflow import DAG\nfrom airflow.models import Variable\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('variables_example', start_date=datetime(2024, 1, 1))\n\ndef use_variable():\n    api_key = Variable.get(\"api_key\")\n    print(f\"API Key: {api_key}\")\n\ntask = PythonOperator(\n    task_id='use_variable',\n    python_callable=use_variable,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/02-concepts/#poaczenia","title":"Po\u0142\u0105czenia","text":"<p>Po\u0142\u0105czenia = Informacje o po\u0142\u0105czeniu</p> <p>Utworzy\u0107 po\u0142\u0105czenie :</p> <pre><code># Przez CLI\nairflow connections add 'my_postgres' \\\n    --conn-type 'postgres' \\\n    --conn-host 'localhost' \\\n    --conn-login 'user' \\\n    --conn-password 'password' \\\n    --conn-port 5432 \\\n    --conn-schema 'mydb'\n</code></pre> <p>U\u017cywa\u0107 po\u0142\u0105czenia :</p> <pre><code>from airflow.hooks.base import BaseHook\n\n# Pobra\u0107 po\u0142\u0105czenie\nconn = BaseHook.get_connection('my_postgres')\nprint(f\"Host: {conn.host}\")\nprint(f\"Login: {conn.login}\")\nprint(f\"Password: {conn.password}\")\n</code></pre>"},{"location":"AirFlow/PL/02-concepts/#wykonania-i-stany","title":"Wykonania i stany","text":""},{"location":"AirFlow/PL/02-concepts/#stany-zadan","title":"Stany zada\u0144","text":"<ul> <li>None : Jeszcze nie wykonane</li> <li>Scheduled : Zaplanowane</li> <li>Queued : W kolejce</li> <li>Running : W trakcie wykonania</li> <li>Success : Zako\u0144czone sukcesem</li> <li>Failed : Nieudane</li> <li>Skipped : Pomini\u0119te</li> <li>Retry : Ponawianie</li> <li>Up for retry : Gotowe do ponowienia</li> </ul>"},{"location":"AirFlow/PL/02-concepts/#stany-dagow","title":"Stany DAG\u00f3w","text":"<ul> <li>Running : W trakcie wykonania</li> <li>Success : Wszystkie zadania zako\u0144czone sukcesem</li> <li>Failed : Przynajmniej jedno zadanie nieudane</li> </ul>"},{"location":"AirFlow/PL/02-concepts/#zarzadzac-wykonaniami","title":"Zarz\u0105dza\u0107 wykonaniami","text":"<p>Przez interfejs web : - Widzie\u0107 stan wykonania - Uruchomi\u0107 ponownie zadanie - Oznaczy\u0107 jako sukces/niepowodzenie - Widzie\u0107 logi</p> <p>Przez CLI :</p> <pre><code># Listowa\u0107 runy\nairflow dags list-runs -d my_dag\n\n# Wyzwoli\u0107 DAG\nairflow dags trigger my_dag\n\n# Oznaczy\u0107 zadanie jako sukces\nairflow tasks clear my_dag task_id -s 2024-01-01\n</code></pre>"},{"location":"AirFlow/PL/02-concepts/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"AirFlow/PL/02-concepts/#przykad-1-dag-ze-zmiennymi","title":"Przyk\u0142ad 1 : DAG ze zmiennymi","text":"<pre><code>from airflow import DAG\nfrom airflow.models import Variable\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('variables_dag', start_date=datetime(2024, 1, 1))\n\ndef process_data():\n    # Pobra\u0107 zmienne\n    input_path = Variable.get(\"input_path\")\n    output_path = Variable.get(\"output_path\")\n\n    print(f\"Processing: {input_path} -&gt; {output_path}\")\n\ntask = PythonOperator(\n    task_id='process',\n    python_callable=process_data,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/02-concepts/#przykad-2-dag-z-poaczeniem","title":"Przyk\u0142ad 2 : DAG z po\u0142\u0105czeniem","text":"<pre><code>from airflow import DAG\nfrom airflow.hooks.postgres import PostgresHook\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('connection_dag', start_date=datetime(2024, 1, 1))\n\ndef query_database():\n    # U\u017cywa\u0107 po\u0142\u0105czenia PostgreSQL\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    records = hook.get_records(\"SELECT * FROM users LIMIT 10\")\n    print(records)\n\ntask = PythonOperator(\n    task_id='query',\n    python_callable=query_database,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/02-concepts/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>DAGi definiuj\u0105 workflows</li> <li>Zadania to pojedyncze kroki</li> <li>Zale\u017cno\u015bci definiuj\u0105 kolejno\u015b\u0107 wykonania</li> <li>Harmonogramowanie planuje wykonania</li> <li>Zmienne i Po\u0142\u0105czenia dla konfiguracji</li> </ol>"},{"location":"AirFlow/PL/02-concepts/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 3. Operatory, aby nauczy\u0107 si\u0119 u\u017cywa\u0107 r\u00f3\u017cnych operator\u00f3w Airflow.</p>"},{"location":"AirFlow/PL/03-operators/","title":"3. Operatory Airflow","text":""},{"location":"AirFlow/PL/03-operators/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 r\u00f3\u017cne typy operator\u00f3w</li> <li>U\u017cywa\u0107 operator\u00f3w Python, Bash, SQL</li> <li>Tworzy\u0107 operatory niestandardowe</li> <li>Zarz\u0105dza\u0107 danymi mi\u0119dzy zadaniami</li> </ul>"},{"location":"AirFlow/PL/03-operators/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Typy operator\u00f3w</li> <li>PythonOperator</li> <li>BashOperator</li> <li>Operatory SQL</li> <li>Operatory niestandardowe</li> </ol>"},{"location":"AirFlow/PL/03-operators/#typy-operatorow","title":"Typy operator\u00f3w","text":""},{"location":"AirFlow/PL/03-operators/#operatory-podstawowe","title":"Operatory podstawowe","text":"<ul> <li>PythonOperator : Wykonuje kod Python</li> <li>BashOperator : Wykonuje polecenia bash</li> <li>SQLExecuteQueryOperator : Wykonuje zapytania SQL</li> <li>EmailOperator : Wysy\u0142a emaile</li> <li>HttpOperator : Wykonuje \u017c\u0105dania HTTP</li> </ul>"},{"location":"AirFlow/PL/03-operators/#operatory-transferu","title":"Operatory transferu","text":"<ul> <li>FileTransferOperator : Transferuje pliki</li> <li>FTPOperator : Operacje FTP</li> <li>S3FileTransformOperator : Przekszta\u0142ca pliki S3</li> </ul>"},{"location":"AirFlow/PL/03-operators/#pythonoperator","title":"PythonOperator","text":""},{"location":"AirFlow/PL/03-operators/#podstawowe-uzycie","title":"Podstawowe u\u017cycie","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG('python_operator', start_date=datetime(2024, 1, 1))\n\ndef my_function():\n    print(\"Hello from Python!\")\n    return \"Success\"\n\ntask = PythonOperator(\n    task_id='python_task',\n    python_callable=my_function,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/03-operators/#przekazywac-argumenty","title":"Przekazywa\u0107 argumenty","text":"<pre><code>def process_data(file_path, output_path):\n    print(f\"Processing {file_path} -&gt; {output_path}\")\n    # Przetwarzanie...\n\ntask = PythonOperator(\n    task_id='process',\n    python_callable=process_data,\n    op_args=['/path/to/input.csv', '/path/to/output.csv'],\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/03-operators/#uzywac-xcom-do-dzielenia-danych","title":"U\u017cywa\u0107 XCom do dzielenia danych","text":"<pre><code>def extract_data(**context):\n    data = {'key': 'value'}\n    return data  # Automatycznie zwraca przez XCom\n\ndef process_data(**context):\n    # Pobra\u0107 dane z poprzedniego zadania\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    print(f\"Received: {data}\")\n\nextract = PythonOperator(\n    task_id='extract',\n    python_callable=extract_data,\n    dag=dag,\n)\n\nprocess = PythonOperator(\n    task_id='process',\n    python_callable=process_data,\n    dag=dag,\n)\n\nextract &gt;&gt; process\n</code></pre>"},{"location":"AirFlow/PL/03-operators/#bashoperator","title":"BashOperator","text":""},{"location":"AirFlow/PL/03-operators/#podstawowe-uzycie_1","title":"Podstawowe u\u017cycie","text":"<pre><code>from airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='bash_task',\n    bash_command='echo \"Hello from Bash\"',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/03-operators/#uzywac-szablonow","title":"U\u017cywa\u0107 szablon\u00f3w","text":"<pre><code>task = BashOperator(\n    task_id='bash_template',\n    bash_command='echo \"Date: {{ ds }}\"',  # ds = data wykonania\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/03-operators/#dostepne-zmienne-szablonu","title":"Dost\u0119pne zmienne szablonu","text":"<ul> <li><code>{{ ds }}</code> : Data wykonania (YYYY-MM-DD)</li> <li><code>{{ ds_nodash }}</code> : Data bez my\u015blnik\u00f3w (YYYYMMDD)</li> <li><code>{{ ts }}</code> : Timestamp wykonania</li> <li><code>{{ dag }}</code> : Obiekt DAG</li> <li><code>{{ task }}</code> : Obiekt Task</li> </ul>"},{"location":"AirFlow/PL/03-operators/#operatory-sql","title":"Operatory SQL","text":""},{"location":"AirFlow/PL/03-operators/#sqlexecutequeryoperator","title":"SQLExecuteQueryOperator","text":"<pre><code>from airflow.providers.postgres.operators.postgres import PostgresOperator\n\ntask = PostgresOperator(\n    task_id='sql_task',\n    postgres_conn_id='my_postgres',\n    sql='SELECT * FROM users LIMIT 10;',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/03-operators/#uzywac-szablonow-sql","title":"U\u017cywa\u0107 szablon\u00f3w SQL","text":"<pre><code>task = PostgresOperator(\n    task_id='sql_template',\n    postgres_conn_id='my_postgres',\n    sql='''\n        SELECT * FROM users\n        WHERE created_at &gt;= '{{ ds }}'\n    ''',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/03-operators/#operatory-niestandardowe","title":"Operatory niestandardowe","text":""},{"location":"AirFlow/PL/03-operators/#utworzyc-operator-niestandardowy","title":"Utworzy\u0107 operator niestandardowy","text":"<pre><code>from airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass MyCustomOperator(BaseOperator):\n    @apply_defaults\n    def __init__(self, my_param, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.my_param = my_param\n\n    def execute(self, context):\n        print(f\"Executing with param: {self.my_param}\")\n        # Twoja logika tutaj\n        return \"Success\"\n\n# U\u017cycie\ntask = MyCustomOperator(\n    task_id='custom_task',\n    my_param='value',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/03-operators/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>PythonOperator dla logiki Python</li> <li>BashOperator dla polece\u0144 shell</li> <li>Operatory SQL dla zapyta\u0144 SQL</li> <li>XCom do dzielenia danych</li> <li>Szablony dla warto\u015bci dynamicznych</li> </ol>"},{"location":"AirFlow/PL/03-operators/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 4. Sensory, aby nauczy\u0107 si\u0119 u\u017cywa\u0107 sensor\u00f3w.</p>"},{"location":"AirFlow/PL/04-sensors/","title":"4. Sensory Airflow","text":""},{"location":"AirFlow/PL/04-sensors/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 sensory</li> <li>U\u017cywa\u0107 FileSensor, SqlSensor, HttpSensor</li> <li>Tworzy\u0107 sensory niestandardowe</li> <li>Zarz\u0105dza\u0107 timeoutami i ponowieniami</li> </ul>"},{"location":"AirFlow/PL/04-sensors/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do sensor\u00f3w</li> <li>FileSensor</li> <li>SqlSensor</li> <li>HttpSensor</li> <li>Sensory niestandardowe</li> </ol>"},{"location":"AirFlow/PL/04-sensors/#wprowadzenie-do-sensorow","title":"Wprowadzenie do sensor\u00f3w","text":""},{"location":"AirFlow/PL/04-sensors/#czym-jest-sensor","title":"Czym jest Sensor?","text":"<p>Sensor = Zadanie kt\u00f3re czeka na warunek</p> <ul> <li>Polling : Sprawdza okresowo warunek</li> <li>Timeout : Zatrzymuje si\u0119 po okre\u015blonym czasie</li> <li>Poke interval : Interwa\u0142 mi\u0119dzy sprawdzeniami</li> <li>Mode : Resume lub poke</li> </ul>"},{"location":"AirFlow/PL/04-sensors/#typy-sensorow","title":"Typy sensor\u00f3w","text":"<ul> <li>FileSensor : Czeka na plik</li> <li>SqlSensor : Czeka na warunek SQL</li> <li>HttpSensor : Czeka na odpowied\u017a HTTP</li> <li>S3KeySensor : Czeka na klucz S3</li> </ul>"},{"location":"AirFlow/PL/04-sensors/#filesensor","title":"FileSensor","text":""},{"location":"AirFlow/PL/04-sensors/#podstawowe-uzycie","title":"Podstawowe u\u017cycie","text":"<pre><code>from airflow.sensors.filesystem import FileSensor\n\ntask = FileSensor(\n    task_id='wait_for_file',\n    filepath='/path/to/file.csv',\n    poke_interval=30,  # Sprawdza\u0107 co 30 sekund\n    timeout=3600,  # Timeout po 1 godzinie\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/04-sensors/#filesensor-z-wildcards","title":"FileSensor z wildcards","text":"<pre><code>task = FileSensor(\n    task_id='wait_for_files',\n    filepath='/path/to/data/*.csv',\n    poke_interval=60,\n    timeout=7200,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/04-sensors/#sqlsensor","title":"SqlSensor","text":""},{"location":"AirFlow/PL/04-sensors/#podstawowe-uzycie_1","title":"Podstawowe u\u017cycie","text":"<pre><code>from airflow.sensors.sql import SqlSensor\n\ntask = SqlSensor(\n    task_id='wait_for_data',\n    conn_id='my_postgres',\n    sql=\"SELECT COUNT(*) FROM users WHERE status = 'active'\",\n    poke_interval=60,\n    timeout=3600,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/04-sensors/#sqlsensor-z-warunkiem","title":"SqlSensor z warunkiem","text":"<pre><code>task = SqlSensor(\n    task_id='wait_for_count',\n    conn_id='my_postgres',\n    sql=\"SELECT COUNT(*) as count FROM orders WHERE date = '{{ ds }}'\",\n    poke_interval=30,\n    timeout=1800,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/04-sensors/#httpsensor","title":"HttpSensor","text":""},{"location":"AirFlow/PL/04-sensors/#podstawowe-uzycie_2","title":"Podstawowe u\u017cycie","text":"<pre><code>from airflow.sensors.http import HttpSensor\n\ntask = HttpSensor(\n    task_id='wait_for_api',\n    http_conn_id='my_http',\n    endpoint='/api/status',\n    method='GET',\n    poke_interval=30,\n    timeout=3600,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/04-sensors/#httpsensor-z-odpowiedzia","title":"HttpSensor z odpowiedzi\u0105","text":"<pre><code>def check_response(response):\n    return response.status_code == 200\n\ntask = HttpSensor(\n    task_id='wait_for_api_ready',\n    http_conn_id='my_http',\n    endpoint='/api/health',\n    response_check=check_response,\n    poke_interval=60,\n    timeout=7200,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/04-sensors/#sensory-niestandardowe","title":"Sensory niestandardowe","text":""},{"location":"AirFlow/PL/04-sensors/#utworzyc-sensor-niestandardowy","title":"Utworzy\u0107 sensor niestandardowy","text":"<pre><code>from airflow.sensors.base import BaseSensorOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass MyCustomSensor(BaseSensorOperator):\n    @apply_defaults\n    def __init__(self, my_param, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.my_param = my_param\n\n    def poke(self, context):\n        # Sprawdzi\u0107 warunek\n        # Zwr\u00f3ci\u0107 True je\u015bli warunek spe\u0142niony\n        # Zwr\u00f3ci\u0107 False aby kontynuowa\u0107 oczekiwanie\n        return self.check_condition()\n\n    def check_condition(self):\n        # Twoja logika weryfikacji\n        return True  # lub False\n\n# U\u017cycie\ntask = MyCustomSensor(\n    task_id='custom_sensor',\n    my_param='value',\n    poke_interval=30,\n    timeout=3600,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/04-sensors/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Sensory czekaj\u0105 na warunki</li> <li>Polling sprawdza okresowo</li> <li>Timeout ogranicza czas oczekiwania</li> <li>Poke interval definiuje cz\u0119stotliwo\u015b\u0107</li> <li>Mode kontroluje zachowanie</li> </ol>"},{"location":"AirFlow/PL/04-sensors/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 5. Hooki, aby nauczy\u0107 si\u0119 u\u017cywa\u0107 hook\u00f3w.</p>"},{"location":"AirFlow/PL/05-hooks/","title":"5. Hooki Airflow","text":""},{"location":"AirFlow/PL/05-hooks/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 hooki</li> <li>U\u017cywa\u0107 hook\u00f3w baz danych</li> <li>U\u017cywa\u0107 hook\u00f3w chmurowych</li> <li>Tworzy\u0107 hooki niestandardowe</li> </ul>"},{"location":"AirFlow/PL/05-hooks/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do hook\u00f3w</li> <li>Hooki baz danych</li> <li>Hooki chmurowe</li> <li>Hooki HTTP</li> <li>Hooki niestandardowe</li> </ol>"},{"location":"AirFlow/PL/05-hooks/#wprowadzenie-do-hookow","title":"Wprowadzenie do hook\u00f3w","text":""},{"location":"AirFlow/PL/05-hooks/#czym-jest-hook","title":"Czym jest Hook?","text":"<p>Hook = Interfejs do interakcji z systemami zewn\u0119trznymi</p> <ul> <li>Reu\u017cywalny : Mo\u017ce by\u0107 u\u017cywany w wielu zadaniach</li> <li>Zarz\u0105dza po\u0142\u0105czeniami : U\u017cywa po\u0142\u0105cze\u0144 Airflow</li> <li>Abstrakcja : Ukrywa szczeg\u00f3\u0142y implementacji</li> </ul>"},{"location":"AirFlow/PL/05-hooks/#typy-hookow","title":"Typy hook\u00f3w","text":"<ul> <li>Hooki baz danych : PostgreSQL, MySQL, SQLite</li> <li>Hooki chmurowe : AWS, Azure, GCP</li> <li>Hooki HTTP : \u017b\u0105dania HTTP</li> <li>Hooki plik\u00f3w : Operacje na plikach</li> </ul>"},{"location":"AirFlow/PL/05-hooks/#hooki-baz-danych","title":"Hooki baz danych","text":""},{"location":"AirFlow/PL/05-hooks/#postgreshook","title":"PostgresHook","text":"<pre><code>from airflow.hooks.postgres import PostgresHook\n\ndef query_database():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n\n    # Wykona\u0107 zapytanie\n    records = hook.get_records(\"SELECT * FROM users LIMIT 10\")\n\n    # Wykona\u0107 polecenie\n    hook.run(\"INSERT INTO logs VALUES ('test')\")\n\n    # Pobra\u0107 pandas DataFrame\n    df = hook.get_pandas_df(\"SELECT * FROM users\")\n\ntask = PythonOperator(\n    task_id='query_db',\n    python_callable=query_database,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/05-hooks/#mysqlhook","title":"MySqlHook","text":"<pre><code>from airflow.providers.mysql.hooks.mysql import MySqlHook\n\ndef query_mysql():\n    hook = MySqlHook(mysql_conn_id='my_mysql')\n    records = hook.get_records(\"SELECT * FROM orders\")\n\ntask = PythonOperator(\n    task_id='query_mysql',\n    python_callable=query_mysql,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/05-hooks/#hooki-chmurowe","title":"Hooki chmurowe","text":""},{"location":"AirFlow/PL/05-hooks/#s3hook-aws","title":"S3Hook (AWS)","text":"<pre><code>from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n\ndef upload_to_s3():\n    hook = S3Hook(aws_conn_id='my_aws')\n\n    # Przes\u0142a\u0107 plik\n    hook.load_file(\n        filename='/local/path/file.csv',\n        key='s3/path/file.csv',\n        bucket_name='my-bucket',\n    )\n\n    # Pobra\u0107 plik\n    hook.download_file(\n        key='s3/path/file.csv',\n        bucket_name='my-bucket',\n        local_path='/local/path/file.csv',\n    )\n\ntask = PythonOperator(\n    task_id='s3_upload',\n    python_callable=upload_to_s3,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/05-hooks/#azure-blob-storage-hook","title":"Azure Blob Storage Hook","text":"<pre><code>from airflow.providers.microsoft.azure.hooks.wasb import WasbHook\n\ndef upload_to_azure():\n    hook = WasbHook(wasb_conn_id='my_azure')\n\n    # Przes\u0142a\u0107 plik\n    hook.upload(\n        container_name='my-container',\n        blob_name='file.csv',\n        file_path='/local/path/file.csv',\n    )\n\ntask = PythonOperator(\n    task_id='azure_upload',\n    python_callable=upload_to_azure,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/05-hooks/#hooki-http","title":"Hooki HTTP","text":""},{"location":"AirFlow/PL/05-hooks/#httphook","title":"HttpHook","text":"<pre><code>from airflow.providers.http.hooks.http import HttpHook\n\ndef call_api():\n    hook = HttpHook(http_conn_id='my_api', method='GET')\n\n    # Wykona\u0107 \u017c\u0105danie GET\n    response = hook.run(endpoint='/api/data')\n    print(response.json())\n\n    # Wykona\u0107 \u017c\u0105danie POST\n    hook = HttpHook(http_conn_id='my_api', method='POST')\n    response = hook.run(\n        endpoint='/api/data',\n        data={'key': 'value'},\n    )\n\ntask = PythonOperator(\n    task_id='call_api',\n    python_callable=call_api,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/05-hooks/#hooki-niestandardowe","title":"Hooki niestandardowe","text":""},{"location":"AirFlow/PL/05-hooks/#utworzyc-hook-niestandardowy","title":"Utworzy\u0107 hook niestandardowy","text":"<pre><code>from airflow.hooks.base import BaseHook\n\nclass MyCustomHook(BaseHook):\n    def __init__(self, my_conn_id):\n        super().__init__()\n        self.conn_id = my_conn_id\n        self.conn = self.get_connection(my_conn_id)\n\n    def do_something(self):\n        # Twoja logika\n        print(f\"Connecting to {self.conn.host}\")\n        return \"Success\"\n\n# U\u017cycie\ndef use_custom_hook():\n    hook = MyCustomHook(my_conn_id='my_connection')\n    result = hook.do_something()\n\ntask = PythonOperator(\n    task_id='use_hook',\n    python_callable=use_custom_hook,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/05-hooks/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Hooki abstrahuj\u0105 po\u0142\u0105czenia</li> <li>Reu\u017cywalne w wielu zadaniach</li> <li>Zarz\u0105dzaj\u0105 po\u0142\u0105czeniami przez Airflow</li> <li>Wspieraj\u0105 bazy danych, chmur\u0119, HTTP</li> <li>Rozszerzalne z hookami niestandardowymi</li> </ol>"},{"location":"AirFlow/PL/05-hooks/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 6. Zmienne i Po\u0142\u0105czenia, aby nauczy\u0107 si\u0119 zarz\u0105dza\u0107 konfiguracj\u0105.</p>"},{"location":"AirFlow/PL/06-variables-connections/","title":"6. Zmienne i Po\u0142\u0105czenia","text":""},{"location":"AirFlow/PL/06-variables-connections/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zarz\u0105dza\u0107 zmiennymi Airflow</li> <li>Konfigurowa\u0107 po\u0142\u0105czenia</li> <li>Zabezpiecza\u0107 credentials</li> <li>U\u017cywa\u0107 zmiennych dynamicznych</li> </ul>"},{"location":"AirFlow/PL/06-variables-connections/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Zmienne</li> <li>Po\u0142\u0105czenia</li> <li>Bezpiecze\u0144stwo</li> <li>Dobre praktyki</li> </ol>"},{"location":"AirFlow/PL/06-variables-connections/#zmienne","title":"Zmienne","text":""},{"location":"AirFlow/PL/06-variables-connections/#tworzyc-zmienne","title":"Tworzy\u0107 zmienne","text":"<p>Przez CLI :</p> <pre><code># Utworzy\u0107 zmienn\u0105\nairflow variables set my_key \"my_value\"\n\n# Utworzy\u0107 zmienn\u0105 JSON\nairflow variables set my_config '{\"key\": \"value\"}'\n\n# Usun\u0105\u0107 zmienn\u0105\nairflow variables delete my_key\n\n# Listowa\u0107 zmienne\nairflow variables list\n</code></pre> <p>Przez interfejs web : 1. Admin \u2192 Variables 2. Klikn\u0105\u0107 \"+\" 3. Wprowadzi\u0107 Key i Value 4. Zapisa\u0107</p>"},{"location":"AirFlow/PL/06-variables-connections/#uzywac-zmiennych","title":"U\u017cywa\u0107 zmiennych","text":"<pre><code>from airflow.models import Variable\n\n# Pobra\u0107 zmienn\u0105\nmy_value = Variable.get(\"my_key\")\n\n# Z warto\u015bci\u0105 domy\u015bln\u0105\nmy_value = Variable.get(\"my_key\", default_var=\"default\")\n\n# Zmienna JSON\nconfig = Variable.get(\"my_config\", deserialize_json=True)\nprint(config['key'])\n</code></pre>"},{"location":"AirFlow/PL/06-variables-connections/#zmienne-w-szablonach","title":"Zmienne w szablonach","text":"<pre><code>from airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='use_var',\n    bash_command='echo \"Value: {{ var.value.my_key }}\"',\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/06-variables-connections/#poaczenia","title":"Po\u0142\u0105czenia","text":""},{"location":"AirFlow/PL/06-variables-connections/#utworzyc-poaczenie","title":"Utworzy\u0107 po\u0142\u0105czenie","text":"<p>Przez CLI :</p> <pre><code># PostgreSQL\nairflow connections add 'my_postgres' \\\n    --conn-type 'postgres' \\\n    --conn-host 'localhost' \\\n    --conn-login 'user' \\\n    --conn-password 'password' \\\n    --conn-port 5432 \\\n    --conn-schema 'mydb'\n\n# MySQL\nairflow connections add 'my_mysql' \\\n    --conn-type 'mysql' \\\n    --conn-host 'localhost' \\\n    --conn-login 'user' \\\n    --conn-password 'password' \\\n    --conn-port 3306\n\n# HTTP\nairflow connections add 'my_api' \\\n    --conn-type 'http' \\\n    --conn-host 'https://api.example.com'\n</code></pre> <p>Przez interfejs web : 1. Admin \u2192 Connections 2. Klikn\u0105\u0107 \"+\" 3. Wype\u0142ni\u0107 pola 4. Zapisa\u0107</p>"},{"location":"AirFlow/PL/06-variables-connections/#uzywac-poaczenia","title":"U\u017cywa\u0107 po\u0142\u0105czenia","text":"<pre><code>from airflow.hooks.base import BaseHook\n\n# Pobra\u0107 po\u0142\u0105czenie\nconn = BaseHook.get_connection('my_postgres')\nprint(f\"Host: {conn.host}\")\nprint(f\"Login: {conn.login}\")\nprint(f\"Password: {conn.password}\")\n</code></pre>"},{"location":"AirFlow/PL/06-variables-connections/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":""},{"location":"AirFlow/PL/06-variables-connections/#ukrywac-hasa","title":"Ukrywa\u0107 has\u0142a","text":"<p>U\u017cywa\u0107 po\u0142\u0105cze\u0144 : - Has\u0142a s\u0105 szyfrowane w bazie - Nigdy nie hardkodowa\u0107 credentials</p> <p>U\u017cywa\u0107 zmiennych : - Dla sekret\u00f3w wra\u017cliwych - U\u017cywa\u0107 narz\u0119dzi zarz\u0105dzania sekretami (Vault, itp.)</p>"},{"location":"AirFlow/PL/06-variables-connections/#dobre-praktyki","title":"Dobre praktyki","text":"<ol> <li>Nigdy nie committowa\u0107 credentials</li> <li>U\u017cywa\u0107 po\u0142\u0105cze\u0144 dla dost\u0119pu</li> <li>U\u017cywa\u0107 zmiennych dla konfiguracji</li> <li>Szyfrowa\u0107 dane wra\u017cliwe</li> <li>Ogranicza\u0107 dost\u0119p do po\u0142\u0105cze\u0144</li> </ol>"},{"location":"AirFlow/PL/06-variables-connections/#dobre-praktyki_1","title":"Dobre praktyki","text":""},{"location":"AirFlow/PL/06-variables-connections/#organizacja-zmiennych","title":"Organizacja zmiennych","text":"<ul> <li>Prefiksy : <code>project_name_key</code></li> <li>Grupy : <code>db_</code>, <code>api_</code>, <code>s3_</code></li> <li>Dokumentacja : Dokumentowa\u0107 u\u017cycie</li> </ul>"},{"location":"AirFlow/PL/06-variables-connections/#organizacja-poaczen","title":"Organizacja po\u0142\u0105cze\u0144","text":"<ul> <li>Jasne nazwy : <code>postgres_prod</code>, <code>postgres_dev</code></li> <li>Prawid\u0142owe typy : U\u017cywa\u0107 prawid\u0142owego typu po\u0142\u0105czenia</li> <li>Testy : Testowa\u0107 po\u0142\u0105czenia regularnie</li> </ul>"},{"location":"AirFlow/PL/06-variables-connections/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Zmienne dla konfiguracji</li> <li>Po\u0142\u0105czenia dla dost\u0119pu</li> <li>Bezpiecze\u0144stwo : Nigdy nie hardkodowa\u0107</li> <li>Organizacja : Prefiksy i grupy</li> <li>Dokumentacja : Dokumentowa\u0107 u\u017cycie</li> </ol>"},{"location":"AirFlow/PL/06-variables-connections/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 7. Dobre praktyki, aby nauczy\u0107 si\u0119 najlepszych praktyk.</p>"},{"location":"AirFlow/PL/07-best-practices/","title":"7. Dobre praktyki Airflow","text":""},{"location":"AirFlow/PL/07-best-practices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Strukturyzowa\u0107 DAGi efektywnie</li> <li>Obs\u0142ugiwa\u0107 b\u0142\u0119dy</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> <li>Testowa\u0107 i debugowa\u0107</li> </ul>"},{"location":"AirFlow/PL/07-best-practices/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Struktura DAG\u00f3w</li> <li>Obs\u0142uga b\u0142\u0119d\u00f3w</li> <li>Wydajno\u015b\u0107</li> <li>Testy</li> <li>Debugowanie</li> </ol>"},{"location":"AirFlow/PL/07-best-practices/#struktura-dagow","title":"Struktura DAG\u00f3w","text":""},{"location":"AirFlow/PL/07-best-practices/#organizacja-plikow","title":"Organizacja plik\u00f3w","text":"<pre><code>airflow/\n\u251c\u2500\u2500 dags/\n\u2502   \u251c\u2500\u2500 etl/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 extract.py\n\u2502   \u2502   \u251c\u2500\u2500 transform.py\n\u2502   \u2502   \u2514\u2500\u2500 load.py\n\u2502   \u2514\u2500\u2500 analytics/\n\u2502       \u2514\u2500\u2500 reports.py\n\u251c\u2500\u2500 plugins/\n\u2502   \u2514\u2500\u2500 custom_operators.py\n\u2514\u2500\u2500 config/\n    \u2514\u2500\u2500 settings.py\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#dobre-praktyki-kodu","title":"Dobre praktyki kodu","text":"<p>1. Zorganizowane importy :</p> <pre><code># Standardowa biblioteka\nfrom datetime import datetime, timedelta\n\n# Zewn\u0119trzne\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\n# Lokalne\nfrom utils.helpers import process_data\n</code></pre> <p>2. Funkcje reu\u017cywalne :</p> <pre><code># utils/helpers.py\ndef extract_data(source):\n    # Logika ekstrakcji\n    return data\n\ndef transform_data(data):\n    # Logika transformacji\n    return transformed_data\n\n# dags/etl_pipeline.py\nfrom utils.helpers import extract_data, transform_data\n\nextract_task = PythonOperator(\n    task_id='extract',\n    python_callable=extract_data,\n    op_args=['source'],\n    dag=dag,\n)\n</code></pre> <p>3. Konfiguracja scentralizowana :</p> <pre><code># config/settings.py\nDEFAULT_ARGS = {\n    'owner': 'data_team',\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# dags/my_dag.py\nfrom config.settings import DEFAULT_ARGS\n\ndag = DAG('my_dag', default_args=DEFAULT_ARGS)\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#obsuga-bedow","title":"Obs\u0142uga b\u0142\u0119d\u00f3w","text":""},{"location":"AirFlow/PL/07-best-practices/#retry-i-backoff","title":"Retry i backoff","text":"<pre><code>default_args = {\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'retry_exponential_backoff': True,\n    'max_retry_delay': timedelta(hours=1),\n}\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#obsuga-wyjatkow","title":"Obs\u0142uga wyj\u0105tk\u00f3w","text":"<pre><code>def process_with_error_handling():\n    try:\n        # Kod kt\u00f3ry mo\u017ce si\u0119 nie powie\u015b\u0107\n        result = risky_operation()\n        return result\n    except SpecificError as e:\n        # Obs\u0142u\u017cy\u0107 b\u0142\u0105d specyficzny\n        logger.error(f\"Error: {e}\")\n        raise\n    except Exception as e:\n        # Obs\u0142u\u017cy\u0107 inne b\u0142\u0119dy\n        logger.error(f\"Unexpected error: {e}\")\n        raise\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#callbacki","title":"Callbacki","text":"<pre><code>def on_failure_callback(context):\n    logger.error(\"Task failed!\")\n    # Wys\u0142a\u0107 alert\n    send_alert(context)\n\ndef on_success_callback(context):\n    logger.info(\"Task succeeded!\")\n\ntask = PythonOperator(\n    task_id='task',\n    python_callable=my_function,\n    on_failure_callback=on_failure_callback,\n    on_success_callback=on_success_callback,\n    dag=dag,\n)\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#wydajnosc","title":"Wydajno\u015b\u0107","text":""},{"location":"AirFlow/PL/07-best-practices/#optymalizowac-dagi","title":"Optymalizowa\u0107 DAGi","text":"<p>1. Unika\u0107 niepotrzebnych zale\u017cno\u015bci :</p> <pre><code># Z\u0142e : niepotrzebne zale\u017cno\u015bci sekwencyjne\ntask1 &gt;&gt; task2 &gt;&gt; task3 &gt;&gt; task4\n\n# Dobre : r\u00f3wnoleg\u0142o\u015b\u0107 gdy mo\u017cliwe\n[task1, task2] &gt;&gt; task3 &gt;&gt; task4\n</code></pre> <p>2. U\u017cywa\u0107 trybu reschedule dla sensor\u00f3w :</p> <pre><code>sensor = FileSensor(\n    task_id='wait_file',\n    filepath='/path/to/file',\n    mode='reschedule',  # Uwalnia slot worker\n    poke_interval=60,\n    dag=dag,\n)\n</code></pre> <p>3. Ogranicza\u0107 r\u00f3wnoleg\u0142o\u015b\u0107 :</p> <pre><code>dag = DAG(\n    'my_dag',\n    max_active_runs=1,  # Tylko jeden run na raz\n    max_active_tasks=10,  # Maksimum 10 zada\u0144 r\u00f3wnolegle\n)\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#testy","title":"Testy","text":""},{"location":"AirFlow/PL/07-best-practices/#testy-jednostkowe","title":"Testy jednostkowe","text":"<pre><code># tests/test_dag.py\nimport pytest\nfrom airflow.models import DagBag\n\ndef test_dag_loaded():\n    dagbag = DagBag()\n    dag = dagbag.get_dag(dag_id='my_dag')\n    assert dag is not None\n    assert len(dag.tasks) == 3\n\ndef test_dag_structure():\n    dagbag = DagBag()\n    dag = dagbag.get_dag(dag_id='my_dag')\n    assert 'extract' in dag.task_ids\n    assert 'transform' in dag.task_ids\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#testy-integracyjne","title":"Testy integracyjne","text":"<pre><code>from airflow.operators.python import PythonOperator\n\ndef test_task_execution():\n    task = PythonOperator(\n        task_id='test_task',\n        python_callable=lambda: \"success\",\n    )\n    result = task.execute({})\n    assert result == \"success\"\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#debugowanie","title":"Debugowanie","text":""},{"location":"AirFlow/PL/07-best-practices/#logi","title":"Logi","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef my_function():\n    logger.info(\"Starting process\")\n    logger.debug(\"Debug information\")\n    logger.error(\"Error occurred\")\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#sprawdzac-logi","title":"Sprawdza\u0107 logi","text":"<pre><code># Logi zadania\nairflow tasks logs my_dag my_task 2024-01-01\n\n# Logi scheduler\ntail -f ~/airflow/logs/scheduler/*.log\n</code></pre>"},{"location":"AirFlow/PL/07-best-practices/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Struktura : Organizowa\u0107 kod czysto</li> <li>B\u0142\u0119dy : Obs\u0142ugiwa\u0107 z retry i callbackami</li> <li>Wydajno\u015b\u0107 : Optymalizowa\u0107 r\u00f3wnoleg\u0142o\u015b\u0107</li> <li>Testy : Testowa\u0107 DAGi</li> <li>Logi : U\u017cywa\u0107 logowania efektywnie</li> </ol>"},{"location":"AirFlow/PL/07-best-practices/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 8. Projekty praktyczne, aby tworzy\u0107 kompletne projekty.</p>"},{"location":"AirFlow/PL/08-projets/","title":"8. Projekty praktyczne Airflow","text":""},{"location":"AirFlow/PL/08-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 kompletny pipeline ETL</li> <li>Orkiestrowa\u0107 z\u0142o\u017cone workflows</li> <li>Integrowa\u0107 z bazami danych</li> <li>Tworzy\u0107 projekty do portfolio</li> </ul>"},{"location":"AirFlow/PL/08-projets/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Projekt 1 : Prosty pipeline ETL</li> <li>Projekt 2 : Orkiestracja workflows</li> <li>Projekt 3 : Integracja z bazami danych</li> <li>Projekt 4 : Kompletny pipeline</li> </ol>"},{"location":"AirFlow/PL/08-projets/#projekt-1-prosty-pipeline-etl","title":"Projekt 1 : Prosty pipeline ETL","text":""},{"location":"AirFlow/PL/08-projets/#cel","title":"Cel","text":"<p>Utworzy\u0107 pipeline ETL kt\u00f3ry ekstrahuje, przekszta\u0142ca i \u0142aduje dane.</p>"},{"location":"AirFlow/PL/08-projets/#architektura","title":"Architektura","text":"<pre><code>Extract (API) \u2192 Transform (Python) \u2192 Load (Plik)\n</code></pre>"},{"location":"AirFlow/PL/08-projets/#kod","title":"Kod","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\nimport requests\nimport pandas as pd\n\ndefault_args = {\n    'owner': 'data_analyst',\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'etl_pipeline',\n    default_args=default_args,\n    description='Prosty pipeline ETL',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\ndef extract_data(**context):\n    # Ekstrahowa\u0107 dane z API\n    response = requests.get('https://api.example.com/data')\n    data = response.json()\n\n    # Zapisa\u0107 tymczasowo\n    df = pd.DataFrame(data)\n    df.to_csv('/tmp/raw_data.csv', index=False)\n\n    return '/tmp/raw_data.csv'\n\ndef transform_data(**context):\n    # Pobra\u0107 plik z poprzedniego zadania\n    ti = context['ti']\n    input_file = ti.xcom_pull(task_ids='extract')\n\n    # Czyta\u0107 i przekszta\u0142ca\u0107\n    df = pd.read_csv(input_file)\n    df['processed_at'] = datetime.now()\n    df = df.dropna()\n\n    # Zapisa\u0107\n    output_file = '/tmp/transformed_data.csv'\n    df.to_csv(output_file, index=False)\n\n    return output_file\n\ndef load_data(**context):\n    # Pobra\u0107 przekszta\u0142cony plik\n    ti = context['ti']\n    input_file = ti.xcom_pull(task_ids='transform')\n\n    # Za\u0142adowa\u0107 do miejsca docelowego (np. baza danych)\n    df = pd.read_csv(input_file)\n    print(f\"Loaded {len(df)} rows\")\n\n    # Tutaj, mo\u017cesz za\u0142adowa\u0107 do bazy danych\n    # df.to_sql('table', con=engine, if_exists='append')\n\nextract = PythonOperator(\n    task_id='extract',\n    python_callable=extract_data,\n    dag=dag,\n)\n\ntransform = PythonOperator(\n    task_id='transform',\n    python_callable=transform_data,\n    dag=dag,\n)\n\nload = PythonOperator(\n    task_id='load',\n    python_callable=load_data,\n    dag=dag,\n)\n\nextract &gt;&gt; transform &gt;&gt; load\n</code></pre>"},{"location":"AirFlow/PL/08-projets/#projekt-2-orkiestracja-workflows","title":"Projekt 2 : Orkiestracja workflows","text":""},{"location":"AirFlow/PL/08-projets/#cel_1","title":"Cel","text":"<p>Orkiestrowa\u0107 wiele workflows z z\u0142o\u017conymi zale\u017cno\u015bciami.</p>"},{"location":"AirFlow/PL/08-projets/#architektura_1","title":"Architektura","text":"<pre><code>Zbieranie danych \u2192 Walidacja \u2192 Przetwarzanie \u2192 Raportowanie\n                      \u2193\n                  Alertowanie\n</code></pre>"},{"location":"AirFlow/PL/08-projets/#kod_1","title":"Kod","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.branch import BranchPythonOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'workflow_orchestration',\n    description='Orkiestracja workflows',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\ndef collect_data():\n    print(\"Collecting data...\")\n    return \"success\"\n\ndef validate_data(**context):\n    data = context['ti'].xcom_pull(task_ids='collect')\n    if data == \"success\":\n        return \"valid\"\n    return \"invalid\"\n\ndef process_valid():\n    print(\"Processing valid data...\")\n\ndef process_invalid():\n    print(\"Alerting: Invalid data!\")\n\ndef generate_report():\n    print(\"Generating report...\")\n\ncollect = PythonOperator(\n    task_id='collect',\n    python_callable=collect_data,\n    dag=dag,\n)\n\nvalidate = BranchPythonOperator(\n    task_id='validate',\n    python_callable=validate_data,\n    dag=dag,\n)\n\nprocess_valid_task = PythonOperator(\n    task_id='process_valid',\n    python_callable=process_valid,\n    dag=dag,\n)\n\nprocess_invalid_task = PythonOperator(\n    task_id='process_invalid',\n    python_callable=process_invalid,\n    dag=dag,\n)\n\nreport = PythonOperator(\n    task_id='report',\n    python_callable=generate_report,\n    dag=dag,\n)\n\ncollect &gt;&gt; validate &gt;&gt; [process_valid_task, process_invalid_task] &gt;&gt; report\n</code></pre>"},{"location":"AirFlow/PL/08-projets/#projekt-3-integracja-z-bazami-danych","title":"Projekt 3 : Integracja z bazami danych","text":""},{"location":"AirFlow/PL/08-projets/#cel_2","title":"Cel","text":"<p>Integrowa\u0107 Airflow z PostgreSQL dla pipeline'u danych.</p>"},{"location":"AirFlow/PL/08-projets/#kod_2","title":"Kod","text":"<pre><code>from airflow import DAG\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'database_pipeline',\n    description='Pipeline z baz\u0105 danych',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n)\n\n# Utworzy\u0107 tabel\u0119\ncreate_table = PostgresOperator(\n    task_id='create_table',\n    postgres_conn_id='my_postgres',\n    sql='''\n        CREATE TABLE IF NOT EXISTS users_processed (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100),\n            email VARCHAR(100),\n            created_at TIMESTAMP\n        );\n    ''',\n    dag=dag,\n)\n\n# Wstawi\u0107 dane\ndef insert_data():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    hook.run(\"\"\"\n        INSERT INTO users_processed (name, email, created_at)\n        VALUES ('John Doe', 'john@example.com', NOW());\n    \"\"\")\n\ninsert = PythonOperator(\n    task_id='insert',\n    python_callable=insert_data,\n    dag=dag,\n)\n\n# Zapytanie analityczne\nanalytics_query = PostgresOperator(\n    task_id='analytics',\n    postgres_conn_id='my_postgres',\n    sql='''\n        SELECT \n            DATE(created_at) as date,\n            COUNT(*) as user_count\n        FROM users_processed\n        GROUP BY DATE(created_at);\n    ''',\n    dag=dag,\n)\n\ncreate_table &gt;&gt; insert &gt;&gt; analytics_query\n</code></pre>"},{"location":"AirFlow/PL/08-projets/#projekt-4-kompletny-pipeline","title":"Projekt 4 : Kompletny pipeline","text":""},{"location":"AirFlow/PL/08-projets/#cel_3","title":"Cel","text":"<p>Utworzy\u0107 kompletny pipeline ETL z wieloma \u017ar\u00f3d\u0142ami i miejscami docelowymi.</p>"},{"location":"AirFlow/PL/08-projets/#architektura_2","title":"Architektura","text":"<pre><code>API \u2192 Walidacja \u2192 Przekszta\u0142canie \u2192 Baza danych \u2192 Raportowanie\n  \u2193\nPlik \u2192 Walidacja \u2192 Przekszta\u0142canie \u2192 Baza danych\n</code></pre>"},{"location":"AirFlow/PL/08-projets/#kod_3","title":"Kod","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport requests\n\ndag = DAG(\n    'complete_pipeline',\n    description='Kompletny pipeline ETL',\n    schedule_interval='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    default_args={\n        'retries': 2,\n        'retry_delay': timedelta(minutes=5),\n    },\n)\n\n# Czeka\u0107 na plik\nwait_file = FileSensor(\n    task_id='wait_file',\n    filepath='/data/input/file.csv',\n    poke_interval=60,\n    timeout=3600,\n    dag=dag,\n)\n\n# Ekstrahowa\u0107 z API\ndef extract_api():\n    response = requests.get('https://api.example.com/data')\n    data = response.json()\n    df = pd.DataFrame(data)\n    df.to_csv('/tmp/api_data.csv', index=False)\n    return '/tmp/api_data.csv'\n\nextract_api_task = PythonOperator(\n    task_id='extract_api',\n    python_callable=extract_api,\n    dag=dag,\n)\n\n# Przekszta\u0142ci\u0107\ndef transform(**context):\n    # Plik API\n    api_file = context['ti'].xcom_pull(task_ids='extract_api')\n    df_api = pd.read_csv(api_file)\n\n    # Plik lokalny\n    df_file = pd.read_csv('/data/input/file.csv')\n\n    # Po\u0142\u0105czy\u0107 i przekszta\u0142ci\u0107\n    df_combined = pd.concat([df_api, df_file])\n    df_combined = df_combined.dropna()\n    df_combined['processed_at'] = datetime.now()\n\n    # Zapisa\u0107\n    df_combined.to_csv('/tmp/transformed.csv', index=False)\n    return '/tmp/transformed.csv'\n\ntransform_task = PythonOperator(\n    task_id='transform',\n    python_callable=transform,\n    dag=dag,\n)\n\n# Za\u0142adowa\u0107 do bazy danych\nload_db = PostgresOperator(\n    task_id='load_db',\n    postgres_conn_id='my_postgres',\n    sql='''\n        COPY users FROM '/tmp/transformed.csv' \n        WITH (FORMAT csv, HEADER true);\n    ''',\n    dag=dag,\n)\n\n# Wygenerowa\u0107 raport\ndef generate_report():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    df = hook.get_pandas_df(\"SELECT * FROM users\")\n    report = df.describe()\n    report.to_csv('/tmp/report.csv')\n    print(\"Report generated!\")\n\nreport_task = PythonOperator(\n    task_id='report',\n    python_callable=generate_report,\n    dag=dag,\n)\n\n# Zale\u017cno\u015bci\n[wait_file, extract_api_task] &gt;&gt; transform_task &gt;&gt; load_db &gt;&gt; report_task\n</code></pre>"},{"location":"AirFlow/PL/08-projets/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>ETL : Extract, Transform, Load</li> <li>Orkiestracja : Koordynowa\u0107 wiele workflows</li> <li>Integracja : Bazy danych, API, pliki</li> <li>Monitorowanie : Monitorowa\u0107 wykonania</li> <li>Portfolio : Tworzy\u0107 projekty demonstrowalne</li> </ol>"},{"location":"AirFlow/PL/08-projets/#zasoby","title":"\ud83d\udd17 Zasoby","text":"<ul> <li>Przyk\u0142ady Airflow</li> <li>Dokumentacja Airflow</li> </ul> <p>Gratulacje ! Uko\u0144czy\u0142e\u015b szkolenie Airflow. Mo\u017cesz teraz tworzy\u0107 z\u0142o\u017cone pipeline'y danych z Airflow.</p>"},{"location":"Clickhouse/Data/EN/","title":"ClickHouse Training for Data Analyst","text":""},{"location":"Clickhouse/Data/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through learning ClickHouse as a Data Analyst. ClickHouse is a column-oriented OLAP (Online Analytical Processing) database, optimized for analytical queries on very large volumes of data.</p>"},{"location":"Clickhouse/Data/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand ClickHouse and OLAP analysis</li> <li>Install and configure ClickHouse</li> <li>Master analytical SQL with ClickHouse</li> <li>Optimize queries for large volumes</li> <li>Use advanced analytical functions</li> <li>Manage partitions and compression</li> <li>Integrate ClickHouse into your analysis workflows</li> <li>Create practical projects for your portfolio</li> </ul>"},{"location":"Clickhouse/Data/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 ClickHouse Community Edition : Free and open-source - \u2705 ClickHouse Client : Free command-line tool - \u2705 ClickHouse Playground : Free online environment - \u2705 Official documentation : Complete free guides</p> <p>Total budget: $0</p>"},{"location":"Clickhouse/Data/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"Clickhouse/Data/EN/#1-getting-started-with-clickhouse","title":"1. Getting Started with ClickHouse","text":""},{"location":"Clickhouse/Data/EN/#2-analytical-sql-with-clickhouse","title":"2. Analytical SQL with ClickHouse","text":""},{"location":"Clickhouse/Data/EN/#3-data-types-and-tables","title":"3. Data Types and Tables","text":""},{"location":"Clickhouse/Data/EN/#4-performance-and-optimization","title":"4. Performance and Optimization","text":""},{"location":"Clickhouse/Data/EN/#5-advanced-functions","title":"5. Advanced Functions","text":""},{"location":"Clickhouse/Data/EN/#6-integration-and-etl","title":"6. Integration and ETL","text":""},{"location":"Clickhouse/Data/EN/#7-best-practices","title":"7. Best Practices","text":""},{"location":"Clickhouse/Data/EN/#8-practical-projects","title":"8. Practical Projects","text":""},{"location":"Clickhouse/Data/EN/#prerequisites","title":"\ud83d\ude80 Prerequisites","text":"<ul> <li>Basic SQL knowledge</li> <li>Database concepts</li> <li>Python (optional)</li> </ul>"},{"location":"Clickhouse/Data/EN/#estimated-duration","title":"\u23f1\ufe0f Estimated Duration","text":"<ul> <li>Total : 40-50 hours</li> <li>Per module : 5-6 hours</li> </ul> <p>Happy learning! \ud83d\ude80</p>"},{"location":"Clickhouse/Data/EN/01-getting-started/","title":"1. Getting Started with ClickHouse","text":""},{"location":"Clickhouse/Data/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand ClickHouse and OLAP</li> <li>Install ClickHouse</li> <li>Understand basic concepts</li> <li>Use ClickHouse Client</li> <li>First queries</li> </ul>"},{"location":"Clickhouse/Data/EN/01-getting-started/#introduction-to-clickhouse","title":"Introduction to ClickHouse","text":"<p>ClickHouse = Column-oriented OLAP database</p> <ul> <li>OLAP : Online Analytical Processing</li> <li>Column-oriented : Column storage</li> <li>High performance : Very fast on large volumes</li> <li>Open-source : Free and open-source</li> </ul>"},{"location":"Clickhouse/Data/EN/01-getting-started/#why-clickhouse-for-data-analyst","title":"Why ClickHouse for Data Analyst?","text":"<ul> <li>Large volumes : Handles billions of rows</li> <li>Performance : Ultra-fast analytical queries</li> <li>Compression : Efficient compression</li> <li>SQL : Standard SQL language</li> </ul>"},{"location":"Clickhouse/Data/EN/01-getting-started/#typical-use-cases","title":"Typical Use Cases","text":"<ul> <li>Web analytics (logs, events)</li> <li>Data warehousing</li> <li>Business Intelligence</li> <li>IoT (sensors)</li> </ul>"},{"location":"Clickhouse/Data/EN/01-getting-started/#installation","title":"Installation","text":""},{"location":"Clickhouse/Data/EN/01-getting-started/#option-1-clickhouse-playground-recommended","title":"Option 1 : ClickHouse Playground (recommended)","text":"<ol> <li>Go to ClickHouse Playground</li> <li>No installation needed</li> </ol>"},{"location":"Clickhouse/Data/EN/01-getting-started/#option-2-docker","title":"Option 2 : Docker","text":"<pre><code>docker run -d --name clickhouse-server -p 8123:8123 -p 9000:9000 clickhouse/clickhouse-server\n</code></pre>"},{"location":"Clickhouse/Data/EN/01-getting-started/#option-3-local-installation-linux","title":"Option 3 : Local Installation (Linux)","text":"<pre><code>sudo apt-get install -y apt-transport-https ca-certificates\necho \"deb https://repo.clickhouse.com/deb stable main\" | sudo tee /etc/apt/sources.list.d/clickhouse.list\nsudo apt-get update\nsudo apt-get install -y clickhouse-server clickhouse-client\nsudo service clickhouse-server start\n</code></pre>"},{"location":"Clickhouse/Data/EN/01-getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"Clickhouse/Data/EN/01-getting-started/#column-storage","title":"Column Storage","text":"<p>Advantages : - Efficient compression - Fast analytical queries - Optimized aggregations</p>"},{"location":"Clickhouse/Data/EN/01-getting-started/#table-engines","title":"Table Engines","text":"<ul> <li>MergeTree : For persistent data</li> <li>Memory : For in-memory data</li> <li>Log : For small volumes</li> </ul>"},{"location":"Clickhouse/Data/EN/01-getting-started/#clickhouse-client","title":"ClickHouse Client","text":"<pre><code>clickhouse-client\nclickhouse-client --host localhost --port 9000\n</code></pre>"},{"location":"Clickhouse/Data/EN/01-getting-started/#basic-commands","title":"Basic Commands","text":"<pre><code>SHOW DATABASES;\nUSE default;\nSHOW TABLES;\nDESCRIBE table_name;\n</code></pre>"},{"location":"Clickhouse/Data/EN/01-getting-started/#first-queries","title":"First Queries","text":""},{"location":"Clickhouse/Data/EN/01-getting-started/#create-database","title":"Create Database","text":"<pre><code>CREATE DATABASE IF NOT EXISTS analytics;\nUSE analytics;\n</code></pre>"},{"location":"Clickhouse/Data/EN/01-getting-started/#create-table","title":"Create Table","text":"<pre><code>CREATE TABLE IF NOT EXISTS events\n(\n    id UInt64,\n    event_date Date,\n    event_time DateTime,\n    user_id UInt32,\n    event_type String,\n    value Float64\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(event_date)\nORDER BY (event_date, event_time);\n</code></pre>"},{"location":"Clickhouse/Data/EN/01-getting-started/#insert-data","title":"Insert Data","text":"<pre><code>INSERT INTO events VALUES\n(1, '2024-01-15', '2024-01-15 10:00:00', 100, 'click', 1.5),\n(2, '2024-01-15', '2024-01-15 10:01:00', 101, 'view', 2.0);\n</code></pre>"},{"location":"Clickhouse/Data/EN/01-getting-started/#select-query","title":"SELECT Query","text":"<pre><code>SELECT * FROM events;\n\nSELECT \n    event_type,\n    COUNT(*) as count,\n    SUM(value) as total_value\nFROM events\nWHERE event_date = '2024-01-15'\nGROUP BY event_type;\n</code></pre> <p>Next step : Analytical SQL with ClickHouse</p>"},{"location":"Clickhouse/Data/EN/02-sql-analytique/","title":"2. Analytical SQL with ClickHouse","text":""},{"location":"Clickhouse/Data/EN/02-sql-analytique/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Master advanced SELECT queries</li> <li>Use aggregations and GROUP BY</li> <li>Understand analytical functions</li> <li>Use Window Functions</li> </ul>"},{"location":"Clickhouse/Data/EN/02-sql-analytique/#advanced-aggregations","title":"Advanced Aggregations","text":""},{"location":"Clickhouse/Data/EN/02-sql-analytique/#group-by-with-multiple-columns","title":"GROUP BY with multiple columns","text":"<pre><code>SELECT \n    event_date,\n    event_type,\n    COUNT(*) as count,\n    SUM(value) as total_value,\n    AVG(value) as avg_value\nFROM events\nGROUP BY event_date, event_type\nORDER BY event_date, event_type;\n</code></pre>"},{"location":"Clickhouse/Data/EN/02-sql-analytique/#having-to-filter-aggregations","title":"HAVING to filter aggregations","text":"<pre><code>SELECT \n    event_type,\n    COUNT(*) as count\nFROM events\nGROUP BY event_type\nHAVING count &gt; 10;\n</code></pre>"},{"location":"Clickhouse/Data/EN/02-sql-analytique/#aggregation-functions","title":"Aggregation Functions","text":""},{"location":"Clickhouse/Data/EN/02-sql-analytique/#basic-functions","title":"Basic functions","text":"<pre><code>SELECT \n    COUNT(*) as total_rows,\n    COUNT(DISTINCT user_id) as unique_users,\n    SUM(value) as total_value,\n    AVG(value) as avg_value,\n    MIN(value) as min_value,\n    MAX(value) as max_value\nFROM events;\n</code></pre>"},{"location":"Clickhouse/Data/EN/02-sql-analytique/#statistical-functions","title":"Statistical functions","text":"<pre><code>SELECT \n    event_type,\n    quantile(0.5)(value) as median,\n    quantile(0.95)(value) as p95,\n    stddevPop(value) as std_dev\nFROM events\nGROUP BY event_type;\n</code></pre>"},{"location":"Clickhouse/Data/EN/02-sql-analytique/#window-functions","title":"Window Functions","text":""},{"location":"Clickhouse/Data/EN/02-sql-analytique/#row_number-rank-dense_rank","title":"ROW_NUMBER, RANK, DENSE_RANK","text":"<pre><code>SELECT \n    user_id,\n    event_date,\n    value,\n    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY value DESC) as rank\nFROM events;\n</code></pre>"},{"location":"Clickhouse/Data/EN/02-sql-analytique/#lag-and-lead","title":"LAG and LEAD","text":"<pre><code>SELECT \n    event_date,\n    value,\n    LAG(value) OVER (ORDER BY event_date) as prev_value,\n    LEAD(value) OVER (ORDER BY event_date) as next_value\nFROM events\nORDER BY event_date;\n</code></pre>"},{"location":"Clickhouse/Data/EN/02-sql-analytique/#sum-over-cumulative","title":"SUM OVER (cumulative)","text":"<pre><code>SELECT \n    event_date,\n    value,\n    SUM(value) OVER (ORDER BY event_date) as cumulative_sum\nFROM events\nORDER BY event_date;\n</code></pre> <p>Next step : Data Types and Tables</p>"},{"location":"Clickhouse/Data/EN/03-types-tables/","title":"3. Data Types and Tables","text":""},{"location":"Clickhouse/Data/EN/03-types-tables/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand ClickHouse data types</li> <li>Create optimized tables</li> <li>Choose the right table engine</li> <li>Configure partitioning</li> </ul>"},{"location":"Clickhouse/Data/EN/03-types-tables/#data-types","title":"Data Types","text":""},{"location":"Clickhouse/Data/EN/03-types-tables/#integer-types","title":"Integer types","text":"<pre><code>UInt8, UInt16, UInt32, UInt64  -- Unsigned integers\nInt8, Int16, Int32, Int64      -- Signed integers\n</code></pre>"},{"location":"Clickhouse/Data/EN/03-types-tables/#decimal-types","title":"Decimal types","text":"<pre><code>Float32, Float64               -- Floating point numbers\nDecimal32, Decimal64, Decimal128  -- Precise decimals\n</code></pre>"},{"location":"Clickhouse/Data/EN/03-types-tables/#string-types","title":"String types","text":"<pre><code>String                        -- Character string\nFixedString(N)                -- Fixed length string\n</code></pre>"},{"location":"Clickhouse/Data/EN/03-types-tables/#datetime-types","title":"Date/Time types","text":"<pre><code>Date                          -- Date (YYYY-MM-DD)\nDateTime                      -- Date and time\nDateTime64                    -- Date and time with precision\n</code></pre>"},{"location":"Clickhouse/Data/EN/03-types-tables/#creating-tables","title":"Creating Tables","text":""},{"location":"Clickhouse/Data/EN/03-types-tables/#mergetree-table-recommended","title":"MergeTree table (recommended)","text":"<pre><code>CREATE TABLE events\n(\n    id UInt64,\n    event_date Date,\n    event_time DateTime,\n    user_id UInt32,\n    event_type String,\n    value Float64\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(event_date)\nORDER BY (event_date, user_id);\n</code></pre>"},{"location":"Clickhouse/Data/EN/03-types-tables/#memory-table","title":"Memory table","text":"<pre><code>CREATE TABLE temp_data\n(\n    id UInt64,\n    name String\n)\nENGINE = Memory;\n</code></pre>"},{"location":"Clickhouse/Data/EN/03-types-tables/#partitioning","title":"Partitioning","text":""},{"location":"Clickhouse/Data/EN/03-types-tables/#by-date-recommended","title":"By date (recommended)","text":"<pre><code>PARTITION BY toYYYYMM(event_date)\n</code></pre>"},{"location":"Clickhouse/Data/EN/03-types-tables/#by-hash","title":"By hash","text":"<pre><code>PARTITION BY intHash32(user_id) % 10\n</code></pre>"},{"location":"Clickhouse/Data/EN/03-types-tables/#table-engines","title":"Table Engines","text":"<ul> <li>MergeTree : For persistent data, large volumes</li> <li>Memory : For temporary data</li> <li>Log : For small volumes, logs</li> </ul> <p>Next step : Performance and Optimization</p>"},{"location":"Clickhouse/Data/EN/04-performance/","title":"4. Performance and Optimization","text":""},{"location":"Clickhouse/Data/EN/04-performance/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand ClickHouse indexes</li> <li>Optimize queries</li> <li>Manage compression</li> <li>Monitor performance</li> </ul>"},{"location":"Clickhouse/Data/EN/04-performance/#indexes-and-projections","title":"Indexes and Projections","text":""},{"location":"Clickhouse/Data/EN/04-performance/#primary-index-order-by","title":"Primary index (ORDER BY)","text":"<pre><code>CREATE TABLE events\n(\n    id UInt64,\n    event_date Date,\n    user_id UInt32\n)\nENGINE = MergeTree()\nORDER BY (event_date, user_id);  -- Primary index\n</code></pre>"},{"location":"Clickhouse/Data/EN/04-performance/#query-optimization","title":"Query Optimization","text":""},{"location":"Clickhouse/Data/EN/04-performance/#use-where-on-indexed-columns","title":"Use WHERE on indexed columns","text":"<pre><code>-- \u2705 Good : uses index\nSELECT * FROM events \nWHERE event_date = '2024-01-15';\n\n-- \u274c Less good : full scan\nSELECT * FROM events \nWHERE value &gt; 100;\n</code></pre>"},{"location":"Clickhouse/Data/EN/04-performance/#limit-to-limit-results","title":"LIMIT to limit results","text":"<pre><code>SELECT * FROM events \nORDER BY event_date DESC \nLIMIT 100;\n</code></pre>"},{"location":"Clickhouse/Data/EN/04-performance/#avoid-select","title":"Avoid SELECT *","text":"<pre><code>-- \u2705 Good\nSELECT event_date, COUNT(*) \nFROM events \nGROUP BY event_date;\n\n-- \u274c Less good\nSELECT * FROM events;\n</code></pre>"},{"location":"Clickhouse/Data/EN/04-performance/#compression","title":"Compression","text":""},{"location":"Clickhouse/Data/EN/04-performance/#check-compression","title":"Check compression","text":"<pre><code>SELECT \n    table,\n    formatReadableSize(sum(data_compressed_bytes)) as compressed,\n    formatReadableSize(sum(data_uncompressed_bytes)) as uncompressed,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) as ratio\nFROM system.parts\nWHERE active\nGROUP BY table;\n</code></pre>"},{"location":"Clickhouse/Data/EN/04-performance/#monitoring","title":"Monitoring","text":""},{"location":"Clickhouse/Data/EN/04-performance/#slow-queries","title":"Slow queries","text":"<pre><code>SELECT \n    query,\n    query_duration_ms,\n    read_rows,\n    read_bytes\nFROM system.query_log\nWHERE type = 'QueryFinish'\nORDER BY query_duration_ms DESC\nLIMIT 10;\n</code></pre> <p>Next step : Advanced Functions</p>"},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/","title":"5. Advanced Functions","text":""},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Master date/time functions</li> <li>Use mathematical functions</li> <li>Manipulate strings</li> <li>Use advanced aggregation functions</li> </ul>"},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#date-and-time-functions","title":"Date and Time Functions","text":""},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#extraction","title":"Extraction","text":"<pre><code>SELECT \n    toYear(event_time) as year,\n    toMonth(event_time) as month,\n    toDayOfMonth(event_time) as day,\n    toHour(event_time) as hour;\n</code></pre>"},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#temporal-aggregation","title":"Temporal aggregation","text":"<pre><code>SELECT \n    toStartOfDay(event_time) as day,\n    toStartOfHour(event_time) as hour,\n    toStartOfWeek(event_time) as week,\n    toStartOfMonth(event_time) as month;\n</code></pre>"},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#mathematical-functions","title":"Mathematical Functions","text":""},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#rounding","title":"Rounding","text":"<pre><code>SELECT \n    round(value, 2) as rounded,\n    floor(value) as floor_val,\n    ceil(value) as ceil_val;\n</code></pre>"},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#statistics","title":"Statistics","text":"<pre><code>SELECT \n    quantile(0.5)(value) as median,\n    quantile(0.95)(value) as p95,\n    stddevPop(value) as std_dev;\n</code></pre>"},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#string-functions","title":"String Functions","text":""},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#manipulation","title":"Manipulation","text":"<pre><code>SELECT \n    length(name) as name_length,\n    upper(name) as upper_name,\n    lower(name) as lower_name;\n</code></pre>"},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#search","title":"Search","text":"<pre><code>SELECT \n    position(name, 'test') as pos,\n    match(name, 'test.*') as matches,\n    replace(name, 'old', 'new') as replaced;\n</code></pre>"},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#advanced-aggregation-functions","title":"Advanced Aggregation Functions","text":""},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#quantiles","title":"Quantiles","text":"<pre><code>SELECT \n    quantile(0.5)(value) as median,\n    quantiles(0.5, 0.9, 0.99)(value) as quantiles;\n</code></pre>"},{"location":"Clickhouse/Data/EN/05-fonctions-avancees/#topk","title":"TopK","text":"<pre><code>SELECT \n    topK(10)(event_type) as top_10_types,\n    topKWeighted(10)(event_type, value) as top_10_weighted;\n</code></pre> <p>Next step : Integration and ETL</p>"},{"location":"Clickhouse/Data/EN/06-integration-etl/","title":"6. Integration and ETL","text":""},{"location":"Clickhouse/Data/EN/06-integration-etl/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Import data into ClickHouse</li> <li>Export data from ClickHouse</li> <li>Integrate with Python</li> <li>Integrate with PowerBI/Tableau</li> </ul>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#data-import","title":"Data Import","text":""},{"location":"Clickhouse/Data/EN/06-integration-etl/#from-csv","title":"From CSV","text":"<pre><code>INSERT INTO events\nFROM INFILE '/path/to/file.csv'\nFORMAT CSV;\n</code></pre>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#from-json","title":"From JSON","text":"<pre><code>INSERT INTO events\nFROM INFILE '/path/to/file.json'\nFORMAT JSONEachRow;\n</code></pre>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#via-clickhouse-client","title":"Via clickhouse-client","text":"<pre><code>clickhouse-client --query \"INSERT INTO events FORMAT CSV\" &lt; data.csv\n</code></pre>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#data-export","title":"Data Export","text":""},{"location":"Clickhouse/Data/EN/06-integration-etl/#to-csv","title":"To CSV","text":"<pre><code>SELECT * FROM events\nINTO OUTFILE '/path/to/output.csv'\nFORMAT CSV;\n</code></pre>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#via-clickhouse-client_1","title":"Via clickhouse-client","text":"<pre><code>clickhouse-client --query \"SELECT * FROM events FORMAT CSV\" &gt; output.csv\n</code></pre>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#python-integration","title":"Python Integration","text":""},{"location":"Clickhouse/Data/EN/06-integration-etl/#installation","title":"Installation","text":"<pre><code>pip install clickhouse-driver\n</code></pre>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#connection","title":"Connection","text":"<pre><code>from clickhouse_driver import Client\n\nclient = Client(host='localhost', port=9000, database='analytics')\n</code></pre>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#queries","title":"Queries","text":"<pre><code># Execute query\nresult = client.execute('SELECT * FROM events LIMIT 10')\n\n# Insert data\nclient.execute('INSERT INTO events VALUES', [\n    (1, '2024-01-15', 100, 'click', 1.5),\n    (2, '2024-01-15', 101, 'view', 2.0)\n])\n</code></pre>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#pandas","title":"Pandas","text":"<pre><code>import pandas as pd\n\n# Read from ClickHouse\ndf = pd.read_sql('SELECT * FROM events', client.connection)\n\n# Write to ClickHouse\ndf.to_sql('events', client.connection, if_exists='append')\n</code></pre>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#powerbi-integration","title":"PowerBI Integration","text":""},{"location":"Clickhouse/Data/EN/06-integration-etl/#odbc-connection","title":"ODBC Connection","text":"<ol> <li>Install ClickHouse ODBC driver</li> <li>Create ODBC data source</li> <li>Connect from PowerBI</li> </ol>"},{"location":"Clickhouse/Data/EN/06-integration-etl/#etl-with-python","title":"ETL with Python","text":""},{"location":"Clickhouse/Data/EN/06-integration-etl/#complete-example","title":"Complete example","text":"<pre><code>from clickhouse_driver import Client\nimport pandas as pd\n\n# Connection\nclient = Client(host='localhost', port=9000)\n\n# Read from source\ndf = pd.read_csv('source_data.csv')\n\n# Transformation\ndf['processed_date'] = pd.to_datetime(df['date'])\ndf = df[df['value'] &gt; 0]\n\n# Write to ClickHouse\nclient.execute('INSERT INTO events VALUES', df.values.tolist())\n</code></pre> <p>Next step : Best Practices</p>"},{"location":"Clickhouse/Data/EN/07-best-practices/","title":"7. Best Practices","text":""},{"location":"Clickhouse/Data/EN/07-best-practices/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Model data efficiently</li> <li>Choose partitioning strategies</li> <li>Manage memory</li> <li>Secure access</li> </ul>"},{"location":"Clickhouse/Data/EN/07-best-practices/#data-modeling","title":"Data Modeling","text":""},{"location":"Clickhouse/Data/EN/07-best-practices/#choose-right-types","title":"Choose right types","text":"<pre><code>-- \u2705 Good : UInt32 for IDs\nuser_id UInt32\n\n-- \u274c Less good : UInt64 unnecessary\nuser_id UInt64\n\n-- \u2705 Good : Date for dates\nevent_date Date\n\n-- \u274c Less good : String for dates\nevent_date String\n</code></pre>"},{"location":"Clickhouse/Data/EN/07-best-practices/#partitioning-strategies","title":"Partitioning Strategies","text":""},{"location":"Clickhouse/Data/EN/07-best-practices/#by-date-recommended","title":"By date (recommended)","text":"<pre><code>PARTITION BY toYYYYMM(event_date)\n</code></pre>"},{"location":"Clickhouse/Data/EN/07-best-practices/#avoid-too-many-partitions","title":"Avoid too many partitions","text":"<pre><code>-- \u2705 Good : Monthly partition\nPARTITION BY toYYYYMM(date)\n\n-- \u274c Less good : Daily partition (too many)\nPARTITION BY date\n</code></pre>"},{"location":"Clickhouse/Data/EN/07-best-practices/#memory-management","title":"Memory Management","text":""},{"location":"Clickhouse/Data/EN/07-best-practices/#limit-queries","title":"LIMIT queries","text":"<pre><code>-- \u2705 Good\nSELECT * FROM events LIMIT 1000;\n\n-- \u274c Less good\nSELECT * FROM events;\n</code></pre>"},{"location":"Clickhouse/Data/EN/07-best-practices/#security","title":"Security","text":""},{"location":"Clickhouse/Data/EN/07-best-practices/#create-users","title":"Create users","text":"<pre><code>CREATE USER analyst IDENTIFIED BY 'password';\nGRANT SELECT ON analytics.* TO analyst;\n</code></pre>"},{"location":"Clickhouse/Data/EN/07-best-practices/#granular-permissions","title":"Granular permissions","text":"<pre><code>GRANT SELECT ON analytics.events TO analyst;\nGRANT INSERT ON analytics.temp_table TO analyst;\n</code></pre>"},{"location":"Clickhouse/Data/EN/07-best-practices/#maintenance","title":"Maintenance","text":""},{"location":"Clickhouse/Data/EN/07-best-practices/#check-partitions","title":"Check partitions","text":"<pre><code>SELECT \n    partition,\n    rows,\n    formatReadableSize(bytes_on_disk) as size\nFROM system.parts\nWHERE active\nORDER BY partition;\n</code></pre>"},{"location":"Clickhouse/Data/EN/07-best-practices/#clean-old-data","title":"Clean old data","text":"<pre><code>ALTER TABLE events DROP PARTITION '202301';\n</code></pre>"},{"location":"Clickhouse/Data/EN/07-best-practices/#optimize-tables","title":"Optimize tables","text":"<pre><code>OPTIMIZE TABLE events FINAL;\n</code></pre> <p>Next step : Practical Projects</p>"},{"location":"Clickhouse/Data/EN/08-projets/","title":"8. Practical Projects","text":""},{"location":"Clickhouse/Data/EN/08-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Apply acquired knowledge</li> <li>Create projects for your portfolio</li> <li>Solve real problems</li> <li>Optimize performance</li> </ul>"},{"location":"Clickhouse/Data/EN/08-projets/#project-1-web-analytics","title":"Project 1 : Web Analytics","text":""},{"location":"Clickhouse/Data/EN/08-projets/#objective","title":"Objective","text":"<p>Analyze website logs to understand user behavior.</p>"},{"location":"Clickhouse/Data/EN/08-projets/#data","title":"Data","text":"<pre><code>CREATE TABLE web_logs\n(\n    timestamp DateTime,\n    user_id UInt32,\n    page String,\n    action String,\n    duration UInt32\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (timestamp, user_id);\n</code></pre>"},{"location":"Clickhouse/Data/EN/08-projets/#analysis-queries","title":"Analysis Queries","text":"<pre><code>-- Most visited pages\nSELECT \n    page,\n    COUNT(*) as visits,\n    AVG(duration) as avg_duration\nFROM web_logs\nGROUP BY page\nORDER BY visits DESC\nLIMIT 10;\n\n-- Active users per day\nSELECT \n    toStartOfDay(timestamp) as day,\n    COUNT(DISTINCT user_id) as active_users\nFROM web_logs\nGROUP BY day\nORDER BY day;\n</code></pre>"},{"location":"Clickhouse/Data/EN/08-projets/#project-2-analytical-data-warehouse","title":"Project 2 : Analytical Data Warehouse","text":""},{"location":"Clickhouse/Data/EN/08-projets/#objective_1","title":"Objective","text":"<p>Create a data warehouse for sales analysis.</p>"},{"location":"Clickhouse/Data/EN/08-projets/#schema","title":"Schema","text":"<pre><code>CREATE TABLE sales\n(\n    sale_id UInt64,\n    sale_date Date,\n    product_id UInt32,\n    customer_id UInt32,\n    quantity UInt32,\n    price Decimal64(2),\n    total Decimal64(2) MATERIALIZED quantity * price\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(sale_date)\nORDER BY (sale_date, product_id);\n</code></pre>"},{"location":"Clickhouse/Data/EN/08-projets/#analyses","title":"Analyses","text":"<pre><code>-- Sales by month\nSELECT \n    toStartOfMonth(sale_date) as month,\n    SUM(total) as monthly_revenue,\n    COUNT(*) as sales_count\nFROM sales\nGROUP BY month\nORDER BY month;\n\n-- Top products\nSELECT \n    product_id,\n    SUM(quantity) as total_sold,\n    SUM(total) as revenue\nFROM sales\nGROUP BY product_id\nORDER BY revenue DESC\nLIMIT 10;\n</code></pre>"},{"location":"Clickhouse/Data/EN/08-projets/#project-3-real-time-dashboard","title":"Project 3 : Real-Time Dashboard","text":""},{"location":"Clickhouse/Data/EN/08-projets/#objective_2","title":"Objective","text":"<p>Create real-time metrics for a dashboard.</p>"},{"location":"Clickhouse/Data/EN/08-projets/#event-table","title":"Event Table","text":"<pre><code>CREATE TABLE realtime_events\n(\n    event_time DateTime,\n    event_type String,\n    value Float64\n)\nENGINE = MergeTree()\nORDER BY event_time;\n</code></pre>"},{"location":"Clickhouse/Data/EN/08-projets/#real-time-metrics","title":"Real-time Metrics","text":"<pre><code>-- Events from last 24h\nSELECT \n    toStartOfHour(event_time) as hour,\n    event_type,\n    COUNT(*) as count,\n    SUM(value) as total\nFROM realtime_events\nWHERE event_time &gt;= now() - INTERVAL 24 HOUR\nGROUP BY hour, event_type\nORDER BY hour DESC;\n</code></pre>"},{"location":"Clickhouse/Data/EN/08-projets/#portfolio-tips","title":"Portfolio Tips","text":"<ol> <li>Document your projects : Explain problem and solution</li> <li>Show results : Visualizations, metrics</li> <li>Clean code : Optimized and commented queries</li> <li>Performance : Show optimizations made</li> <li>GitHub : Share projects on GitHub</li> </ol> <p>Congratulations! You have completed the ClickHouse training for Data Analyst! \ud83c\udf89</p>"},{"location":"Clickhouse/Data/FR/","title":"Formation ClickHouse pour Data Analyst","text":""},{"location":"Clickhouse/Data/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'apprentissage de ClickHouse en tant que Data Analyst. ClickHouse est une base de donn\u00e9es OLAP (Online Analytical Processing) orient\u00e9e colonnes, optimis\u00e9e pour les requ\u00eates analytiques sur de tr\u00e8s gros volumes de donn\u00e9es.</p>"},{"location":"Clickhouse/Data/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre ClickHouse et l'analyse OLAP</li> <li>Installer et configurer ClickHouse</li> <li>Ma\u00eetriser le SQL analytique avec ClickHouse</li> <li>Optimiser les requ\u00eates pour de gros volumes</li> <li>Utiliser les fonctions analytiques avanc\u00e9es</li> <li>G\u00e9rer les partitions et la compression</li> <li>Int\u00e9grer ClickHouse dans vos workflows d'analyse</li> <li>Cr\u00e9er des projets pratiques pour votre portfolio</li> </ul>"},{"location":"Clickhouse/Data/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 ClickHouse Community Edition : Gratuit et open-source - \u2705 ClickHouse Client : Outil en ligne de commande gratuit - \u2705 ClickHouse Playground : Environnement en ligne gratuit - \u2705 Documentation officielle : Guides complets gratuits</p> <p>Budget total : 0\u20ac</p>"},{"location":"Clickhouse/Data/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"Clickhouse/Data/FR/#1-prise-en-main-clickhouse","title":"1. Prise en main ClickHouse","text":""},{"location":"Clickhouse/Data/FR/#2-sql-analytique-avec-clickhouse","title":"2. SQL Analytique avec ClickHouse","text":""},{"location":"Clickhouse/Data/FR/#3-types-de-donnees-et-tables","title":"3. Types de donn\u00e9es et Tables","text":""},{"location":"Clickhouse/Data/FR/#4-performance-et-optimisation","title":"4. Performance et Optimisation","text":""},{"location":"Clickhouse/Data/FR/#5-fonctions-avancees","title":"5. Fonctions Avanc\u00e9es","text":""},{"location":"Clickhouse/Data/FR/#6-integration-et-etl","title":"6. Int\u00e9gration et ETL","text":""},{"location":"Clickhouse/Data/FR/#7-bonnes-pratiques","title":"7. Bonnes Pratiques","text":""},{"location":"Clickhouse/Data/FR/#8-projets-pratiques","title":"8. Projets Pratiques","text":""},{"location":"Clickhouse/Data/FR/#prerequis","title":"\ud83d\ude80 Pr\u00e9requis","text":"<ul> <li>Connaissances de base en SQL</li> <li>Notions de bases de donn\u00e9es</li> <li>Python (optionnel)</li> </ul>"},{"location":"Clickhouse/Data/FR/#duree-estimee","title":"\u23f1\ufe0f Dur\u00e9e estim\u00e9e","text":"<ul> <li>Total : 40-50 heures</li> <li>Par module : 5-6 heures</li> </ul> <p>Bon apprentissage ! \ud83d\ude80</p>"},{"location":"Clickhouse/Data/FR/01-getting-started/","title":"1. Prise en main ClickHouse","text":""},{"location":"Clickhouse/Data/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre ClickHouse et l'OLAP</li> <li>Installer ClickHouse</li> <li>Comprendre les concepts de base</li> <li>Utiliser ClickHouse Client</li> <li>Premi\u00e8res requ\u00eates</li> </ul>"},{"location":"Clickhouse/Data/FR/01-getting-started/#introduction-a-clickhouse","title":"Introduction \u00e0 ClickHouse","text":"<p>ClickHouse = Base de donn\u00e9es OLAP orient\u00e9e colonnes</p> <ul> <li>OLAP : Online Analytical Processing</li> <li>Orient\u00e9e colonnes : Stockage par colonnes</li> <li>Haute performance : Tr\u00e8s rapide sur gros volumes</li> <li>Open-source : Gratuit et open-source</li> </ul>"},{"location":"Clickhouse/Data/FR/01-getting-started/#pourquoi-clickhouse-pour-data-analyst","title":"Pourquoi ClickHouse pour Data Analyst ?","text":"<ul> <li>Gros volumes : G\u00e8re des milliards de lignes</li> <li>Performance : Requ\u00eates analytiques ultra-rapides</li> <li>Compression : Compression efficace</li> <li>SQL : Langage SQL standard</li> </ul>"},{"location":"Clickhouse/Data/FR/01-getting-started/#cas-dusage-typiques","title":"Cas d'usage typiques","text":"<ul> <li>Analytics web (logs, \u00e9v\u00e9nements)</li> <li>Data warehousing</li> <li>Business Intelligence</li> <li>IoT (capteurs)</li> </ul>"},{"location":"Clickhouse/Data/FR/01-getting-started/#installation","title":"Installation","text":""},{"location":"Clickhouse/Data/FR/01-getting-started/#option-1-clickhouse-playground-recommande","title":"Option 1 : ClickHouse Playground (recommand\u00e9)","text":"<ol> <li>Allez sur ClickHouse Playground</li> <li>Aucune installation n\u00e9cessaire</li> </ol>"},{"location":"Clickhouse/Data/FR/01-getting-started/#option-2-docker","title":"Option 2 : Docker","text":"<pre><code>docker run -d --name clickhouse-server -p 8123:8123 -p 9000:9000 clickhouse/clickhouse-server\n</code></pre>"},{"location":"Clickhouse/Data/FR/01-getting-started/#option-3-installation-locale-linux","title":"Option 3 : Installation locale (Linux)","text":"<pre><code>sudo apt-get install -y apt-transport-https ca-certificates\necho \"deb https://repo.clickhouse.com/deb stable main\" | sudo tee /etc/apt/sources.list.d/clickhouse.list\nsudo apt-get update\nsudo apt-get install -y clickhouse-server clickhouse-client\nsudo service clickhouse-server start\n</code></pre>"},{"location":"Clickhouse/Data/FR/01-getting-started/#concepts-de-base","title":"Concepts de base","text":""},{"location":"Clickhouse/Data/FR/01-getting-started/#stockage-par-colonnes","title":"Stockage par colonnes","text":"<p>Avantages : - Compression efficace - Requ\u00eates analytiques rapides - Agr\u00e9gations optimis\u00e9es</p>"},{"location":"Clickhouse/Data/FR/01-getting-started/#moteurs-de-tables","title":"Moteurs de tables","text":"<ul> <li>MergeTree : Pour donn\u00e9es persistantes</li> <li>Memory : Pour donn\u00e9es en m\u00e9moire</li> <li>Log : Pour petits volumes</li> </ul>"},{"location":"Clickhouse/Data/FR/01-getting-started/#clickhouse-client","title":"ClickHouse Client","text":"<pre><code>clickhouse-client\nclickhouse-client --host localhost --port 9000\n</code></pre>"},{"location":"Clickhouse/Data/FR/01-getting-started/#commandes-de-base","title":"Commandes de base","text":"<pre><code>SHOW DATABASES;\nUSE default;\nSHOW TABLES;\nDESCRIBE table_name;\n</code></pre>"},{"location":"Clickhouse/Data/FR/01-getting-started/#premieres-requetes","title":"Premi\u00e8res requ\u00eates","text":""},{"location":"Clickhouse/Data/FR/01-getting-started/#creer-une-base-de-donnees","title":"Cr\u00e9er une base de donn\u00e9es","text":"<pre><code>CREATE DATABASE IF NOT EXISTS analytics;\nUSE analytics;\n</code></pre>"},{"location":"Clickhouse/Data/FR/01-getting-started/#creer-une-table","title":"Cr\u00e9er une table","text":"<pre><code>CREATE TABLE IF NOT EXISTS events\n(\n    id UInt64,\n    event_date Date,\n    event_time DateTime,\n    user_id UInt32,\n    event_type String,\n    value Float64\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(event_date)\nORDER BY (event_date, event_time);\n</code></pre>"},{"location":"Clickhouse/Data/FR/01-getting-started/#inserer-des-donnees","title":"Ins\u00e9rer des donn\u00e9es","text":"<pre><code>INSERT INTO events VALUES\n(1, '2024-01-15', '2024-01-15 10:00:00', 100, 'click', 1.5),\n(2, '2024-01-15', '2024-01-15 10:01:00', 101, 'view', 2.0);\n</code></pre>"},{"location":"Clickhouse/Data/FR/01-getting-started/#requete-select","title":"Requ\u00eate SELECT","text":"<pre><code>SELECT * FROM events;\n\nSELECT \n    event_type,\n    COUNT(*) as count,\n    SUM(value) as total_value\nFROM events\nWHERE event_date = '2024-01-15'\nGROUP BY event_type;\n</code></pre> <p>Prochaine \u00e9tape : SQL Analytique avec ClickHouse</p>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/","title":"2. SQL Analytique avec ClickHouse","text":""},{"location":"Clickhouse/Data/FR/02-sql-analytique/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Ma\u00eetriser les requ\u00eates SELECT avanc\u00e9es</li> <li>Utiliser les agr\u00e9gations et GROUP BY</li> <li>Comprendre les fonctions analytiques</li> <li>Utiliser les fen\u00eatres (Window Functions)</li> </ul>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#agregations-avancees","title":"Agr\u00e9gations avanc\u00e9es","text":""},{"location":"Clickhouse/Data/FR/02-sql-analytique/#group-by-avec-plusieurs-colonnes","title":"GROUP BY avec plusieurs colonnes","text":"<pre><code>SELECT \n    event_date,\n    event_type,\n    COUNT(*) as count,\n    SUM(value) as total_value,\n    AVG(value) as avg_value\nFROM events\nGROUP BY event_date, event_type\nORDER BY event_date, event_type;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#having-pour-filtrer-les-agregations","title":"HAVING pour filtrer les agr\u00e9gations","text":"<pre><code>SELECT \n    event_type,\n    COUNT(*) as count\nFROM events\nGROUP BY event_type\nHAVING count &gt; 10;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#fonctions-dagregation","title":"Fonctions d'agr\u00e9gation","text":""},{"location":"Clickhouse/Data/FR/02-sql-analytique/#fonctions-de-base","title":"Fonctions de base","text":"<pre><code>SELECT \n    COUNT(*) as total_rows,\n    COUNT(DISTINCT user_id) as unique_users,\n    SUM(value) as total_value,\n    AVG(value) as avg_value,\n    MIN(value) as min_value,\n    MAX(value) as max_value\nFROM events;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#fonctions-statistiques","title":"Fonctions statistiques","text":"<pre><code>SELECT \n    event_type,\n    quantile(0.5)(value) as median,\n    quantile(0.95)(value) as p95,\n    stddevPop(value) as std_dev\nFROM events\nGROUP BY event_type;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#window-functions","title":"Window Functions","text":""},{"location":"Clickhouse/Data/FR/02-sql-analytique/#row_number-rank-dense_rank","title":"ROW_NUMBER, RANK, DENSE_RANK","text":"<pre><code>SELECT \n    user_id,\n    event_date,\n    value,\n    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY value DESC) as rank\nFROM events;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#lag-et-lead","title":"LAG et LEAD","text":"<pre><code>SELECT \n    event_date,\n    value,\n    LAG(value) OVER (ORDER BY event_date) as prev_value,\n    LEAD(value) OVER (ORDER BY event_date) as next_value\nFROM events\nORDER BY event_date;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#sum-over-cumulative","title":"SUM OVER (cumulative)","text":"<pre><code>SELECT \n    event_date,\n    value,\n    SUM(value) OVER (ORDER BY event_date) as cumulative_sum\nFROM events\nORDER BY event_date;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#requetes-complexes","title":"Requ\u00eates complexes","text":""},{"location":"Clickhouse/Data/FR/02-sql-analytique/#sous-requetes","title":"Sous-requ\u00eates","text":"<pre><code>SELECT \n    event_type,\n    COUNT(*) as count\nFROM events\nWHERE user_id IN (\n    SELECT DISTINCT user_id \n    FROM events \n    WHERE value &gt; 100\n)\nGROUP BY event_type;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#join","title":"JOIN","text":"<pre><code>SELECT \n    e.event_type,\n    u.user_name,\n    COUNT(*) as count\nFROM events e\nJOIN users u ON e.user_id = u.id\nGROUP BY e.event_type, u.user_name;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#union","title":"UNION","text":"<pre><code>SELECT event_type, COUNT(*) as count FROM events GROUP BY event_type\nUNION ALL\nSELECT 'total' as event_type, COUNT(*) as count FROM events;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#fonctions-de-date","title":"Fonctions de date","text":"<pre><code>SELECT \n    toStartOfDay(event_time) as day,\n    toStartOfHour(event_time) as hour,\n    toStartOfWeek(event_time) as week,\n    toStartOfMonth(event_time) as month,\n    COUNT(*) as count\nFROM events\nGROUP BY day, hour, week, month;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#exercices-pratiques","title":"Exercices pratiques","text":""},{"location":"Clickhouse/Data/FR/02-sql-analytique/#exercice-1-top-10-des-utilisateurs","title":"Exercice 1 : Top 10 des utilisateurs","text":"<pre><code>SELECT \n    user_id,\n    COUNT(*) as event_count,\n    SUM(value) as total_value\nFROM events\nGROUP BY user_id\nORDER BY total_value DESC\nLIMIT 10;\n</code></pre>"},{"location":"Clickhouse/Data/FR/02-sql-analytique/#exercice-2-evolution-quotidienne","title":"Exercice 2 : \u00c9volution quotidienne","text":"<pre><code>SELECT \n    event_date,\n    COUNT(*) as daily_count,\n    SUM(value) as daily_total,\n    AVG(value) as daily_avg\nFROM events\nGROUP BY event_date\nORDER BY event_date;\n</code></pre> <p>Prochaine \u00e9tape : Types de donn\u00e9es et Tables</p>"},{"location":"Clickhouse/Data/FR/03-types-tables/","title":"3. Types de donn\u00e9es et Tables","text":""},{"location":"Clickhouse/Data/FR/03-types-tables/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les types de donn\u00e9es ClickHouse</li> <li>Cr\u00e9er des tables optimis\u00e9es</li> <li>Choisir le bon moteur de table</li> <li>Configurer le partitionnement</li> </ul>"},{"location":"Clickhouse/Data/FR/03-types-tables/#types-de-donnees","title":"Types de donn\u00e9es","text":""},{"location":"Clickhouse/Data/FR/03-types-tables/#types-entiers","title":"Types entiers","text":"<pre><code>UInt8, UInt16, UInt32, UInt64  -- Entiers non sign\u00e9s\nInt8, Int16, Int32, Int64      -- Entiers sign\u00e9s\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#types-decimaux","title":"Types d\u00e9cimaux","text":"<pre><code>Float32, Float64               -- Nombres flottants\nDecimal32, Decimal64, Decimal128  -- Nombres d\u00e9cimaux pr\u00e9cis\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#types-de-chaines","title":"Types de cha\u00eenes","text":"<pre><code>String                        -- Cha\u00eene de caract\u00e8res\nFixedString(N)                -- Cha\u00eene de longueur fixe\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#types-de-dateheure","title":"Types de date/heure","text":"<pre><code>Date                          -- Date (YYYY-MM-DD)\nDateTime                      -- Date et heure\nDateTime64                    -- Date et heure avec pr\u00e9cision\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#types-speciaux","title":"Types sp\u00e9ciaux","text":"<pre><code>Array(T)                      -- Tableau de type T\nTuple(T1, T2, ...)            -- Tuple\nNullable(T)                   -- Type nullable\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#creation-de-tables","title":"Cr\u00e9ation de tables","text":""},{"location":"Clickhouse/Data/FR/03-types-tables/#table-mergetree-recommande","title":"Table MergeTree (recommand\u00e9)","text":"<pre><code>CREATE TABLE events\n(\n    id UInt64,\n    event_date Date,\n    event_time DateTime,\n    user_id UInt32,\n    event_type String,\n    value Float64\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(event_date)\nORDER BY (event_date, user_id)\nSETTINGS index_granularity = 8192;\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#table-memory","title":"Table Memory","text":"<pre><code>CREATE TABLE temp_data\n(\n    id UInt64,\n    name String\n)\nENGINE = Memory;\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#table-log","title":"Table Log","text":"<pre><code>CREATE TABLE logs\n(\n    id UInt64,\n    message String\n)\nENGINE = Log;\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#partitionnement","title":"Partitionnement","text":""},{"location":"Clickhouse/Data/FR/03-types-tables/#par-date-recommande","title":"Par date (recommand\u00e9)","text":"<pre><code>PARTITION BY toYYYYMM(event_date)\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#par-hash","title":"Par hash","text":"<pre><code>PARTITION BY intHash32(user_id) % 10\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#par-valeur","title":"Par valeur","text":"<pre><code>PARTITION BY event_type\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#order-by-cle-de-tri","title":"ORDER BY (cl\u00e9 de tri)","text":"<pre><code>ORDER BY (event_date, user_id, event_type)\n</code></pre> <p>Important : L'ordre d\u00e9finit l'index primaire</p>"},{"location":"Clickhouse/Data/FR/03-types-tables/#moteurs-de-tables","title":"Moteurs de tables","text":""},{"location":"Clickhouse/Data/FR/03-types-tables/#mergetree","title":"MergeTree","text":"<ul> <li>Usage : Donn\u00e9es persistantes, gros volumes</li> <li>Avantages : Compression, index, partitions</li> <li>Inconv\u00e9nients : Plus complexe</li> </ul>"},{"location":"Clickhouse/Data/FR/03-types-tables/#memory","title":"Memory","text":"<ul> <li>Usage : Donn\u00e9es temporaires</li> <li>Avantages : Tr\u00e8s rapide</li> <li>Inconv\u00e9nients : Perdues au red\u00e9marrage</li> </ul>"},{"location":"Clickhouse/Data/FR/03-types-tables/#log","title":"Log","text":"<ul> <li>Usage : Petits volumes, logs</li> <li>Avantages : Simple</li> <li>Inconv\u00e9nients : Pas de partitions</li> </ul>"},{"location":"Clickhouse/Data/FR/03-types-tables/#exemples-complets","title":"Exemples complets","text":""},{"location":"Clickhouse/Data/FR/03-types-tables/#table-de-ventes","title":"Table de ventes","text":"<pre><code>CREATE TABLE sales\n(\n    id UInt64,\n    sale_date Date,\n    product_id UInt32,\n    customer_id UInt32,\n    quantity UInt32,\n    price Decimal64(2),\n    total Decimal64(2) MATERIALIZED quantity * price\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(sale_date)\nORDER BY (sale_date, product_id);\n</code></pre>"},{"location":"Clickhouse/Data/FR/03-types-tables/#table-avec-array","title":"Table avec Array","text":"<pre><code>CREATE TABLE user_tags\n(\n    user_id UInt32,\n    tags Array(String),\n    created_at DateTime\n)\nENGINE = MergeTree()\nORDER BY user_id;\n</code></pre> <p>Prochaine \u00e9tape : Performance et Optimisation</p>"},{"location":"Clickhouse/Data/FR/04-performance/","title":"4. Performance et Optimisation","text":""},{"location":"Clickhouse/Data/FR/04-performance/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les index ClickHouse</li> <li>Optimiser les requ\u00eates</li> <li>G\u00e9rer la compression</li> <li>Monitorer les performances</li> </ul>"},{"location":"Clickhouse/Data/FR/04-performance/#index-et-projections","title":"Index et projections","text":""},{"location":"Clickhouse/Data/FR/04-performance/#index-primaire-order-by","title":"Index primaire (ORDER BY)","text":"<pre><code>CREATE TABLE events\n(\n    id UInt64,\n    event_date Date,\n    user_id UInt32\n)\nENGINE = MergeTree()\nORDER BY (event_date, user_id);  -- Index primaire\n</code></pre>"},{"location":"Clickhouse/Data/FR/04-performance/#projections-clickhouse-213","title":"Projections (ClickHouse 21.3+)","text":"<pre><code>ALTER TABLE events\nADD PROJECTION projection_by_user\n(\n    SELECT \n        user_id,\n        event_date,\n        COUNT(*) as count\n    GROUP BY user_id, event_date\n);\n</code></pre>"},{"location":"Clickhouse/Data/FR/04-performance/#optimisation-des-requetes","title":"Optimisation des requ\u00eates","text":""},{"location":"Clickhouse/Data/FR/04-performance/#utiliser-where-sur-colonnes-indexees","title":"Utiliser WHERE sur colonnes index\u00e9es","text":"<pre><code>-- \u2705 Bon : utilise l'index\nSELECT * FROM events \nWHERE event_date = '2024-01-15';\n\n-- \u274c Moins bon : scan complet\nSELECT * FROM events \nWHERE value &gt; 100;\n</code></pre>"},{"location":"Clickhouse/Data/FR/04-performance/#limit-pour-limiter-les-resultats","title":"LIMIT pour limiter les r\u00e9sultats","text":"<pre><code>SELECT * FROM events \nORDER BY event_date DESC \nLIMIT 100;\n</code></pre>"},{"location":"Clickhouse/Data/FR/04-performance/#eviter-select","title":"\u00c9viter SELECT *","text":"<pre><code>-- \u2705 Bon\nSELECT event_date, COUNT(*) \nFROM events \nGROUP BY event_date;\n\n-- \u274c Moins bon\nSELECT * FROM events;\n</code></pre>"},{"location":"Clickhouse/Data/FR/04-performance/#compression","title":"Compression","text":""},{"location":"Clickhouse/Data/FR/04-performance/#verifier-la-compression","title":"V\u00e9rifier la compression","text":"<pre><code>SELECT \n    table,\n    formatReadableSize(sum(data_compressed_bytes)) as compressed,\n    formatReadableSize(sum(data_uncompressed_bytes)) as uncompressed,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) as ratio\nFROM system.parts\nWHERE active\nGROUP BY table;\n</code></pre>"},{"location":"Clickhouse/Data/FR/04-performance/#types-de-compression","title":"Types de compression","text":"<ul> <li>LZ4 : Rapide, compression moyenne (par d\u00e9faut)</li> <li>ZSTD : Plus lent, meilleure compression</li> </ul> <pre><code>SETTINGS compression_codec = 'ZSTD(3)';\n</code></pre>"},{"location":"Clickhouse/Data/FR/04-performance/#monitoring","title":"Monitoring","text":""},{"location":"Clickhouse/Data/FR/04-performance/#requetes-lentes","title":"Requ\u00eates lentes","text":"<pre><code>SELECT \n    query,\n    query_duration_ms,\n    read_rows,\n    read_bytes\nFROM system.query_log\nWHERE type = 'QueryFinish'\nORDER BY query_duration_ms DESC\nLIMIT 10;\n</code></pre>"},{"location":"Clickhouse/Data/FR/04-performance/#utilisation-disque","title":"Utilisation disque","text":"<pre><code>SELECT \n    database,\n    table,\n    formatReadableSize(sum(bytes)) as size\nFROM system.parts\nWHERE active\nGROUP BY database, table\nORDER BY size DESC;\n</code></pre>"},{"location":"Clickhouse/Data/FR/04-performance/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ol> <li>Partitionner par date pour les donn\u00e9es temporelles</li> <li>ORDER BY sur colonnes fr\u00e9quemment filtr\u00e9es</li> <li>\u00c9viter les colonnes trop larges (String vs FixedString)</li> <li>Utiliser les types appropri\u00e9s (UInt32 vs UInt64)</li> <li>Monitorer r\u00e9guli\u00e8rement les performances</li> </ol> <p>Prochaine \u00e9tape : Fonctions Avanc\u00e9es</p>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/","title":"5. Fonctions Avanc\u00e9es","text":""},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Ma\u00eetriser les fonctions de date/heure</li> <li>Utiliser les fonctions math\u00e9matiques</li> <li>Manipuler les cha\u00eenes de caract\u00e8res</li> <li>Utiliser les fonctions d'agr\u00e9gation avanc\u00e9es</li> </ul>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#fonctions-de-date-et-heure","title":"Fonctions de date et heure","text":""},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#extraction","title":"Extraction","text":"<pre><code>SELECT \n    toYear(event_time) as year,\n    toMonth(event_time) as month,\n    toDayOfMonth(event_time) as day,\n    toHour(event_time) as hour,\n    toMinute(event_time) as minute;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#agregation-temporelle","title":"Agr\u00e9gation temporelle","text":"<pre><code>SELECT \n    toStartOfDay(event_time) as day,\n    toStartOfHour(event_time) as hour,\n    toStartOfWeek(event_time) as week,\n    toStartOfMonth(event_time) as month,\n    toStartOfQuarter(event_time) as quarter,\n    toStartOfYear(event_time) as year;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#calculs-de-dates","title":"Calculs de dates","text":"<pre><code>SELECT \n    event_date,\n    addDays(event_date, 7) as next_week,\n    addMonths(event_date, 1) as next_month,\n    dateDiff('day', event_date, today()) as days_ago;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#fonctions-mathematiques","title":"Fonctions math\u00e9matiques","text":""},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#arrondis","title":"Arrondis","text":"<pre><code>SELECT \n    round(value, 2) as rounded,\n    floor(value) as floor_val,\n    ceil(value) as ceil_val,\n    trunc(value, 2) as truncated;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#statistiques","title":"Statistiques","text":"<pre><code>SELECT \n    quantile(0.5)(value) as median,\n    quantile(0.95)(value) as p95,\n    quantile(0.99)(value) as p99,\n    stddevPop(value) as std_dev,\n    varPop(value) as variance;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#logarithmes","title":"Logarithmes","text":"<pre><code>SELECT \n    log(value) as ln,\n    log2(value) as log2,\n    log10(value) as log10,\n    exp(value) as exp_val;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#fonctions-de-chaines","title":"Fonctions de cha\u00eenes","text":""},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#manipulation","title":"Manipulation","text":"<pre><code>SELECT \n    length(name) as name_length,\n    upper(name) as upper_name,\n    lower(name) as lower_name,\n    reverse(name) as reversed;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#extraction_1","title":"Extraction","text":"<pre><code>SELECT \n    substring(name, 1, 5) as first_5,\n    left(name, 3) as left_3,\n    right(name, 3) as right_3;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#recherche","title":"Recherche","text":"<pre><code>SELECT \n    position(name, 'test') as pos,\n    match(name, 'test.*') as matches,\n    replace(name, 'old', 'new') as replaced;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#fonctions-dagregation-avancees","title":"Fonctions d'agr\u00e9gation avanc\u00e9es","text":""},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#quantiles","title":"Quantiles","text":"<pre><code>SELECT \n    quantile(0.5)(value) as median,\n    quantiles(0.5, 0.9, 0.99)(value) as quantiles;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#topk","title":"TopK","text":"<pre><code>SELECT \n    topK(10)(event_type) as top_10_types,\n    topKWeighted(10)(event_type, value) as top_10_weighted;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#argmaxargmin","title":"ArgMax/ArgMin","text":"<pre><code>SELECT \n    argMax(user_id, value) as user_with_max_value,\n    argMin(user_id, value) as user_with_min_value;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#fonctions-conditionnelles","title":"Fonctions conditionnelles","text":""},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#if","title":"IF","text":"<pre><code>SELECT \n    IF(value &gt; 100, 'high', 'low') as category;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#case","title":"CASE","text":"<pre><code>SELECT \n    CASE \n        WHEN value &gt; 100 THEN 'high'\n        WHEN value &gt; 50 THEN 'medium'\n        ELSE 'low'\n    END as category;\n</code></pre>"},{"location":"Clickhouse/Data/FR/05-fonctions-avancees/#multiif","title":"multiIf","text":"<pre><code>SELECT \n    multiIf(\n        value &gt; 100, 'high',\n        value &gt; 50, 'medium',\n        'low'\n    ) as category;\n</code></pre> <p>Prochaine \u00e9tape : Int\u00e9gration et ETL</p>"},{"location":"Clickhouse/Data/FR/06-integration-etl/","title":"6. Int\u00e9gration et ETL","text":""},{"location":"Clickhouse/Data/FR/06-integration-etl/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Importer des donn\u00e9es dans ClickHouse</li> <li>Exporter des donn\u00e9es depuis ClickHouse</li> <li>Int\u00e9grer avec Python</li> <li>Int\u00e9grer avec PowerBI/Tableau</li> </ul>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#import-de-donnees","title":"Import de donn\u00e9es","text":""},{"location":"Clickhouse/Data/FR/06-integration-etl/#depuis-csv","title":"Depuis CSV","text":"<pre><code>INSERT INTO events\nFROM INFILE '/path/to/file.csv'\nFORMAT CSV;\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#depuis-json","title":"Depuis JSON","text":"<pre><code>INSERT INTO events\nFROM INFILE '/path/to/file.json'\nFORMAT JSONEachRow;\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#via-clickhouse-client","title":"Via clickhouse-client","text":"<pre><code>clickhouse-client --query \"INSERT INTO events FORMAT CSV\" &lt; data.csv\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#export-de-donnees","title":"Export de donn\u00e9es","text":""},{"location":"Clickhouse/Data/FR/06-integration-etl/#vers-csv","title":"Vers CSV","text":"<pre><code>SELECT * FROM events\nINTO OUTFILE '/path/to/output.csv'\nFORMAT CSV;\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#vers-json","title":"Vers JSON","text":"<pre><code>SELECT * FROM events\nINTO OUTFILE '/path/to/output.json'\nFORMAT JSONEachRow;\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#via-clickhouse-client_1","title":"Via clickhouse-client","text":"<pre><code>clickhouse-client --query \"SELECT * FROM events FORMAT CSV\" &gt; output.csv\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#integration-python","title":"Int\u00e9gration Python","text":""},{"location":"Clickhouse/Data/FR/06-integration-etl/#installation","title":"Installation","text":"<pre><code>pip install clickhouse-driver\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#connexion","title":"Connexion","text":"<pre><code>from clickhouse_driver import Client\n\nclient = Client(host='localhost', port=9000, database='analytics')\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#requetes","title":"Requ\u00eates","text":"<pre><code># Ex\u00e9cuter une requ\u00eate\nresult = client.execute('SELECT * FROM events LIMIT 10')\n\n# Ins\u00e9rer des donn\u00e9es\nclient.execute('INSERT INTO events VALUES', [\n    (1, '2024-01-15', 100, 'click', 1.5),\n    (2, '2024-01-15', 101, 'view', 2.0)\n])\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#pandas","title":"Pandas","text":"<pre><code>import pandas as pd\n\n# Lire depuis ClickHouse\ndf = pd.read_sql('SELECT * FROM events', client.connection)\n\n# \u00c9crire vers ClickHouse\ndf.to_sql('events', client.connection, if_exists='append')\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#integration-powerbi","title":"Int\u00e9gration PowerBI","text":""},{"location":"Clickhouse/Data/FR/06-integration-etl/#connexion-odbc","title":"Connexion ODBC","text":"<ol> <li>Installer le driver ODBC ClickHouse</li> <li>Cr\u00e9er une source de donn\u00e9es ODBC</li> <li>Se connecter depuis PowerBI</li> </ol>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#requete-directe","title":"Requ\u00eate directe","text":"<pre><code>SELECT \n    event_date,\n    COUNT(*) as count\nFROM events\nGROUP BY event_date\n</code></pre>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#integration-tableau","title":"Int\u00e9gration Tableau","text":""},{"location":"Clickhouse/Data/FR/06-integration-etl/#connexion-native","title":"Connexion native","text":"<ol> <li>S\u00e9lectionner \"ClickHouse\" comme source</li> <li>Entrer les param\u00e8tres de connexion</li> <li>Cr\u00e9er des visualisations</li> </ol>"},{"location":"Clickhouse/Data/FR/06-integration-etl/#etl-avec-python","title":"ETL avec Python","text":""},{"location":"Clickhouse/Data/FR/06-integration-etl/#exemple-complet","title":"Exemple complet","text":"<pre><code>from clickhouse_driver import Client\nimport pandas as pd\n\n# Connexion\nclient = Client(host='localhost', port=9000)\n\n# Lire depuis source\ndf = pd.read_csv('source_data.csv')\n\n# Transformation\ndf['processed_date'] = pd.to_datetime(df['date'])\ndf = df[df['value'] &gt; 0]\n\n# \u00c9crire vers ClickHouse\nclient.execute('INSERT INTO events VALUES', df.values.tolist())\n</code></pre> <p>Prochaine \u00e9tape : Bonnes Pratiques</p>"},{"location":"Clickhouse/Data/FR/07-best-practices/","title":"7. Bonnes Pratiques","text":""},{"location":"Clickhouse/Data/FR/07-best-practices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Mod\u00e9liser les donn\u00e9es efficacement</li> <li>Choisir les strat\u00e9gies de partitionnement</li> <li>G\u00e9rer la m\u00e9moire</li> <li>S\u00e9curiser l'acc\u00e8s</li> </ul>"},{"location":"Clickhouse/Data/FR/07-best-practices/#modelisation-des-donnees","title":"Mod\u00e9lisation des donn\u00e9es","text":""},{"location":"Clickhouse/Data/FR/07-best-practices/#choisir-les-bons-types","title":"Choisir les bons types","text":"<pre><code>-- \u2705 Bon : UInt32 pour IDs\nuser_id UInt32\n\n-- \u274c Moins bon : UInt64 inutile\nuser_id UInt64\n\n-- \u2705 Bon : Date pour dates\nevent_date Date\n\n-- \u274c Moins bon : String pour dates\nevent_date String\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#eviter-les-colonnes-trop-larges","title":"\u00c9viter les colonnes trop larges","text":"<pre><code>-- \u2705 Bon : String pour textes variables\ndescription String\n\n-- \u274c Moins bon : FixedString trop large\ndescription FixedString(10000)\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#strategies-de-partitionnement","title":"Strat\u00e9gies de partitionnement","text":""},{"location":"Clickhouse/Data/FR/07-best-practices/#par-date-recommande","title":"Par date (recommand\u00e9)","text":"<pre><code>PARTITION BY toYYYYMM(event_date)\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#par-hash-pour-distribution","title":"Par hash pour distribution","text":"<pre><code>PARTITION BY intHash32(user_id) % 10\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#eviter-trop-de-partitions","title":"\u00c9viter trop de partitions","text":"<pre><code>-- \u2705 Bon : Partition mensuelle\nPARTITION BY toYYYYMM(date)\n\n-- \u274c Moins bon : Partition quotidienne (trop de partitions)\nPARTITION BY date\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#gestion-de-la-memoire","title":"Gestion de la m\u00e9moire","text":""},{"location":"Clickhouse/Data/FR/07-best-practices/#limit-les-requetes","title":"LIMIT les requ\u00eates","text":"<pre><code>-- \u2705 Bon\nSELECT * FROM events LIMIT 1000;\n\n-- \u274c Moins bon\nSELECT * FROM events;\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#eviter-les-select","title":"\u00c9viter les SELECT *","text":"<pre><code>-- \u2705 Bon\nSELECT event_date, COUNT(*) FROM events;\n\n-- \u274c Moins bon\nSELECT * FROM events;\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#securite","title":"S\u00e9curit\u00e9","text":""},{"location":"Clickhouse/Data/FR/07-best-practices/#creer-des-utilisateurs","title":"Cr\u00e9er des utilisateurs","text":"<pre><code>CREATE USER analyst IDENTIFIED BY 'password';\nGRANT SELECT ON analytics.* TO analyst;\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#permissions-granulaires","title":"Permissions granulaires","text":"<pre><code>GRANT SELECT ON analytics.events TO analyst;\nGRANT INSERT ON analytics.temp_table TO analyst;\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#maintenance","title":"Maintenance","text":""},{"location":"Clickhouse/Data/FR/07-best-practices/#verifier-les-partitions","title":"V\u00e9rifier les partitions","text":"<pre><code>SELECT \n    partition,\n    rows,\n    formatReadableSize(bytes_on_disk) as size\nFROM system.parts\nWHERE active\nORDER BY partition;\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#nettoyer-les-anciennes-donnees","title":"Nettoyer les anciennes donn\u00e9es","text":"<pre><code>ALTER TABLE events DROP PARTITION '202301';\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#optimiser-les-tables","title":"Optimiser les tables","text":"<pre><code>OPTIMIZE TABLE events FINAL;\n</code></pre>"},{"location":"Clickhouse/Data/FR/07-best-practices/#checklist","title":"Checklist","text":"<ul> <li>[ ] Types de donn\u00e9es appropri\u00e9s</li> <li>[ ] Partitionnement configur\u00e9</li> <li>[ ] ORDER BY optimis\u00e9</li> <li>[ ] Index sur colonnes filtr\u00e9es</li> <li>[ ] Requ\u00eates avec LIMIT</li> <li>[ ] Utilisateurs et permissions</li> <li>[ ] Monitoring en place</li> </ul> <p>Prochaine \u00e9tape : Projets Pratiques</p>"},{"location":"Clickhouse/Data/FR/08-projets/","title":"8. Projets Pratiques","text":""},{"location":"Clickhouse/Data/FR/08-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Appliquer les connaissances acquises</li> <li>Cr\u00e9er des projets pour votre portfolio</li> <li>R\u00e9soudre des probl\u00e8mes r\u00e9els</li> <li>Optimiser les performances</li> </ul>"},{"location":"Clickhouse/Data/FR/08-projets/#projet-1-analytics-web","title":"Projet 1 : Analytics Web","text":""},{"location":"Clickhouse/Data/FR/08-projets/#objectif","title":"Objectif","text":"<p>Analyser les logs d'un site web pour comprendre le comportement des utilisateurs.</p>"},{"location":"Clickhouse/Data/FR/08-projets/#donnees","title":"Donn\u00e9es","text":"<pre><code>CREATE TABLE web_logs\n(\n    timestamp DateTime,\n    user_id UInt32,\n    page String,\n    action String,\n    duration UInt32\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (timestamp, user_id);\n</code></pre>"},{"location":"Clickhouse/Data/FR/08-projets/#requetes-danalyse","title":"Requ\u00eates d'analyse","text":"<pre><code>-- Pages les plus visit\u00e9es\nSELECT \n    page,\n    COUNT(*) as visits,\n    AVG(duration) as avg_duration\nFROM web_logs\nGROUP BY page\nORDER BY visits DESC\nLIMIT 10;\n\n-- Utilisateurs actifs par jour\nSELECT \n    toStartOfDay(timestamp) as day,\n    COUNT(DISTINCT user_id) as active_users\nFROM web_logs\nGROUP BY day\nORDER BY day;\n</code></pre>"},{"location":"Clickhouse/Data/FR/08-projets/#projet-2-data-warehouse-analytique","title":"Projet 2 : Data Warehouse Analytique","text":""},{"location":"Clickhouse/Data/FR/08-projets/#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er un entrep\u00f4t de donn\u00e9es pour l'analyse des ventes.</p>"},{"location":"Clickhouse/Data/FR/08-projets/#schema","title":"Sch\u00e9ma","text":"<pre><code>CREATE TABLE sales\n(\n    sale_id UInt64,\n    sale_date Date,\n    product_id UInt32,\n    customer_id UInt32,\n    quantity UInt32,\n    price Decimal64(2),\n    total Decimal64(2) MATERIALIZED quantity * price\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(sale_date)\nORDER BY (sale_date, product_id);\n</code></pre>"},{"location":"Clickhouse/Data/FR/08-projets/#analyses","title":"Analyses","text":"<pre><code>-- Ventes par mois\nSELECT \n    toStartOfMonth(sale_date) as month,\n    SUM(total) as monthly_revenue,\n    COUNT(*) as sales_count\nFROM sales\nGROUP BY month\nORDER BY month;\n\n-- Top produits\nSELECT \n    product_id,\n    SUM(quantity) as total_sold,\n    SUM(total) as revenue\nFROM sales\nGROUP BY product_id\nORDER BY revenue DESC\nLIMIT 10;\n</code></pre>"},{"location":"Clickhouse/Data/FR/08-projets/#projet-3-dashboard-temps-reel","title":"Projet 3 : Dashboard Temps R\u00e9el","text":""},{"location":"Clickhouse/Data/FR/08-projets/#objectif_2","title":"Objectif","text":"<p>Cr\u00e9er des m\u00e9triques en temps r\u00e9el pour un dashboard.</p>"},{"location":"Clickhouse/Data/FR/08-projets/#table-devenements","title":"Table d'\u00e9v\u00e9nements","text":"<pre><code>CREATE TABLE realtime_events\n(\n    event_time DateTime,\n    event_type String,\n    value Float64\n)\nENGINE = MergeTree()\nORDER BY event_time;\n</code></pre>"},{"location":"Clickhouse/Data/FR/08-projets/#metriques-temps-reel","title":"M\u00e9triques temps r\u00e9el","text":"<pre><code>-- \u00c9v\u00e9nements des derni\u00e8res 24h\nSELECT \n    toStartOfHour(event_time) as hour,\n    event_type,\n    COUNT(*) as count,\n    SUM(value) as total\nFROM realtime_events\nWHERE event_time &gt;= now() - INTERVAL 24 HOUR\nGROUP BY hour, event_type\nORDER BY hour DESC;\n</code></pre>"},{"location":"Clickhouse/Data/FR/08-projets/#projet-4-analyse-iot","title":"Projet 4 : Analyse IoT","text":""},{"location":"Clickhouse/Data/FR/08-projets/#objectif_3","title":"Objectif","text":"<p>Analyser les donn\u00e9es de capteurs IoT.</p>"},{"location":"Clickhouse/Data/FR/08-projets/#schema_1","title":"Sch\u00e9ma","text":"<pre><code>CREATE TABLE sensor_data\n(\n    timestamp DateTime,\n    sensor_id UInt32,\n    temperature Float32,\n    humidity Float32,\n    pressure Float32\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (timestamp, sensor_id);\n</code></pre>"},{"location":"Clickhouse/Data/FR/08-projets/#analyses_1","title":"Analyses","text":"<pre><code>-- Temp\u00e9rature moyenne par capteur\nSELECT \n    sensor_id,\n    AVG(temperature) as avg_temp,\n    MIN(temperature) as min_temp,\n    MAX(temperature) as max_temp\nFROM sensor_data\nGROUP BY sensor_id;\n\n-- D\u00e9tection d'anomalies\nSELECT \n    sensor_id,\n    timestamp,\n    temperature\nFROM sensor_data\nWHERE temperature &gt; (\n    SELECT AVG(temperature) + 2 * stddevPop(temperature)\n    FROM sensor_data\n);\n</code></pre>"},{"location":"Clickhouse/Data/FR/08-projets/#conseils-pour-votre-portfolio","title":"Conseils pour votre portfolio","text":"<ol> <li>Documentez vos projets : Expliquez le probl\u00e8me et la solution</li> <li>Montrez les r\u00e9sultats : Visualisations, m\u00e9triques</li> <li>Code propre : Requ\u00eates optimis\u00e9es et comment\u00e9es</li> <li>Performance : Montrez les optimisations effectu\u00e9es</li> <li>GitHub : Partagez vos projets sur GitHub</li> </ol> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation ClickHouse pour Data Analyst ! \ud83c\udf89</p>"},{"location":"Clickhouse/Data/PL/","title":"Szkolenie ClickHouse dla Data Analyst","text":""},{"location":"Clickhouse/Data/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>To szkolenie poprowadzi Ci\u0119 przez nauk\u0119 ClickHouse jako Data Analyst. ClickHouse to baza danych OLAP (Online Analytical Processing) zorientowana na kolumny, zoptymalizowana pod k\u0105tem zapyta\u0144 analitycznych na bardzo du\u017cych wolumenach danych.</p>"},{"location":"Clickhouse/Data/PL/#cele-szkoleniowe","title":"\ud83c\udfaf Cele szkoleniowe","text":"<ul> <li>Zrozumie\u0107 ClickHouse i analiz\u0119 OLAP</li> <li>Zainstalowa\u0107 i skonfigurowa\u0107 ClickHouse</li> <li>Opanowa\u0107 analityczny SQL z ClickHouse</li> <li>Optymalizowa\u0107 zapytania dla du\u017cych wolumen\u00f3w</li> <li>U\u017cywa\u0107 zaawansowanych funkcji analitycznych</li> <li>Zarz\u0105dza\u0107 partycjami i kompresj\u0105</li> <li>Integrowa\u0107 ClickHouse w przep\u0142ywach pracy analitycznych</li> <li>Tworzy\u0107 praktyczne projekty do portfolio</li> </ul>"},{"location":"Clickhouse/Data/PL/#wszystko-jest-darmowe","title":"\ud83d\udcb0 Wszystko jest darmowe!","text":"<p>To szkolenie wykorzystuje tylko: - \u2705 ClickHouse Community Edition : Darmowe i open-source - \u2705 ClickHouse Client : Darmowe narz\u0119dzie wiersza polece\u0144 - \u2705 ClickHouse Playground : Darmowe \u015brodowisko online - \u2705 Oficjalna dokumentacja : Kompletne darmowe przewodniki</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"Clickhouse/Data/PL/#struktura-szkolenia","title":"\ud83d\udcd6 Struktura szkolenia","text":""},{"location":"Clickhouse/Data/PL/#1-rozpoczecie-pracy-z-clickhouse","title":"1. Rozpocz\u0119cie pracy z ClickHouse","text":""},{"location":"Clickhouse/Data/PL/#2-analityczny-sql-z-clickhouse","title":"2. Analityczny SQL z ClickHouse","text":""},{"location":"Clickhouse/Data/PL/#3-typy-danych-i-tabele","title":"3. Typy danych i tabele","text":""},{"location":"Clickhouse/Data/PL/#4-wydajnosc-i-optymalizacja","title":"4. Wydajno\u015b\u0107 i optymalizacja","text":""},{"location":"Clickhouse/Data/PL/#5-zaawansowane-funkcje","title":"5. Zaawansowane funkcje","text":""},{"location":"Clickhouse/Data/PL/#6-integracja-i-etl","title":"6. Integracja i ETL","text":""},{"location":"Clickhouse/Data/PL/#7-najlepsze-praktyki","title":"7. Najlepsze praktyki","text":""},{"location":"Clickhouse/Data/PL/#8-projekty-praktyczne","title":"8. Projekty praktyczne","text":""},{"location":"Clickhouse/Data/PL/#wymagania-wstepne","title":"\ud83d\ude80 Wymagania wst\u0119pne","text":"<ul> <li>Podstawowa znajomo\u015b\u0107 SQL</li> <li>Poj\u0119cia dotycz\u0105ce baz danych</li> <li>Python (opcjonalnie)</li> </ul>"},{"location":"Clickhouse/Data/PL/#szacowany-czas-trwania","title":"\u23f1\ufe0f Szacowany czas trwania","text":"<ul> <li>Ca\u0142kowity : 40-50 godzin</li> <li>Na modu\u0142 : 5-6 godzin</li> </ul> <p>Powodzenia w nauce! \ud83d\ude80</p>"},{"location":"Clickhouse/Data/PL/01-getting-started/","title":"1. Rozpocz\u0119cie pracy z ClickHouse","text":""},{"location":"Clickhouse/Data/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 ClickHouse i OLAP</li> <li>Zainstalowa\u0107 ClickHouse</li> <li>Zrozumie\u0107 podstawowe koncepcje</li> <li>U\u017cywa\u0107 ClickHouse Client</li> <li>Pierwsze zapytania</li> </ul>"},{"location":"Clickhouse/Data/PL/01-getting-started/#wprowadzenie-do-clickhouse","title":"Wprowadzenie do ClickHouse","text":"<p>ClickHouse = Baza danych OLAP zorientowana na kolumny</p> <ul> <li>OLAP : Online Analytical Processing</li> <li>Zorientowana na kolumny : Przechowywanie kolumnowe</li> <li>Wysoka wydajno\u015b\u0107 : Bardzo szybka na du\u017cych wolumenach</li> <li>Open-source : Darmowa i open-source</li> </ul>"},{"location":"Clickhouse/Data/PL/01-getting-started/#dlaczego-clickhouse-dla-data-analyst","title":"Dlaczego ClickHouse dla Data Analyst?","text":"<ul> <li>Du\u017ce wolumeny : Obs\u0142uguje miliardy wierszy</li> <li>Wydajno\u015b\u0107 : Ultra-szybkie zapytania analityczne</li> <li>Kompresja : Efektywna kompresja</li> <li>SQL : Standardowy j\u0119zyk SQL</li> </ul>"},{"location":"Clickhouse/Data/PL/01-getting-started/#typowe-przypadki-uzycia","title":"Typowe przypadki u\u017cycia","text":"<ul> <li>Analityka webowa (logi, zdarzenia)</li> <li>Hurtownia danych</li> <li>Business Intelligence</li> <li>IoT (czujniki)</li> </ul>"},{"location":"Clickhouse/Data/PL/01-getting-started/#instalacja","title":"Instalacja","text":""},{"location":"Clickhouse/Data/PL/01-getting-started/#opcja-1-clickhouse-playground-zalecane","title":"Opcja 1 : ClickHouse Playground (zalecane)","text":"<ol> <li>Przejd\u017a do ClickHouse Playground</li> <li>Brak potrzeby instalacji</li> </ol>"},{"location":"Clickhouse/Data/PL/01-getting-started/#opcja-2-docker","title":"Opcja 2 : Docker","text":"<pre><code>docker run -d --name clickhouse-server -p 8123:8123 -p 9000:9000 clickhouse/clickhouse-server\n</code></pre>"},{"location":"Clickhouse/Data/PL/01-getting-started/#opcja-3-instalacja-lokalna-linux","title":"Opcja 3 : Instalacja lokalna (Linux)","text":"<pre><code>sudo apt-get install -y apt-transport-https ca-certificates\necho \"deb https://repo.clickhouse.com/deb stable main\" | sudo tee /etc/apt/sources.list.d/clickhouse.list\nsudo apt-get update\nsudo apt-get install -y clickhouse-server clickhouse-client\nsudo service clickhouse-server start\n</code></pre>"},{"location":"Clickhouse/Data/PL/01-getting-started/#podstawowe-koncepcje","title":"Podstawowe koncepcje","text":""},{"location":"Clickhouse/Data/PL/01-getting-started/#przechowywanie-kolumnowe","title":"Przechowywanie kolumnowe","text":"<p>Zalety : - Efektywna kompresja - Szybkie zapytania analityczne - Zoptymalizowane agregacje</p>"},{"location":"Clickhouse/Data/PL/01-getting-started/#silniki-tabel","title":"Silniki tabel","text":"<ul> <li>MergeTree : Do danych trwa\u0142ych</li> <li>Memory : Do danych w pami\u0119ci</li> <li>Log : Do ma\u0142ych wolumen\u00f3w</li> </ul>"},{"location":"Clickhouse/Data/PL/01-getting-started/#clickhouse-client","title":"ClickHouse Client","text":"<pre><code>clickhouse-client\nclickhouse-client --host localhost --port 9000\n</code></pre>"},{"location":"Clickhouse/Data/PL/01-getting-started/#podstawowe-polecenia","title":"Podstawowe polecenia","text":"<pre><code>SHOW DATABASES;\nUSE default;\nSHOW TABLES;\nDESCRIBE table_name;\n</code></pre>"},{"location":"Clickhouse/Data/PL/01-getting-started/#pierwsze-zapytania","title":"Pierwsze zapytania","text":""},{"location":"Clickhouse/Data/PL/01-getting-started/#utworzenie-bazy-danych","title":"Utworzenie bazy danych","text":"<pre><code>CREATE DATABASE IF NOT EXISTS analytics;\nUSE analytics;\n</code></pre>"},{"location":"Clickhouse/Data/PL/01-getting-started/#utworzenie-tabeli","title":"Utworzenie tabeli","text":"<pre><code>CREATE TABLE IF NOT EXISTS events\n(\n    id UInt64,\n    event_date Date,\n    event_time DateTime,\n    user_id UInt32,\n    event_type String,\n    value Float64\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(event_date)\nORDER BY (event_date, event_time);\n</code></pre>"},{"location":"Clickhouse/Data/PL/01-getting-started/#wstawienie-danych","title":"Wstawienie danych","text":"<pre><code>INSERT INTO events VALUES\n(1, '2024-01-15', '2024-01-15 10:00:00', 100, 'click', 1.5),\n(2, '2024-01-15', '2024-01-15 10:01:00', 101, 'view', 2.0);\n</code></pre>"},{"location":"Clickhouse/Data/PL/01-getting-started/#zapytanie-select","title":"Zapytanie SELECT","text":"<pre><code>SELECT * FROM events;\n\nSELECT \n    event_type,\n    COUNT(*) as count,\n    SUM(value) as total_value\nFROM events\nWHERE event_date = '2024-01-15'\nGROUP BY event_type;\n</code></pre> <p>Nast\u0119pny krok : Analityczny SQL z ClickHouse</p>"},{"location":"Clickhouse/Data/PL/02-sql-analytique/","title":"2. Analityczny SQL z ClickHouse","text":""},{"location":"Clickhouse/Data/PL/02-sql-analytique/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Opanowa\u0107 zaawansowane zapytania SELECT</li> <li>U\u017cywa\u0107 agregacji i GROUP BY</li> <li>Zrozumie\u0107 funkcje analityczne</li> <li>U\u017cywa\u0107 funkcji okienkowych</li> </ul>"},{"location":"Clickhouse/Data/PL/02-sql-analytique/#zaawansowane-agregacje","title":"Zaawansowane agregacje","text":""},{"location":"Clickhouse/Data/PL/02-sql-analytique/#group-by-z-wieloma-kolumnami","title":"GROUP BY z wieloma kolumnami","text":"<pre><code>SELECT \n    event_date,\n    event_type,\n    COUNT(*) as count,\n    SUM(value) as total_value,\n    AVG(value) as avg_value\nFROM events\nGROUP BY event_date, event_type\nORDER BY event_date, event_type;\n</code></pre>"},{"location":"Clickhouse/Data/PL/02-sql-analytique/#having-do-filtrowania-agregacji","title":"HAVING do filtrowania agregacji","text":"<pre><code>SELECT \n    event_type,\n    COUNT(*) as count\nFROM events\nGROUP BY event_type\nHAVING count &gt; 10;\n</code></pre>"},{"location":"Clickhouse/Data/PL/02-sql-analytique/#funkcje-agregacji","title":"Funkcje agregacji","text":""},{"location":"Clickhouse/Data/PL/02-sql-analytique/#funkcje-podstawowe","title":"Funkcje podstawowe","text":"<pre><code>SELECT \n    COUNT(*) as total_rows,\n    COUNT(DISTINCT user_id) as unique_users,\n    SUM(value) as total_value,\n    AVG(value) as avg_value,\n    MIN(value) as min_value,\n    MAX(value) as max_value\nFROM events;\n</code></pre>"},{"location":"Clickhouse/Data/PL/02-sql-analytique/#funkcje-statystyczne","title":"Funkcje statystyczne","text":"<pre><code>SELECT \n    event_type,\n    quantile(0.5)(value) as median,\n    quantile(0.95)(value) as p95,\n    stddevPop(value) as std_dev\nFROM events\nGROUP BY event_type;\n</code></pre>"},{"location":"Clickhouse/Data/PL/02-sql-analytique/#funkcje-okienkowe","title":"Funkcje okienkowe","text":""},{"location":"Clickhouse/Data/PL/02-sql-analytique/#row_number-rank-dense_rank","title":"ROW_NUMBER, RANK, DENSE_RANK","text":"<pre><code>SELECT \n    user_id,\n    event_date,\n    value,\n    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY value DESC) as rank\nFROM events;\n</code></pre>"},{"location":"Clickhouse/Data/PL/02-sql-analytique/#lag-i-lead","title":"LAG i LEAD","text":"<pre><code>SELECT \n    event_date,\n    value,\n    LAG(value) OVER (ORDER BY event_date) as prev_value,\n    LEAD(value) OVER (ORDER BY event_date) as next_value\nFROM events\nORDER BY event_date;\n</code></pre> <p>Nast\u0119pny krok : Typy danych i tabele</p>"},{"location":"Clickhouse/Data/PL/03-types-tables/","title":"3. Typy danych i tabele","text":""},{"location":"Clickhouse/Data/PL/03-types-tables/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 typy danych ClickHouse</li> <li>Tworzy\u0107 zoptymalizowane tabele</li> <li>Wybiera\u0107 odpowiedni silnik tabeli</li> <li>Konfigurowa\u0107 partycjonowanie</li> </ul>"},{"location":"Clickhouse/Data/PL/03-types-tables/#typy-danych","title":"Typy danych","text":""},{"location":"Clickhouse/Data/PL/03-types-tables/#typy-cakowite","title":"Typy ca\u0142kowite","text":"<pre><code>UInt8, UInt16, UInt32, UInt64  -- Liczby ca\u0142kowite bez znaku\nInt8, Int16, Int32, Int64      -- Liczby ca\u0142kowite ze znakiem\n</code></pre>"},{"location":"Clickhouse/Data/PL/03-types-tables/#typy-dziesietne","title":"Typy dziesi\u0119tne","text":"<pre><code>Float32, Float64               -- Liczby zmiennoprzecinkowe\nDecimal32, Decimal64, Decimal128  -- Dok\u0142adne liczby dziesi\u0119tne\n</code></pre>"},{"location":"Clickhouse/Data/PL/03-types-tables/#typy-ancuchowe","title":"Typy \u0142a\u0144cuchowe","text":"<pre><code>String                        -- \u0141a\u0144cuch znak\u00f3w\nFixedString(N)                -- \u0141a\u0144cuch o sta\u0142ej d\u0142ugo\u015bci\n</code></pre>"},{"location":"Clickhouse/Data/PL/03-types-tables/#typy-datyczasu","title":"Typy daty/czasu","text":"<pre><code>Date                          -- Data (YYYY-MM-DD)\nDateTime                      -- Data i czas\nDateTime64                    -- Data i czas z precyzj\u0105\n</code></pre>"},{"location":"Clickhouse/Data/PL/03-types-tables/#tworzenie-tabel","title":"Tworzenie tabel","text":""},{"location":"Clickhouse/Data/PL/03-types-tables/#tabela-mergetree-zalecana","title":"Tabela MergeTree (zalecana)","text":"<pre><code>CREATE TABLE events\n(\n    id UInt64,\n    event_date Date,\n    event_time DateTime,\n    user_id UInt32,\n    event_type String,\n    value Float64\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(event_date)\nORDER BY (event_date, user_id);\n</code></pre>"},{"location":"Clickhouse/Data/PL/03-types-tables/#tabela-memory","title":"Tabela Memory","text":"<pre><code>CREATE TABLE temp_data\n(\n    id UInt64,\n    name String\n)\nENGINE = Memory;\n</code></pre>"},{"location":"Clickhouse/Data/PL/03-types-tables/#partycjonowanie","title":"Partycjonowanie","text":""},{"location":"Clickhouse/Data/PL/03-types-tables/#wedug-daty-zalecane","title":"Wed\u0142ug daty (zalecane)","text":"<pre><code>PARTITION BY toYYYYMM(event_date)\n</code></pre>"},{"location":"Clickhouse/Data/PL/03-types-tables/#wedug-hash","title":"Wed\u0142ug hash","text":"<pre><code>PARTITION BY intHash32(user_id) % 10\n</code></pre>"},{"location":"Clickhouse/Data/PL/03-types-tables/#silniki-tabel","title":"Silniki tabel","text":"<ul> <li>MergeTree : Do danych trwa\u0142ych, du\u017cych wolumen\u00f3w</li> <li>Memory : Do danych tymczasowych</li> <li>Log : Do ma\u0142ych wolumen\u00f3w, log\u00f3w</li> </ul> <p>Nast\u0119pny krok : Wydajno\u015b\u0107 i optymalizacja</p>"},{"location":"Clickhouse/Data/PL/04-performance/","title":"4. Wydajno\u015b\u0107 i optymalizacja","text":""},{"location":"Clickhouse/Data/PL/04-performance/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 indeksy ClickHouse</li> <li>Optymalizowa\u0107 zapytania</li> <li>Zarz\u0105dza\u0107 kompresj\u0105</li> <li>Monitorowa\u0107 wydajno\u015b\u0107</li> </ul>"},{"location":"Clickhouse/Data/PL/04-performance/#indeksy-i-projekcje","title":"Indeksy i projekcje","text":""},{"location":"Clickhouse/Data/PL/04-performance/#indeks-podstawowy-order-by","title":"Indeks podstawowy (ORDER BY)","text":"<pre><code>CREATE TABLE events\n(\n    id UInt64,\n    event_date Date,\n    user_id UInt32\n)\nENGINE = MergeTree()\nORDER BY (event_date, user_id);  -- Indeks podstawowy\n</code></pre>"},{"location":"Clickhouse/Data/PL/04-performance/#optymalizacja-zapytan","title":"Optymalizacja zapyta\u0144","text":""},{"location":"Clickhouse/Data/PL/04-performance/#uzywac-where-na-kolumnach-zindeksowanych","title":"U\u017cywa\u0107 WHERE na kolumnach zindeksowanych","text":"<pre><code>-- \u2705 Dobrze : u\u017cywa indeksu\nSELECT * FROM events \nWHERE event_date = '2024-01-15';\n\n-- \u274c Mniej dobrze : pe\u0142ne skanowanie\nSELECT * FROM events \nWHERE value &gt; 100;\n</code></pre>"},{"location":"Clickhouse/Data/PL/04-performance/#limit-do-ograniczenia-wynikow","title":"LIMIT do ograniczenia wynik\u00f3w","text":"<pre><code>SELECT * FROM events \nORDER BY event_date DESC \nLIMIT 100;\n</code></pre>"},{"location":"Clickhouse/Data/PL/04-performance/#unikac-select","title":"Unika\u0107 SELECT *","text":"<pre><code>-- \u2705 Dobrze\nSELECT event_date, COUNT(*) \nFROM events \nGROUP BY event_date;\n\n-- \u274c Mniej dobrze\nSELECT * FROM events;\n</code></pre>"},{"location":"Clickhouse/Data/PL/04-performance/#kompresja","title":"Kompresja","text":""},{"location":"Clickhouse/Data/PL/04-performance/#sprawdzic-kompresje","title":"Sprawdzi\u0107 kompresj\u0119","text":"<pre><code>SELECT \n    table,\n    formatReadableSize(sum(data_compressed_bytes)) as compressed,\n    formatReadableSize(sum(data_uncompressed_bytes)) as uncompressed,\n    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) as ratio\nFROM system.parts\nWHERE active\nGROUP BY table;\n</code></pre>"},{"location":"Clickhouse/Data/PL/04-performance/#monitorowanie","title":"Monitorowanie","text":""},{"location":"Clickhouse/Data/PL/04-performance/#wolne-zapytania","title":"Wolne zapytania","text":"<pre><code>SELECT \n    query,\n    query_duration_ms,\n    read_rows,\n    read_bytes\nFROM system.query_log\nWHERE type = 'QueryFinish'\nORDER BY query_duration_ms DESC\nLIMIT 10;\n</code></pre> <p>Nast\u0119pny krok : Zaawansowane funkcje</p>"},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/","title":"5. Zaawansowane funkcje","text":""},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Opanowa\u0107 funkcje daty/czasu</li> <li>U\u017cywa\u0107 funkcji matematycznych</li> <li>Manipulowa\u0107 \u0142a\u0144cuchami znak\u00f3w</li> <li>U\u017cywa\u0107 zaawansowanych funkcji agregacji</li> </ul>"},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#funkcje-daty-i-czasu","title":"Funkcje daty i czasu","text":""},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#ekstrakcja","title":"Ekstrakcja","text":"<pre><code>SELECT \n    toYear(event_time) as year,\n    toMonth(event_time) as month,\n    toDayOfMonth(event_time) as day,\n    toHour(event_time) as hour;\n</code></pre>"},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#agregacja-czasowa","title":"Agregacja czasowa","text":"<pre><code>SELECT \n    toStartOfDay(event_time) as day,\n    toStartOfHour(event_time) as hour,\n    toStartOfWeek(event_time) as week,\n    toStartOfMonth(event_time) as month;\n</code></pre>"},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#funkcje-matematyczne","title":"Funkcje matematyczne","text":""},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#zaokraglanie","title":"Zaokr\u0105glanie","text":"<pre><code>SELECT \n    round(value, 2) as rounded,\n    floor(value) as floor_val,\n    ceil(value) as ceil_val;\n</code></pre>"},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#statystyki","title":"Statystyki","text":"<pre><code>SELECT \n    quantile(0.5)(value) as median,\n    quantile(0.95)(value) as p95,\n    stddevPop(value) as std_dev;\n</code></pre>"},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#funkcje-ancuchowe","title":"Funkcje \u0142a\u0144cuchowe","text":""},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#manipulacja","title":"Manipulacja","text":"<pre><code>SELECT \n    length(name) as name_length,\n    upper(name) as upper_name,\n    lower(name) as lower_name;\n</code></pre>"},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#wyszukiwanie","title":"Wyszukiwanie","text":"<pre><code>SELECT \n    position(name, 'test') as pos,\n    match(name, 'test.*') as matches,\n    replace(name, 'old', 'new') as replaced;\n</code></pre>"},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#zaawansowane-funkcje-agregacji","title":"Zaawansowane funkcje agregacji","text":""},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#kwantyle","title":"Kwantyle","text":"<pre><code>SELECT \n    quantile(0.5)(value) as median,\n    quantiles(0.5, 0.9, 0.99)(value) as quantiles;\n</code></pre>"},{"location":"Clickhouse/Data/PL/05-fonctions-avancees/#topk","title":"TopK","text":"<pre><code>SELECT \n    topK(10)(event_type) as top_10_types,\n    topKWeighted(10)(event_type, value) as top_10_weighted;\n</code></pre> <p>Nast\u0119pny krok : Integracja i ETL</p>"},{"location":"Clickhouse/Data/PL/06-integration-etl/","title":"6. Integracja i ETL","text":""},{"location":"Clickhouse/Data/PL/06-integration-etl/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Importowa\u0107 dane do ClickHouse</li> <li>Eksportowa\u0107 dane z ClickHouse</li> <li>Integrowa\u0107 z Python</li> <li>Integrowa\u0107 z PowerBI/Tableau</li> </ul>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#import-danych","title":"Import danych","text":""},{"location":"Clickhouse/Data/PL/06-integration-etl/#z-csv","title":"Z CSV","text":"<pre><code>INSERT INTO events\nFROM INFILE '/path/to/file.csv'\nFORMAT CSV;\n</code></pre>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#z-json","title":"Z JSON","text":"<pre><code>INSERT INTO events\nFROM INFILE '/path/to/file.json'\nFORMAT JSONEachRow;\n</code></pre>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#przez-clickhouse-client","title":"Przez clickhouse-client","text":"<pre><code>clickhouse-client --query \"INSERT INTO events FORMAT CSV\" &lt; data.csv\n</code></pre>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#eksport-danych","title":"Eksport danych","text":""},{"location":"Clickhouse/Data/PL/06-integration-etl/#do-csv","title":"Do CSV","text":"<pre><code>SELECT * FROM events\nINTO OUTFILE '/path/to/output.csv'\nFORMAT CSV;\n</code></pre>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#przez-clickhouse-client_1","title":"Przez clickhouse-client","text":"<pre><code>clickhouse-client --query \"SELECT * FROM events FORMAT CSV\" &gt; output.csv\n</code></pre>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#integracja-python","title":"Integracja Python","text":""},{"location":"Clickhouse/Data/PL/06-integration-etl/#instalacja","title":"Instalacja","text":"<pre><code>pip install clickhouse-driver\n</code></pre>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#poaczenie","title":"Po\u0142\u0105czenie","text":"<pre><code>from clickhouse_driver import Client\n\nclient = Client(host='localhost', port=9000, database='analytics')\n</code></pre>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#zapytania","title":"Zapytania","text":"<pre><code># Wykonaj zapytanie\nresult = client.execute('SELECT * FROM events LIMIT 10')\n\n# Wstaw dane\nclient.execute('INSERT INTO events VALUES', [\n    (1, '2024-01-15', 100, 'click', 1.5),\n    (2, '2024-01-15', 101, 'view', 2.0)\n])\n</code></pre>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#pandas","title":"Pandas","text":"<pre><code>import pandas as pd\n\n# Czytaj z ClickHouse\ndf = pd.read_sql('SELECT * FROM events', client.connection)\n\n# Zapisz do ClickHouse\ndf.to_sql('events', client.connection, if_exists='append')\n</code></pre>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#integracja-powerbi","title":"Integracja PowerBI","text":""},{"location":"Clickhouse/Data/PL/06-integration-etl/#poaczenie-odbc","title":"Po\u0142\u0105czenie ODBC","text":"<ol> <li>Zainstaluj sterownik ODBC ClickHouse</li> <li>Utw\u00f3rz \u017ar\u00f3d\u0142o danych ODBC</li> <li>Po\u0142\u0105cz z PowerBI</li> </ol>"},{"location":"Clickhouse/Data/PL/06-integration-etl/#etl-z-python","title":"ETL z Python","text":""},{"location":"Clickhouse/Data/PL/06-integration-etl/#kompletny-przykad","title":"Kompletny przyk\u0142ad","text":"<pre><code>from clickhouse_driver import Client\nimport pandas as pd\n\n# Po\u0142\u0105czenie\nclient = Client(host='localhost', port=9000)\n\n# Czytaj ze \u017ar\u00f3d\u0142a\ndf = pd.read_csv('source_data.csv')\n\n# Transformacja\ndf['processed_date'] = pd.to_datetime(df['date'])\ndf = df[df['value'] &gt; 0]\n\n# Zapisz do ClickHouse\nclient.execute('INSERT INTO events VALUES', df.values.tolist())\n</code></pre> <p>Nast\u0119pny krok : Najlepsze praktyki</p>"},{"location":"Clickhouse/Data/PL/07-best-practices/","title":"7. Najlepsze praktyki","text":""},{"location":"Clickhouse/Data/PL/07-best-practices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Modelowa\u0107 dane efektywnie</li> <li>Wybiera\u0107 strategie partycjonowania</li> <li>Zarz\u0105dza\u0107 pami\u0119ci\u0105</li> <li>Zabezpiecza\u0107 dost\u0119p</li> </ul>"},{"location":"Clickhouse/Data/PL/07-best-practices/#modelowanie-danych","title":"Modelowanie danych","text":""},{"location":"Clickhouse/Data/PL/07-best-practices/#wybierac-odpowiednie-typy","title":"Wybiera\u0107 odpowiednie typy","text":"<pre><code>-- \u2705 Dobrze : UInt32 dla ID\nuser_id UInt32\n\n-- \u274c Mniej dobrze : UInt64 niepotrzebne\nuser_id UInt64\n\n-- \u2705 Dobrze : Date dla dat\nevent_date Date\n\n-- \u274c Mniej dobrze : String dla dat\nevent_date String\n</code></pre>"},{"location":"Clickhouse/Data/PL/07-best-practices/#strategie-partycjonowania","title":"Strategie partycjonowania","text":""},{"location":"Clickhouse/Data/PL/07-best-practices/#wedug-daty-zalecane","title":"Wed\u0142ug daty (zalecane)","text":"<pre><code>PARTITION BY toYYYYMM(event_date)\n</code></pre>"},{"location":"Clickhouse/Data/PL/07-best-practices/#unikac-zbyt-wielu-partycji","title":"Unika\u0107 zbyt wielu partycji","text":"<pre><code>-- \u2705 Dobrze : Partycja miesi\u0119czna\nPARTITION BY toYYYYMM(date)\n\n-- \u274c Mniej dobrze : Partycja dzienna (zbyt wiele)\nPARTITION BY date\n</code></pre>"},{"location":"Clickhouse/Data/PL/07-best-practices/#zarzadzanie-pamiecia","title":"Zarz\u0105dzanie pami\u0119ci\u0105","text":""},{"location":"Clickhouse/Data/PL/07-best-practices/#limit-zapytan","title":"LIMIT zapyta\u0144","text":"<pre><code>-- \u2705 Dobrze\nSELECT * FROM events LIMIT 1000;\n\n-- \u274c Mniej dobrze\nSELECT * FROM events;\n</code></pre>"},{"location":"Clickhouse/Data/PL/07-best-practices/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":""},{"location":"Clickhouse/Data/PL/07-best-practices/#tworzenie-uzytkownikow","title":"Tworzenie u\u017cytkownik\u00f3w","text":"<pre><code>CREATE USER analyst IDENTIFIED BY 'password';\nGRANT SELECT ON analytics.* TO analyst;\n</code></pre>"},{"location":"Clickhouse/Data/PL/07-best-practices/#szczegoowe-uprawnienia","title":"Szczeg\u00f3\u0142owe uprawnienia","text":"<pre><code>GRANT SELECT ON analytics.events TO analyst;\nGRANT INSERT ON analytics.temp_table TO analyst;\n</code></pre>"},{"location":"Clickhouse/Data/PL/07-best-practices/#konserwacja","title":"Konserwacja","text":""},{"location":"Clickhouse/Data/PL/07-best-practices/#sprawdzac-partycje","title":"Sprawdza\u0107 partycje","text":"<pre><code>SELECT \n    partition,\n    rows,\n    formatReadableSize(bytes_on_disk) as size\nFROM system.parts\nWHERE active\nORDER BY partition;\n</code></pre>"},{"location":"Clickhouse/Data/PL/07-best-practices/#czyscic-stare-dane","title":"Czy\u015bci\u0107 stare dane","text":"<pre><code>ALTER TABLE events DROP PARTITION '202301';\n</code></pre>"},{"location":"Clickhouse/Data/PL/07-best-practices/#optymalizowac-tabele","title":"Optymalizowa\u0107 tabele","text":"<pre><code>OPTIMIZE TABLE events FINAL;\n</code></pre> <p>Nast\u0119pny krok : Projekty praktyczne</p>"},{"location":"Clickhouse/Data/PL/08-projets/","title":"8. Projekty praktyczne","text":""},{"location":"Clickhouse/Data/PL/08-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zastosowa\u0107 zdobyt\u0105 wiedz\u0119</li> <li>Tworzy\u0107 projekty do portfolio</li> <li>Rozwi\u0105zywa\u0107 rzeczywiste problemy</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> </ul>"},{"location":"Clickhouse/Data/PL/08-projets/#projekt-1-analityka-webowa","title":"Projekt 1 : Analityka webowa","text":""},{"location":"Clickhouse/Data/PL/08-projets/#cel","title":"Cel","text":"<p>Analizowa\u0107 logi strony internetowej, aby zrozumie\u0107 zachowanie u\u017cytkownik\u00f3w.</p>"},{"location":"Clickhouse/Data/PL/08-projets/#dane","title":"Dane","text":"<pre><code>CREATE TABLE web_logs\n(\n    timestamp DateTime,\n    user_id UInt32,\n    page String,\n    action String,\n    duration UInt32\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(timestamp)\nORDER BY (timestamp, user_id);\n</code></pre>"},{"location":"Clickhouse/Data/PL/08-projets/#zapytania-analityczne","title":"Zapytania analityczne","text":"<pre><code>-- Najcz\u0119\u015bciej odwiedzane strony\nSELECT \n    page,\n    COUNT(*) as visits,\n    AVG(duration) as avg_duration\nFROM web_logs\nGROUP BY page\nORDER BY visits DESC\nLIMIT 10;\n\n-- Aktywni u\u017cytkownicy dziennie\nSELECT \n    toStartOfDay(timestamp) as day,\n    COUNT(DISTINCT user_id) as active_users\nFROM web_logs\nGROUP BY day\nORDER BY day;\n</code></pre>"},{"location":"Clickhouse/Data/PL/08-projets/#projekt-2-hurtownia-danych-analityczna","title":"Projekt 2 : Hurtownia danych analityczna","text":""},{"location":"Clickhouse/Data/PL/08-projets/#cel_1","title":"Cel","text":"<p>Utworzy\u0107 hurtowni\u0119 danych do analizy sprzeda\u017cy.</p>"},{"location":"Clickhouse/Data/PL/08-projets/#schemat","title":"Schemat","text":"<pre><code>CREATE TABLE sales\n(\n    sale_id UInt64,\n    sale_date Date,\n    product_id UInt32,\n    customer_id UInt32,\n    quantity UInt32,\n    price Decimal64(2),\n    total Decimal64(2) MATERIALIZED quantity * price\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(sale_date)\nORDER BY (sale_date, product_id);\n</code></pre>"},{"location":"Clickhouse/Data/PL/08-projets/#analizy","title":"Analizy","text":"<pre><code>-- Sprzeda\u017c wed\u0142ug miesi\u0105ca\nSELECT \n    toStartOfMonth(sale_date) as month,\n    SUM(total) as monthly_revenue,\n    COUNT(*) as sales_count\nFROM sales\nGROUP BY month\nORDER BY month;\n\n-- Top produkty\nSELECT \n    product_id,\n    SUM(quantity) as total_sold,\n    SUM(total) as revenue\nFROM sales\nGROUP BY product_id\nORDER BY revenue DESC\nLIMIT 10;\n</code></pre>"},{"location":"Clickhouse/Data/PL/08-projets/#projekt-3-dashboard-czasu-rzeczywistego","title":"Projekt 3 : Dashboard czasu rzeczywistego","text":""},{"location":"Clickhouse/Data/PL/08-projets/#cel_2","title":"Cel","text":"<p>Tworzy\u0107 metryki czasu rzeczywistego dla dashboardu.</p>"},{"location":"Clickhouse/Data/PL/08-projets/#tabela-zdarzen","title":"Tabela zdarze\u0144","text":"<pre><code>CREATE TABLE realtime_events\n(\n    event_time DateTime,\n    event_type String,\n    value Float64\n)\nENGINE = MergeTree()\nORDER BY event_time;\n</code></pre>"},{"location":"Clickhouse/Data/PL/08-projets/#metryki-czasu-rzeczywistego","title":"Metryki czasu rzeczywistego","text":"<pre><code>-- Zdarzenia z ostatnich 24h\nSELECT \n    toStartOfHour(event_time) as hour,\n    event_type,\n    COUNT(*) as count,\n    SUM(value) as total\nFROM realtime_events\nWHERE event_time &gt;= now() - INTERVAL 24 HOUR\nGROUP BY hour, event_type\nORDER BY hour DESC;\n</code></pre>"},{"location":"Clickhouse/Data/PL/08-projets/#wskazowki-do-portfolio","title":"Wskaz\u00f3wki do portfolio","text":"<ol> <li>Dokumentuj projekty : Wyja\u015bnij problem i rozwi\u0105zanie</li> <li>Poka\u017c wyniki : Wizualizacje, metryki</li> <li>Czysty kod : Zoptymalizowane i skomentowane zapytania</li> <li>Wydajno\u015b\u0107 : Poka\u017c wykonane optymalizacje</li> <li>GitHub : Udost\u0119pnij projekty na GitHub</li> </ol> <p>Gratulacje! Uko\u0144czy\u0142e\u015b szkolenie ClickHouse dla Data Analyst! \ud83c\udf89</p>"},{"location":"Clickhouse/Dev/EN/","title":"ClickHouse Training for Java Developer","text":""},{"location":"Clickhouse/Dev/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through integrating ClickHouse into your Java applications. You will learn to use ClickHouse with Java, JDBC drivers, queries, optimization and best practices.</p>"},{"location":"Clickhouse/Dev/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Integrate ClickHouse into Java applications</li> <li>Use ClickHouse JDBC driver</li> <li>Execute SQL queries from Java</li> <li>Manage connections and pooling</li> <li>Optimize performance</li> <li>Handle transactions and errors</li> <li>Create complete applications</li> </ul>"},{"location":"Clickhouse/Dev/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 ClickHouse Community Edition : Free - \u2705 ClickHouse JDBC Driver : Open-source - \u2705 Java : OpenJDK free - \u2705 Maven/Gradle : Free tools</p> <p>Total budget: $0</p>"},{"location":"Clickhouse/Dev/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"Clickhouse/Dev/EN/#1-getting-started-with-clickhouse-and-java","title":"1. Getting Started with ClickHouse and Java","text":""},{"location":"Clickhouse/Dev/EN/#2-jdbc-driver-and-connection","title":"2. JDBC Driver and Connection","text":""},{"location":"Clickhouse/Dev/EN/#3-sql-queries-from-java","title":"3. SQL Queries from Java","text":""},{"location":"Clickhouse/Dev/EN/#4-data-management","title":"4. Data Management","text":""},{"location":"Clickhouse/Dev/EN/#5-performance-and-optimization","title":"5. Performance and Optimization","text":""},{"location":"Clickhouse/Dev/EN/#6-error-handling","title":"6. Error Handling","text":""},{"location":"Clickhouse/Dev/EN/#7-best-practices","title":"7. Best Practices","text":""},{"location":"Clickhouse/Dev/EN/#8-practical-projects","title":"8. Practical Projects","text":""},{"location":"Clickhouse/Dev/EN/#prerequisites","title":"\ud83d\ude80 Prerequisites","text":"<ul> <li>Java knowledge (basics)</li> <li>Database concepts</li> <li>Maven or Gradle</li> <li>Java IDE (IntelliJ, Eclipse, VS Code)</li> </ul>"},{"location":"Clickhouse/Dev/EN/#estimated-duration","title":"\u23f1\ufe0f Estimated Duration","text":"<ul> <li>Total : 30-40 hours</li> <li>Per module : 4-5 hours</li> </ul> <p>Happy learning! \ud83d\ude80</p>"},{"location":"Clickhouse/Dev/EN/01-getting-started/","title":"1. Getting Started with ClickHouse and Java","text":""},{"location":"Clickhouse/Dev/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand ClickHouse and Java</li> <li>Set up development environment</li> <li>Install JDBC driver</li> <li>Create first project</li> </ul>"},{"location":"Clickhouse/Dev/EN/01-getting-started/#maven-configuration","title":"Maven Configuration","text":""},{"location":"Clickhouse/Dev/EN/01-getting-started/#pomxml","title":"pom.xml","text":"<pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.clickhouse&lt;/groupId&gt;\n        &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt;\n        &lt;version&gt;0.6.0&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"Clickhouse/Dev/EN/01-getting-started/#first-example","title":"First Example","text":"<pre><code>import com.clickhouse.jdbc.ClickHouseConnection;\nimport com.clickhouse.jdbc.ClickHouseDataSource;\n\npublic class ClickHouseExample {\n    public static void main(String[] args) {\n        String url = \"jdbc:clickhouse://localhost:8123/default\";\n\n        try (ClickHouseConnection conn = \n             new ClickHouseDataSource(url).getConnection()) {\n            System.out.println(\"Connected to ClickHouse!\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre> <p>Next step : JDBC Driver and Connection</p>"},{"location":"Clickhouse/Dev/EN/02-jdbc-connection/","title":"2. JDBC Driver and Connection","text":""},{"location":"Clickhouse/Dev/EN/02-jdbc-connection/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Configure JDBC connection</li> <li>Manage connection pooling</li> <li>Use different connection types</li> <li>Manage connection parameters</li> </ul>"},{"location":"Clickhouse/Dev/EN/02-jdbc-connection/#connection-url","title":"Connection URL","text":"<pre><code>String url = \"jdbc:clickhouse://localhost:8123/default\";\nString url = \"jdbc:clickhouse://localhost:8123/default?user=default&amp;password=password\";\n</code></pre>"},{"location":"Clickhouse/Dev/EN/02-jdbc-connection/#connection-pooling","title":"Connection Pooling","text":""},{"location":"Clickhouse/Dev/EN/02-jdbc-connection/#with-hikaricp","title":"With HikariCP","text":"<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.zaxxer&lt;/groupId&gt;\n    &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;\n    &lt;version&gt;5.0.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>HikariConfig config = new HikariConfig();\nconfig.setJdbcUrl(\"jdbc:clickhouse://localhost:8123/default\");\nconfig.setMaximumPoolSize(10);\nHikariDataSource dataSource = new HikariDataSource(config);\n</code></pre> <p>Next step : SQL Queries from Java</p>"},{"location":"Clickhouse/Dev/EN/03-queries/","title":"3. SQL Queries from Java","text":""},{"location":"Clickhouse/Dev/EN/03-queries/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Execute SELECT queries</li> <li>Use PreparedStatement</li> <li>Handle results</li> <li>Execute INSERT/UPDATE/DELETE</li> </ul>"},{"location":"Clickhouse/Dev/EN/03-queries/#select-query","title":"SELECT Query","text":"<pre><code>try (Connection conn = ConnectionManager.getConnection();\n     Statement stmt = conn.createStatement();\n     ResultSet rs = stmt.executeQuery(\"SELECT * FROM events LIMIT 10\")) {\n\n    while (rs.next()) {\n        long id = rs.getLong(\"id\");\n        String eventType = rs.getString(\"event_type\");\n        System.out.println(id + \" - \" + eventType);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/EN/03-queries/#preparedstatement","title":"PreparedStatement","text":"<pre><code>String sql = \"SELECT * FROM events WHERE event_date = ?\";\ntry (PreparedStatement pstmt = conn.prepareStatement(sql)) {\n    pstmt.setDate(1, Date.valueOf(\"2024-01-15\"));\n    try (ResultSet rs = pstmt.executeQuery()) {\n        // Process results\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/EN/03-queries/#insert","title":"INSERT","text":"<pre><code>String sql = \"INSERT INTO events (id, event_date, user_id, event_type, value) VALUES (?, ?, ?, ?, ?)\";\ntry (PreparedStatement pstmt = conn.prepareStatement(sql)) {\n    pstmt.setLong(1, 1L);\n    pstmt.setDate(2, Date.valueOf(\"2024-01-15\"));\n    pstmt.setInt(3, 100);\n    pstmt.setString(4, \"click\");\n    pstmt.setDouble(5, 1.5);\n    pstmt.executeUpdate();\n}\n</code></pre> <p>Next step : Data Management</p>"},{"location":"Clickhouse/Dev/EN/04-data-management/","title":"4. Data Management","text":""},{"location":"Clickhouse/Dev/EN/04-data-management/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create tables from Java</li> <li>Manage schemas</li> <li>Import/export data</li> <li>Manage partitions</li> </ul>"},{"location":"Clickhouse/Dev/EN/04-data-management/#create-table","title":"Create Table","text":"<pre><code>String createTableSQL = \"\"\"\n    CREATE TABLE IF NOT EXISTS events (\n        id UInt64,\n        event_date Date,\n        user_id UInt32,\n        event_type String,\n        value Float64\n    )\n    ENGINE = MergeTree()\n    PARTITION BY toYYYYMM(event_date)\n    ORDER BY (event_date, user_id)\n    \"\"\";\n\ntry (Connection conn = ConnectionManager.getConnection();\n     Statement stmt = conn.createStatement()) {\n    stmt.execute(createTableSQL);\n}\n</code></pre>"},{"location":"Clickhouse/Dev/EN/04-data-management/#check-table-exists","title":"Check Table Exists","text":"<pre><code>public boolean tableExists(String tableName) throws SQLException {\n    String sql = \"EXISTS TABLE \" + tableName;\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n        return rs.next() &amp;&amp; rs.getInt(1) == 1;\n    }\n}\n</code></pre> <p>Next step : Performance and Optimization</p>"},{"location":"Clickhouse/Dev/EN/05-performance/","title":"5. Performance and Optimization","text":""},{"location":"Clickhouse/Dev/EN/05-performance/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Optimize queries</li> <li>Use batch processing</li> <li>Manage memory</li> <li>Monitor performance</li> </ul>"},{"location":"Clickhouse/Dev/EN/05-performance/#batch-processing","title":"Batch Processing","text":"<pre><code>public void batchInsert(List&lt;Event&gt; events) throws SQLException {\n    String sql = \"INSERT INTO events (id, event_date, user_id, event_type, value) VALUES\";\n\n    try (Connection conn = ConnectionManager.getConnection();\n         PreparedStatement pstmt = conn.prepareStatement(sql)) {\n\n        conn.setAutoCommit(false);\n        int batchSize = 1000;\n\n        for (Event event : events) {\n            pstmt.setLong(1, event.getId());\n            pstmt.setDate(2, new Date(event.getEventDate().getTime()));\n            pstmt.setInt(3, event.getUserId());\n            pstmt.setString(4, event.getEventType());\n            pstmt.setDouble(5, event.getValue());\n            pstmt.addBatch();\n\n            if (count % batchSize == 0) {\n                pstmt.executeBatch();\n                conn.commit();\n            }\n        }\n\n        pstmt.executeBatch();\n        conn.commit();\n    }\n}\n</code></pre> <p>Next step : Error Handling</p>"},{"location":"Clickhouse/Dev/EN/06-error-handling/","title":"6. Error Handling","text":""},{"location":"Clickhouse/Dev/EN/06-error-handling/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Handle SQL exceptions</li> <li>Implement retry logic</li> <li>Log errors</li> <li>Validate data</li> </ul>"},{"location":"Clickhouse/Dev/EN/06-error-handling/#exception-handling","title":"Exception Handling","text":"<pre><code>public void executeQuery(String sql) {\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n        // Process results\n    } catch (SQLException e) {\n        System.err.println(\"SQL Error: \" + e.getMessage());\n        e.printStackTrace();\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/EN/06-error-handling/#retry-logic","title":"Retry Logic","text":"<pre><code>public &lt;T&gt; T executeWithRetry(Supplier&lt;T&gt; operation, int maxRetries) {\n    int attempts = 0;\n    while (attempts &lt; maxRetries) {\n        try {\n            return operation.get();\n        } catch (SQLException e) {\n            attempts++;\n            if (attempts &gt;= maxRetries) throw new RuntimeException(e);\n            try { Thread.sleep(1000 * attempts); } \n            catch (InterruptedException ie) { throw new RuntimeException(ie); }\n        }\n    }\n    throw new RuntimeException(\"Failed to execute\");\n}\n</code></pre> <p>Next step : Best Practices</p>"},{"location":"Clickhouse/Dev/EN/07-best-practices/","title":"7. Best Practices","text":""},{"location":"Clickhouse/Dev/EN/07-best-practices/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Structure code</li> <li>Use appropriate patterns</li> <li>Optimize resources</li> <li>Secure application</li> </ul>"},{"location":"Clickhouse/Dev/EN/07-best-practices/#service-layer","title":"Service Layer","text":"<pre><code>public class EventService {\n    private final ConnectionManager connectionManager;\n\n    public EventService(ConnectionManager connectionManager) {\n        this.connectionManager = connectionManager;\n    }\n\n    public List&lt;Event&gt; getEvents(Date date) throws SQLException {\n        // Business logic\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/EN/07-best-practices/#resource-management","title":"Resource Management","text":"<pre><code>// \u2705 Good\ntry (Connection conn = getConnection();\n     Statement stmt = conn.createStatement();\n     ResultSet rs = stmt.executeQuery(sql)) {\n    // ...\n}\n</code></pre>"},{"location":"Clickhouse/Dev/EN/07-best-practices/#security","title":"Security","text":"<pre><code>// \u2705 Good : PreparedStatement\nString sql = \"SELECT * FROM events WHERE id = ?\";\nPreparedStatement pstmt = conn.prepareStatement(sql);\npstmt.setLong(1, eventId);\n</code></pre> <p>Next step : Practical Projects</p>"},{"location":"Clickhouse/Dev/EN/08-projets/","title":"8. Practical Projects","text":""},{"location":"Clickhouse/Dev/EN/08-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create complete application</li> <li>Integrate ClickHouse in real project</li> <li>Apply best practices</li> <li>Optimize performance</li> </ul>"},{"location":"Clickhouse/Dev/EN/08-projets/#project-1-analytics-service","title":"Project 1 : Analytics Service","text":""},{"location":"Clickhouse/Dev/EN/08-projets/#structure","title":"Structure","text":"<pre><code>analytics-service/\n\u251c\u2500\u2500 src/main/java/com/analytics/\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 model/\n\u2502   \u251c\u2500\u2500 repository/\n\u2502   \u2514\u2500\u2500 service/\n\u2514\u2500\u2500 pom.xml\n</code></pre>"},{"location":"Clickhouse/Dev/EN/08-projets/#service-example","title":"Service Example","text":"<pre><code>@Service\npublic class AnalyticsService {\n    private final EventRepository eventRepository;\n\n    public Map&lt;String, Long&gt; getEventCountsByType(Date date) {\n        return eventRepository.countByType(date);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/EN/08-projets/#project-2-spring-boot-application","title":"Project 2 : Spring Boot Application","text":""},{"location":"Clickhouse/Dev/EN/08-projets/#configuration","title":"Configuration","text":"<pre><code>@Configuration\npublic class ClickHouseConfig {\n    @Bean\n    public DataSource clickHouseDataSource() {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(\"jdbc:clickhouse://localhost:8123/default\");\n        return new HikariDataSource(config);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/EN/08-projets/#project-3-rest-api","title":"Project 3 : REST API","text":"<pre><code>@RestController\n@RequestMapping(\"/api/events\")\npublic class EventController {\n    @GetMapping(\"/stats\")\n    public ResponseEntity&lt;EventStats&gt; getStats(@RequestParam Date date) {\n        EventStats stats = eventService.getStats(date);\n        return ResponseEntity.ok(stats);\n    }\n}\n</code></pre> <p>Congratulations! You have completed the ClickHouse training for Java Developer! \ud83c\udf89</p>"},{"location":"Clickhouse/Dev/FR/","title":"Formation ClickHouse pour D\u00e9veloppeur Java","text":""},{"location":"Clickhouse/Dev/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'int\u00e9gration de ClickHouse dans vos applications Java. Vous apprendrez \u00e0 utiliser ClickHouse avec Java, les drivers JDBC, les requ\u00eates, l'optimisation et les bonnes pratiques.</p>"},{"location":"Clickhouse/Dev/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Int\u00e9grer ClickHouse dans des applications Java</li> <li>Utiliser le driver JDBC ClickHouse</li> <li>Ex\u00e9cuter des requ\u00eates SQL depuis Java</li> <li>G\u00e9rer les connexions et le pooling</li> <li>Optimiser les performances</li> <li>G\u00e9rer les transactions et les erreurs</li> <li>Cr\u00e9er des applications compl\u00e8tes</li> </ul>"},{"location":"Clickhouse/Dev/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 ClickHouse Community Edition : Gratuit - \u2705 ClickHouse JDBC Driver : Open-source - \u2705 Java : OpenJDK gratuit - \u2705 Maven/Gradle : Outils gratuits</p> <p>Budget total : 0\u20ac</p>"},{"location":"Clickhouse/Dev/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"Clickhouse/Dev/FR/#1-prise-en-main-clickhouse-avec-java","title":"1. Prise en main ClickHouse avec Java","text":""},{"location":"Clickhouse/Dev/FR/#2-driver-jdbc-et-connexion","title":"2. Driver JDBC et Connexion","text":""},{"location":"Clickhouse/Dev/FR/#3-requetes-sql-depuis-java","title":"3. Requ\u00eates SQL depuis Java","text":""},{"location":"Clickhouse/Dev/FR/#4-gestion-des-donnees","title":"4. Gestion des Donn\u00e9es","text":""},{"location":"Clickhouse/Dev/FR/#5-performance-et-optimisation","title":"5. Performance et Optimisation","text":""},{"location":"Clickhouse/Dev/FR/#6-gestion-des-erreurs","title":"6. Gestion des Erreurs","text":""},{"location":"Clickhouse/Dev/FR/#7-bonnes-pratiques","title":"7. Bonnes Pratiques","text":""},{"location":"Clickhouse/Dev/FR/#8-projets-pratiques","title":"8. Projets Pratiques","text":""},{"location":"Clickhouse/Dev/FR/#prerequis","title":"\ud83d\ude80 Pr\u00e9requis","text":"<ul> <li>Connaissances Java (basiques)</li> <li>Notions de bases de donn\u00e9es</li> <li>Maven ou Gradle</li> <li>IDE Java (IntelliJ, Eclipse, VS Code)</li> </ul>"},{"location":"Clickhouse/Dev/FR/#duree-estimee","title":"\u23f1\ufe0f Dur\u00e9e estim\u00e9e","text":"<ul> <li>Total : 30-40 heures</li> <li>Par module : 4-5 heures</li> </ul> <p>Bon apprentissage ! \ud83d\ude80</p>"},{"location":"Clickhouse/Dev/FR/01-getting-started/","title":"1. Prise en main ClickHouse avec Java","text":""},{"location":"Clickhouse/Dev/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre ClickHouse et Java</li> <li>Configurer l'environnement de d\u00e9veloppement</li> <li>Installer le driver JDBC</li> <li>Cr\u00e9er un premier projet</li> </ul>"},{"location":"Clickhouse/Dev/FR/01-getting-started/#introduction","title":"Introduction","text":"<p>ClickHouse peut \u00eatre utilis\u00e9 avec Java via : - JDBC Driver : Connexion standard JDBC - HTTP Interface : Requ\u00eates HTTP REST - Native Protocol : Protocole natif ClickHouse</p>"},{"location":"Clickhouse/Dev/FR/01-getting-started/#configuration-maven","title":"Configuration Maven","text":""},{"location":"Clickhouse/Dev/FR/01-getting-started/#pomxml","title":"pom.xml","text":"<pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.clickhouse&lt;/groupId&gt;\n        &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt;\n        &lt;version&gt;0.6.0&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"Clickhouse/Dev/FR/01-getting-started/#configuration-gradle","title":"Configuration Gradle","text":""},{"location":"Clickhouse/Dev/FR/01-getting-started/#buildgradle","title":"build.gradle","text":"<pre><code>dependencies {\n    implementation 'com.clickhouse:clickhouse-jdbc:0.6.0'\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/01-getting-started/#premier-exemple","title":"Premier exemple","text":""},{"location":"Clickhouse/Dev/FR/01-getting-started/#connexion-simple","title":"Connexion simple","text":"<pre><code>import com.clickhouse.jdbc.ClickHouseConnection;\nimport com.clickhouse.jdbc.ClickHouseDataSource;\n\npublic class ClickHouseExample {\n    public static void main(String[] args) {\n        String url = \"jdbc:clickhouse://localhost:8123/default\";\n\n        try (ClickHouseConnection conn = \n             new ClickHouseDataSource(url).getConnection()) {\n            System.out.println(\"Connected to ClickHouse!\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/01-getting-started/#structure-de-projet","title":"Structure de projet","text":"<pre><code>clickhouse-java-project/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 main/\n\u2502       \u2514\u2500\u2500 java/\n\u2502           \u2514\u2500\u2500 com/\n\u2502               \u2514\u2500\u2500 example/\n\u2502                   \u2514\u2500\u2500 ClickHouseApp.java\n\u251c\u2500\u2500 pom.xml\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Prochaine \u00e9tape : Driver JDBC et Connexion</p>"},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/","title":"2. Driver JDBC et Connexion","text":""},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Configurer la connexion JDBC</li> <li>G\u00e9rer le pooling de connexions</li> <li>Utiliser diff\u00e9rents types de connexion</li> <li>G\u00e9rer les param\u00e8tres de connexion</li> </ul>"},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/#url-de-connexion","title":"URL de connexion","text":""},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/#format-de-base","title":"Format de base","text":"<pre><code>String url = \"jdbc:clickhouse://localhost:8123/default\";\n</code></pre>"},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/#avec-authentification","title":"Avec authentification","text":"<pre><code>String url = \"jdbc:clickhouse://localhost:8123/default?user=default&amp;password=password\";\n</code></pre>"},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/#avec-parametres","title":"Avec param\u00e8tres","text":"<pre><code>String url = \"jdbc:clickhouse://localhost:8123/default?\" +\n             \"socket_timeout=300000&amp;\" +\n             \"connect_timeout=10000\";\n</code></pre>"},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/#connexion-simple","title":"Connexion simple","text":"<pre><code>import com.clickhouse.jdbc.ClickHouseConnection;\nimport com.clickhouse.jdbc.ClickHouseDataSource;\nimport java.sql.Statement;\n\npublic class ConnectionExample {\n    public static void main(String[] args) {\n        String url = \"jdbc:clickhouse://localhost:8123/default\";\n\n        try (ClickHouseConnection conn = \n             new ClickHouseDataSource(url).getConnection();\n             Statement stmt = conn.createStatement()) {\n\n            // Test de connexion\n            stmt.execute(\"SELECT 1\");\n            System.out.println(\"Connection successful!\");\n\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/#pooling-de-connexions","title":"Pooling de connexions","text":""},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/#avec-hikaricp","title":"Avec HikariCP","text":"<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.zaxxer&lt;/groupId&gt;\n    &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;\n    &lt;version&gt;5.0.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>import com.zaxxer.hikari.HikariConfig;\nimport com.zaxxer.hikari.HikariDataSource;\n\nHikariConfig config = new HikariConfig();\nconfig.setJdbcUrl(\"jdbc:clickhouse://localhost:8123/default\");\nconfig.setMaximumPoolSize(10);\nconfig.setMinimumIdle(2);\n\nHikariDataSource dataSource = new HikariDataSource(config);\n</code></pre>"},{"location":"Clickhouse/Dev/FR/02-jdbc-connection/#gestion-des-connexions","title":"Gestion des connexions","text":"<pre><code>public class ConnectionManager {\n    private static HikariDataSource dataSource;\n\n    static {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(\"jdbc:clickhouse://localhost:8123/default\");\n        config.setMaximumPoolSize(10);\n        dataSource = new HikariDataSource(config);\n    }\n\n    public static Connection getConnection() throws SQLException {\n        return dataSource.getConnection();\n    }\n\n    public static void close() {\n        if (dataSource != null) {\n            dataSource.close();\n        }\n    }\n}\n</code></pre> <p>Prochaine \u00e9tape : Requ\u00eates SQL depuis Java</p>"},{"location":"Clickhouse/Dev/FR/03-queries/","title":"3. Requ\u00eates SQL depuis Java","text":""},{"location":"Clickhouse/Dev/FR/03-queries/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Ex\u00e9cuter des requ\u00eates SELECT</li> <li>Utiliser PreparedStatement</li> <li>G\u00e9rer les r\u00e9sultats</li> <li>Ex\u00e9cuter des INSERT/UPDATE/DELETE</li> </ul>"},{"location":"Clickhouse/Dev/FR/03-queries/#requetes-select","title":"Requ\u00eates SELECT","text":""},{"location":"Clickhouse/Dev/FR/03-queries/#requete-simple","title":"Requ\u00eate simple","text":"<pre><code>try (Connection conn = ConnectionManager.getConnection();\n     Statement stmt = conn.createStatement();\n     ResultSet rs = stmt.executeQuery(\"SELECT * FROM events LIMIT 10\")) {\n\n    while (rs.next()) {\n        long id = rs.getLong(\"id\");\n        String eventType = rs.getString(\"event_type\");\n        double value = rs.getDouble(\"value\");\n        System.out.println(id + \" - \" + eventType + \" - \" + value);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/03-queries/#preparedstatement","title":"PreparedStatement","text":""},{"location":"Clickhouse/Dev/FR/03-queries/#requete-parametree","title":"Requ\u00eate param\u00e9tr\u00e9e","text":"<pre><code>String sql = \"SELECT * FROM events WHERE event_date = ? AND event_type = ?\";\n\ntry (Connection conn = ConnectionManager.getConnection();\n     PreparedStatement pstmt = conn.prepareStatement(sql)) {\n\n    pstmt.setDate(1, Date.valueOf(\"2024-01-15\"));\n    pstmt.setString(2, \"click\");\n\n    try (ResultSet rs = pstmt.executeQuery()) {\n        while (rs.next()) {\n            // Traiter les r\u00e9sultats\n        }\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/03-queries/#insert-de-donnees","title":"INSERT de donn\u00e9es","text":""},{"location":"Clickhouse/Dev/FR/03-queries/#insert-simple","title":"INSERT simple","text":"<pre><code>String sql = \"INSERT INTO events (id, event_date, event_time, user_id, event_type, value) VALUES (?, ?, ?, ?, ?, ?)\";\n\ntry (Connection conn = ConnectionManager.getConnection();\n     PreparedStatement pstmt = conn.prepareStatement(sql)) {\n\n    pstmt.setLong(1, 1L);\n    pstmt.setDate(2, Date.valueOf(\"2024-01-15\"));\n    pstmt.setTimestamp(3, Timestamp.valueOf(\"2024-01-15 10:00:00\"));\n    pstmt.setInt(4, 100);\n    pstmt.setString(5, \"click\");\n    pstmt.setDouble(6, 1.5);\n\n    pstmt.executeUpdate();\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/03-queries/#insert-batch","title":"INSERT batch","text":"<pre><code>String sql = \"INSERT INTO events (id, event_date, user_id, event_type, value) VALUES (?, ?, ?, ?, ?)\";\n\ntry (Connection conn = ConnectionManager.getConnection();\n     PreparedStatement pstmt = conn.prepareStatement(sql)) {\n\n    for (int i = 0; i &lt; 1000; i++) {\n        pstmt.setLong(1, i);\n        pstmt.setDate(2, Date.valueOf(\"2024-01-15\"));\n        pstmt.setInt(3, 100 + i);\n        pstmt.setString(4, \"event_\" + i);\n        pstmt.setDouble(5, Math.random() * 100);\n        pstmt.addBatch();\n    }\n\n    pstmt.executeBatch();\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/03-queries/#gestion-des-resultats","title":"Gestion des r\u00e9sultats","text":""},{"location":"Clickhouse/Dev/FR/03-queries/#mapper-vers-objets","title":"Mapper vers objets","text":"<pre><code>public class Event {\n    private long id;\n    private Date eventDate;\n    private int userId;\n    private String eventType;\n    private double value;\n\n    // Constructeurs, getters, setters...\n}\n\npublic List&lt;Event&gt; getEvents() throws SQLException {\n    List&lt;Event&gt; events = new ArrayList&lt;&gt;();\n    String sql = \"SELECT * FROM events LIMIT 100\";\n\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n\n        while (rs.next()) {\n            Event event = new Event();\n            event.setId(rs.getLong(\"id\"));\n            event.setEventDate(rs.getDate(\"event_date\"));\n            event.setUserId(rs.getInt(\"user_id\"));\n            event.setEventType(rs.getString(\"event_type\"));\n            event.setValue(rs.getDouble(\"value\"));\n            events.add(event);\n        }\n    }\n\n    return events;\n}\n</code></pre> <p>Prochaine \u00e9tape : Gestion des Donn\u00e9es</p>"},{"location":"Clickhouse/Dev/FR/04-data-management/","title":"4. Gestion des Donn\u00e9es","text":""},{"location":"Clickhouse/Dev/FR/04-data-management/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er des tables depuis Java</li> <li>G\u00e9rer les sch\u00e9mas</li> <li>Importer/exporter des donn\u00e9es</li> <li>G\u00e9rer les partitions</li> </ul>"},{"location":"Clickhouse/Dev/FR/04-data-management/#creation-de-tables","title":"Cr\u00e9ation de tables","text":""},{"location":"Clickhouse/Dev/FR/04-data-management/#creer-une-table","title":"Cr\u00e9er une table","text":"<pre><code>String createTableSQL = \"\"\"\n    CREATE TABLE IF NOT EXISTS events (\n        id UInt64,\n        event_date Date,\n        event_time DateTime,\n        user_id UInt32,\n        event_type String,\n        value Float64\n    )\n    ENGINE = MergeTree()\n    PARTITION BY toYYYYMM(event_date)\n    ORDER BY (event_date, user_id)\n    \"\"\";\n\ntry (Connection conn = ConnectionManager.getConnection();\n     Statement stmt = conn.createStatement()) {\n    stmt.execute(createTableSQL);\n    System.out.println(\"Table created successfully!\");\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/04-data-management/#gestion-des-schemas","title":"Gestion des sch\u00e9mas","text":""},{"location":"Clickhouse/Dev/FR/04-data-management/#verifier-lexistence-dune-table","title":"V\u00e9rifier l'existence d'une table","text":"<pre><code>public boolean tableExists(String tableName) throws SQLException {\n    String sql = \"EXISTS TABLE \" + tableName;\n\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n        return rs.next() &amp;&amp; rs.getInt(1) == 1;\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/04-data-management/#obtenir-la-structure-dune-table","title":"Obtenir la structure d'une table","text":"<pre><code>public void describeTable(String tableName) throws SQLException {\n    String sql = \"DESCRIBE TABLE \" + tableName;\n\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n\n        System.out.println(\"Column\\tType\");\n        while (rs.next()) {\n            System.out.println(rs.getString(\"name\") + \"\\t\" + \n                             rs.getString(\"type\"));\n        }\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/04-data-management/#import-de-donnees","title":"Import de donn\u00e9es","text":""},{"location":"Clickhouse/Dev/FR/04-data-management/#depuis-csv","title":"Depuis CSV","text":"<pre><code>String insertSQL = \"INSERT INTO events FROM INFILE '/path/to/file.csv' FORMAT CSV\";\n\ntry (Connection conn = ConnectionManager.getConnection();\n     Statement stmt = conn.createStatement()) {\n    stmt.execute(insertSQL);\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/04-data-management/#depuis-fichier-local","title":"Depuis fichier local","text":"<pre><code>public void importFromFile(String filePath) throws SQLException, IOException {\n    String sql = \"INSERT INTO events FORMAT CSV\";\n\n    try (Connection conn = ConnectionManager.getConnection();\n         PreparedStatement pstmt = conn.prepareStatement(sql);\n         BufferedReader reader = Files.newBufferedReader(Paths.get(filePath))) {\n\n        // Lire et ins\u00e9rer ligne par ligne\n        String line;\n        while ((line = reader.readLine()) != null) {\n            // Parser et ins\u00e9rer\n        }\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/04-data-management/#export-de-donnees","title":"Export de donn\u00e9es","text":""},{"location":"Clickhouse/Dev/FR/04-data-management/#vers-csv","title":"Vers CSV","text":"<pre><code>public void exportToCSV(String outputPath) throws SQLException, IOException {\n    String sql = \"SELECT * FROM events\";\n\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql);\n         BufferedWriter writer = Files.newBufferedWriter(Paths.get(outputPath))) {\n\n        // \u00c9crire l'en-t\u00eate\n        ResultSetMetaData metaData = rs.getMetaData();\n        for (int i = 1; i &lt;= metaData.getColumnCount(); i++) {\n            writer.write(metaData.getColumnName(i));\n            if (i &lt; metaData.getColumnCount()) writer.write(\",\");\n        }\n        writer.newLine();\n\n        // \u00c9crire les donn\u00e9es\n        while (rs.next()) {\n            for (int i = 1; i &lt;= metaData.getColumnCount(); i++) {\n                writer.write(rs.getString(i));\n                if (i &lt; metaData.getColumnCount()) writer.write(\",\");\n            }\n            writer.newLine();\n        }\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/04-data-management/#gestion-des-partitions","title":"Gestion des partitions","text":""},{"location":"Clickhouse/Dev/FR/04-data-management/#lister-les-partitions","title":"Lister les partitions","text":"<pre><code>public void listPartitions(String tableName) throws SQLException {\n    String sql = \"SELECT partition, rows, bytes_on_disk \" +\n                 \"FROM system.parts \" +\n                 \"WHERE table = ? AND active = 1\";\n\n    try (Connection conn = ConnectionManager.getConnection();\n         PreparedStatement pstmt = conn.prepareStatement(sql)) {\n        pstmt.setString(1, tableName);\n\n        try (ResultSet rs = pstmt.executeQuery()) {\n            while (rs.next()) {\n                System.out.println(\"Partition: \" + rs.getString(\"partition\") +\n                                 \", Rows: \" + rs.getLong(\"rows\"));\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/04-data-management/#supprimer-une-partition","title":"Supprimer une partition","text":"<pre><code>public void dropPartition(String tableName, String partition) throws SQLException {\n    String sql = \"ALTER TABLE \" + tableName + \" DROP PARTITION ?\";\n\n    try (Connection conn = ConnectionManager.getConnection();\n         PreparedStatement pstmt = conn.prepareStatement(sql)) {\n        pstmt.setString(1, partition);\n        pstmt.executeUpdate();\n    }\n}\n</code></pre> <p>Prochaine \u00e9tape : Performance et Optimisation</p>"},{"location":"Clickhouse/Dev/FR/05-performance/","title":"5. Performance et Optimisation","text":""},{"location":"Clickhouse/Dev/FR/05-performance/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Optimiser les requ\u00eates</li> <li>Utiliser le batch processing</li> <li>G\u00e9rer la m\u00e9moire</li> <li>Monitorer les performances</li> </ul>"},{"location":"Clickhouse/Dev/FR/05-performance/#batch-processing","title":"Batch Processing","text":""},{"location":"Clickhouse/Dev/FR/05-performance/#insert-batch-optimise","title":"INSERT batch optimis\u00e9","text":"<pre><code>public void batchInsert(List&lt;Event&gt; events) throws SQLException {\n    String sql = \"INSERT INTO events (id, event_date, user_id, event_type, value) VALUES\";\n\n    try (Connection conn = ConnectionManager.getConnection();\n         PreparedStatement pstmt = conn.prepareStatement(sql)) {\n\n        conn.setAutoCommit(false);\n\n        int batchSize = 1000;\n        int count = 0;\n\n        for (Event event : events) {\n            pstmt.setLong(1, event.getId());\n            pstmt.setDate(2, new Date(event.getEventDate().getTime()));\n            pstmt.setInt(3, event.getUserId());\n            pstmt.setString(4, event.getEventType());\n            pstmt.setDouble(5, event.getValue());\n            pstmt.addBatch();\n\n            if (++count % batchSize == 0) {\n                pstmt.executeBatch();\n                conn.commit();\n            }\n        }\n\n        pstmt.executeBatch();\n        conn.commit();\n        conn.setAutoCommit(true);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/05-performance/#requetes-optimisees","title":"Requ\u00eates optimis\u00e9es","text":""},{"location":"Clickhouse/Dev/FR/05-performance/#utiliser-limit","title":"Utiliser LIMIT","text":"<pre><code>String sql = \"SELECT * FROM events WHERE event_date = ? LIMIT ?\";\n\ntry (PreparedStatement pstmt = conn.prepareStatement(sql)) {\n    pstmt.setDate(1, date);\n    pstmt.setInt(2, 1000); // Limiter les r\u00e9sultats\n    // ...\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/05-performance/#eviter-select","title":"\u00c9viter SELECT *","text":"<pre><code>// \u2705 Bon\nString sql = \"SELECT id, event_type, value FROM events\";\n\n// \u274c Moins bon\nString sql = \"SELECT * FROM events\";\n</code></pre>"},{"location":"Clickhouse/Dev/FR/05-performance/#monitoring","title":"Monitoring","text":""},{"location":"Clickhouse/Dev/FR/05-performance/#mesurer-le-temps-dexecution","title":"Mesurer le temps d'ex\u00e9cution","text":"<pre><code>public long executeQueryWithTiming(String sql) throws SQLException {\n    long startTime = System.currentTimeMillis();\n\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n\n        // Traiter les r\u00e9sultats\n        while (rs.next()) {\n            // ...\n        }\n    }\n\n    long endTime = System.currentTimeMillis();\n    return endTime - startTime;\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/05-performance/#requetes-lentes","title":"Requ\u00eates lentes","text":"<pre><code>public void findSlowQueries() throws SQLException {\n    String sql = \"\"\"\n        SELECT query, query_duration_ms, read_rows\n        FROM system.query_log\n        WHERE type = 'QueryFinish'\n        ORDER BY query_duration_ms DESC\n        LIMIT 10\n        \"\"\";\n\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n\n        while (rs.next()) {\n            System.out.println(\"Query: \" + rs.getString(\"query\"));\n            System.out.println(\"Duration: \" + rs.getLong(\"query_duration_ms\") + \"ms\");\n        }\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/05-performance/#connection-pooling","title":"Connection Pooling","text":""},{"location":"Clickhouse/Dev/FR/05-performance/#configuration-optimale","title":"Configuration optimale","text":"<pre><code>HikariConfig config = new HikariConfig();\nconfig.setJdbcUrl(\"jdbc:clickhouse://localhost:8123/default\");\nconfig.setMaximumPoolSize(20); // Ajuster selon charge\nconfig.setMinimumIdle(5);\nconfig.setConnectionTimeout(30000);\nconfig.setIdleTimeout(600000);\nconfig.setMaxLifetime(1800000);\n</code></pre> <p>Prochaine \u00e9tape : Gestion des Erreurs</p>"},{"location":"Clickhouse/Dev/FR/06-error-handling/","title":"6. Gestion des Erreurs","text":""},{"location":"Clickhouse/Dev/FR/06-error-handling/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>G\u00e9rer les exceptions SQL</li> <li>Impl\u00e9menter la retry logic</li> <li>Logger les erreurs</li> <li>Valider les donn\u00e9es</li> </ul>"},{"location":"Clickhouse/Dev/FR/06-error-handling/#gestion-des-exceptions","title":"Gestion des exceptions","text":""},{"location":"Clickhouse/Dev/FR/06-error-handling/#try-catch-basique","title":"Try-catch basique","text":"<pre><code>public void executeQuery(String sql) {\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n\n        // Traiter les r\u00e9sultats\n    } catch (SQLException e) {\n        System.err.println(\"SQL Error: \" + e.getMessage());\n        System.err.println(\"SQL State: \" + e.getSQLState());\n        System.err.println(\"Error Code: \" + e.getErrorCode());\n        e.printStackTrace();\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/06-error-handling/#retry-logic","title":"Retry Logic","text":""},{"location":"Clickhouse/Dev/FR/06-error-handling/#implementation-simple","title":"Impl\u00e9mentation simple","text":"<pre><code>public &lt;T&gt; T executeWithRetry(Supplier&lt;T&gt; operation, int maxRetries) {\n    int attempts = 0;\n\n    while (attempts &lt; maxRetries) {\n        try {\n            return operation.get();\n        } catch (SQLException e) {\n            attempts++;\n            if (attempts &gt;= maxRetries) {\n                throw new RuntimeException(\"Failed after \" + maxRetries + \" attempts\", e);\n            }\n\n            try {\n                Thread.sleep(1000 * attempts); // Backoff exponentiel\n            } catch (InterruptedException ie) {\n                Thread.currentThread().interrupt();\n                throw new RuntimeException(ie);\n            }\n        }\n    }\n\n    throw new RuntimeException(\"Failed to execute operation\");\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/06-error-handling/#utilisation","title":"Utilisation","text":"<pre><code>List&lt;Event&gt; events = executeWithRetry(() -&gt; {\n    return getEventsFromDatabase();\n}, 3);\n</code></pre>"},{"location":"Clickhouse/Dev/FR/06-error-handling/#logging","title":"Logging","text":""},{"location":"Clickhouse/Dev/FR/06-error-handling/#avec-slf4j","title":"Avec SLF4J","text":"<pre><code>import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\npublic class ClickHouseService {\n    private static final Logger logger = LoggerFactory.getLogger(ClickHouseService.class);\n\n    public void insertEvent(Event event) {\n        try {\n            // Insertion\n            logger.info(\"Event inserted: {}\", event.getId());\n        } catch (SQLException e) {\n            logger.error(\"Failed to insert event: {}\", event.getId(), e);\n            throw new RuntimeException(e);\n        }\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/06-error-handling/#validation-des-donnees","title":"Validation des donn\u00e9es","text":""},{"location":"Clickhouse/Dev/FR/06-error-handling/#valider-avant-insertion","title":"Valider avant insertion","text":"<pre><code>public void insertEvent(Event event) throws ValidationException {\n    if (event.getId() &lt;= 0) {\n        throw new ValidationException(\"Invalid event ID\");\n    }\n\n    if (event.getEventDate() == null) {\n        throw new ValidationException(\"Event date is required\");\n    }\n\n    if (event.getEventType() == null || event.getEventType().isEmpty()) {\n        throw new ValidationException(\"Event type is required\");\n    }\n\n    // Ins\u00e9rer apr\u00e8s validation\n    // ...\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/06-error-handling/#gestion-des-transactions","title":"Gestion des transactions","text":""},{"location":"Clickhouse/Dev/FR/06-error-handling/#rollback-en-cas-derreur","title":"Rollback en cas d'erreur","text":"<pre><code>public void insertMultipleEvents(List&lt;Event&gt; events) throws SQLException {\n    try (Connection conn = ConnectionManager.getConnection()) {\n        conn.setAutoCommit(false);\n\n        try (PreparedStatement pstmt = conn.prepareStatement(insertSQL)) {\n            for (Event event : events) {\n                // Pr\u00e9parer l'insertion\n                pstmt.addBatch();\n            }\n            pstmt.executeBatch();\n            conn.commit();\n        } catch (SQLException e) {\n            conn.rollback();\n            throw e;\n        } finally {\n            conn.setAutoCommit(true);\n        }\n    }\n}\n</code></pre> <p>Prochaine \u00e9tape : Bonnes Pratiques</p>"},{"location":"Clickhouse/Dev/FR/07-best-practices/","title":"7. Bonnes Pratiques","text":""},{"location":"Clickhouse/Dev/FR/07-best-practices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Structurer le code</li> <li>Utiliser des patterns appropri\u00e9s</li> <li>Optimiser les ressources</li> <li>S\u00e9curiser l'application</li> </ul>"},{"location":"Clickhouse/Dev/FR/07-best-practices/#architecture","title":"Architecture","text":""},{"location":"Clickhouse/Dev/FR/07-best-practices/#service-layer","title":"Service Layer","text":"<pre><code>public class EventService {\n    private final ConnectionManager connectionManager;\n\n    public EventService(ConnectionManager connectionManager) {\n        this.connectionManager = connectionManager;\n    }\n\n    public List&lt;Event&gt; getEvents(Date date) throws SQLException {\n        // Logique m\u00e9tier\n    }\n\n    public void insertEvent(Event event) throws SQLException {\n        // Validation et insertion\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/07-best-practices/#repository-pattern","title":"Repository Pattern","text":"<pre><code>public interface EventRepository {\n    List&lt;Event&gt; findByDate(Date date) throws SQLException;\n    void save(Event event) throws SQLException;\n    void saveAll(List&lt;Event&gt; events) throws SQLException;\n}\n\npublic class ClickHouseEventRepository implements EventRepository {\n    // Impl\u00e9mentation\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/07-best-practices/#gestion-des-ressources","title":"Gestion des ressources","text":""},{"location":"Clickhouse/Dev/FR/07-best-practices/#try-with-resources","title":"Try-with-resources","text":"<pre><code>// \u2705 Bon\ntry (Connection conn = getConnection();\n     Statement stmt = conn.createStatement();\n     ResultSet rs = stmt.executeQuery(sql)) {\n    // ...\n}\n\n// \u274c Moins bon\nConnection conn = getConnection();\nStatement stmt = conn.createStatement();\n// Risque de fuite de ressources\n</code></pre>"},{"location":"Clickhouse/Dev/FR/07-best-practices/#configuration","title":"Configuration","text":""},{"location":"Clickhouse/Dev/FR/07-best-practices/#externaliser-la-configuration","title":"Externaliser la configuration","text":"<pre><code>public class ClickHouseConfig {\n    private String host;\n    private int port;\n    private String database;\n    private String user;\n    private String password;\n\n    // Charger depuis properties file\n    public static ClickHouseConfig load() {\n        Properties props = new Properties();\n        try (InputStream is = ClickHouseConfig.class\n                .getResourceAsStream(\"/clickhouse.properties\")) {\n            props.load(is);\n        }\n        // ...\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/07-best-practices/#securite","title":"S\u00e9curit\u00e9","text":""},{"location":"Clickhouse/Dev/FR/07-best-practices/#preparer-les-requetes","title":"Pr\u00e9parer les requ\u00eates","text":"<pre><code>// \u2705 Bon : PreparedStatement\nString sql = \"SELECT * FROM events WHERE id = ?\";\nPreparedStatement pstmt = conn.prepareStatement(sql);\npstmt.setLong(1, eventId);\n\n// \u274c Moins bon : Concatenation\nString sql = \"SELECT * FROM events WHERE id = \" + eventId;\n</code></pre>"},{"location":"Clickhouse/Dev/FR/07-best-practices/#gestion-des-credentials","title":"Gestion des credentials","text":"<pre><code>// Utiliser des variables d'environnement ou fichiers s\u00e9curis\u00e9s\nString password = System.getenv(\"CLICKHOUSE_PASSWORD\");\n// Ou utiliser un gestionnaire de secrets\n</code></pre>"},{"location":"Clickhouse/Dev/FR/07-best-practices/#tests","title":"Tests","text":""},{"location":"Clickhouse/Dev/FR/07-best-practices/#tests-unitaires","title":"Tests unitaires","text":"<pre><code>@Test\npublic void testInsertEvent() throws SQLException {\n    EventService service = new EventService(mockConnectionManager);\n    Event event = new Event(1L, new Date(), 100, \"click\", 1.5);\n\n    service.insertEvent(event);\n\n    // V\u00e9rifications\n}\n</code></pre> <p>Prochaine \u00e9tape : Projets Pratiques</p>"},{"location":"Clickhouse/Dev/FR/08-projets/","title":"8. Projets Pratiques","text":""},{"location":"Clickhouse/Dev/FR/08-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er une application compl\u00e8te</li> <li>Int\u00e9grer ClickHouse dans un projet r\u00e9el</li> <li>Appliquer les bonnes pratiques</li> <li>Optimiser les performances</li> </ul>"},{"location":"Clickhouse/Dev/FR/08-projets/#projet-1-service-danalytics","title":"Projet 1 : Service d'Analytics","text":""},{"location":"Clickhouse/Dev/FR/08-projets/#objectif","title":"Objectif","text":"<p>Cr\u00e9er un service Java pour analyser les \u00e9v\u00e9nements web.</p>"},{"location":"Clickhouse/Dev/FR/08-projets/#structure","title":"Structure","text":"<pre><code>analytics-service/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 main/\n\u2502       \u251c\u2500\u2500 java/\n\u2502       \u2502   \u2514\u2500\u2500 com/\n\u2502       \u2502       \u2514\u2500\u2500 analytics/\n\u2502       \u2502           \u251c\u2500\u2500 config/\n\u2502       \u2502           \u251c\u2500\u2500 model/\n\u2502       \u2502           \u251c\u2500\u2500 repository/\n\u2502       \u2502           \u2514\u2500\u2500 service/\n\u2502       \u2514\u2500\u2500 resources/\n\u2502           \u2514\u2500\u2500 application.properties\n\u2514\u2500\u2500 pom.xml\n</code></pre>"},{"location":"Clickhouse/Dev/FR/08-projets/#exemple-de-service","title":"Exemple de service","text":"<pre><code>@Service\npublic class AnalyticsService {\n    private final EventRepository eventRepository;\n\n    public AnalyticsService(EventRepository eventRepository) {\n        this.eventRepository = eventRepository;\n    }\n\n    public Map&lt;String, Long&gt; getEventCountsByType(Date date) {\n        return eventRepository.countByType(date);\n    }\n\n    public List&lt;TopUser&gt; getTopUsers(int limit) {\n        return eventRepository.findTopUsers(limit);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/08-projets/#projet-2-application-spring-boot","title":"Projet 2 : Application Spring Boot","text":""},{"location":"Clickhouse/Dev/FR/08-projets/#dependances","title":"D\u00e9pendances","text":"<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.clickhouse&lt;/groupId&gt;\n    &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"Clickhouse/Dev/FR/08-projets/#configuration","title":"Configuration","text":"<pre><code>@Configuration\npublic class ClickHouseConfig {\n    @Bean\n    public DataSource clickHouseDataSource() {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(\"jdbc:clickhouse://localhost:8123/default\");\n        return new HikariDataSource(config);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/08-projets/#repository","title":"Repository","text":"<pre><code>@Repository\npublic class EventRepository {\n    private final JdbcTemplate jdbcTemplate;\n\n    public List&lt;Event&gt; findByDate(Date date) {\n        String sql = \"SELECT * FROM events WHERE event_date = ?\";\n        return jdbcTemplate.query(sql, new EventRowMapper(), date);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/08-projets/#projet-3-api-rest","title":"Projet 3 : API REST","text":""},{"location":"Clickhouse/Dev/FR/08-projets/#controller","title":"Controller","text":"<pre><code>@RestController\n@RequestMapping(\"/api/events\")\npublic class EventController {\n    private final EventService eventService;\n\n    @GetMapping(\"/stats\")\n    public ResponseEntity&lt;EventStats&gt; getStats(@RequestParam Date date) {\n        EventStats stats = eventService.getStats(date);\n        return ResponseEntity.ok(stats);\n    }\n\n    @PostMapping\n    public ResponseEntity&lt;Void&gt; createEvent(@RequestBody Event event) {\n        eventService.insertEvent(event);\n        return ResponseEntity.status(HttpStatus.CREATED).build();\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/FR/08-projets/#conseils","title":"Conseils","text":"<ol> <li>Structure claire : S\u00e9parer les couches (Service, Repository, Model)</li> <li>Configuration externe : Utiliser properties files</li> <li>Tests : \u00c9crire des tests unitaires et d'int\u00e9gration</li> <li>Documentation : Documenter l'API et le code</li> <li>Monitoring : Ajouter des logs et m\u00e9triques</li> </ol> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation ClickHouse pour D\u00e9veloppeur Java ! \ud83c\udf89</p>"},{"location":"Clickhouse/Dev/PL/","title":"Szkolenie ClickHouse dla Dewelopera Java","text":""},{"location":"Clickhouse/Dev/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>To szkolenie poprowadzi Ci\u0119 przez integracj\u0119 ClickHouse w aplikacjach Java. Nauczysz si\u0119 u\u017cywa\u0107 ClickHouse z Java, sterownik\u00f3w JDBC, zapyta\u0144, optymalizacji i najlepszych praktyk.</p>"},{"location":"Clickhouse/Dev/PL/#cele-szkoleniowe","title":"\ud83c\udfaf Cele szkoleniowe","text":"<ul> <li>Integrowa\u0107 ClickHouse w aplikacjach Java</li> <li>U\u017cywa\u0107 sterownika JDBC ClickHouse</li> <li>Wykonywa\u0107 zapytania SQL z Java</li> <li>Zarz\u0105dza\u0107 po\u0142\u0105czeniami i pul\u0105</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> <li>Obs\u0142ugiwa\u0107 transakcje i b\u0142\u0119dy</li> <li>Tworzy\u0107 kompletne aplikacje</li> </ul>"},{"location":"Clickhouse/Dev/PL/#wszystko-jest-darmowe","title":"\ud83d\udcb0 Wszystko jest darmowe!","text":"<p>To szkolenie wykorzystuje tylko: - \u2705 ClickHouse Community Edition : Darmowe - \u2705 Sterownik JDBC ClickHouse : Open-source - \u2705 Java : OpenJDK darmowe - \u2705 Maven/Gradle : Darmowe narz\u0119dzia</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"Clickhouse/Dev/PL/#struktura-szkolenia","title":"\ud83d\udcd6 Struktura szkolenia","text":""},{"location":"Clickhouse/Dev/PL/#1-rozpoczecie-pracy-z-clickhouse-i-java","title":"1. Rozpocz\u0119cie pracy z ClickHouse i Java","text":""},{"location":"Clickhouse/Dev/PL/#2-sterownik-jdbc-i-poaczenie","title":"2. Sterownik JDBC i po\u0142\u0105czenie","text":""},{"location":"Clickhouse/Dev/PL/#3-zapytania-sql-z-java","title":"3. Zapytania SQL z Java","text":""},{"location":"Clickhouse/Dev/PL/#4-zarzadzanie-danymi","title":"4. Zarz\u0105dzanie danymi","text":""},{"location":"Clickhouse/Dev/PL/#5-wydajnosc-i-optymalizacja","title":"5. Wydajno\u015b\u0107 i optymalizacja","text":""},{"location":"Clickhouse/Dev/PL/#6-obsuga-bedow","title":"6. Obs\u0142uga b\u0142\u0119d\u00f3w","text":""},{"location":"Clickhouse/Dev/PL/#7-najlepsze-praktyki","title":"7. Najlepsze praktyki","text":""},{"location":"Clickhouse/Dev/PL/#8-projekty-praktyczne","title":"8. Projekty praktyczne","text":""},{"location":"Clickhouse/Dev/PL/#wymagania-wstepne","title":"\ud83d\ude80 Wymagania wst\u0119pne","text":"<ul> <li>Znajomo\u015b\u0107 Java (podstawy)</li> <li>Poj\u0119cia dotycz\u0105ce baz danych</li> <li>Maven lub Gradle</li> <li>IDE Java (IntelliJ, Eclipse, VS Code)</li> </ul>"},{"location":"Clickhouse/Dev/PL/#szacowany-czas-trwania","title":"\u23f1\ufe0f Szacowany czas trwania","text":"<ul> <li>Ca\u0142kowity : 30-40 godzin</li> <li>Na modu\u0142 : 4-5 godzin</li> </ul> <p>Powodzenia w nauce! \ud83d\ude80</p>"},{"location":"Clickhouse/Dev/PL/01-getting-started/","title":"1. Rozpocz\u0119cie pracy z ClickHouse i Java","text":""},{"location":"Clickhouse/Dev/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 ClickHouse i Java</li> <li>Skonfigurowa\u0107 \u015brodowisko deweloperskie</li> <li>Zainstalowa\u0107 sterownik JDBC</li> <li>Utworzy\u0107 pierwszy projekt</li> </ul>"},{"location":"Clickhouse/Dev/PL/01-getting-started/#konfiguracja-maven","title":"Konfiguracja Maven","text":""},{"location":"Clickhouse/Dev/PL/01-getting-started/#pomxml","title":"pom.xml","text":"<pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;com.clickhouse&lt;/groupId&gt;\n        &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt;\n        &lt;version&gt;0.6.0&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"Clickhouse/Dev/PL/01-getting-started/#pierwszy-przykad","title":"Pierwszy przyk\u0142ad","text":"<pre><code>import com.clickhouse.jdbc.ClickHouseConnection;\nimport com.clickhouse.jdbc.ClickHouseDataSource;\n\npublic class ClickHouseExample {\n    public static void main(String[] args) {\n        String url = \"jdbc:clickhouse://localhost:8123/default\";\n\n        try (ClickHouseConnection conn = \n             new ClickHouseDataSource(url).getConnection()) {\n            System.out.println(\"Po\u0142\u0105czono z ClickHouse!\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre> <p>Nast\u0119pny krok : Sterownik JDBC i po\u0142\u0105czenie</p>"},{"location":"Clickhouse/Dev/PL/02-jdbc-connection/","title":"2. Sterownik JDBC i po\u0142\u0105czenie","text":""},{"location":"Clickhouse/Dev/PL/02-jdbc-connection/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Skonfigurowa\u0107 po\u0142\u0105czenie JDBC</li> <li>Zarz\u0105dza\u0107 pul\u0105 po\u0142\u0105cze\u0144</li> <li>U\u017cywa\u0107 r\u00f3\u017cnych typ\u00f3w po\u0142\u0105cze\u0144</li> </ul>"},{"location":"Clickhouse/Dev/PL/02-jdbc-connection/#url-poaczenia","title":"URL po\u0142\u0105czenia","text":"<pre><code>String url = \"jdbc:clickhouse://localhost:8123/default\";\nString url = \"jdbc:clickhouse://localhost:8123/default?user=default&amp;password=password\";\n</code></pre>"},{"location":"Clickhouse/Dev/PL/02-jdbc-connection/#pula-poaczen","title":"Pula po\u0142\u0105cze\u0144","text":""},{"location":"Clickhouse/Dev/PL/02-jdbc-connection/#z-hikaricp","title":"Z HikariCP","text":"<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.zaxxer&lt;/groupId&gt;\n    &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;\n    &lt;version&gt;5.0.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>HikariConfig config = new HikariConfig();\nconfig.setJdbcUrl(\"jdbc:clickhouse://localhost:8123/default\");\nconfig.setMaximumPoolSize(10);\nHikariDataSource dataSource = new HikariDataSource(config);\n</code></pre> <p>Nast\u0119pny krok : Zapytania SQL z Java</p>"},{"location":"Clickhouse/Dev/PL/03-queries/","title":"3. Zapytania SQL z Java","text":""},{"location":"Clickhouse/Dev/PL/03-queries/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Wykonywa\u0107 zapytania SELECT</li> <li>U\u017cywa\u0107 PreparedStatement</li> <li>Obs\u0142ugiwa\u0107 wyniki</li> <li>Wykonywa\u0107 INSERT/UPDATE/DELETE</li> </ul>"},{"location":"Clickhouse/Dev/PL/03-queries/#zapytanie-select","title":"Zapytanie SELECT","text":"<pre><code>try (Connection conn = ConnectionManager.getConnection();\n     Statement stmt = conn.createStatement();\n     ResultSet rs = stmt.executeQuery(\"SELECT * FROM events LIMIT 10\")) {\n\n    while (rs.next()) {\n        long id = rs.getLong(\"id\");\n        String eventType = rs.getString(\"event_type\");\n        System.out.println(id + \" - \" + eventType);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/PL/03-queries/#preparedstatement","title":"PreparedStatement","text":"<pre><code>String sql = \"SELECT * FROM events WHERE event_date = ?\";\ntry (PreparedStatement pstmt = conn.prepareStatement(sql)) {\n    pstmt.setDate(1, Date.valueOf(\"2024-01-15\"));\n    try (ResultSet rs = pstmt.executeQuery()) {\n        // Przetwarzaj wyniki\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/PL/03-queries/#insert","title":"INSERT","text":"<pre><code>String sql = \"INSERT INTO events (id, event_date, user_id, event_type, value) VALUES (?, ?, ?, ?, ?)\";\ntry (PreparedStatement pstmt = conn.prepareStatement(sql)) {\n    pstmt.setLong(1, 1L);\n    pstmt.setDate(2, Date.valueOf(\"2024-01-15\"));\n    pstmt.setInt(3, 100);\n    pstmt.setString(4, \"click\");\n    pstmt.setDouble(5, 1.5);\n    pstmt.executeUpdate();\n}\n</code></pre> <p>Nast\u0119pny krok : Zarz\u0105dzanie danymi</p>"},{"location":"Clickhouse/Dev/PL/04-data-management/","title":"4. Zarz\u0105dzanie danymi","text":""},{"location":"Clickhouse/Dev/PL/04-data-management/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 tabele z Java</li> <li>Zarz\u0105dza\u0107 schematami</li> <li>Importowa\u0107/eksportowa\u0107 dane</li> <li>Zarz\u0105dza\u0107 partycjami</li> </ul>"},{"location":"Clickhouse/Dev/PL/04-data-management/#utworzenie-tabeli","title":"Utworzenie tabeli","text":"<pre><code>String createTableSQL = \"\"\"\n    CREATE TABLE IF NOT EXISTS events (\n        id UInt64,\n        event_date Date,\n        user_id UInt32,\n        event_type String,\n        value Float64\n    )\n    ENGINE = MergeTree()\n    PARTITION BY toYYYYMM(event_date)\n    ORDER BY (event_date, user_id)\n    \"\"\";\n\ntry (Connection conn = ConnectionManager.getConnection();\n     Statement stmt = conn.createStatement()) {\n    stmt.execute(createTableSQL);\n}\n</code></pre>"},{"location":"Clickhouse/Dev/PL/04-data-management/#sprawdzenie-istnienia-tabeli","title":"Sprawdzenie istnienia tabeli","text":"<pre><code>public boolean tableExists(String tableName) throws SQLException {\n    String sql = \"EXISTS TABLE \" + tableName;\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n        return rs.next() &amp;&amp; rs.getInt(1) == 1;\n    }\n}\n</code></pre> <p>Nast\u0119pny krok : Wydajno\u015b\u0107 i optymalizacja</p>"},{"location":"Clickhouse/Dev/PL/05-performance/","title":"5. Wydajno\u015b\u0107 i optymalizacja","text":""},{"location":"Clickhouse/Dev/PL/05-performance/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Optymalizowa\u0107 zapytania</li> <li>U\u017cywa\u0107 przetwarzania wsadowego</li> <li>Zarz\u0105dza\u0107 pami\u0119ci\u0105</li> <li>Monitorowa\u0107 wydajno\u015b\u0107</li> </ul>"},{"location":"Clickhouse/Dev/PL/05-performance/#przetwarzanie-wsadowe","title":"Przetwarzanie wsadowe","text":"<pre><code>public void batchInsert(List&lt;Event&gt; events) throws SQLException {\n    String sql = \"INSERT INTO events (id, event_date, user_id, event_type, value) VALUES\";\n\n    try (Connection conn = ConnectionManager.getConnection();\n         PreparedStatement pstmt = conn.prepareStatement(sql)) {\n\n        conn.setAutoCommit(false);\n        int batchSize = 1000;\n\n        for (Event event : events) {\n            pstmt.setLong(1, event.getId());\n            pstmt.setDate(2, new Date(event.getEventDate().getTime()));\n            pstmt.setInt(3, event.getUserId());\n            pstmt.setString(4, event.getEventType());\n            pstmt.setDouble(5, event.getValue());\n            pstmt.addBatch();\n\n            if (count % batchSize == 0) {\n                pstmt.executeBatch();\n                conn.commit();\n            }\n        }\n\n        pstmt.executeBatch();\n        conn.commit();\n    }\n}\n</code></pre> <p>Nast\u0119pny krok : Obs\u0142uga b\u0142\u0119d\u00f3w</p>"},{"location":"Clickhouse/Dev/PL/06-error-handling/","title":"6. Obs\u0142uga b\u0142\u0119d\u00f3w","text":""},{"location":"Clickhouse/Dev/PL/06-error-handling/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Obs\u0142ugiwa\u0107 wyj\u0105tki SQL</li> <li>Implementowa\u0107 logik\u0119 ponawiania</li> <li>Logowa\u0107 b\u0142\u0119dy</li> <li>Walidowa\u0107 dane</li> </ul>"},{"location":"Clickhouse/Dev/PL/06-error-handling/#obsuga-wyjatkow","title":"Obs\u0142uga wyj\u0105tk\u00f3w","text":"<pre><code>public void executeQuery(String sql) {\n    try (Connection conn = ConnectionManager.getConnection();\n         Statement stmt = conn.createStatement();\n         ResultSet rs = stmt.executeQuery(sql)) {\n        // Przetwarzaj wyniki\n    } catch (SQLException e) {\n        System.err.println(\"B\u0142\u0105d SQL: \" + e.getMessage());\n        e.printStackTrace();\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/PL/06-error-handling/#logika-ponawiania","title":"Logika ponawiania","text":"<pre><code>public &lt;T&gt; T executeWithRetry(Supplier&lt;T&gt; operation, int maxRetries) {\n    int attempts = 0;\n    while (attempts &lt; maxRetries) {\n        try {\n            return operation.get();\n        } catch (SQLException e) {\n            attempts++;\n            if (attempts &gt;= maxRetries) throw new RuntimeException(e);\n            try { Thread.sleep(1000 * attempts); } \n            catch (InterruptedException ie) { throw new RuntimeException(ie); }\n        }\n    }\n    throw new RuntimeException(\"Nie uda\u0142o si\u0119 wykona\u0107\");\n}\n</code></pre> <p>Nast\u0119pny krok : Najlepsze praktyki</p>"},{"location":"Clickhouse/Dev/PL/07-best-practices/","title":"7. Najlepsze praktyki","text":""},{"location":"Clickhouse/Dev/PL/07-best-practices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Strukturyzowa\u0107 kod</li> <li>U\u017cywa\u0107 odpowiednich wzorc\u00f3w</li> <li>Optymalizowa\u0107 zasoby</li> <li>Zabezpiecza\u0107 aplikacj\u0119</li> </ul>"},{"location":"Clickhouse/Dev/PL/07-best-practices/#warstwa-serwisowa","title":"Warstwa serwisowa","text":"<pre><code>public class EventService {\n    private final ConnectionManager connectionManager;\n\n    public EventService(ConnectionManager connectionManager) {\n        this.connectionManager = connectionManager;\n    }\n\n    public List&lt;Event&gt; getEvents(Date date) throws SQLException {\n        // Logika biznesowa\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/PL/07-best-practices/#zarzadzanie-zasobami","title":"Zarz\u0105dzanie zasobami","text":"<pre><code>// \u2705 Dobrze\ntry (Connection conn = getConnection();\n     Statement stmt = conn.createStatement();\n     ResultSet rs = stmt.executeQuery(sql)) {\n    // ...\n}\n</code></pre>"},{"location":"Clickhouse/Dev/PL/07-best-practices/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":"<pre><code>// \u2705 Dobrze : PreparedStatement\nString sql = \"SELECT * FROM events WHERE id = ?\";\nPreparedStatement pstmt = conn.prepareStatement(sql);\npstmt.setLong(1, eventId);\n</code></pre> <p>Nast\u0119pny krok : Projekty praktyczne</p>"},{"location":"Clickhouse/Dev/PL/08-projets/","title":"8. Projekty praktyczne","text":""},{"location":"Clickhouse/Dev/PL/08-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 kompletn\u0105 aplikacj\u0119</li> <li>Integrowa\u0107 ClickHouse w rzeczywistym projekcie</li> <li>Stosowa\u0107 najlepsze praktyki</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> </ul>"},{"location":"Clickhouse/Dev/PL/08-projets/#projekt-1-serwis-analityczny","title":"Projekt 1 : Serwis analityczny","text":""},{"location":"Clickhouse/Dev/PL/08-projets/#struktura","title":"Struktura","text":"<pre><code>analytics-service/\n\u251c\u2500\u2500 src/main/java/com/analytics/\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 model/\n\u2502   \u251c\u2500\u2500 repository/\n\u2502   \u2514\u2500\u2500 service/\n\u2514\u2500\u2500 pom.xml\n</code></pre>"},{"location":"Clickhouse/Dev/PL/08-projets/#przykad-serwisu","title":"Przyk\u0142ad serwisu","text":"<pre><code>@Service\npublic class AnalyticsService {\n    private final EventRepository eventRepository;\n\n    public Map&lt;String, Long&gt; getEventCountsByType(Date date) {\n        return eventRepository.countByType(date);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/PL/08-projets/#projekt-2-aplikacja-spring-boot","title":"Projekt 2 : Aplikacja Spring Boot","text":""},{"location":"Clickhouse/Dev/PL/08-projets/#konfiguracja","title":"Konfiguracja","text":"<pre><code>@Configuration\npublic class ClickHouseConfig {\n    @Bean\n    public DataSource clickHouseDataSource() {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(\"jdbc:clickhouse://localhost:8123/default\");\n        return new HikariDataSource(config);\n    }\n}\n</code></pre>"},{"location":"Clickhouse/Dev/PL/08-projets/#projekt-3-api-rest","title":"Projekt 3 : API REST","text":"<pre><code>@RestController\n@RequestMapping(\"/api/events\")\npublic class EventController {\n    @GetMapping(\"/stats\")\n    public ResponseEntity&lt;EventStats&gt; getStats(@RequestParam Date date) {\n        EventStats stats = eventService.getStats(date);\n        return ResponseEntity.ok(stats);\n    }\n}\n</code></pre> <p>Gratulacje! Uko\u0144czy\u0142e\u015b szkolenie ClickHouse dla Dewelopera Java! \ud83c\udf89</p>"},{"location":"Cloud/AWS/EN/","title":"AWS Training for Data Analyst - Free Guide","text":""},{"location":"Cloud/AWS/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through learning Amazon Web Services (AWS) as a Data Analyst, using only free resources. You'll learn to use essential AWS services for data analysis without spending a penny.</p>"},{"location":"Cloud/AWS/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand essential AWS services for Data Analyst</li> <li>Create and manage free AWS accounts</li> <li>Use S3, Glue, Redshift and other data services</li> <li>Build ETL pipelines on AWS</li> <li>Analyze data with AWS</li> <li>Create practical projects for your portfolio</li> </ul>"},{"location":"Cloud/AWS/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 AWS Free Tier : Free services for 12 months - \u2705 AWS Training : Free courses and labs - \u2705 AWS Documentation : Complete free guides - \u2705 AWS Workshops : Free practical workshops</p> <p>Total Budget: $0</p>"},{"location":"Cloud/AWS/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"Cloud/AWS/EN/#1-aws-getting-started","title":"1. AWS Getting Started","text":"<ul> <li>Create a free AWS account</li> <li>Understand Free Tier</li> <li>Navigate AWS Console</li> <li>Security configuration (IAM)</li> </ul>"},{"location":"Cloud/AWS/EN/#2-amazon-s3-data-storage","title":"2. Amazon S3 - Data Storage","text":"<ul> <li>Create S3 buckets</li> <li>Upload and manage files</li> <li>Data organization</li> <li>Integration with other services</li> </ul>"},{"location":"Cloud/AWS/EN/#3-aws-glue-serverless-etl","title":"3. AWS Glue - Serverless ETL","text":"<ul> <li>Create ETL jobs</li> <li>Transform data</li> <li>Crawlers and data catalogs</li> <li>S3 integration</li> </ul>"},{"location":"Cloud/AWS/EN/#4-amazon-redshift-data-warehouse","title":"4. Amazon Redshift - Data Warehouse","text":"<ul> <li>Create Redshift cluster (free 2 months)</li> <li>Load data</li> <li>Advanced SQL queries</li> <li>Optimization</li> </ul>"},{"location":"Cloud/AWS/EN/#5-amazon-athena-sql-queries-on-s3","title":"5. Amazon Athena - SQL Queries on S3","text":"<ul> <li>SQL queries on S3 files</li> <li>Table creation</li> <li>Cost optimization</li> <li>Glue integration</li> </ul>"},{"location":"Cloud/AWS/EN/#6-aws-lambda-serverless-computing","title":"6. AWS Lambda - Serverless Computing","text":"<ul> <li>Create Lambda functions</li> <li>Data processing</li> <li>Workflow automation</li> <li>Integration with other services</li> </ul>"},{"location":"Cloud/AWS/EN/#7-practical-projects","title":"7. Practical Projects","text":"<ul> <li>Complete ETL pipeline</li> <li>Data Lake on AWS</li> <li>Portfolio project</li> <li>Best practices</li> </ul>"},{"location":"Cloud/AWS/EN/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"Cloud/AWS/EN/#step-1-create-a-free-aws-account","title":"Step 1: Create a Free AWS Account","text":"<ol> <li>Go to: https://aws.amazon.com/free/</li> <li>Click \"Create a Free Account\"</li> <li>Fill out the form (credit card required but not charged)</li> <li>Verify your identity by phone</li> <li>Choose a support plan (free)</li> </ol> <p>Important: AWS won't charge you as long as you stay within Free Tier limits.</p>"},{"location":"Cloud/AWS/EN/#step-2-explore-free-tier","title":"Step 2: Explore Free Tier","text":"<p>Free services useful for Data Analyst:</p> <ul> <li>Amazon S3 : 5 GB storage (always free)</li> <li>AWS Glue : 10,000 objects/month (always free)</li> <li>Amazon Redshift : 2 months free (750 hours)</li> <li>AWS Lambda : 1 million requests/month (always free)</li> <li>Amazon Athena : 10 GB data scanned/month (always free)</li> </ul>"},{"location":"Cloud/AWS/EN/#step-3-follow-the-training","title":"Step 3: Follow the Training","text":"<ol> <li>Start with module 1 (Getting Started)</li> <li>Follow module order</li> <li>Practice with module 7 projects</li> <li>Monitor your usage in AWS Billing</li> </ol>"},{"location":"Cloud/AWS/EN/#essential-aws-services-for-data-analyst","title":"\ud83d\udcca Essential AWS Services for Data Analyst","text":"Service Usage Free Tier S3 Data storage 5 GB (always) Glue Serverless ETL 10K objects/month Redshift Data warehouse 2 months free Athena SQL queries on S3 10 GB/month Lambda Serverless processing 1M requests/month QuickSight Visualization 1 free user"},{"location":"Cloud/AWS/EN/#cost-management","title":"\u26a0\ufe0f Cost Management","text":""},{"location":"Cloud/AWS/EN/#tips-to-stay-free","title":"Tips to Stay Free","text":"<ol> <li>Monitor Billing</li> <li>Enable billing alerts</li> <li>Regularly check AWS Cost Explorer</li> <li> <p>Recommended limit: $5 alert</p> </li> <li> <p>Respect Free Tier Limits</p> </li> <li>Read conditions carefully</li> <li>Some services free for 12 months</li> <li> <p>Others always free (with limits)</p> </li> <li> <p>Delete Unused Resources</p> </li> <li>Stop unused instances</li> <li>Delete empty S3 buckets</li> <li> <p>Regular cleanup</p> </li> <li> <p>Use Free Regions</p> </li> <li>Some regions offer more free services</li> <li>Check availability by region</li> </ol>"},{"location":"Cloud/AWS/EN/#free-resources","title":"\ud83d\udcda Free Resources","text":""},{"location":"Cloud/AWS/EN/#official-aws-training","title":"Official AWS Training","text":"<ul> <li>AWS Training : https://aws.amazon.com/training/</li> <li>Free courses</li> <li>Practical labs</li> <li> <p>Certification preparation</p> </li> <li> <p>AWS Workshops : https://workshops.aws/</p> </li> <li>Guided workshops</li> <li>Practical labs</li> <li> <p>Complete projects</p> </li> <li> <p>AWS Documentation : https://docs.aws.amazon.com/</p> </li> <li>Complete guides</li> <li>Code examples</li> <li>API Reference</li> </ul>"},{"location":"Cloud/AWS/EN/#external-free-resources","title":"External Free Resources","text":"<ul> <li>YouTube : Official AWS channel</li> <li>GitHub : AWS examples</li> <li>AWS Blog : Articles and tutorials</li> </ul>"},{"location":"Cloud/AWS/EN/#certifications-optional","title":"\ud83c\udf93 Certifications (Optional)","text":""},{"location":"Cloud/AWS/EN/#aws-certified-cloud-practitioner","title":"AWS Certified Cloud Practitioner","text":"<ul> <li>Cost : ~$100</li> <li>Preparation : Free (AWS Training)</li> <li>Duration : 2-3 weeks</li> <li>Level : Beginner</li> </ul>"},{"location":"Cloud/AWS/EN/#aws-certified-data-analytics-specialty","title":"AWS Certified Data Analytics - Specialty","text":"<ul> <li>Cost : ~$300</li> <li>Preparation : Free (AWS Training)</li> <li>Duration : 2-3 months</li> <li>Level : Advanced</li> </ul>"},{"location":"Cloud/AWS/EN/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>All examples use Free Tier</li> <li>Costs indicated if exceeded</li> <li>Commands tested on AWS Console</li> <li>Times may vary by region</li> </ul>"},{"location":"Cloud/AWS/EN/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>This training is designed to be evolving. Feel free to suggest improvements or additional use cases.</p>"},{"location":"Cloud/AWS/EN/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>AWS Free Tier</li> <li>AWS Training</li> <li>AWS Documentation</li> <li>AWS Workshops</li> </ul>"},{"location":"Cloud/AWS/EN/01-getting-started/","title":"1. AWS Getting Started","text":""},{"location":"Cloud/AWS/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create a free AWS account</li> <li>Understand AWS Free Tier</li> <li>Navigate AWS Console</li> <li>Configure basic security (IAM)</li> <li>Monitor costs</li> </ul>"},{"location":"Cloud/AWS/EN/01-getting-started/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Create a Free AWS Account</li> <li>Understand Free Tier</li> <li>Navigate AWS Console</li> <li>IAM Configuration (Security)</li> <li>Cost Monitoring</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#create-a-free-aws-account","title":"Create a Free AWS Account","text":""},{"location":"Cloud/AWS/EN/01-getting-started/#step-1-registration","title":"Step 1: Registration","text":"<ol> <li>Go to AWS website</li> <li>URL: https://aws.amazon.com/free/</li> <li> <p>Click \"Create a Free Account\"</p> </li> <li> <p>Fill out the form</p> </li> <li>Email</li> <li>Strong password</li> <li> <p>AWS account name</p> </li> <li> <p>Contact information</p> </li> <li>Full name</li> <li>Phone number</li> <li> <p>Country</p> </li> <li> <p>Verification</p> </li> <li>Code received by SMS</li> <li> <p>Enter verification code</p> </li> <li> <p>Payment method</p> </li> <li>Important: Credit card required but not charged</li> <li>AWS won't charge you as long as you stay within Free Tier</li> <li> <p>You can remove the card later (not recommended)</p> </li> <li> <p>Identity verification</p> </li> <li>Automatic call</li> <li> <p>Enter 4-digit code</p> </li> <li> <p>Support plan</p> </li> <li>Choose \"Basic Plan\" (free)</li> <li>Other plans are paid</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#step-2-confirmation","title":"Step 2: Confirmation","text":"<ul> <li>Confirmation email received</li> <li>AWS account active immediately</li> <li>Access to AWS Console</li> </ul> <p>\u26a0\ufe0f Important: Don't create multiple accounts with the same credit card (risk of suspension).</p>"},{"location":"Cloud/AWS/EN/01-getting-started/#understand-free-tier","title":"Understand Free Tier","text":""},{"location":"Cloud/AWS/EN/01-getting-started/#types-of-free-tier","title":"Types of Free Tier","text":"<p>AWS offers 3 types of free services:</p>"},{"location":"Cloud/AWS/EN/01-getting-started/#1-free-services-for-12-months","title":"1. Free services for 12 months","text":"<p>Services useful for Data Analyst:</p> <ul> <li>Amazon EC2: 750 hours/month (t2.micro)</li> <li>Amazon RDS: 750 hours/month</li> <li>Amazon Redshift: 750 hours/month (2 months only)</li> <li>Amazon Elasticsearch: 750 hours/month</li> </ul> <p>Conditions: - Free for 12 months after registration - Monthly limits - Beyond: normal billing</p>"},{"location":"Cloud/AWS/EN/01-getting-started/#2-always-free-services-with-limits","title":"2. Always free services (with limits)","text":"<p>Services useful for Data Analyst:</p> <ul> <li>Amazon S3: 5 GB storage (always free)</li> <li>AWS Lambda: 1 million requests/month (always free)</li> <li>AWS Glue: 10,000 objects/month (always free)</li> <li>Amazon Athena: 10 GB data scanned/month (always free)</li> <li>Amazon CloudWatch: 10 custom metrics (always free)</li> </ul> <p>Conditions: - Free indefinitely - Monthly limits - Beyond: billing beyond the limit</p>"},{"location":"Cloud/AWS/EN/01-getting-started/#3-short-term-free-trials","title":"3. Short-term free trials","text":"<ul> <li>Amazon Redshift: 2 months free</li> <li>Amazon QuickSight: 1 free user</li> </ul>"},{"location":"Cloud/AWS/EN/01-getting-started/#check-your-free-tier","title":"Check Your Free Tier","text":"<ol> <li>Go to AWS Console</li> <li>Menu \"Services\" \u2192 \"Billing\"</li> <li>Click \"Free Tier\"</li> <li>View usage by service</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#navigate-aws-console","title":"Navigate AWS Console","text":""},{"location":"Cloud/AWS/EN/01-getting-started/#main-interface","title":"Main Interface","text":"<p>Key elements:</p> <ol> <li>Search bar (top)</li> <li>Quickly search for services</li> <li> <p>Example: type \"S3\" to access Amazon S3</p> </li> <li> <p>Services menu (top left)</p> </li> <li>All AWS services</li> <li> <p>Organized by category</p> </li> <li> <p>Region (top right)</p> </li> <li>Choose AWS region</li> <li>Recommendation: Choose closest region</li> <li> <p>Example: <code>eu-west-3</code> (Paris) for France</p> </li> <li> <p>Account name (top right)</p> </li> <li>Account settings</li> <li>Billing</li> <li>Support</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#essential-services-for-data-analyst","title":"Essential Services for Data Analyst","text":"<p>In Services menu, search for:</p> <ul> <li>S3: Data storage</li> <li>Glue: Serverless ETL</li> <li>Redshift: Data warehouse</li> <li>Athena: SQL queries on S3</li> <li>Lambda: Serverless processing</li> <li>IAM: Access management</li> </ul>"},{"location":"Cloud/AWS/EN/01-getting-started/#first-connection","title":"First Connection","text":"<ol> <li>Sign in: https://console.aws.amazon.com/</li> <li>Explore the dashboard</li> <li>Click \"Services\" to see all services</li> <li>Use search bar to find a service</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#iam-configuration-security","title":"IAM Configuration (Security)","text":""},{"location":"Cloud/AWS/EN/01-getting-started/#what-is-iam","title":"What is IAM?","text":"<p>IAM (Identity and Access Management) = Access and identity management</p> <ul> <li>Create users</li> <li>Manage permissions</li> <li>Secure access to services</li> </ul>"},{"location":"Cloud/AWS/EN/01-getting-started/#security-best-practices","title":"Security Best Practices","text":""},{"location":"Cloud/AWS/EN/01-getting-started/#1-enable-two-factor-authentication-mfa","title":"1. Enable Two-Factor Authentication (MFA)","text":"<p>For root account:</p> <ol> <li>Go to IAM</li> <li>Click \"Activate MFA\"</li> <li>Choose a device (phone)</li> <li>Scan QR code with MFA app</li> <li>Enter verification codes</li> </ol> <p>\u26a0\ufe0f Important: Always enable MFA for root account.</p>"},{"location":"Cloud/AWS/EN/01-getting-started/#2-create-iam-user-recommended","title":"2. Create IAM User (Recommended)","text":"<p>Don't use root account for daily work.</p> <ol> <li>Go to IAM</li> <li>Click \"Users\" \u2192 \"Add users\"</li> <li>Username: <code>data-analyst</code></li> <li>Access type: \"Programmatic access\" + \"AWS Management Console access\"</li> <li>Permissions: \"Attach existing policies directly\"</li> <li>Select: <code>PowerUserAccess</code> (to start)</li> <li>Or create custom permissions</li> <li>Create user</li> <li>Save credentials (access key + secret)</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#3-iam-groups-optional","title":"3. IAM Groups (Optional)","text":"<p>Create groups to organize users:</p> <ol> <li>IAM \u2192 \"Groups\" \u2192 \"Create group\"</li> <li>Name: <code>DataAnalystGroup</code></li> <li>Attach policies</li> <li>Add users to group</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#recommended-iam-policies-for-data-analyst","title":"Recommended IAM Policies for Data Analyst","text":"<p>Essential policies:</p> <ul> <li><code>AmazonS3FullAccess</code>: Full access to S3</li> <li><code>AWSGlueServiceRole</code>: Access to Glue</li> <li><code>AmazonRedshiftFullAccess</code>: Access to Redshift</li> <li><code>AmazonAthenaFullAccess</code>: Access to Athena</li> <li><code>AWSLambdaFullAccess</code>: Access to Lambda</li> </ul> <p>\u26a0\ufe0f Principle of least privilege: Give only necessary permissions.</p>"},{"location":"Cloud/AWS/EN/01-getting-started/#cost-monitoring","title":"Cost Monitoring","text":""},{"location":"Cloud/AWS/EN/01-getting-started/#enable-billing-alerts","title":"Enable Billing Alerts","text":"<p>Step 1: Enable alerts</p> <ol> <li>Go to \"Billing\" \u2192 \"Preferences\"</li> <li>Enable \"Receive Billing Alerts\"</li> <li>Enable \"Receive Free Tier Usage Alerts\"</li> </ol> <p>Step 2: Create CloudWatch Alert</p> <ol> <li>Go to CloudWatch</li> <li>\"Alarms\" \u2192 \"Create alarm\"</li> <li>Metric: \"EstimatedCharges\"</li> <li>Threshold: $5 (recommended)</li> <li>Notification: Email</li> </ol> <p>Result: Email received if costs exceed $5.</p>"},{"location":"Cloud/AWS/EN/01-getting-started/#check-free-tier-usage","title":"Check Free Tier Usage","text":"<ol> <li>\"Billing\" \u2192 \"Free Tier\"</li> <li>View usage by service</li> <li>Check remaining limits</li> <li>Monitor expiration dates (12 months)</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#aws-cost-explorer","title":"AWS Cost Explorer","text":"<ol> <li>\"Billing\" \u2192 \"Cost Explorer\"</li> <li>View costs by service</li> <li>Filter by period</li> <li>Export reports</li> </ol> <p>\u26a0\ufe0f Important: Check regularly (weekly recommended).</p>"},{"location":"Cloud/AWS/EN/01-getting-started/#tips-to-stay-free","title":"Tips to Stay Free","text":"<ol> <li>Delete unused resources</li> <li>Stop unused EC2 instances</li> <li>Delete empty S3 buckets</li> <li> <p>Clean up snapshots</p> </li> <li> <p>Respect Free Tier limits</p> </li> <li>Read conditions carefully</li> <li>Monitor usage</li> <li> <p>Set alerts</p> </li> <li> <p>Use free regions</p> </li> <li>Some regions offer more free services</li> <li> <p>Check availability</p> </li> <li> <p>Stop unused services</p> </li> <li>Redshift: stop cluster when not used</li> <li>EC2: stop instances</li> <li>RDS: stop databases</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#key-points-to-remember","title":"\ud83d\udcca Key Points to Remember","text":"<ol> <li>Free AWS account: $200 credit + Free Tier</li> <li>Free Tier: 3 types (12 months, always free, trials)</li> <li>IAM Security: Enable MFA, create users</li> <li>Monitoring: Billing alerts essential</li> <li>Stay free: Delete unused resources</li> </ol>"},{"location":"Cloud/AWS/EN/01-getting-started/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Go to module 2. Amazon S3 - Data Storage to learn how to store data on AWS.</p>"},{"location":"Cloud/AWS/EN/02-s3/","title":"2. Amazon S3 - Data Storage","text":""},{"location":"Cloud/AWS/EN/02-s3/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Amazon S3 and its usage</li> <li>Create and manage S3 buckets</li> <li>Upload and organize files</li> <li>Understand storage classes</li> <li>Integrate S3 with other AWS services</li> </ul>"},{"location":"Cloud/AWS/EN/02-s3/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to S3</li> <li>Create an S3 Bucket</li> <li>Upload and Manage Files</li> <li>Storage Classes</li> <li>Data Organization</li> <li>Integration with Other Services</li> </ol>"},{"location":"Cloud/AWS/EN/02-s3/#introduction-to-s3","title":"Introduction to S3","text":""},{"location":"Cloud/AWS/EN/02-s3/#what-is-amazon-s3","title":"What is Amazon S3?","text":"<p>Amazon S3 (Simple Storage Service) = Object storage service</p> <ul> <li>Unlimited storage</li> <li>High availability (99.99%)</li> <li>Secure by default</li> <li>Integration with all AWS services</li> </ul>"},{"location":"Cloud/AWS/EN/02-s3/#use-cases-for-data-analyst","title":"Use Cases for Data Analyst","text":"<ul> <li>Data Lake: Store raw data</li> <li>Backup: Backup data</li> <li>ETL: Source/destination for pipelines</li> <li>Analytics: Data for Athena, Redshift</li> <li>Archive: Historical data</li> </ul>"},{"location":"Cloud/AWS/EN/02-s3/#s3-free-tier","title":"S3 Free Tier","text":"<p>Free forever: - 5 GB standard storage - 20,000 GET requests - 2,000 PUT requests - 15 GB data transfer out</p> <p>\u26a0\ufe0f Important: Beyond these limits, normal billing.</p>"},{"location":"Cloud/AWS/EN/02-s3/#create-an-s3-bucket","title":"Create an S3 Bucket","text":""},{"location":"Cloud/AWS/EN/02-s3/#step-1-access-s3","title":"Step 1: Access S3","text":"<ol> <li>AWS Console \u2192 Search \"S3\"</li> <li>Click \"Amazon S3\"</li> <li>Click \"Create bucket\"</li> </ol>"},{"location":"Cloud/AWS/EN/02-s3/#step-2-bucket-configuration","title":"Step 2: Bucket Configuration","text":"<p>Basic information: - Bucket name: Globally unique name (ex: <code>my-data-analyst-bucket</code>) - Region: Choose closest region (ex: <code>eu-west-3</code> Paris)</p> <p>Configuration options:</p> <ol> <li>Object Ownership</li> <li>\"ACLs disabled\" (recommended)</li> <li> <p>\"Bucket owner enforced\"</p> </li> <li> <p>Block Public Access</p> </li> <li>\u2705 Enable all (default security)</li> <li> <p>Disable only if specific need</p> </li> <li> <p>Versioning</p> </li> <li>Disabled by default (free)</li> <li> <p>Enable if need multiple versions</p> </li> <li> <p>Tags (optional)</p> </li> <li>Add tags for organization</li> <li> <p>Ex: <code>Project: Data-Analyst-Training</code></p> </li> <li> <p>Default encryption</p> </li> <li>\u2705 Enable (recommended)</li> <li>\"Amazon S3 managed keys (SSE-S3)\" (free)</li> </ol>"},{"location":"Cloud/AWS/EN/02-s3/#step-3-create-bucket","title":"Step 3: Create Bucket","text":"<ol> <li>Click \"Create bucket\"</li> <li>Bucket created and visible in list</li> <li>Ready to use</li> </ol> <p>\u26a0\ufe0f Important: Bucket name must be globally unique in AWS.</p>"},{"location":"Cloud/AWS/EN/02-s3/#upload-and-manage-files","title":"Upload and Manage Files","text":""},{"location":"Cloud/AWS/EN/02-s3/#upload-a-file","title":"Upload a File","text":"<p>Method 1: Web Interface</p> <ol> <li>Click bucket name</li> <li>Click \"Upload\"</li> <li>\"Add files\" or \"Add folder\"</li> <li>Select files</li> <li>Click \"Upload\"</li> </ol> <p>Method 2: AWS CLI</p> <pre><code># Install AWS CLI (if not already)\n# Windows: https://aws.amazon.com/cli/\n# Linux/Mac: pip install awscli\n\n# Configure credentials\naws configure\n\n# Upload a file\naws s3 cp local-file.csv s3://my-data-analyst-bucket/data/\n</code></pre> <p>Method 3: Python SDK (boto3)</p> <pre><code>import boto3\n\n# Create S3 client\ns3 = boto3.client('s3')\n\n# Upload file\ns3.upload_file('local-file.csv', 'my-data-analyst-bucket', 'data/file.csv')\n</code></pre>"},{"location":"Cloud/AWS/EN/02-s3/#download-a-file","title":"Download a File","text":"<p>Web interface: 1. Click file 2. Click \"Download\"</p> <p>AWS CLI: <pre><code>aws s3 cp s3://my-data-analyst-bucket/data/file.csv local-file.csv\n</code></pre></p> <p>Python: <pre><code>s3.download_file('my-data-analyst-bucket', 'data/file.csv', 'local-file.csv')\n</code></pre></p>"},{"location":"Cloud/AWS/EN/02-s3/#manage-files","title":"Manage Files","text":"<p>Available actions: - Download: Download - Open: Open in browser - Copy: Copy to another location - Move: Move - Delete: Delete - Make public: Make public (security attention)</p>"},{"location":"Cloud/AWS/EN/02-s3/#storage-classes","title":"Storage Classes","text":""},{"location":"Cloud/AWS/EN/02-s3/#s3-standard-default","title":"S3 Standard (default)","text":"<p>Usage: - Frequently accessed data - Production applications</p> <p>Characteristics: - Fast access - 99.99% availability - Cost: ~$0.023 per GB/month</p> <p>Free Tier: 5 GB free</p>"},{"location":"Cloud/AWS/EN/02-s3/#s3-intelligent-tiering","title":"S3 Intelligent-Tiering","text":"<p>Usage: - Data with variable access - Automatic cost optimization</p> <p>Characteristics: - Automatically moves between classes - No retrieval fees - Cost: ~$0.023 per GB/month</p>"},{"location":"Cloud/AWS/EN/02-s3/#s3-standard-ia-infrequent-access","title":"S3 Standard-IA (Infrequent Access)","text":"<p>Usage: - Rarely accessed data - Backup, archives</p> <p>Characteristics: - Fast access when needed - Storage cost: ~$0.0125 per GB/month - Retrieval cost: ~$0.01 per GB</p>"},{"location":"Cloud/AWS/EN/02-s3/#s3-one-zone-ia","title":"S3 One Zone-IA","text":"<p>Usage: - Reproducible data - Secondary backup</p> <p>Characteristics: - Storage in single zone - Cost: ~$0.01 per GB/month - \u26a0\ufe0f Risk of loss if zone fails</p>"},{"location":"Cloud/AWS/EN/02-s3/#s3-glacier","title":"S3 Glacier","text":"<p>Usage: - Long-term archiving - Rarely needed data</p> <p>Characteristics: - Retrieval: 1-5 minutes to several hours - Cost: ~$0.004 per GB/month - Retrieval fees by speed</p>"},{"location":"Cloud/AWS/EN/02-s3/#choose-storage-class","title":"Choose Storage Class","text":"<p>For Data Analyst: - S3 Standard: Active data (frequent analysis) - S3 Standard-IA: Historical data (occasional analysis) - S3 Glacier: Archives (rarely used)</p> <p>Automatic transition: - Configure transition rules - Example: Standard \u2192 Standard-IA after 30 days</p>"},{"location":"Cloud/AWS/EN/02-s3/#data-organization","title":"Data Organization","text":""},{"location":"Cloud/AWS/EN/02-s3/#recommended-structure","title":"Recommended Structure","text":"<p>Organization by project: <pre><code>bucket-name/\n\u251c\u2500\u2500 raw/              # Raw data\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2502   \u251c\u2500\u2500 01/\n\u2502   \u2502   \u251c\u2500\u2500 02/\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 processed/        # Transformed data\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 analytics/        # Data for analysis\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 archive/          # Archives\n    \u2514\u2500\u2500 ...\n</code></pre></p> <p>Organization by type: <pre><code>bucket-name/\n\u251c\u2500\u2500 csv/\n\u251c\u2500\u2500 json/\n\u251c\u2500\u2500 parquet/\n\u2514\u2500\u2500 logs/\n</code></pre></p>"},{"location":"Cloud/AWS/EN/02-s3/#prefixes-and-folders","title":"Prefixes and Folders","text":"<p>S3 doesn't have \"real\" folders, but uses prefixes:</p> <ul> <li><code>data/2024/01/file.csv</code> = Prefix <code>data/2024/01/</code></li> <li>Web interface simulates folders</li> <li>Use <code>/</code> to organize</li> </ul> <p>Best practices: - Use consistent prefixes - Include date in path - Separate by data type</p>"},{"location":"Cloud/AWS/EN/02-s3/#integration-with-other-services","title":"Integration with Other Services","text":""},{"location":"Cloud/AWS/EN/02-s3/#s3-aws-glue","title":"S3 + AWS Glue","text":"<p>Usage: - S3 as data source - Glue transforms data - Result to S3 or other destination</p> <p>Example: <pre><code># Glue job reads from S3\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"my_database\",\n    table_name = \"s3_data\"\n)\n</code></pre></p>"},{"location":"Cloud/AWS/EN/02-s3/#s3-amazon-athena","title":"S3 + Amazon Athena","text":"<p>Usage: - SQL queries directly on S3 files - No need to load into database - Pay-per-query</p> <p>Example: <pre><code>-- Create external table pointing to S3\nCREATE EXTERNAL TABLE my_table (\n    id INT,\n    name STRING\n)\nSTORED AS PARQUET\nLOCATION 's3://my-bucket/data/';\n</code></pre></p>"},{"location":"Cloud/AWS/EN/02-s3/#s3-amazon-redshift","title":"S3 + Amazon Redshift","text":"<p>Usage: - S3 as source for COPY - Redshift as data warehouse - Fast loading of large amounts</p> <p>Example: <pre><code>COPY my_table\nFROM 's3://my-bucket/data/file.csv'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV;\n</code></pre></p>"},{"location":"Cloud/AWS/EN/02-s3/#s3-aws-lambda","title":"S3 + AWS Lambda","text":"<p>Usage: - Trigger Lambda on upload - Automatic file processing - Transformation, validation, etc.</p> <p>Configuration: 1. S3 \u2192 Properties \u2192 Event notifications 2. Create notification 3. Trigger: \"All object create events\" 4. Destination: Lambda function</p>"},{"location":"Cloud/AWS/EN/02-s3/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/AWS/EN/02-s3/#security","title":"Security","text":"<ol> <li>Never make buckets public (except specific need)</li> <li>Use IAM to control access</li> <li>Enable encryption by default</li> <li>Use bucket policies for granular permissions</li> </ol>"},{"location":"Cloud/AWS/EN/02-s3/#performance","title":"Performance","text":"<ol> <li>Use prefixes to distribute load</li> <li>Avoid sequential names (ex: file1, file2, file3)</li> <li>Use Multipart Upload for large files (&gt;100MB)</li> <li>Enable Transfer Acceleration if needed (paid)</li> </ol>"},{"location":"Cloud/AWS/EN/02-s3/#costs","title":"Costs","text":"<ol> <li>Monitor usage regularly</li> <li>Use right storage classes</li> <li>Delete unused files</li> <li>Configure automatic transitions</li> <li>Use S3 Lifecycle to automate</li> </ol>"},{"location":"Cloud/AWS/EN/02-s3/#organization","title":"Organization","text":"<ol> <li>Name buckets consistently</li> <li>Use tags for organization</li> <li>Document data structure</li> <li>Create naming conventions</li> </ol>"},{"location":"Cloud/AWS/EN/02-s3/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/AWS/EN/02-s3/#example-1-upload-csv-file","title":"Example 1: Upload CSV File","text":"<pre><code>import boto3\nimport pandas as pd\n\n# Create S3 client\ns3 = boto3.client('s3')\n\n# Read local file\ndf = pd.read_csv('data.csv')\n\n# Upload to S3\ns3.upload_file('data.csv', 'my-bucket', 'raw/2024/data.csv')\n</code></pre>"},{"location":"Cloud/AWS/EN/02-s3/#example-2-list-files-in-prefix","title":"Example 2: List Files in Prefix","text":"<pre><code># List all files in a prefix\nresponse = s3.list_objects_v2(\n    Bucket='my-bucket',\n    Prefix='raw/2024/'\n)\n\nfor obj in response.get('Contents', []):\n    print(obj['Key'], obj['Size'])\n</code></pre>"},{"location":"Cloud/AWS/EN/02-s3/#example-3-download-and-process","title":"Example 3: Download and Process","text":"<pre><code># Download from S3\ns3.download_file('my-bucket', 'raw/data.csv', 'local-data.csv')\n\n# Process\ndf = pd.read_csv('local-data.csv')\n# ... processing ...\n\n# Upload result\ndf.to_csv('processed-data.csv', index=False)\ns3.upload_file('processed-data.csv', 'my-bucket', 'processed/data.csv')\n</code></pre>"},{"location":"Cloud/AWS/EN/02-s3/#key-points-to-remember","title":"\ud83d\udcca Key Points to Remember","text":"<ol> <li>S3 = Unlimited storage and highly available</li> <li>Free Tier: 5 GB always free</li> <li>Organize with prefixes for better performance</li> <li>Choose right storage class according to usage</li> <li>S3 integrates with all AWS data services</li> </ol>"},{"location":"Cloud/AWS/EN/02-s3/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Go to module 3. AWS Glue - Serverless ETL to learn how to transform data with AWS Glue.</p>"},{"location":"Cloud/AWS/EN/03-glue/","title":"3. AWS Glue - Serverless ETL","text":""},{"location":"Cloud/AWS/EN/03-glue/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand AWS Glue and its role in ETL</li> <li>Create crawlers to discover data</li> <li>Create ETL jobs with Glue</li> <li>Transform data with PySpark</li> <li>Integrate Glue with S3 and other services</li> </ul>"},{"location":"Cloud/AWS/EN/03-glue/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to AWS Glue</li> <li>Create a Data Catalog</li> <li>Crawlers - Discover Data</li> <li>Create an ETL Job</li> <li>Data Transformation</li> <li>Orchestration and Scheduling</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#introduction-to-aws-glue","title":"Introduction to AWS Glue","text":""},{"location":"Cloud/AWS/EN/03-glue/#what-is-aws-glue","title":"What is AWS Glue?","text":"<p>AWS Glue = Managed serverless ETL service</p> <ul> <li>ETL : Extract, Transform, Load</li> <li>Serverless : No servers to manage</li> <li>Managed : AWS manages the infrastructure</li> <li>Scalable : Automatically adapts</li> </ul>"},{"location":"Cloud/AWS/EN/03-glue/#glue-components","title":"Glue Components","text":"<ol> <li>Data Catalog : Metadata catalog</li> <li>Crawlers : Automatically discover schemas</li> <li>ETL Jobs : Transformation scripts (Python/PySpark)</li> <li>Triggers : Automatic triggering</li> <li>Workflows : Orchestration of multiple jobs</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#glue-free-tier","title":"Glue Free Tier","text":"<p>Free forever: - 10,000 objects/month in Data Catalog - 1 million queries/month to Data Catalog - $0.44 per DPU-hour (first million free)</p> <p>\u26a0\ufe0f Important: Glue jobs consume DPU (Data Processing Units). Monitor costs.</p>"},{"location":"Cloud/AWS/EN/03-glue/#create-a-data-catalog","title":"Create a Data Catalog","text":""},{"location":"Cloud/AWS/EN/03-glue/#what-is-the-data-catalog","title":"What is the Data Catalog?","text":"<p>Data Catalog = Centralized metadata catalog</p> <ul> <li>Data schemas</li> <li>Locations (S3, databases)</li> <li>Data types</li> <li>Partitions</li> </ul>"},{"location":"Cloud/AWS/EN/03-glue/#data-catalog-structure","title":"Data Catalog Structure","text":"<ul> <li>Databases : Groups of tables</li> <li>Tables : Data metadata</li> <li>Partitions : Data organization</li> </ul>"},{"location":"Cloud/AWS/EN/03-glue/#create-a-database","title":"Create a Database","text":"<ol> <li>AWS Console \u2192 Glue \u2192 \"Databases\"</li> <li>\"Add database\"</li> <li>Name: <code>data_analyst_db</code></li> <li>Description (optional)</li> <li>\"Create\"</li> </ol> <p>Usage: - Organize tables by project - Example: <code>raw_data_db</code>, <code>processed_data_db</code></p>"},{"location":"Cloud/AWS/EN/03-glue/#crawlers-discover-data","title":"Crawlers - Discover Data","text":""},{"location":"Cloud/AWS/EN/03-glue/#what-is-a-crawler","title":"What is a Crawler?","text":"<p>Crawler = Service that scans data and automatically creates tables</p> <ul> <li>Analyzes files in S3</li> <li>Automatically detects schema</li> <li>Creates tables in Data Catalog</li> <li>Supports: CSV, JSON, Parquet, etc.</li> </ul>"},{"location":"Cloud/AWS/EN/03-glue/#create-a-crawler","title":"Create a Crawler","text":"<p>Step 1: Basic Configuration</p> <ol> <li>Glue \u2192 \"Crawlers\" \u2192 \"Add crawler\"</li> <li>Name: <code>s3-csv-crawler</code></li> <li>Description (optional)</li> </ol> <p>Step 2: Data Source</p> <ol> <li>\"Add a data source\"</li> <li>Type: \"S3\"</li> <li>S3 path: <code>s3://my-bucket/raw/</code></li> <li>Include subfolders (optional)</li> </ol> <p>Step 3: IAM Role</p> <ol> <li>Create a new role or use existing</li> <li>Name: <code>AWSGlueServiceRole-default</code></li> <li>Permissions: S3 and Glue access</li> </ol> <p>Step 4: Output</p> <ol> <li>Database: <code>data_analyst_db</code></li> <li>Table prefix (optional)</li> </ol> <p>Step 5: Execute</p> <ol> <li>\"Run crawler now\" or schedule</li> <li>Wait for completion (a few minutes)</li> <li>Check created tables</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#crawler-result","title":"Crawler Result","text":"<p>Automatically created table: - Detected columns - Inferred data types - S3 location - File format</p> <p>Example of created table: <pre><code>Table: raw_data\nColumns:\n  - id (bigint)\n  - name (string)\n  - created_at (timestamp)\nLocation: s3://my-bucket/raw/\nFormat: csv\n</code></pre></p>"},{"location":"Cloud/AWS/EN/03-glue/#create-an-etl-job","title":"Create an ETL Job","text":""},{"location":"Cloud/AWS/EN/03-glue/#glue-job-types","title":"Glue Job Types","text":"<ol> <li>Spark : PySpark jobs (recommended)</li> <li>Python shell : Simple Python scripts</li> <li>Ray : Advanced distributed processing</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#create-a-spark-job","title":"Create a Spark Job","text":"<p>Step 1: Configuration</p> <ol> <li>Glue \u2192 \"ETL jobs\" \u2192 \"Add job\"</li> <li>Name: <code>transform-csv-job</code></li> <li>IAM Role: <code>AWSGlueServiceRole-default</code></li> <li>Type: \"Spark\"</li> <li>Glue version: \"4.0\" (recommended)</li> <li>DPU: 2 (minimum, adjustable)</li> </ol> <p>Step 2: Data Source</p> <ol> <li>\"Data source\": Select a table from Data Catalog</li> <li>Or: Direct S3 path</li> </ol> <p>Step 3: Destination</p> <ol> <li>\"Data target\": S3</li> <li>Format: Parquet (recommended for analytics)</li> <li>Path: <code>s3://my-bucket/processed/</code></li> </ol> <p>Step 4: Script</p> <ol> <li>Generate an automatic script</li> <li>Or: Write a custom script</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#basic-etl-script","title":"Basic ETL Script","text":"<p>Automatically generated script:</p> <pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Read from Data Catalog\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"raw_data\"\n)\n\n# Transform (example: filter)\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"status\"] == \"active\"\n)\n\n# Write to S3\nglueContext.write_dynamic_frame.from_options(\n    frame = filtered,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://my-bucket/processed/\"},\n    format = \"parquet\"\n)\n\njob.commit()\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#data-transformation","title":"Data Transformation","text":""},{"location":"Cloud/AWS/EN/03-glue/#common-transformations","title":"Common Transformations","text":""},{"location":"Cloud/AWS/EN/03-glue/#1-filter-rows","title":"1. Filter Rows","text":"<pre><code>from awsglue.transforms import Filter\n\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"age\"] &gt; 18\n)\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#2-select-columns","title":"2. Select Columns","text":"<pre><code>from awsglue.transforms import SelectFields\n\nselected = SelectFields.apply(\n    frame = datasource,\n    paths = [\"id\", \"name\", \"email\"]\n)\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#3-rename-columns","title":"3. Rename Columns","text":"<pre><code>from awsglue.transforms import RenameField\n\nrenamed = RenameField.apply(\n    frame = datasource,\n    old_name = \"old_column\",\n    new_name = \"new_column\"\n)\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#4-join-data","title":"4. Join Data","text":"<pre><code>joined = Join.apply(\n    frame1 = datasource1,\n    frame2 = datasource2,\n    keys1 = [\"id\"],\n    keys2 = [\"user_id\"]\n)\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#5-aggregations","title":"5. Aggregations","text":"<pre><code># Convert to Spark DataFrame for aggregations\ndf = datasource.toDF()\n\naggregated = df.groupBy(\"category\").agg({\n    \"amount\": \"sum\",\n    \"id\": \"count\"\n})\n\n# Convert back to DynamicFrame\nfrom awsglue.dynamicframe import DynamicFrame\nresult = DynamicFrame.fromDF(aggregated, glueContext, \"result\")\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#complete-example-csv-parquet-transformation","title":"Complete Example: CSV \u2192 Parquet Transformation","text":"<pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# 1. Read from S3 (via Data Catalog)\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"raw_data\"\n)\n\n# 2. Filter data\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"status\"] == \"active\"\n)\n\n# 3. Select columns\nselected = SelectFields.apply(\n    frame = filtered,\n    paths = [\"id\", \"name\", \"email\", \"created_at\"]\n)\n\n# 4. Convert to DataFrame for advanced transformations\ndf = selected.toDF()\n\n# 5. Add a calculated column\nfrom pyspark.sql.functions import col, year\ndf = df.withColumn(\"year\", year(col(\"created_at\")))\n\n# 6. Convert back to DynamicFrame\nfrom awsglue.dynamicframe import DynamicFrame\nresult = DynamicFrame.fromDF(df, glueContext, \"result\")\n\n# 7. Write to S3 in Parquet (partitioned by year)\nglueContext.write_dynamic_frame.from_options(\n    frame = result,\n    connection_type = \"s3\",\n    connection_options = {\n        \"path\": \"s3://my-bucket/processed/\",\n        \"partitionKeys\": [\"year\"]\n    },\n    format = \"parquet\"\n)\n\njob.commit()\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#orchestration-and-scheduling","title":"Orchestration and Scheduling","text":""},{"location":"Cloud/AWS/EN/03-glue/#manually-trigger-a-job","title":"Manually Trigger a Job","text":"<ol> <li>Glue \u2192 \"ETL jobs\"</li> <li>Select the job</li> <li>\"Run job\"</li> <li>View real-time logs</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#schedule-a-job-trigger","title":"Schedule a Job (Trigger)","text":"<p>Create a trigger:</p> <ol> <li>Glue \u2192 \"Triggers\" \u2192 \"Add trigger\"</li> <li>Name: <code>daily-etl-trigger</code></li> <li>Type: \"Scheduled\"</li> <li>Frequency: \"Cron expression\"</li> <li>Example: <code>cron(0 2 * * ? *)</code> = Every day at 2 AM</li> <li>Actions: Select the job to execute</li> <li>\"Add\"</li> </ol> <p>Trigger types: - On-demand : Manual triggering - Scheduled : Scheduled (cron) - Event-driven : Triggered by event (e.g., new S3 file)</p>"},{"location":"Cloud/AWS/EN/03-glue/#workflows-complex-orchestration","title":"Workflows (Complex Orchestration)","text":"<p>Create a workflow:</p> <ol> <li>Glue \u2192 \"Workflows\" \u2192 \"Add workflow\"</li> <li>Name: <code>etl-pipeline-workflow</code></li> <li>Add steps:</li> <li>Crawler \u2192 ETL Job \u2192 Another Job</li> <li>Define dependencies</li> <li>Trigger the workflow</li> </ol> <p>Example workflow: <pre><code>1. S3 Crawler \u2192 Discovers new files\n2. ETL Job 1 \u2192 Transforms raw data\n3. ETL Job 2 \u2192 Aggregates data\n4. ETL Job 3 \u2192 Loads into Redshift\n</code></pre></p>"},{"location":"Cloud/AWS/EN/03-glue/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/AWS/EN/03-glue/#performance","title":"Performance","text":"<ol> <li>Use Parquet instead of CSV (faster)</li> <li>Partition data (improves performance)</li> <li>Adjust DPU according to data size</li> <li>Use Spark cache to reuse data</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#costs","title":"Costs","text":"<ol> <li>Monitor DPU-hours used</li> <li>Optimize scripts to reduce execution time</li> <li>Use appropriate S3 classes (Standard-IA for archives)</li> <li>Stop jobs that fail quickly</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#organization","title":"Organization","text":"<ol> <li>Name jobs consistently</li> <li>Document transformations</li> <li>Version scripts (Git)</li> <li>Test locally before deploying</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/AWS/EN/03-glue/#example-1-transform-csv-parquet","title":"Example 1: Transform CSV \u2192 Parquet","text":"<pre><code># Read CSV from S3\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"raw_csv_data\"\n)\n\n# Write in Parquet\nglueContext.write_dynamic_frame.from_options(\n    frame = datasource,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://my-bucket/parquet/\"},\n    format = \"parquet\"\n)\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#example-2-clean-and-validate","title":"Example 2: Clean and Validate","text":"<pre><code># Filter invalid rows\ncleaned = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"email\"] is not None and \"@\" in x[\"email\"]\n)\n\n# Remove duplicates (via DataFrame)\ndf = cleaned.toDF()\ndf = df.dropDuplicates([\"id\"])\n\nresult = DynamicFrame.fromDF(df, glueContext, \"result\")\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#example-3-join-multiple-sources","title":"Example 3: Join Multiple Sources","text":"<pre><code># Read two tables\nusers = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"users\"\n)\n\norders = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"orders\"\n)\n\n# Join\njoined = Join.apply(\n    frame1 = users,\n    frame2 = orders,\n    keys1 = [\"id\"],\n    keys2 = [\"user_id\"]\n)\n</code></pre>"},{"location":"Cloud/AWS/EN/03-glue/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Glue = Serverless ETL managed by AWS</li> <li>Crawlers automatically discover schemas</li> <li>ETL Jobs use PySpark for transformations</li> <li>Data Catalog centralizes metadata</li> <li>Triggers enable automation</li> </ol>"},{"location":"Cloud/AWS/EN/03-glue/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 4. Amazon Redshift - Data Warehouse to learn how to use Redshift for data analysis.</p>"},{"location":"Cloud/AWS/EN/04-redshift/","title":"4. Amazon Redshift - Data Warehouse","text":""},{"location":"Cloud/AWS/EN/04-redshift/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Amazon Redshift and its role</li> <li>Create a Redshift cluster (free for 2 months)</li> <li>Load data into Redshift</li> <li>Optimize Redshift queries</li> <li>Integrate with S3 and other services</li> </ul>"},{"location":"Cloud/AWS/EN/04-redshift/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Redshift</li> <li>Create a Redshift Cluster</li> <li>Load Data</li> <li>Advanced SQL Queries</li> <li>Optimization</li> <li>Integration with Other Services</li> </ol>"},{"location":"Cloud/AWS/EN/04-redshift/#introduction-to-redshift","title":"Introduction to Redshift","text":""},{"location":"Cloud/AWS/EN/04-redshift/#what-is-amazon-redshift","title":"What is Amazon Redshift?","text":"<p>Amazon Redshift = Managed cloud data warehouse</p> <ul> <li>OLAP : Optimized for analysis (not transactions)</li> <li>Columnar : Column-oriented storage</li> <li>Massively parallel : Distributed processing</li> <li>Scalable : From a few GB to several PB</li> </ul>"},{"location":"Cloud/AWS/EN/04-redshift/#use-cases-for-data-analyst","title":"Use Cases for Data Analyst","text":"<ul> <li>Data Warehouse : Centralize data</li> <li>Analytics : Complex queries on large volumes</li> <li>Business Intelligence : Dashboards and reports</li> <li>Data Mining : In-depth analysis</li> </ul>"},{"location":"Cloud/AWS/EN/04-redshift/#redshift-free-tier","title":"Redshift Free Tier","text":"<p>Free for 2 months: - 750 hours/month of <code>dc2.large</code> cluster - 32 GB storage per node - After 2 months: normal billing</p> <p>\u26a0\ufe0f Important: Stop the cluster when not in use to avoid costs.</p>"},{"location":"Cloud/AWS/EN/04-redshift/#create-a-redshift-cluster","title":"Create a Redshift Cluster","text":""},{"location":"Cloud/AWS/EN/04-redshift/#step-1-access-redshift","title":"Step 1: Access Redshift","text":"<ol> <li>AWS Console \u2192 Search \"Redshift\"</li> <li>Click on \"Amazon Redshift\"</li> <li>\"Create cluster\"</li> </ol>"},{"location":"Cloud/AWS/EN/04-redshift/#step-2-cluster-configuration","title":"Step 2: Cluster Configuration","text":"<p>Basic Configuration:</p> <ol> <li>Cluster identifier: <code>data-analyst-cluster</code></li> <li>Node type: <code>dc2.large</code> (free for 2 months)</li> <li>Number of nodes: 1 (sufficient to start)</li> <li>Database name: <code>analytics</code> (default: <code>dev</code>)</li> <li>Database port: 5439 (default)</li> <li>Master username: <code>admin</code> (or other)</li> <li>Master password: Strong password</li> </ol> <p>Network Configuration:</p> <ol> <li>VPC: Choose an existing VPC</li> <li>Subnet group: Create or use existing</li> <li>Publicly accessible: \u2705 Yes (for easy access)</li> <li>Availability zone: Choose a zone</li> </ol> <p>Security:</p> <ol> <li>VPC security groups: Create a security group</li> <li>Allow port 5439 from your IP</li> <li>Encryption: Enable (recommended)</li> </ol>"},{"location":"Cloud/AWS/EN/04-redshift/#step-3-create-the-cluster","title":"Step 3: Create the Cluster","text":"<ol> <li>Click on \"Create cluster\"</li> <li>Wait 5-10 minutes (creation)</li> <li>Cluster ready when status = \"Available\"</li> </ol> <p>\u26a0\ufe0f Important: Note the cluster endpoint (e.g., <code>data-analyst-cluster.xxxxx.eu-west-3.redshift.amazonaws.com:5439</code>)</p>"},{"location":"Cloud/AWS/EN/04-redshift/#load-data","title":"Load Data","text":""},{"location":"Cloud/AWS/EN/04-redshift/#method-1-copy-from-s3-recommended","title":"Method 1: COPY from S3 (Recommended)","text":"<p>Fastest for large quantities:</p> <pre><code>-- Create a table\nCREATE TABLE users (\n    id INTEGER,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at TIMESTAMP\n);\n\n-- Load from S3\nCOPY users\nFROM 's3://my-bucket/data/users.csv'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV\nIGNOREHEADER 1;\n</code></pre> <p>IAM Role Configuration:</p> <ol> <li>IAM \u2192 \"Roles\" \u2192 \"Create role\"</li> <li>Type: \"Redshift\"</li> <li>Attach policy: <code>AmazonS3ReadOnlyAccess</code></li> <li>Name: <code>RedshiftS3Role</code></li> <li>Copy the ARN for COPY</li> </ol>"},{"location":"Cloud/AWS/EN/04-redshift/#method-2-insert-small-quantities","title":"Method 2: INSERT (Small Quantities)","text":"<pre><code>INSERT INTO users (id, name, email, created_at)\nVALUES (1, 'John Doe', 'john@example.com', '2024-01-01');\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#method-3-insert-from-query","title":"Method 3: INSERT from Query","text":"<pre><code>INSERT INTO users_aggregated\nSELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count\nFROM users\nGROUP BY DATE_TRUNC('month', created_at);\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#supported-formats","title":"Supported Formats","text":"<ul> <li>CSV: CSV files</li> <li>JSON: JSON files</li> <li>Parquet: Optimized format (recommended)</li> <li>Avro: Avro format</li> </ul>"},{"location":"Cloud/AWS/EN/04-redshift/#advanced-sql-queries","title":"Advanced SQL Queries","text":""},{"location":"Cloud/AWS/EN/04-redshift/#analytical-functions","title":"Analytical Functions","text":"<p>Window functions:</p> <pre><code>-- ROW_NUMBER\nSELECT \n    id,\n    name,\n    ROW_NUMBER() OVER (PARTITION BY category ORDER BY created_at) AS rank\nFROM products;\n\n-- LAG/LEAD\nSELECT \n    date,\n    sales,\n    LAG(sales, 1) OVER (ORDER BY date) AS previous_sales,\n    LEAD(sales, 1) OVER (ORDER BY date) AS next_sales\nFROM daily_sales;\n\n-- RANK\nSELECT \n    user_id,\n    total_spent,\n    RANK() OVER (ORDER BY total_spent DESC) AS spending_rank\nFROM user_totals;\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#complex-aggregations","title":"Complex Aggregations","text":"<pre><code>-- GROUP BY with ROLLUP\nSELECT \n    category,\n    region,\n    SUM(amount) AS total\nFROM sales\nGROUP BY ROLLUP(category, region);\n\n-- GROUP BY with CUBE\nSELECT \n    category,\n    region,\n    SUM(amount) AS total\nFROM sales\nGROUP BY CUBE(category, region);\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#optimized-joins","title":"Optimized Joins","text":"<pre><code>-- Join with distribution key\nSELECT \n    u.name,\n    o.amount,\n    o.created_at\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01';\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#optimization","title":"Optimization","text":""},{"location":"Cloud/AWS/EN/04-redshift/#distribution-keys","title":"Distribution Keys","text":"<p>Choose the right distribution key:</p> <pre><code>-- Key distribution (for joins)\nCREATE TABLE users (\n    id INTEGER DISTKEY,\n    name VARCHAR(100),\n    email VARCHAR(100)\n);\n\n-- ALL distribution (for small tables)\nCREATE TABLE categories (\n    id INTEGER,\n    name VARCHAR(100)\n) DISTSTYLE ALL;\n\n-- EVEN distribution (default)\nCREATE TABLE logs (\n    id INTEGER,\n    message TEXT\n) DISTSTYLE EVEN;\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#sort-keys","title":"Sort Keys","text":"<p>Improve query performance:</p> <pre><code>-- Simple sort key\nCREATE TABLE orders (\n    id INTEGER,\n    user_id INTEGER,\n    created_at TIMESTAMP,\n    amount DECIMAL(10,2)\n) SORTKEY (created_at);\n\n-- Composite sort key\nCREATE TABLE sales (\n    date DATE,\n    region VARCHAR(50),\n    amount DECIMAL(10,2)\n) SORTKEY (date, region);\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#compression","title":"Compression","text":"<p>Reduce storage space:</p> <pre><code>-- Automatic compression\nCREATE TABLE users (\n    id INTEGER,\n    name VARCHAR(100) ENCODE lzo,\n    email VARCHAR(100) ENCODE lzo,\n    created_at TIMESTAMP ENCODE delta\n);\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#analyze","title":"ANALYZE","text":"<p>Update statistics:</p> <pre><code>-- Analyze a table\nANALYZE users;\n\n-- Analyze all tables\nANALYZE;\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#integration-with-other-services","title":"Integration with Other Services","text":""},{"location":"Cloud/AWS/EN/04-redshift/#redshift-s3","title":"Redshift + S3","text":"<p>Unload to S3:</p> <pre><code>UNLOAD ('SELECT * FROM users WHERE created_at &gt; ''2024-01-01''')\nTO 's3://my-bucket/exports/users/'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV\nPARALLEL OFF;\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#redshift-glue","title":"Redshift + Glue","text":"<p>Glue can load into Redshift:</p> <pre><code># In a Glue job\nglueContext.write_dynamic_frame.from_jdbc_conf(\n    frame = transformed_data,\n    catalog_connection = \"redshift-connection\",\n    connection_options = {\n        \"dbtable\": \"users\",\n        \"database\": \"analytics\"\n    }\n)\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#redshift-quicksight","title":"Redshift + QuickSight","text":"<p>Connect QuickSight to Redshift:</p> <ol> <li>QuickSight \u2192 \"Data sources\"</li> <li>\"Redshift\"</li> <li>Enter connection information</li> <li>Select tables</li> <li>Create visualizations</li> </ol>"},{"location":"Cloud/AWS/EN/04-redshift/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/AWS/EN/04-redshift/#performance","title":"Performance","text":"<ol> <li>Use COPY instead of INSERT for large quantities</li> <li>Choose the right distribution keys</li> <li>Use sort keys for frequent queries</li> <li>Compress columns to save space</li> <li>VACUUM regularly to optimize</li> </ol>"},{"location":"Cloud/AWS/EN/04-redshift/#costs","title":"Costs","text":"<ol> <li>Stop the cluster when not in use</li> <li>Use the right node type according to needs</li> <li>Monitor storage usage</li> <li>Clean up unnecessary data</li> </ol>"},{"location":"Cloud/AWS/EN/04-redshift/#security","title":"Security","text":"<ol> <li>Encrypt data in transit and at rest</li> <li>Use VPC to isolate the cluster</li> <li>Limit access with security groups</li> <li>Audit access with CloudTrail</li> </ol>"},{"location":"Cloud/AWS/EN/04-redshift/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/AWS/EN/04-redshift/#example-1-complete-s3-redshift-pipeline","title":"Example 1: Complete S3 \u2192 Redshift Pipeline","text":"<pre><code>-- 1. Create the table\nCREATE TABLE sales (\n    id INTEGER,\n    product_id INTEGER,\n    amount DECIMAL(10,2),\n    sale_date DATE\n) DISTKEY(product_id) SORTKEY(sale_date);\n\n-- 2. Load from S3\nCOPY sales\nFROM 's3://my-bucket/data/sales/'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV\nIGNOREHEADER 1;\n\n-- 3. Analyze\nANALYZE sales;\n\n-- 4. Analytical queries\nSELECT \n    DATE_TRUNC('month', sale_date) AS month,\n    SUM(amount) AS total_sales\nFROM sales\nGROUP BY DATE_TRUNC('month', sale_date)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#example-2-aggregations-with-window-functions","title":"Example 2: Aggregations with Window Functions","text":"<pre><code>-- Top 10 products per month\nSELECT \n    product_id,\n    month,\n    total_sales,\n    RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) AS rank\nFROM (\n    SELECT \n        product_id,\n        DATE_TRUNC('month', sale_date) AS month,\n        SUM(amount) AS total_sales\n    FROM sales\n    GROUP BY product_id, DATE_TRUNC('month', sale_date)\n) monthly_sales\nWHERE RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) &lt;= 10;\n</code></pre>"},{"location":"Cloud/AWS/EN/04-redshift/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Redshift = Data warehouse for analytics</li> <li>Free Tier: 2 months free (750 hours)</li> <li>COPY from S3 = fastest method</li> <li>Distribution and sort keys = performance keys</li> <li>Stop the cluster when not in use</li> </ol>"},{"location":"Cloud/AWS/EN/04-redshift/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 5. Amazon Athena - SQL Queries on S3 to learn how to query S3 files directly.</p>"},{"location":"Cloud/AWS/EN/05-athena/","title":"5. Amazon Athena - SQL Queries on S3","text":""},{"location":"Cloud/AWS/EN/05-athena/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Amazon Athena and its usage</li> <li>Create external tables pointing to S3</li> <li>Execute SQL queries on S3 files</li> <li>Optimize costs and performance</li> <li>Integrate with Glue Data Catalog</li> </ul>"},{"location":"Cloud/AWS/EN/05-athena/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Athena</li> <li>Create External Tables</li> <li>Execute Queries</li> <li>Cost Optimization</li> <li>Integration with Glue</li> <li>Best Practices</li> </ol>"},{"location":"Cloud/AWS/EN/05-athena/#introduction-to-athena","title":"Introduction to Athena","text":""},{"location":"Cloud/AWS/EN/05-athena/#what-is-amazon-athena","title":"What is Amazon Athena?","text":"<p>Amazon Athena = Serverless SQL query service on S3</p> <ul> <li>Serverless : No infrastructure to manage</li> <li>Pay-per-query : Pay only what you use</li> <li>Standard SQL : Standard SQL syntax</li> <li>Directly on S3 : No need to load into database</li> </ul>"},{"location":"Cloud/AWS/EN/05-athena/#use-cases-for-data-analyst","title":"Use Cases for Data Analyst","text":"<ul> <li>Data exploration : Quickly analyze S3 files</li> <li>Data Lake queries : Queries on data lake</li> <li>Ad-hoc analysis : One-off analyses</li> <li>Log analysis : Analyze logs stored in S3</li> </ul>"},{"location":"Cloud/AWS/EN/05-athena/#athena-free-tier","title":"Athena Free Tier","text":"<p>Free forever: - 10 GB of data scanned/month - Beyond: $5 per Terabyte scanned</p> <p>\u26a0\ufe0f Important: Costs depend on the amount of data scanned. Optimize queries to reduce costs.</p>"},{"location":"Cloud/AWS/EN/05-athena/#create-external-tables","title":"Create External Tables","text":""},{"location":"Cloud/AWS/EN/05-athena/#method-1-via-athena-editor","title":"Method 1: Via Athena Editor","text":"<p>Step 1: Access Athena</p> <ol> <li>AWS Console \u2192 Search \"Athena\"</li> <li>Click on \"Amazon Athena\"</li> <li>First use: Configure S3 result</li> </ol> <p>Step 2: Configure Result</p> <ol> <li>\"Settings\" \u2192 \"Manage\"</li> <li>\"Query result location\": <code>s3://my-bucket/athena-results/</code></li> <li>\"Save\"</li> </ol> <p>Step 3: Create a Table</p> <pre><code>-- Table for CSV files\nCREATE EXTERNAL TABLE users (\n    id INT,\n    name STRING,\n    email STRING,\n    created_at TIMESTAMP\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nWITH SERDEPROPERTIES (\n    'serialization.format' = ',',\n    'field.delim' = ','\n)\nSTORED AS TEXTFILE\nLOCATION 's3://my-bucket/data/users/'\nTBLPROPERTIES ('skip.header.line.count'='1');\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#method-2-via-glue-data-catalog-recommended","title":"Method 2: Via Glue Data Catalog (Recommended)","text":"<p>Use tables created by Glue:</p> <ol> <li>Glue \u2192 Create a crawler for S3</li> <li>Crawler automatically creates the table</li> <li>Athena uses this table directly</li> </ol> <p>Advantages: - Automatically detected schema - No need to define manually - Reusable by other services</p>"},{"location":"Cloud/AWS/EN/05-athena/#supported-formats","title":"Supported Formats","text":"<p>CSV: <pre><code>CREATE EXTERNAL TABLE csv_data (\n    col1 STRING,\n    col2 INT\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/csv/';\n</code></pre></p> <p>JSON: <pre><code>CREATE EXTERNAL TABLE json_data (\n    id INT,\n    name STRING\n)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/json/';\n</code></pre></p> <p>Parquet (recommended): <pre><code>CREATE EXTERNAL TABLE parquet_data (\n    id INT,\n    name STRING,\n    created_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket/parquet/';\n</code></pre></p>"},{"location":"Cloud/AWS/EN/05-athena/#execute-queries","title":"Execute Queries","text":""},{"location":"Cloud/AWS/EN/05-athena/#basic-queries","title":"Basic Queries","text":"<p>Simple SELECT:</p> <pre><code>SELECT * FROM users LIMIT 10;\n</code></pre> <p>Filter:</p> <pre><code>SELECT \n    id,\n    name,\n    email\nFROM users\nWHERE created_at &gt; DATE '2024-01-01'\nORDER BY created_at DESC;\n</code></pre> <p>Aggregations:</p> <pre><code>SELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count,\n    COUNT(DISTINCT email) AS unique_emails\nFROM users\nGROUP BY DATE_TRUNC('month', created_at)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#advanced-queries","title":"Advanced Queries","text":"<p>Window functions:</p> <pre><code>SELECT \n    id,\n    name,\n    created_at,\n    ROW_NUMBER() OVER (PARTITION BY DATE_TRUNC('month', created_at) ORDER BY created_at) AS rank\nFROM users;\n</code></pre> <p>Joins:</p> <pre><code>SELECT \n    u.name,\n    o.amount,\n    o.created_at\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.created_at &gt; DATE '2024-01-01';\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#queries-on-partitions","title":"Queries on Partitions","text":"<p>If data is partitioned:</p> <pre><code>-- Table partitioned by date\nCREATE EXTERNAL TABLE sales (\n    id INT,\n    product_id INT,\n    amount DECIMAL(10,2)\n)\nPARTITIONED BY (sale_date DATE)\nSTORED AS PARQUET\nLOCATION 's3://bucket/sales/';\n\n-- Add partitions\nALTER TABLE sales ADD PARTITION (sale_date='2024-01-01')\nLOCATION 's3://bucket/sales/year=2024/month=01/day=01/';\n\n-- Query with partition (faster and cheaper)\nSELECT * FROM sales\nWHERE sale_date = DATE '2024-01-01';\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#cost-optimization","title":"Cost Optimization","text":""},{"location":"Cloud/AWS/EN/05-athena/#reduce-data-scanned","title":"Reduce Data Scanned","text":"<p>1. Use WHERE to filter early:</p> <pre><code>-- \u274c Bad: Scans everything then filters\nSELECT * FROM large_table\nWHERE date = '2024-01-01';\n\n-- \u2705 Good: Filters from the start (if partitioned)\nSELECT * FROM large_table\nWHERE date = '2024-01-01';\n</code></pre> <p>2. Select only necessary columns:</p> <pre><code>-- \u274c Bad: Scans all columns\nSELECT * FROM large_table;\n\n-- \u2705 Good: Scans only necessary columns\nSELECT id, name FROM large_table;\n</code></pre> <p>3. Use LIMIT:</p> <pre><code>-- Limit the number of results\nSELECT * FROM large_table LIMIT 100;\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#use-parquet","title":"Use Parquet","text":"<p>Parquet is more efficient than CSV:</p> <ul> <li>Compression : Less data scanned</li> <li>Columns : Scans only necessary columns</li> <li>Reduced cost : Up to 90% reduction</li> </ul> <p>Convert CSV \u2192 Parquet with Glue:</p> <pre><code># Glue job to convert\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"csv_data\"\n)\n\nglueContext.write_dynamic_frame.from_options(\n    frame = datasource,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bucket/parquet/\"},\n    format = \"parquet\"\n)\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#partition-data","title":"Partition Data","text":"<p>Partition by date (recommended):</p> <pre><code>s3://bucket/data/\n\u251c\u2500\u2500 year=2024/\n\u2502   \u251c\u2500\u2500 month=01/\n\u2502   \u2502   \u251c\u2500\u2500 day=01/\n\u2502   \u2502   \u2514\u2500\u2500 day=02/\n\u2502   \u2514\u2500\u2500 month=02/\n</code></pre> <p>Create partitioned table:</p> <pre><code>CREATE EXTERNAL TABLE partitioned_data (\n    id INT,\n    name STRING\n)\nPARTITIONED BY (year INT, month INT, day INT)\nSTORED AS PARQUET\nLOCATION 's3://bucket/data/';\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#integration-with-glue","title":"Integration with Glue","text":""},{"location":"Cloud/AWS/EN/05-athena/#use-glue-tables","title":"Use Glue Tables","text":"<p>Tables created by Glue are automatically available in Athena:</p> <ol> <li>Glue \u2192 Crawler creates a table</li> <li>Athena \u2192 \"Tables\" \u2192 See all Glue tables</li> <li>Use directly in queries</li> </ol> <p>Advantages: - Automatic schema - No manual definition - Automatic synchronization</p>"},{"location":"Cloud/AWS/EN/05-athena/#update-partitions","title":"Update Partitions","text":"<p>If new data added:</p> <pre><code>-- Update partitions\nMSCK REPAIR TABLE sales;\n\n-- Or add manually\nALTER TABLE sales ADD PARTITION (sale_date='2024-01-02')\nLOCATION 's3://bucket/sales/year=2024/month=01/day=02/';\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/AWS/EN/05-athena/#performance","title":"Performance","text":"<ol> <li>Use Parquet instead of CSV</li> <li>Partition data by date/category</li> <li>Select only necessary columns</li> <li>Filter early with WHERE</li> <li>Use LIMIT for exploration</li> </ol>"},{"location":"Cloud/AWS/EN/05-athena/#costs","title":"Costs","text":"<ol> <li>Monitor scanned data in results</li> <li>Optimize queries to reduce scan</li> <li>Use Parquet for compression</li> <li>Partition to reduce scan</li> <li>Cache frequent results</li> </ol>"},{"location":"Cloud/AWS/EN/05-athena/#organization","title":"Organization","text":"<ol> <li>Organize S3 with consistent prefixes</li> <li>Name tables clearly</li> <li>Document schemas</li> <li>Use databases to organize</li> </ol>"},{"location":"Cloud/AWS/EN/05-athena/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/AWS/EN/05-athena/#example-1-analyze-logs","title":"Example 1: Analyze Logs","text":"<pre><code>-- Table for logs\nCREATE EXTERNAL TABLE logs (\n    timestamp TIMESTAMP,\n    level STRING,\n    message STRING,\n    user_id INT\n)\nPARTITIONED BY (date DATE)\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/logs/';\n\n-- Query: Errors per day\nSELECT \n    date,\n    COUNT(*) AS error_count\nFROM logs\nWHERE level = 'ERROR'\nGROUP BY date\nORDER BY date DESC;\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#example-2-analyze-csv-data","title":"Example 2: Analyze CSV Data","text":"<pre><code>-- CSV table\nCREATE EXTERNAL TABLE sales_csv (\n    id INT,\n    product_id INT,\n    amount DECIMAL(10,2),\n    sale_date DATE\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/sales/csv/'\nTBLPROPERTIES ('skip.header.line.count'='1');\n\n-- Analysis: Sales per month\nSELECT \n    DATE_TRUNC('month', sale_date) AS month,\n    SUM(amount) AS total_sales,\n    COUNT(*) AS transaction_count\nFROM sales_csv\nGROUP BY DATE_TRUNC('month', sale_date)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#example-3-join-multiple-tables","title":"Example 3: Join Multiple Tables","text":"<pre><code>-- Analyze with joins\nSELECT \n    p.name AS product_name,\n    c.name AS category_name,\n    SUM(s.amount) AS total_sales\nFROM sales s\nJOIN products p ON s.product_id = p.id\nJOIN categories c ON p.category_id = c.id\nWHERE s.sale_date &gt;= DATE '2024-01-01'\nGROUP BY p.name, c.name\nORDER BY total_sales DESC\nLIMIT 10;\n</code></pre>"},{"location":"Cloud/AWS/EN/05-athena/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Athena = Serverless SQL on S3 files</li> <li>Free Tier: 10 GB/month of scanned data</li> <li>Parquet = most efficient format</li> <li>Partition = reduce costs</li> <li>Glue integration = automatic schemas</li> </ol>"},{"location":"Cloud/AWS/EN/05-athena/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 6. AWS Lambda - Serverless Computing to learn how to automate data processing.</p>"},{"location":"Cloud/AWS/EN/06-lambda/","title":"6. AWS Lambda - Serverless Computing","text":""},{"location":"Cloud/AWS/EN/06-lambda/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand AWS Lambda and its usage</li> <li>Create Lambda functions</li> <li>Process data with Lambda</li> <li>Trigger Lambda from S3</li> <li>Integrate Lambda with other services</li> </ul>"},{"location":"Cloud/AWS/EN/06-lambda/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Lambda</li> <li>Create a Lambda Function</li> <li>Data Processing</li> <li>Triggers</li> <li>Integration with Other Services</li> <li>Best Practices</li> </ol>"},{"location":"Cloud/AWS/EN/06-lambda/#introduction-to-lambda","title":"Introduction to Lambda","text":""},{"location":"Cloud/AWS/EN/06-lambda/#what-is-aws-lambda","title":"What is AWS Lambda?","text":"<p>AWS Lambda = Serverless computing service</p> <ul> <li>Serverless : No servers to manage</li> <li>Event-driven : Triggered by events</li> <li>Auto-scaling : Automatically adapts</li> <li>Pay-per-use : Pay only for execution</li> </ul>"},{"location":"Cloud/AWS/EN/06-lambda/#use-cases-for-data-analyst","title":"Use Cases for Data Analyst","text":"<ul> <li>File processing : Transform uploaded files</li> <li>Automated ETL : Trigger Glue jobs</li> <li>Data validation : Verify data</li> <li>Notifications : Alert on events</li> <li>Orchestration : Coordinate multiple services</li> </ul>"},{"location":"Cloud/AWS/EN/06-lambda/#lambda-free-tier","title":"Lambda Free Tier","text":"<p>Free forever: - 1 million requests/month - 400,000 GB-seconds of compute time/month - Beyond: usage-based billing</p> <p>\u26a0\ufe0f Important: Very generous for most use cases.</p>"},{"location":"Cloud/AWS/EN/06-lambda/#create-a-lambda-function","title":"Create a Lambda Function","text":""},{"location":"Cloud/AWS/EN/06-lambda/#step-1-access-lambda","title":"Step 1: Access Lambda","text":"<ol> <li>AWS Console \u2192 Search \"Lambda\"</li> <li>Click on \"AWS Lambda\"</li> <li>\"Create function\"</li> </ol>"},{"location":"Cloud/AWS/EN/06-lambda/#step-2-basic-configuration","title":"Step 2: Basic Configuration","text":"<p>Options:</p> <ol> <li>Author from scratch : Create from scratch</li> <li>Use a blueprint : Use a template</li> <li>Browse serverless app repository : Pre-built applications</li> </ol> <p>Configuration:</p> <ol> <li>Function name: <code>process-data-file</code></li> <li>Runtime: Python 3.11 (or other)</li> <li>Architecture: x86_64 (default)</li> <li>Permissions: Create a new role with basic permissions</li> </ol>"},{"location":"Cloud/AWS/EN/06-lambda/#step-3-function-code","title":"Step 3: Function Code","text":"<p>Simple example:</p> <pre><code>import json\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Basic Lambda function\n    \"\"\"\n    # Processing\n    result = {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n\n    return result\n</code></pre> <p>Test the function:</p> <ol> <li>Click on \"Test\"</li> <li>Create a test event</li> <li>Execute</li> <li>View results</li> </ol>"},{"location":"Cloud/AWS/EN/06-lambda/#data-processing","title":"Data Processing","text":""},{"location":"Cloud/AWS/EN/06-lambda/#example-1-process-a-csv-file","title":"Example 1: Process a CSV File","text":"<pre><code>import json\nimport csv\nimport boto3\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # Get bucket and key from event\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # Download the file\n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response['Body'].read().decode('utf-8')\n\n    # Parse CSV\n    reader = csv.DictReader(content.splitlines())\n    rows = list(reader)\n\n    # Process data\n    processed = []\n    for row in rows:\n        processed.append({\n            'id': row['id'],\n            'name': row['name'].upper(),\n            'email': row['email'].lower()\n        })\n\n    # Upload result\n    output_key = key.replace('raw/', 'processed/')\n    s3.put_object(\n        Bucket=bucket,\n        Key=output_key,\n        Body=json.dumps(processed)\n    )\n\n    return {\n        'statusCode': 200,\n        'body': f'Processed {len(processed)} rows'\n    }\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#example-2-validate-data","title":"Example 2: Validate Data","text":"<pre><code>import json\nimport boto3\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    bucket = event['bucket']\n    key = event['key']\n\n    # Download the file\n    response = s3.get_object(Bucket=bucket, Key=key)\n    data = json.loads(response['Body'].read())\n\n    # Validate\n    errors = []\n    for item in data:\n        if 'email' not in item or '@' not in item['email']:\n            errors.append(f\"Invalid email for id {item.get('id')}\")\n        if 'age' in item and item['age'] &lt; 0:\n            errors.append(f\"Invalid age for id {item.get('id')}\")\n\n    # Upload report\n    if errors:\n        s3.put_object(\n            Bucket=bucket,\n            Key=f'validation-errors/{key}',\n            Body=json.dumps(errors)\n        )\n        return {\n            'statusCode': 400,\n            'body': f'Found {len(errors)} validation errors'\n        }\n\n    return {\n        'statusCode': 200,\n        'body': 'Validation passed'\n    }\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#example-3-trigger-a-glue-job","title":"Example 3: Trigger a Glue Job","text":"<pre><code>import boto3\n\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    # Name of Glue job to execute\n    job_name = 'my-etl-job'\n\n    # Trigger the job\n    response = glue.start_job_run(\n        JobName=job_name\n    )\n\n    return {\n        'statusCode': 200,\n        'body': f'Started Glue job: {response[\"JobRunId\"]}'\n    }\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#triggers","title":"Triggers","text":""},{"location":"Cloud/AWS/EN/06-lambda/#trigger-from-s3","title":"Trigger from S3","text":"<p>Configuration:</p> <ol> <li>Lambda \u2192 Function \u2192 \"Add trigger\"</li> <li>Source: \"S3\"</li> <li>Bucket: Select the bucket</li> <li>Event type: \"All object create events\" (or specific)</li> <li>Prefix (optional): <code>raw/</code> (only files in this prefix)</li> <li>Suffix (optional): <code>.csv</code> (only CSV files)</li> </ol> <p>Result: Lambda executes automatically when a file is uploaded.</p>"},{"location":"Cloud/AWS/EN/06-lambda/#trigger-from-eventbridge-schedule","title":"Trigger from EventBridge (Schedule)","text":"<p>Schedule an execution:</p> <ol> <li>Lambda \u2192 Function \u2192 \"Add trigger\"</li> <li>Source: \"EventBridge (CloudWatch Events)\"</li> <li>Rule: Create a new rule</li> <li>Schedule expression: <code>cron(0 2 * * ? *)</code> (every day at 2 AM)</li> </ol> <p>Cron examples: - <code>cron(0 2 * * ? *)</code> : Every day at 2 AM - <code>cron(0 */6 * * ? *)</code> : Every 6 hours - <code>cron(0 0 ? * MON *)</code> : Every Monday at midnight</p>"},{"location":"Cloud/AWS/EN/06-lambda/#trigger-from-api-gateway","title":"Trigger from API Gateway","text":"<p>Create a REST API:</p> <ol> <li>API Gateway \u2192 \"Create API\"</li> <li>Type: REST API</li> <li>Create a resource and method</li> <li>Integration: Lambda Function</li> <li>Select the Lambda function</li> </ol> <p>Result: HTTP call triggers Lambda.</p>"},{"location":"Cloud/AWS/EN/06-lambda/#integration-with-other-services","title":"Integration with Other Services","text":""},{"location":"Cloud/AWS/EN/06-lambda/#lambda-s3","title":"Lambda + S3","text":"<p>Automatic file processing:</p> <pre><code>import boto3\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # S3 event\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = record['s3']['object']['key']\n\n        # Process the file\n        # ...\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#lambda-glue","title":"Lambda + Glue","text":"<p>Trigger a Glue job:</p> <pre><code>import boto3\n\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    response = glue.start_job_run(\n        JobName='my-etl-job',\n        Arguments={\n            '--input-path': 's3://bucket/raw/',\n            '--output-path': 's3://bucket/processed/'\n        }\n    )\n    return response\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#lambda-sns-notifications","title":"Lambda + SNS (Notifications)","text":"<p>Send a notification:</p> <pre><code>import boto3\nimport json\n\nsns = boto3.client('sns')\n\ndef lambda_handler(event, context):\n    # Processing...\n\n    # Send notification\n    sns.publish(\n        TopicArn='arn:aws:sns:region:account:topic',\n        Message=json.dumps({\n            'status': 'success',\n            'message': 'Data processing completed'\n        })\n    )\n\n    return {'statusCode': 200}\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#lambda-step-functions","title":"Lambda + Step Functions","text":"<p>Orchestrate multiple Lambdas:</p> <pre><code>{\n  \"Comment\": \"ETL Pipeline\",\n  \"StartAt\": \"ProcessData\",\n  \"States\": {\n    \"ProcessData\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:region:account:function:process-data\",\n      \"Next\": \"ValidateData\"\n    },\n    \"ValidateData\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:region:account:function:validate-data\",\n      \"End\": true\n    }\n  }\n}\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/AWS/EN/06-lambda/#performance","title":"Performance","text":"<ol> <li>Optimize code to reduce execution time</li> <li>Use appropriate memory (128 MB to 10 GB)</li> <li>Reuse connections (boto3 clients)</li> <li>Use layers for common dependencies</li> </ol>"},{"location":"Cloud/AWS/EN/06-lambda/#costs","title":"Costs","text":"<ol> <li>Optimize duration of execution</li> <li>Choose the right memory (more memory = faster but more expensive)</li> <li>Avoid unnecessary timeouts</li> <li>Use reservations if constant usage (not in Free Tier)</li> </ol>"},{"location":"Cloud/AWS/EN/06-lambda/#security","title":"Security","text":"<ol> <li>Use IAM roles for permissions</li> <li>Don't hardcode credentials</li> <li>Use environment variables for configuration</li> <li>Enable VPC if private access needed</li> </ol>"},{"location":"Cloud/AWS/EN/06-lambda/#error-handling","title":"Error Handling","text":"<pre><code>import json\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ndef lambda_handler(event, context):\n    try:\n        # Processing\n        result = process_data(event)\n        return {\n            'statusCode': 200,\n            'body': json.dumps(result)\n        }\n    except Exception as e:\n        logger.error(f'Error: {str(e)}')\n        return {\n            'statusCode': 500,\n            'body': json.dumps({'error': str(e)})\n        }\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/AWS/EN/06-lambda/#example-1-automated-etl-pipeline","title":"Example 1: Automated ETL Pipeline","text":"<pre><code>import boto3\nimport json\n\ns3 = boto3.client('s3')\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Triggers a Glue job when a file is uploaded to S3\n    \"\"\"\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # Check if it's a CSV file\n    if not key.endswith('.csv'):\n        return {'statusCode': 200, 'body': 'Not a CSV file'}\n\n    # Trigger Glue job\n    response = glue.start_job_run(\n        JobName='csv-to-parquet-job',\n        Arguments={\n            '--input-path': f's3://{bucket}/{key}',\n            '--output-path': f's3://{bucket}/processed/'\n        }\n    )\n\n    return {\n        'statusCode': 200,\n        'body': f'Started Glue job: {response[\"JobRunId\"]}'\n    }\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#example-2-validation-and-notification","title":"Example 2: Validation and Notification","text":"<pre><code>import boto3\nimport json\nimport csv\n\ns3 = boto3.client('s3')\nsns = boto3.client('sns')\n\ndef lambda_handler(event, context):\n    bucket = event['bucket']\n    key = event['key']\n\n    # Download and validate\n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response['Body'].read().decode('utf-8')\n    reader = csv.DictReader(content.splitlines())\n\n    errors = []\n    for row in reader:\n        if not row.get('email') or '@' not in row['email']:\n            errors.append(f\"Row {row.get('id')}: Invalid email\")\n\n    # Notification\n    if errors:\n        sns.publish(\n            TopicArn='arn:aws:sns:region:account:alerts',\n            Message=f'Validation failed: {len(errors)} errors found'\n        )\n\n    return {'statusCode': 200 if not errors else 400}\n</code></pre>"},{"location":"Cloud/AWS/EN/06-lambda/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Lambda = Serverless : No infrastructure to manage</li> <li>Free Tier: 1M requests/month : Very generous</li> <li>Event-driven : Triggered by events</li> <li>Easy integration : With all AWS services</li> <li>Pay-per-use : Pay only for execution</li> </ol>"},{"location":"Cloud/AWS/EN/06-lambda/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 7. Practical Projects to create complete projects with AWS.</p>"},{"location":"Cloud/AWS/EN/07-projets/","title":"7. AWS Practical Projects","text":""},{"location":"Cloud/AWS/EN/07-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Apply acquired knowledge</li> <li>Create complete ETL pipelines</li> <li>Build a Data Lake on AWS</li> <li>Create projects for your portfolio</li> <li>Integrate multiple AWS services</li> </ul>"},{"location":"Cloud/AWS/EN/07-projets/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Project 1: ETL Pipeline S3 \u2192 Parquet</li> <li>Project 2: Data Lake on AWS</li> <li>Project 3: Analytics with Athena</li> <li>Project 4: Complete Automated Pipeline</li> <li>Best Practices for Portfolio</li> </ol>"},{"location":"Cloud/AWS/EN/07-projets/#project-1-etl-pipeline-s3-parquet","title":"Project 1: ETL Pipeline S3 \u2192 Parquet","text":""},{"location":"Cloud/AWS/EN/07-projets/#objective","title":"Objective","text":"<p>Create an ETL pipeline that transforms CSV files from S3 into optimized Parquet format.</p>"},{"location":"Cloud/AWS/EN/07-projets/#architecture","title":"Architecture","text":"<pre><code>S3 (raw/) \u2192 Glue Crawler \u2192 Data Catalog \u2192 Glue Job \u2192 S3 (processed/parquet/)\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#steps","title":"Steps","text":""},{"location":"Cloud/AWS/EN/07-projets/#1-prepare-data","title":"1. Prepare Data","text":"<p>Create an S3 bucket: - Name: <code>data-analyst-project-1</code> - Create a <code>raw/</code> folder - Upload a test CSV file</p> <p>Example CSV data: <pre><code>id,name,email,created_at,status\n1,John Doe,john@example.com,2024-01-01,active\n2,Jane Smith,jane@example.com,2024-01-02,inactive\n</code></pre></p>"},{"location":"Cloud/AWS/EN/07-projets/#2-create-a-glue-crawler","title":"2. Create a Glue Crawler","text":"<ol> <li>Glue \u2192 \"Crawlers\" \u2192 \"Add crawler\"</li> <li>Name: <code>csv-crawler</code></li> <li>Data source: <code>s3://data-analyst-project-1/raw/</code></li> <li>IAM Role: Create a role with S3 access</li> <li>Database: <code>project1_db</code></li> <li>Execute the crawler</li> </ol>"},{"location":"Cloud/AWS/EN/07-projets/#3-create-a-glue-job","title":"3. Create a Glue Job","text":"<ol> <li>Glue \u2192 \"ETL jobs\" \u2192 \"Add job\"</li> <li>Name: <code>csv-to-parquet-job</code></li> <li>Type: Spark</li> <li>Script:</li> </ol> <pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Read from Data Catalog\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"project1_db\",\n    table_name = \"raw_data\"\n)\n\n# Filter active data\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"status\"] == \"active\"\n)\n\n# Write in Parquet\nglueContext.write_dynamic_frame.from_options(\n    frame = filtered,\n    connection_type = \"s3\",\n    connection_options = {\n        \"path\": \"s3://data-analyst-project-1/processed/parquet/\"\n    },\n    format = \"parquet\"\n)\n\njob.commit()\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#4-execute-the-job","title":"4. Execute the Job","text":"<ol> <li>Select the job</li> <li>\"Run job\"</li> <li>Check logs</li> <li>Check Parquet files in S3</li> </ol>"},{"location":"Cloud/AWS/EN/07-projets/#result","title":"Result","text":"<ul> <li>CSV files transformed to Parquet</li> <li>Filtered data (only active)</li> <li>Ready for analytics with Athena</li> </ul>"},{"location":"Cloud/AWS/EN/07-projets/#project-2-data-lake-on-aws","title":"Project 2: Data Lake on AWS","text":""},{"location":"Cloud/AWS/EN/07-projets/#objective_1","title":"Objective","text":"<p>Create a complete Data Lake with ingestion, transformation, and analytics.</p>"},{"location":"Cloud/AWS/EN/07-projets/#architecture_1","title":"Architecture","text":"<pre><code>Sources \u2192 S3 (Raw) \u2192 Glue (Transform) \u2192 S3 (Processed) \u2192 Athena (Analytics)\n                \u2193\n            Lambda (Trigger)\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#steps_1","title":"Steps","text":""},{"location":"Cloud/AWS/EN/07-projets/#1-s3-structure","title":"1. S3 Structure","text":"<pre><code>data-lake-bucket/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u251c\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#2-crawlers-for-each-source","title":"2. Crawlers for Each Source","text":"<p>Create 3 crawlers: - <code>users-crawler</code> \u2192 <code>s3://bucket/raw/users/</code> - <code>orders-crawler</code> \u2192 <code>s3://bucket/raw/orders/</code> - <code>products-crawler</code> \u2192 <code>s3://bucket/raw/products/</code></p>"},{"location":"Cloud/AWS/EN/07-projets/#3-etl-jobs-for-transformation","title":"3. ETL Jobs for Transformation","text":"<p>Job for users: <pre><code># users-etl-job\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_lake_db\",\n    table_name = \"users\"\n)\n\n# Clean and transform\ncleaned = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"email\"] is not None\n)\n\nglueContext.write_dynamic_frame.from_options(\n    frame = cleaned,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bucket/processed/users/\"},\n    format = \"parquet\"\n)\n</code></pre></p>"},{"location":"Cloud/AWS/EN/07-projets/#4-athena-tables-for-analytics","title":"4. Athena Tables for Analytics","text":"<pre><code>-- Table users\nCREATE EXTERNAL TABLE users_processed (\n    id INT,\n    name STRING,\n    email STRING,\n    created_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket/processed/users/';\n\n-- Table orders\nCREATE EXTERNAL TABLE orders_processed (\n    id INT,\n    user_id INT,\n    amount DECIMAL(10,2),\n    created_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket/processed/orders/';\n\n-- Analytical query\nSELECT \n    u.name,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_spent\nFROM users_processed u\nLEFT JOIN orders_processed o ON u.id = o.user_id\nGROUP BY u.name\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#5-automation-with-lambda","title":"5. Automation with Lambda","text":"<p>Lambda triggered by S3 upload:</p> <pre><code>import boto3\n\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # Determine which job to run based on prefix\n    if 'users' in key:\n        job_name = 'users-etl-job'\n    elif 'orders' in key:\n        job_name = 'orders-etl-job'\n    else:\n        job_name = 'products-etl-job'\n\n    # Trigger the job\n    glue.start_job_run(JobName=job_name)\n\n    return {'statusCode': 200}\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#result_1","title":"Result","text":"<ul> <li>Functional Data Lake</li> <li>Automated pipeline</li> <li>Analytics with Athena</li> <li>Complete project for portfolio</li> </ul>"},{"location":"Cloud/AWS/EN/07-projets/#project-3-analytics-with-athena","title":"Project 3: Analytics with Athena","text":""},{"location":"Cloud/AWS/EN/07-projets/#objective_2","title":"Objective","text":"<p>Create a complete analytics system with SQL queries on S3 data.</p>"},{"location":"Cloud/AWS/EN/07-projets/#steps_2","title":"Steps","text":""},{"location":"Cloud/AWS/EN/07-projets/#1-prepare-data_1","title":"1. Prepare Data","text":"<p>Upload Parquet files to S3: - <code>s3://analytics-bucket/sales/year=2024/month=01/</code> - <code>s3://analytics-bucket/sales/year=2024/month=02/</code></p>"},{"location":"Cloud/AWS/EN/07-projets/#2-create-partitioned-tables","title":"2. Create Partitioned Tables","text":"<pre><code>CREATE EXTERNAL TABLE sales (\n    id INT,\n    product_id INT,\n    amount DECIMAL(10,2),\n    sale_date TIMESTAMP\n)\nPARTITIONED BY (year INT, month INT)\nSTORED AS PARQUET\nLOCATION 's3://analytics-bucket/sales/';\n\n-- Add partitions\nALTER TABLE sales ADD PARTITION (year=2024, month=1)\nLOCATION 's3://analytics-bucket/sales/year=2024/month=01/';\n\nALTER TABLE sales ADD PARTITION (year=2024, month=2)\nLOCATION 's3://analytics-bucket/sales/year=2024/month=02/';\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#3-analytical-queries","title":"3. Analytical Queries","text":"<p>Sales per month: <pre><code>SELECT \n    year,\n    month,\n    SUM(amount) AS total_sales,\n    COUNT(*) AS transaction_count,\n    AVG(amount) AS avg_transaction\nFROM sales\nWHERE year = 2024\nGROUP BY year, month\nORDER BY year, month;\n</code></pre></p> <p>Top products: <pre><code>SELECT \n    product_id,\n    SUM(amount) AS total_revenue,\n    COUNT(*) AS sales_count\nFROM sales\nWHERE year = 2024\nGROUP BY product_id\nORDER BY total_revenue DESC\nLIMIT 10;\n</code></pre></p> <p>Trends: <pre><code>SELECT \n    DATE_TRUNC('week', sale_date) AS week,\n    SUM(amount) AS weekly_sales,\n    LAG(SUM(amount), 1) OVER (ORDER BY DATE_TRUNC('week', sale_date)) AS previous_week\nFROM sales\nWHERE year = 2024\nGROUP BY DATE_TRUNC('week', sale_date)\nORDER BY week;\n</code></pre></p>"},{"location":"Cloud/AWS/EN/07-projets/#4-save-results","title":"4. Save Results","text":"<p>Create a table for results: <pre><code>CREATE EXTERNAL TABLE analytics_results (\n    metric_name STRING,\n    metric_value DECIMAL(10,2),\n    calculated_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://analytics-bucket/results/';\n</code></pre></p>"},{"location":"Cloud/AWS/EN/07-projets/#project-4-complete-automated-pipeline","title":"Project 4: Complete Automated Pipeline","text":""},{"location":"Cloud/AWS/EN/07-projets/#objective_3","title":"Objective","text":"<p>Create a completely automated ETL pipeline with multiple AWS services.</p>"},{"location":"Cloud/AWS/EN/07-projets/#complete-architecture","title":"Complete Architecture","text":"<pre><code>CSV file uploaded \u2192 S3 (raw/)\n    \u2193 (Event)\nLambda (Validation)\n    \u2193\nS3 (validated/)\n    \u2193 (Event)\nGlue Job (Transform CSV \u2192 Parquet)\n    \u2193\nS3 (processed/parquet/)\n    \u2193\nGlue Crawler (Update Catalog)\n    \u2193\nAthena (Analytics)\n    \u2193\nS3 (results/)\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#implementation","title":"Implementation","text":""},{"location":"Cloud/AWS/EN/07-projets/#1-validation-lambda","title":"1. Validation Lambda","text":"<pre><code>import boto3\nimport csv\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # Download and validate\n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response['Body'].read().decode('utf-8')\n    reader = csv.DictReader(content.splitlines())\n\n    valid_rows = []\n    for row in reader:\n        if row.get('email') and '@' in row['email']:\n            valid_rows.append(row)\n\n    # Upload validated data\n    if valid_rows:\n        validated_key = key.replace('raw/', 'validated/')\n        # Convert to CSV and upload\n        # ...\n\n    return {'statusCode': 200}\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#2-glue-transformation-job","title":"2. Glue Transformation Job","text":"<pre><code># Transform validated CSV to Parquet\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"pipeline_db\",\n    table_name = \"validated_data\"\n)\n\n# Transform\ntransformed = Map.apply(\n    frame = datasource,\n    f = lambda x: {\n        'id': x['id'],\n        'name': x['name'].upper(),\n        'email': x['email'].lower(),\n        'created_at': x['created_at']\n    }\n)\n\n# Write in Parquet\nglueContext.write_dynamic_frame.from_options(\n    frame = transformed,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bucket/processed/\"},\n    format = \"parquet\"\n)\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#3-glue-workflow","title":"3. Glue Workflow","text":"<p>Create a workflow: 1. Trigger: New file in <code>validated/</code> 2. Action: Execute Glue job 3. Next action: Update crawler</p>"},{"location":"Cloud/AWS/EN/07-projets/#result_2","title":"Result","text":"<ul> <li>Completely automated pipeline</li> <li>Automatic validation</li> <li>Automatic transformation</li> <li>Analytics available immediately</li> </ul>"},{"location":"Cloud/AWS/EN/07-projets/#best-practices-for-portfolio","title":"Best Practices for Portfolio","text":""},{"location":"Cloud/AWS/EN/07-projets/#documentation","title":"Documentation","text":"<p>Create a README for each project:</p> <pre><code># Project: AWS ETL Pipeline\n\n## Description\nAutomated ETL pipeline to transform CSV data to Parquet.\n\n## Architecture\n- S3: Storage\n- Glue: Transformation\n- Athena: Analytics\n\n## Results\n- 60% cost reduction\n- 80% processing time reduction\n</code></pre>"},{"location":"Cloud/AWS/EN/07-projets/#visualizations","title":"Visualizations","text":"<p>Create diagrams: - System architecture - Data flow - Data schema</p> <p>Tools: - Draw.io - Lucidchart - ASCII diagrams in README</p>"},{"location":"Cloud/AWS/EN/07-projets/#metrics","title":"Metrics","text":"<p>Include metrics: - Execution time before/after - Costs before/after - Volume of processed data - Query performance</p>"},{"location":"Cloud/AWS/EN/07-projets/#code","title":"Code","text":"<p>Best practices: - Commented code - Environment variables for configuration - Error handling - Logging</p>"},{"location":"Cloud/AWS/EN/07-projets/#github","title":"GitHub","text":"<p>Create a repository: - README with documentation - Lambda scripts - Glue scripts - Configuration - Diagrams</p>"},{"location":"Cloud/AWS/EN/07-projets/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Practical projects : Essential for portfolio</li> <li>Documentation : Explain architecture and results</li> <li>Metrics : Show impact (performance, costs)</li> <li>Clean code : Commented and organized</li> <li>GitHub : Share your projects</li> </ol>"},{"location":"Cloud/AWS/EN/07-projets/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>AWS Architecture Center</li> <li>AWS Solutions</li> <li>GitHub AWS Examples</li> </ul> <p>Congratulations! You have completed the AWS training for Data Analyst. You can now create complete projects on AWS using only free resources.</p>"},{"location":"Cloud/AWS/FR/","title":"Formation AWS pour Data Analyst - Guide Gratuit","text":""},{"location":"Cloud/AWS/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'apprentissage d'Amazon Web Services (AWS) en tant que Data Analyst, en utilisant uniquement des ressources gratuites. Vous apprendrez \u00e0 utiliser les services AWS essentiels pour l'analyse de donn\u00e9es sans d\u00e9penser un centime.</p>"},{"location":"Cloud/AWS/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre les services AWS essentiels pour Data Analyst</li> <li>Cr\u00e9er et g\u00e9rer des comptes AWS gratuits</li> <li>Utiliser S3, Glue, Redshift et autres services data</li> <li>Construire des pipelines ETL sur AWS</li> <li>Analyser des donn\u00e9es avec AWS</li> <li>Cr\u00e9er des projets pratiques pour votre portfolio</li> </ul>"},{"location":"Cloud/AWS/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 AWS Free Tier : Services gratuits pendant 12 mois - \u2705 AWS Training : Cours et labs gratuits - \u2705 AWS Documentation : Guides complets gratuits - \u2705 AWS Workshops : Workshops pratiques gratuits</p> <p>Budget total : 0\u20ac</p>"},{"location":"Cloud/AWS/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"Cloud/AWS/FR/#1-prise-en-main-aws","title":"1. Prise en main AWS","text":"<ul> <li>Cr\u00e9er un compte AWS gratuit</li> <li>Comprendre le Free Tier</li> <li>Naviguer dans la console AWS</li> <li>Configuration de s\u00e9curit\u00e9 (IAM)</li> </ul>"},{"location":"Cloud/AWS/FR/#2-amazon-s3-stockage-de-donnees","title":"2. Amazon S3 - Stockage de donn\u00e9es","text":"<ul> <li>Cr\u00e9er des buckets S3</li> <li>Uploader et g\u00e9rer des fichiers</li> <li>Organisation des donn\u00e9es</li> <li>Int\u00e9gration avec autres services</li> </ul>"},{"location":"Cloud/AWS/FR/#3-aws-glue-etl-serverless","title":"3. AWS Glue - ETL Serverless","text":"<ul> <li>Cr\u00e9er des jobs ETL</li> <li>Transformer des donn\u00e9es</li> <li>Crawlers et catalogues de donn\u00e9es</li> <li>Int\u00e9gration avec S3</li> </ul>"},{"location":"Cloud/AWS/FR/#4-amazon-redshift-data-warehouse","title":"4. Amazon Redshift - Data Warehouse","text":"<ul> <li>Cr\u00e9er un cluster Redshift (gratuit 2 mois)</li> <li>Charger des donn\u00e9es</li> <li>Requ\u00eates SQL avanc\u00e9es</li> <li>Optimisation</li> </ul>"},{"location":"Cloud/AWS/FR/#5-amazon-athena-requetes-sql-sur-s3","title":"5. Amazon Athena - Requ\u00eates SQL sur S3","text":"<ul> <li>Requ\u00eates SQL sur fichiers S3</li> <li>Cr\u00e9ation de tables</li> <li>Optimisation des co\u00fbts</li> <li>Int\u00e9gration avec Glue</li> </ul>"},{"location":"Cloud/AWS/FR/#6-aws-lambda-serverless-computing","title":"6. AWS Lambda - Serverless Computing","text":"<ul> <li>Cr\u00e9er des fonctions Lambda</li> <li>Traitement de donn\u00e9es</li> <li>Automatisation de workflows</li> <li>Int\u00e9gration avec autres services</li> </ul>"},{"location":"Cloud/AWS/FR/#7-projets-pratiques","title":"7. Projets pratiques","text":"<ul> <li>Pipeline ETL complet</li> <li>Data Lake sur AWS</li> <li>Projet pour portfolio</li> <li>Bonnes pratiques</li> </ul>"},{"location":"Cloud/AWS/FR/#demarrage-rapide","title":"\ud83d\ude80 D\u00e9marrage rapide","text":""},{"location":"Cloud/AWS/FR/#etape-1-creer-un-compte-aws-gratuit","title":"\u00c9tape 1 : Cr\u00e9er un compte AWS gratuit","text":"<ol> <li>Aller sur : https://aws.amazon.com/fr/free/</li> <li>Cliquer sur \"Cr\u00e9er un compte gratuit\"</li> <li>Remplir le formulaire (carte bancaire requise mais non d\u00e9bit\u00e9e)</li> <li>V\u00e9rifier votre identit\u00e9 par t\u00e9l\u00e9phone</li> <li>Choisir un plan de support (gratuit)</li> </ol> <p>Important : AWS ne vous facturera rien tant que vous restez dans les limites du Free Tier.</p>"},{"location":"Cloud/AWS/FR/#etape-2-explorer-le-free-tier","title":"\u00c9tape 2 : Explorer le Free Tier","text":"<p>Services gratuits utiles pour Data Analyst :</p> <ul> <li>Amazon S3 : 5 Go de stockage (toujours gratuit)</li> <li>AWS Glue : 10 000 objets/mois (toujours gratuit)</li> <li>Amazon Redshift : 2 mois gratuit (750 heures)</li> <li>AWS Lambda : 1 million de requ\u00eates/mois (toujours gratuit)</li> <li>Amazon Athena : 10 Go de donn\u00e9es scann\u00e9es/mois (toujours gratuit)</li> </ul>"},{"location":"Cloud/AWS/FR/#etape-3-suivre-la-formation","title":"\u00c9tape 3 : Suivre la formation","text":"<ol> <li>Commencez par le module 1 (Prise en main)</li> <li>Suivez l'ordre des modules</li> <li>Pratiquez avec les projets du module 7</li> <li>Surveillez votre utilisation dans AWS Billing</li> </ol>"},{"location":"Cloud/AWS/FR/#services-aws-essentiels-pour-data-analyst","title":"\ud83d\udcca Services AWS essentiels pour Data Analyst","text":"Service Usage Free Tier S3 Stockage de donn\u00e9es 5 Go (toujours) Glue ETL serverless 10K objets/mois Redshift Data warehouse 2 mois gratuit Athena Requ\u00eates SQL sur S3 10 Go/mois Lambda Traitement serverless 1M requ\u00eates/mois QuickSight Visualisation 1 utilisateur gratuit"},{"location":"Cloud/AWS/FR/#gestion-des-couts","title":"\u26a0\ufe0f Gestion des co\u00fbts","text":""},{"location":"Cloud/AWS/FR/#conseils-pour-rester-gratuit","title":"Conseils pour rester gratuit","text":"<ol> <li>Surveiller la facturation</li> <li>Activer les alertes de facturation</li> <li>V\u00e9rifier r\u00e9guli\u00e8rement AWS Cost Explorer</li> <li> <p>Limite recommand\u00e9e : 5\u20ac d'alerte</p> </li> <li> <p>Respecter les limites Free Tier</p> </li> <li>Lire attentivement les conditions</li> <li>Certains services sont gratuits 12 mois</li> <li> <p>D'autres sont toujours gratuits (avec limites)</p> </li> <li> <p>Supprimer les ressources inutilis\u00e9es</p> </li> <li>Arr\u00eater les instances non utilis\u00e9es</li> <li>Supprimer les buckets S3 vides</li> <li> <p>Nettoyer r\u00e9guli\u00e8rement</p> </li> <li> <p>Utiliser les r\u00e9gions gratuites</p> </li> <li>Certaines r\u00e9gions offrent plus de services gratuits</li> <li>V\u00e9rifier la disponibilit\u00e9 par r\u00e9gion</li> </ol>"},{"location":"Cloud/AWS/FR/#ressources-gratuites","title":"\ud83d\udcda Ressources gratuites","text":""},{"location":"Cloud/AWS/FR/#formation-officielle-aws","title":"Formation officielle AWS","text":"<ul> <li>AWS Training : https://aws.amazon.com/fr/training/</li> <li>Cours gratuits</li> <li>Labs pratiques</li> <li> <p>Certifications pr\u00e9par\u00e9es</p> </li> <li> <p>AWS Workshops : https://workshops.aws/</p> </li> <li>Workshops guid\u00e9s</li> <li>Labs pratiques</li> <li> <p>Projets complets</p> </li> <li> <p>AWS Documentation : https://docs.aws.amazon.com/</p> </li> <li>Guides complets</li> <li>Exemples de code</li> <li>API Reference</li> </ul>"},{"location":"Cloud/AWS/FR/#ressources-externes-gratuites","title":"Ressources externes gratuites","text":"<ul> <li>YouTube : Cha\u00eene officielle AWS</li> <li>GitHub : Exemples AWS</li> <li>AWS Blog : Articles et tutoriels</li> </ul>"},{"location":"Cloud/AWS/FR/#certifications-optionnel","title":"\ud83c\udf93 Certifications (optionnel)","text":""},{"location":"Cloud/AWS/FR/#aws-certified-cloud-practitioner","title":"AWS Certified Cloud Practitioner","text":"<ul> <li>Co\u00fbt : ~100\u20ac</li> <li>Pr\u00e9paration : Gratuite (AWS Training)</li> <li>Dur\u00e9e : 2-3 semaines</li> <li>Niveau : D\u00e9butant</li> </ul>"},{"location":"Cloud/AWS/FR/#aws-certified-data-analytics-specialty","title":"AWS Certified Data Analytics - Specialty","text":"<ul> <li>Co\u00fbt : ~300\u20ac</li> <li>Pr\u00e9paration : Gratuite (AWS Training)</li> <li>Dur\u00e9e : 2-3 mois</li> <li>Niveau : Avanc\u00e9</li> </ul>"},{"location":"Cloud/AWS/FR/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>Tous les exemples utilisent le Free Tier</li> <li>Les co\u00fbts sont indiqu\u00e9s si d\u00e9passement</li> <li>Les commandes sont test\u00e9es sur AWS Console</li> <li>Les temps peuvent varier selon la r\u00e9gion</li> </ul>"},{"location":"Cloud/AWS/FR/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>Cette formation est con\u00e7ue pour \u00eatre \u00e9volutive. N'h\u00e9sitez pas \u00e0 proposer des am\u00e9liorations ou des cas d'usage suppl\u00e9mentaires.</p>"},{"location":"Cloud/AWS/FR/#ressources-complementaires","title":"\ud83d\udcda Ressources compl\u00e9mentaires","text":"<ul> <li>AWS Free Tier</li> <li>AWS Training</li> <li>AWS Documentation</li> <li>AWS Workshops</li> </ul>"},{"location":"Cloud/AWS/FR/01-getting-started/","title":"1. Prise en main AWS","text":""},{"location":"Cloud/AWS/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er un compte AWS gratuit</li> <li>Comprendre le Free Tier AWS</li> <li>Naviguer dans la console AWS</li> <li>Configurer la s\u00e9curit\u00e9 de base (IAM)</li> <li>Surveiller les co\u00fbts</li> </ul>"},{"location":"Cloud/AWS/FR/01-getting-started/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Cr\u00e9er un compte AWS gratuit</li> <li>Comprendre le Free Tier</li> <li>Naviguer dans la console AWS</li> <li>Configuration IAM (s\u00e9curit\u00e9)</li> <li>Surveillance des co\u00fbts</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#creer-un-compte-aws-gratuit","title":"Cr\u00e9er un compte AWS gratuit","text":""},{"location":"Cloud/AWS/FR/01-getting-started/#etape-1-inscription","title":"\u00c9tape 1 : Inscription","text":"<ol> <li>Aller sur le site AWS</li> <li>URL : https://aws.amazon.com/fr/free/</li> <li> <p>Cliquer sur \"Cr\u00e9er un compte gratuit\"</p> </li> <li> <p>Remplir le formulaire</p> </li> <li>Email</li> <li>Mot de passe (fort)</li> <li> <p>Nom du compte AWS</p> </li> <li> <p>Informations de contact</p> </li> <li>Nom complet</li> <li>Num\u00e9ro de t\u00e9l\u00e9phone</li> <li> <p>Pays</p> </li> <li> <p>V\u00e9rification</p> </li> <li>Code re\u00e7u par SMS</li> <li> <p>Entrer le code de v\u00e9rification</p> </li> <li> <p>M\u00e9thode de paiement</p> </li> <li>Important : Carte bancaire requise mais non d\u00e9bit\u00e9e</li> <li>AWS ne facture rien tant que vous restez dans le Free Tier</li> <li> <p>Vous pouvez supprimer la carte apr\u00e8s (non recommand\u00e9)</p> </li> <li> <p>V\u00e9rification d'identit\u00e9</p> </li> <li>Appel automatique</li> <li> <p>Entrer le code \u00e0 4 chiffres</p> </li> <li> <p>Plan de support</p> </li> <li>Choisir \"Plan de base\" (gratuit)</li> <li>Les autres plans sont payants</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#etape-2-confirmation","title":"\u00c9tape 2 : Confirmation","text":"<ul> <li>Email de confirmation re\u00e7u</li> <li>Compte AWS actif imm\u00e9diatement</li> <li>Acc\u00e8s \u00e0 la console AWS</li> </ul> <p>\u26a0\ufe0f Important : Ne pas cr\u00e9er plusieurs comptes avec la m\u00eame carte bancaire (risque de suspension).</p>"},{"location":"Cloud/AWS/FR/01-getting-started/#comprendre-le-free-tier","title":"Comprendre le Free Tier","text":""},{"location":"Cloud/AWS/FR/01-getting-started/#types-de-free-tier","title":"Types de Free Tier","text":"<p>AWS offre 3 types de services gratuits :</p>"},{"location":"Cloud/AWS/FR/01-getting-started/#1-services-gratuits-pendant-12-mois","title":"1. Services gratuits pendant 12 mois","text":"<p>Services utiles pour Data Analyst :</p> <ul> <li>Amazon EC2 : 750 heures/mois (t2.micro)</li> <li>Amazon RDS : 750 heures/mois</li> <li>Amazon Redshift : 750 heures/mois (2 mois seulement)</li> <li>Amazon Elasticsearch : 750 heures/mois</li> </ul> <p>Conditions : - Gratuit pendant 12 mois apr\u00e8s inscription - Limites par mois - Au-del\u00e0 : facturation normale</p>"},{"location":"Cloud/AWS/FR/01-getting-started/#2-services-toujours-gratuits-avec-limites","title":"2. Services toujours gratuits (avec limites)","text":"<p>Services utiles pour Data Analyst :</p> <ul> <li>Amazon S3 : 5 Go de stockage (toujours gratuit)</li> <li>AWS Lambda : 1 million de requ\u00eates/mois (toujours gratuit)</li> <li>AWS Glue : 10 000 objets/mois (toujours gratuit)</li> <li>Amazon Athena : 10 Go de donn\u00e9es scann\u00e9es/mois (toujours gratuit)</li> <li>Amazon CloudWatch : 10 m\u00e9triques personnalis\u00e9es (toujours gratuit)</li> </ul> <p>Conditions : - Gratuit ind\u00e9finiment - Limites par mois - Au-del\u00e0 : facturation au-del\u00e0 de la limite</p>"},{"location":"Cloud/AWS/FR/01-getting-started/#3-essais-gratuits-a-court-terme","title":"3. Essais gratuits \u00e0 court terme","text":"<ul> <li>Amazon Redshift : 2 mois gratuit</li> <li>Amazon QuickSight : 1 utilisateur gratuit</li> </ul>"},{"location":"Cloud/AWS/FR/01-getting-started/#verifier-votre-free-tier","title":"V\u00e9rifier votre Free Tier","text":"<ol> <li>Aller dans la console AWS</li> <li>Menu \"Services\" \u2192 \"Billing\"</li> <li>Cliquer sur \"Free Tier\"</li> <li>Voir l'utilisation par service</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#naviguer-dans-la-console-aws","title":"Naviguer dans la console AWS","text":""},{"location":"Cloud/AWS/FR/01-getting-started/#interface-principale","title":"Interface principale","text":"<p>\u00c9l\u00e9ments cl\u00e9s :</p> <ol> <li>Barre de recherche (en haut)</li> <li>Rechercher des services rapidement</li> <li> <p>Exemple : taper \"S3\" pour acc\u00e9der \u00e0 Amazon S3</p> </li> <li> <p>Menu Services (en haut \u00e0 gauche)</p> </li> <li>Tous les services AWS</li> <li> <p>Organis\u00e9s par cat\u00e9gorie</p> </li> <li> <p>R\u00e9gion (en haut \u00e0 droite)</p> </li> <li>Choisir la r\u00e9gion AWS</li> <li>Recommandation : Choisir la r\u00e9gion la plus proche</li> <li> <p>Exemple : <code>eu-west-3</code> (Paris) pour la France</p> </li> <li> <p>Nom du compte (en haut \u00e0 droite)</p> </li> <li>Param\u00e8tres du compte</li> <li>Facturation</li> <li>Support</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#services-essentiels-pour-data-analyst","title":"Services essentiels pour Data Analyst","text":"<p>Dans le menu Services, chercher :</p> <ul> <li>S3 : Stockage de donn\u00e9es</li> <li>Glue : ETL serverless</li> <li>Redshift : Data warehouse</li> <li>Athena : Requ\u00eates SQL sur S3</li> <li>Lambda : Traitement serverless</li> <li>IAM : Gestion des acc\u00e8s</li> </ul>"},{"location":"Cloud/AWS/FR/01-getting-started/#premiere-connexion","title":"Premi\u00e8re connexion","text":"<ol> <li>Se connecter : https://console.aws.amazon.com/</li> <li>Explorer le tableau de bord</li> <li>Cliquer sur \"Services\" pour voir tous les services</li> <li>Utiliser la barre de recherche pour trouver un service</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#configuration-iam-securite","title":"Configuration IAM (s\u00e9curit\u00e9)","text":""},{"location":"Cloud/AWS/FR/01-getting-started/#quest-ce-que-iam","title":"Qu'est-ce que IAM ?","text":"<p>IAM (Identity and Access Management) = Gestion des acc\u00e8s et identit\u00e9s</p> <ul> <li>Cr\u00e9er des utilisateurs</li> <li>G\u00e9rer les permissions</li> <li>S\u00e9curiser l'acc\u00e8s aux services</li> </ul>"},{"location":"Cloud/AWS/FR/01-getting-started/#bonnes-pratiques-de-securite","title":"Bonnes pratiques de s\u00e9curit\u00e9","text":""},{"location":"Cloud/AWS/FR/01-getting-started/#1-activer-lauthentification-a-deux-facteurs-mfa","title":"1. Activer l'authentification \u00e0 deux facteurs (MFA)","text":"<p>Pour le compte root :</p> <ol> <li>Aller dans IAM</li> <li>Cliquer sur \"Activate MFA\"</li> <li>Choisir un appareil (t\u00e9l\u00e9phone)</li> <li>Scanner le QR code avec une app MFA</li> <li>Entrer les codes de v\u00e9rification</li> </ol> <p>\u26a0\ufe0f Important : Toujours activer MFA pour le compte root.</p>"},{"location":"Cloud/AWS/FR/01-getting-started/#2-creer-un-utilisateur-iam-recommande","title":"2. Cr\u00e9er un utilisateur IAM (recommand\u00e9)","text":"<p>Ne pas utiliser le compte root pour le travail quotidien.</p> <ol> <li>Aller dans IAM</li> <li>Cliquer sur \"Users\" \u2192 \"Add users\"</li> <li>Nom d'utilisateur : <code>data-analyst</code></li> <li>Type d'acc\u00e8s : \"Programmatic access\" + \"AWS Management Console access\"</li> <li>Permissions : \"Attach existing policies directly\"</li> <li>S\u00e9lectionner : <code>PowerUserAccess</code> (pour d\u00e9buter)</li> <li>Ou cr\u00e9er des permissions personnalis\u00e9es</li> <li>Cr\u00e9er l'utilisateur</li> <li>Sauvegarder les identifiants (cl\u00e9 d'acc\u00e8s + secret)</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#3-groupes-iam-optionnel","title":"3. Groupes IAM (optionnel)","text":"<p>Cr\u00e9er des groupes pour organiser les utilisateurs :</p> <ol> <li>IAM \u2192 \"Groups\" \u2192 \"Create group\"</li> <li>Nom : <code>DataAnalystGroup</code></li> <li>Attacher des politiques</li> <li>Ajouter des utilisateurs au groupe</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#politiques-iam-recommandees-pour-data-analyst","title":"Politiques IAM recommand\u00e9es pour Data Analyst","text":"<p>Politiques essentielles :</p> <ul> <li><code>AmazonS3FullAccess</code> : Acc\u00e8s complet \u00e0 S3</li> <li><code>AWSGlueServiceRole</code> : Acc\u00e8s \u00e0 Glue</li> <li><code>AmazonRedshiftFullAccess</code> : Acc\u00e8s \u00e0 Redshift</li> <li><code>AmazonAthenaFullAccess</code> : Acc\u00e8s \u00e0 Athena</li> <li><code>AWSLambdaFullAccess</code> : Acc\u00e8s \u00e0 Lambda</li> </ul> <p>\u26a0\ufe0f Principe du moindre privil\u00e8ge : Donner uniquement les permissions n\u00e9cessaires.</p>"},{"location":"Cloud/AWS/FR/01-getting-started/#surveillance-des-couts","title":"Surveillance des co\u00fbts","text":""},{"location":"Cloud/AWS/FR/01-getting-started/#activer-les-alertes-de-facturation","title":"Activer les alertes de facturation","text":"<p>\u00c9tape 1 : Activer les alertes</p> <ol> <li>Aller dans \"Billing\" \u2192 \"Preferences\"</li> <li>Activer \"Receive Billing Alerts\"</li> <li>Activer \"Receive Free Tier Usage Alerts\"</li> </ol> <p>\u00c9tape 2 : Cr\u00e9er une alerte CloudWatch</p> <ol> <li>Aller dans CloudWatch</li> <li>\"Alarms\" \u2192 \"Create alarm\"</li> <li>M\u00e9trique : \"EstimatedCharges\"</li> <li>Seuil : 5\u20ac (recommand\u00e9)</li> <li>Notification : Email</li> </ol> <p>R\u00e9sultat : Email re\u00e7u si les co\u00fbts d\u00e9passent 5\u20ac.</p>"},{"location":"Cloud/AWS/FR/01-getting-started/#verifier-lutilisation-free-tier","title":"V\u00e9rifier l'utilisation Free Tier","text":"<ol> <li>\"Billing\" \u2192 \"Free Tier\"</li> <li>Voir l'utilisation par service</li> <li>V\u00e9rifier les limites restantes</li> <li>Surveiller les dates d'expiration (12 mois)</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#aws-cost-explorer","title":"AWS Cost Explorer","text":"<ol> <li>\"Billing\" \u2192 \"Cost Explorer\"</li> <li>Voir les co\u00fbts par service</li> <li>Filtrer par p\u00e9riode</li> <li>Exporter les rapports</li> </ol> <p>\u26a0\ufe0f Important : V\u00e9rifier r\u00e9guli\u00e8rement (hebdomadaire recommand\u00e9).</p>"},{"location":"Cloud/AWS/FR/01-getting-started/#conseils-pour-rester-gratuit","title":"Conseils pour rester gratuit","text":"<ol> <li>Supprimer les ressources inutilis\u00e9es</li> <li>Arr\u00eater les instances EC2 non utilis\u00e9es</li> <li>Supprimer les buckets S3 vides</li> <li> <p>Nettoyer les snapshots</p> </li> <li> <p>Respecter les limites Free Tier</p> </li> <li>Lire attentivement les conditions</li> <li>Surveiller l'utilisation</li> <li> <p>Mettre des alertes</p> </li> <li> <p>Utiliser les r\u00e9gions gratuites</p> </li> <li>Certaines r\u00e9gions offrent plus de services gratuits</li> <li> <p>V\u00e9rifier la disponibilit\u00e9</p> </li> <li> <p>Arr\u00eater les services non utilis\u00e9s</p> </li> <li>Redshift : arr\u00eater le cluster quand non utilis\u00e9</li> <li>EC2 : arr\u00eater les instances</li> <li>RDS : arr\u00eater les bases de donn\u00e9es</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Compte AWS gratuit : 200$ de cr\u00e9dit + Free Tier</li> <li>Free Tier : 3 types (12 mois, toujours gratuit, essais)</li> <li>S\u00e9curit\u00e9 IAM : Activer MFA, cr\u00e9er utilisateurs</li> <li>Surveillance : Alertes de facturation essentielles</li> <li>Rester gratuit : Supprimer ressources inutilis\u00e9es</li> </ol>"},{"location":"Cloud/AWS/FR/01-getting-started/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 2. Amazon S3 - Stockage de donn\u00e9es pour apprendre \u00e0 stocker des donn\u00e9es sur AWS.</p>"},{"location":"Cloud/AWS/FR/02-s3/","title":"2. Amazon S3 - Stockage de donn\u00e9es","text":""},{"location":"Cloud/AWS/FR/02-s3/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Amazon S3 et son utilisation</li> <li>Cr\u00e9er et g\u00e9rer des buckets S3</li> <li>Uploader et organiser des fichiers</li> <li>Comprendre les classes de stockage</li> <li>Int\u00e9grer S3 avec d'autres services AWS</li> </ul>"},{"location":"Cloud/AWS/FR/02-s3/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 S3</li> <li>Cr\u00e9er un bucket S3</li> <li>Uploader et g\u00e9rer des fichiers</li> <li>Classes de stockage</li> <li>Organisation des donn\u00e9es</li> <li>Int\u00e9gration avec autres services</li> </ol>"},{"location":"Cloud/AWS/FR/02-s3/#introduction-a-s3","title":"Introduction \u00e0 S3","text":""},{"location":"Cloud/AWS/FR/02-s3/#quest-ce-quamazon-s3","title":"Qu'est-ce qu'Amazon S3 ?","text":"<p>Amazon S3 (Simple Storage Service) = Service de stockage d'objets</p> <ul> <li>Stockage illimit\u00e9</li> <li>Haute disponibilit\u00e9 (99.99%)</li> <li>S\u00e9curis\u00e9 par d\u00e9faut</li> <li>Int\u00e9gration avec tous les services AWS</li> </ul>"},{"location":"Cloud/AWS/FR/02-s3/#cas-dusage-pour-data-analyst","title":"Cas d'usage pour Data Analyst","text":"<ul> <li>Data Lake : Stocker des donn\u00e9es brutes</li> <li>Backup : Sauvegarder des donn\u00e9es</li> <li>ETL : Source/destination pour pipelines</li> <li>Analytics : Donn\u00e9es pour Athena, Redshift</li> <li>Archivage : Donn\u00e9es historiques</li> </ul>"},{"location":"Cloud/AWS/FR/02-s3/#free-tier-s3","title":"Free Tier S3","text":"<p>Gratuit \u00e0 vie : - 5 Go de stockage standard - 20 000 requ\u00eates GET - 2 000 requ\u00eates PUT - 15 Go de transfert de donn\u00e9es sortantes</p> <p>\u26a0\ufe0f Important : Au-del\u00e0 de ces limites, facturation normale.</p>"},{"location":"Cloud/AWS/FR/02-s3/#creer-un-bucket-s3","title":"Cr\u00e9er un bucket S3","text":""},{"location":"Cloud/AWS/FR/02-s3/#etape-1-acceder-a-s3","title":"\u00c9tape 1 : Acc\u00e9der \u00e0 S3","text":"<ol> <li>Console AWS \u2192 Rechercher \"S3\"</li> <li>Cliquer sur \"Amazon S3\"</li> <li>Cliquer sur \"Create bucket\"</li> </ol>"},{"location":"Cloud/AWS/FR/02-s3/#etape-2-configuration-du-bucket","title":"\u00c9tape 2 : Configuration du bucket","text":"<p>Informations de base : - Bucket name : Nom unique globalement (ex: <code>my-data-analyst-bucket</code>) - Region : Choisir la r\u00e9gion la plus proche (ex: <code>eu-west-3</code> Paris)</p> <p>Options de configuration :</p> <ol> <li>Object Ownership</li> <li>\"ACLs disabled\" (recommand\u00e9)</li> <li> <p>\"Bucket owner enforced\"</p> </li> <li> <p>Block Public Access</p> </li> <li>\u2705 Tout activer (s\u00e9curit\u00e9 par d\u00e9faut)</li> <li> <p>D\u00e9sactiver seulement si besoin sp\u00e9cifique</p> </li> <li> <p>Versioning</p> </li> <li>D\u00e9sactiv\u00e9 par d\u00e9faut (gratuit)</li> <li> <p>Activer si besoin de versions multiples</p> </li> <li> <p>Tags (optionnel)</p> </li> <li>Ajouter des tags pour organisation</li> <li> <p>Ex: <code>Project: Data-Analyst-Training</code></p> </li> <li> <p>Default encryption</p> </li> <li>\u2705 Activer (recommand\u00e9)</li> <li>\"Amazon S3 managed keys (SSE-S3)\" (gratuit)</li> </ol>"},{"location":"Cloud/AWS/FR/02-s3/#etape-3-creer-le-bucket","title":"\u00c9tape 3 : Cr\u00e9er le bucket","text":"<ol> <li>Cliquer sur \"Create bucket\"</li> <li>Bucket cr\u00e9\u00e9 et visible dans la liste</li> <li>Pr\u00eat \u00e0 utiliser</li> </ol> <p>\u26a0\ufe0f Important : Le nom du bucket doit \u00eatre unique globalement dans AWS.</p>"},{"location":"Cloud/AWS/FR/02-s3/#uploader-et-gerer-des-fichiers","title":"Uploader et g\u00e9rer des fichiers","text":""},{"location":"Cloud/AWS/FR/02-s3/#uploader-un-fichier","title":"Uploader un fichier","text":"<p>M\u00e9thode 1 : Interface web</p> <ol> <li>Cliquer sur le nom du bucket</li> <li>Cliquer sur \"Upload\"</li> <li>\"Add files\" ou \"Add folder\"</li> <li>S\u00e9lectionner les fichiers</li> <li>Cliquer sur \"Upload\"</li> </ol> <p>M\u00e9thode 2 : AWS CLI</p> <pre><code># Installer AWS CLI (si pas d\u00e9j\u00e0 fait)\n# Windows: https://aws.amazon.com/cli/\n# Linux/Mac: pip install awscli\n\n# Configurer les credentials\naws configure\n\n# Uploader un fichier\naws s3 cp local-file.csv s3://my-data-analyst-bucket/data/\n</code></pre> <p>M\u00e9thode 3 : SDK Python (boto3)</p> <pre><code>import boto3\n\n# Cr\u00e9er un client S3\ns3 = boto3.client('s3')\n\n# Uploader un fichier\ns3.upload_file('local-file.csv', 'my-data-analyst-bucket', 'data/file.csv')\n</code></pre>"},{"location":"Cloud/AWS/FR/02-s3/#telecharger-un-fichier","title":"T\u00e9l\u00e9charger un fichier","text":"<p>Interface web : 1. Cliquer sur le fichier 2. Cliquer sur \"Download\"</p> <p>AWS CLI : <pre><code>aws s3 cp s3://my-data-analyst-bucket/data/file.csv local-file.csv\n</code></pre></p> <p>Python : <pre><code>s3.download_file('my-data-analyst-bucket', 'data/file.csv', 'local-file.csv')\n</code></pre></p>"},{"location":"Cloud/AWS/FR/02-s3/#gerer-les-fichiers","title":"G\u00e9rer les fichiers","text":"<p>Actions disponibles : - Download : T\u00e9l\u00e9charger - Open : Ouvrir dans le navigateur - Copy : Copier vers un autre emplacement - Move : D\u00e9placer - Delete : Supprimer - Make public : Rendre public (attention s\u00e9curit\u00e9)</p>"},{"location":"Cloud/AWS/FR/02-s3/#classes-de-stockage","title":"Classes de stockage","text":""},{"location":"Cloud/AWS/FR/02-s3/#s3-standard-par-defaut","title":"S3 Standard (par d\u00e9faut)","text":"<p>Utilisation : - Donn\u00e9es fr\u00e9quemment acc\u00e9d\u00e9es - Applications en production</p> <p>Caract\u00e9ristiques : - Acc\u00e8s rapide - 99.99% de disponibilit\u00e9 - Co\u00fbt : ~0.023$ par Go/mois</p> <p>Free Tier : 5 Go gratuit</p>"},{"location":"Cloud/AWS/FR/02-s3/#s3-intelligent-tiering","title":"S3 Intelligent-Tiering","text":"<p>Utilisation : - Donn\u00e9es avec acc\u00e8s variable - Optimisation automatique des co\u00fbts</p> <p>Caract\u00e9ristiques : - D\u00e9place automatiquement entre classes - Pas de frais de r\u00e9cup\u00e9ration - Co\u00fbt : ~0.023$ par Go/mois</p>"},{"location":"Cloud/AWS/FR/02-s3/#s3-standard-ia-infrequent-access","title":"S3 Standard-IA (Infrequent Access)","text":"<p>Utilisation : - Donn\u00e9es rarement acc\u00e9d\u00e9es - Backup, archives</p> <p>Caract\u00e9ristiques : - Acc\u00e8s rapide quand n\u00e9cessaire - Co\u00fbt stockage : ~0.0125$ par Go/mois - Co\u00fbt r\u00e9cup\u00e9ration : ~0.01$ par Go</p>"},{"location":"Cloud/AWS/FR/02-s3/#s3-one-zone-ia","title":"S3 One Zone-IA","text":"<p>Utilisation : - Donn\u00e9es reproductibles - Backup secondaire</p> <p>Caract\u00e9ristiques : - Stockage dans une seule zone - Co\u00fbt : ~0.01$ par Go/mois - \u26a0\ufe0f Risque de perte si zone d\u00e9faillante</p>"},{"location":"Cloud/AWS/FR/02-s3/#s3-glacier","title":"S3 Glacier","text":"<p>Utilisation : - Archivage long terme - Donn\u00e9es rarement n\u00e9cessaires</p> <p>Caract\u00e9ristiques : - R\u00e9cup\u00e9ration : 1-5 minutes \u00e0 plusieurs heures - Co\u00fbt : ~0.004$ par Go/mois - Frais de r\u00e9cup\u00e9ration selon vitesse</p>"},{"location":"Cloud/AWS/FR/02-s3/#choisir-la-classe-de-stockage","title":"Choisir la classe de stockage","text":"<p>Pour Data Analyst : - S3 Standard : Donn\u00e9es actives (analyses fr\u00e9quentes) - S3 Standard-IA : Donn\u00e9es historiques (analyses occasionnelles) - S3 Glacier : Archives (rarement utilis\u00e9es)</p> <p>Transition automatique : - Configurer des r\u00e8gles de transition - Exemple : Standard \u2192 Standard-IA apr\u00e8s 30 jours</p>"},{"location":"Cloud/AWS/FR/02-s3/#organisation-des-donnees","title":"Organisation des donn\u00e9es","text":""},{"location":"Cloud/AWS/FR/02-s3/#structure-recommandee","title":"Structure recommand\u00e9e","text":"<p>Organisation par projet : <pre><code>bucket-name/\n\u251c\u2500\u2500 raw/              # Donn\u00e9es brutes\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2502   \u251c\u2500\u2500 01/\n\u2502   \u2502   \u251c\u2500\u2500 02/\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 processed/        # Donn\u00e9es transform\u00e9es\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 analytics/        # Donn\u00e9es pour analyse\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 archive/          # Archives\n    \u2514\u2500\u2500 ...\n</code></pre></p> <p>Organisation par type : <pre><code>bucket-name/\n\u251c\u2500\u2500 csv/\n\u251c\u2500\u2500 json/\n\u251c\u2500\u2500 parquet/\n\u2514\u2500\u2500 logs/\n</code></pre></p>"},{"location":"Cloud/AWS/FR/02-s3/#prefixes-et-dossiers","title":"Pr\u00e9fixes et dossiers","text":"<p>S3 n'a pas de \"vrais\" dossiers, mais utilise des pr\u00e9fixes :</p> <ul> <li><code>data/2024/01/file.csv</code> = Pr\u00e9fixe <code>data/2024/01/</code></li> <li>Interface web simule des dossiers</li> <li>Utiliser <code>/</code> pour organiser</li> </ul> <p>Bonnes pratiques : - Utiliser des pr\u00e9fixes coh\u00e9rents - Inclure la date dans le chemin - S\u00e9parer par type de donn\u00e9es</p>"},{"location":"Cloud/AWS/FR/02-s3/#integration-avec-autres-services","title":"Int\u00e9gration avec autres services","text":""},{"location":"Cloud/AWS/FR/02-s3/#s3-aws-glue","title":"S3 + AWS Glue","text":"<p>Utilisation : - S3 comme source de donn\u00e9es - Glue transforme les donn\u00e9es - R\u00e9sultat vers S3 ou autre destination</p> <p>Exemple : <pre><code># Job Glue lit depuis S3\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"my_database\",\n    table_name = \"s3_data\"\n)\n</code></pre></p>"},{"location":"Cloud/AWS/FR/02-s3/#s3-amazon-athena","title":"S3 + Amazon Athena","text":"<p>Utilisation : - Requ\u00eates SQL directement sur fichiers S3 - Pas besoin de charger dans base de donn\u00e9es - Pay-per-query</p> <p>Exemple : <pre><code>-- Cr\u00e9er une table externe pointant vers S3\nCREATE EXTERNAL TABLE my_table (\n    id INT,\n    name STRING\n)\nSTORED AS PARQUET\nLOCATION 's3://my-bucket/data/';\n</code></pre></p>"},{"location":"Cloud/AWS/FR/02-s3/#s3-amazon-redshift","title":"S3 + Amazon Redshift","text":"<p>Utilisation : - S3 comme source pour COPY - Redshift comme data warehouse - Chargement rapide de grandes quantit\u00e9s</p> <p>Exemple : <pre><code>COPY my_table\nFROM 's3://my-bucket/data/file.csv'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV;\n</code></pre></p>"},{"location":"Cloud/AWS/FR/02-s3/#s3-aws-lambda","title":"S3 + AWS Lambda","text":"<p>Utilisation : - D\u00e9clencher Lambda lors d'upload - Traitement automatique des fichiers - Transformation, validation, etc.</p> <p>Configuration : 1. S3 \u2192 Properties \u2192 Event notifications 2. Cr\u00e9er une notification 3. D\u00e9clencher : \"All object create events\" 4. Destination : Lambda function</p>"},{"location":"Cloud/AWS/FR/02-s3/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/AWS/FR/02-s3/#securite","title":"S\u00e9curit\u00e9","text":"<ol> <li>Ne jamais rendre les buckets publics (sauf besoin sp\u00e9cifique)</li> <li>Utiliser IAM pour contr\u00f4ler l'acc\u00e8s</li> <li>Activer le chiffrement par d\u00e9faut</li> <li>Utiliser des bucket policies pour permissions granulaires</li> </ol>"},{"location":"Cloud/AWS/FR/02-s3/#performance","title":"Performance","text":"<ol> <li>Utiliser des pr\u00e9fixes pour distribuer la charge</li> <li>\u00c9viter les noms s\u00e9quentiels (ex: file1, file2, file3)</li> <li>Utiliser Multipart Upload pour gros fichiers (&gt;100MB)</li> <li>Activer Transfer Acceleration si besoin (payant)</li> </ol>"},{"location":"Cloud/AWS/FR/02-s3/#couts","title":"Co\u00fbts","text":"<ol> <li>Surveiller l'utilisation r\u00e9guli\u00e8rement</li> <li>Utiliser les bonnes classes de stockage</li> <li>Supprimer les fichiers inutiles</li> <li>Configurer des transitions automatiques</li> <li>Utiliser S3 Lifecycle pour automatiser</li> </ol>"},{"location":"Cloud/AWS/FR/02-s3/#organisation","title":"Organisation","text":"<ol> <li>Nommer les buckets de mani\u00e8re coh\u00e9rente</li> <li>Utiliser des tags pour organisation</li> <li>Documenter la structure des donn\u00e9es</li> <li>Cr\u00e9er des conventions de nommage</li> </ol>"},{"location":"Cloud/AWS/FR/02-s3/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/AWS/FR/02-s3/#exemple-1-uploader-un-fichier-csv","title":"Exemple 1 : Uploader un fichier CSV","text":"<pre><code>import boto3\nimport pandas as pd\n\n# Cr\u00e9er un client S3\ns3 = boto3.client('s3')\n\n# Lire un fichier local\ndf = pd.read_csv('data.csv')\n\n# Uploader vers S3\ns3.upload_file('data.csv', 'my-bucket', 'raw/2024/data.csv')\n</code></pre>"},{"location":"Cloud/AWS/FR/02-s3/#exemple-2-lister-les-fichiers-dun-prefixe","title":"Exemple 2 : Lister les fichiers d'un pr\u00e9fixe","text":"<pre><code># Lister tous les fichiers dans un pr\u00e9fixe\nresponse = s3.list_objects_v2(\n    Bucket='my-bucket',\n    Prefix='raw/2024/'\n)\n\nfor obj in response.get('Contents', []):\n    print(obj['Key'], obj['Size'])\n</code></pre>"},{"location":"Cloud/AWS/FR/02-s3/#exemple-3-telecharger-et-traiter","title":"Exemple 3 : T\u00e9l\u00e9charger et traiter","text":"<pre><code># T\u00e9l\u00e9charger depuis S3\ns3.download_file('my-bucket', 'raw/data.csv', 'local-data.csv')\n\n# Traiter\ndf = pd.read_csv('local-data.csv')\n# ... traitement ...\n\n# Uploader le r\u00e9sultat\ndf.to_csv('processed-data.csv', index=False)\ns3.upload_file('processed-data.csv', 'my-bucket', 'processed/data.csv')\n</code></pre>"},{"location":"Cloud/AWS/FR/02-s3/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>S3 = Stockage illimit\u00e9 et hautement disponible</li> <li>Free Tier : 5 Go toujours gratuit</li> <li>Organiser avec pr\u00e9fixes pour meilleure performance</li> <li>Choisir la bonne classe selon l'usage</li> <li>S3 s'int\u00e8gre avec tous les services AWS data</li> </ol>"},{"location":"Cloud/AWS/FR/02-s3/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 3. AWS Glue - ETL Serverless pour apprendre \u00e0 transformer des donn\u00e9es avec AWS Glue.</p>"},{"location":"Cloud/AWS/FR/03-glue/","title":"3. AWS Glue - ETL Serverless","text":""},{"location":"Cloud/AWS/FR/03-glue/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre AWS Glue et son r\u00f4le dans l'ETL</li> <li>Cr\u00e9er des crawlers pour d\u00e9couvrir les donn\u00e9es</li> <li>Cr\u00e9er des jobs ETL avec Glue</li> <li>Transformer des donn\u00e9es avec PySpark</li> <li>Int\u00e9grer Glue avec S3 et autres services</li> </ul>"},{"location":"Cloud/AWS/FR/03-glue/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 AWS Glue</li> <li>Cr\u00e9er un Data Catalog</li> <li>Crawlers - D\u00e9couvrir les donn\u00e9es</li> <li>Cr\u00e9er un job ETL</li> <li>Transformation de donn\u00e9es</li> <li>Orchestration et scheduling</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#introduction-a-aws-glue","title":"Introduction \u00e0 AWS Glue","text":""},{"location":"Cloud/AWS/FR/03-glue/#quest-ce-quaws-glue","title":"Qu'est-ce qu'AWS Glue ?","text":"<p>AWS Glue = Service ETL serverless g\u00e9r\u00e9</p> <ul> <li>ETL : Extract, Transform, Load</li> <li>Serverless : Pas de serveurs \u00e0 g\u00e9rer</li> <li>G\u00e9r\u00e9 : AWS g\u00e8re l'infrastructure</li> <li>Scalable : S'adapte automatiquement</li> </ul>"},{"location":"Cloud/AWS/FR/03-glue/#composants-glue","title":"Composants Glue","text":"<ol> <li>Data Catalog : Catalogue de m\u00e9tadonn\u00e9es</li> <li>Crawlers : D\u00e9couvrent automatiquement les sch\u00e9mas</li> <li>ETL Jobs : Scripts de transformation (Python/PySpark)</li> <li>Triggers : D\u00e9clenchement automatique</li> <li>Workflows : Orchestration de plusieurs jobs</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#free-tier-glue","title":"Free Tier Glue","text":"<p>Gratuit \u00e0 vie : - 10 000 objets/mois dans le Data Catalog - 1 million requ\u00eates/mois au Data Catalog - 0.44$ par DPU-heure (premier million gratuit)</p> <p>\u26a0\ufe0f Important : Les jobs Glue consomment des DPU (Data Processing Units). Surveiller les co\u00fbts.</p>"},{"location":"Cloud/AWS/FR/03-glue/#creer-un-data-catalog","title":"Cr\u00e9er un Data Catalog","text":""},{"location":"Cloud/AWS/FR/03-glue/#quest-ce-que-le-data-catalog","title":"Qu'est-ce que le Data Catalog ?","text":"<p>Data Catalog = Catalogue centralis\u00e9 de m\u00e9tadonn\u00e9es</p> <ul> <li>Sch\u00e9mas de donn\u00e9es</li> <li>Emplacements (S3, bases de donn\u00e9es)</li> <li>Types de donn\u00e9es</li> <li>Partitions</li> </ul>"},{"location":"Cloud/AWS/FR/03-glue/#structure-du-data-catalog","title":"Structure du Data Catalog","text":"<ul> <li>Databases : Groupes de tables</li> <li>Tables : M\u00e9tadonn\u00e9es des donn\u00e9es</li> <li>Partitions : Organisation des donn\u00e9es</li> </ul>"},{"location":"Cloud/AWS/FR/03-glue/#creer-une-base-de-donnees","title":"Cr\u00e9er une base de donn\u00e9es","text":"<ol> <li>Console AWS \u2192 Glue \u2192 \"Databases\"</li> <li>\"Add database\"</li> <li>Nom : <code>data_analyst_db</code></li> <li>Description (optionnel)</li> <li>\"Create\"</li> </ol> <p>Utilisation : - Organiser les tables par projet - Exemple : <code>raw_data_db</code>, <code>processed_data_db</code></p>"},{"location":"Cloud/AWS/FR/03-glue/#crawlers-decouvrir-les-donnees","title":"Crawlers - D\u00e9couvrir les donn\u00e9es","text":""},{"location":"Cloud/AWS/FR/03-glue/#quest-ce-quun-crawler","title":"Qu'est-ce qu'un Crawler ?","text":"<p>Crawler = Service qui scanne les donn\u00e9es et cr\u00e9e automatiquement les tables</p> <ul> <li>Analyse les fichiers dans S3</li> <li>D\u00e9tecte le sch\u00e9ma automatiquement</li> <li>Cr\u00e9e les tables dans le Data Catalog</li> <li>Supporte : CSV, JSON, Parquet, etc.</li> </ul>"},{"location":"Cloud/AWS/FR/03-glue/#creer-un-crawler","title":"Cr\u00e9er un Crawler","text":"<p>\u00c9tape 1 : Configuration de base</p> <ol> <li>Glue \u2192 \"Crawlers\" \u2192 \"Add crawler\"</li> <li>Nom : <code>s3-csv-crawler</code></li> <li>Description (optionnel)</li> </ol> <p>\u00c9tape 2 : Source de donn\u00e9es</p> <ol> <li>\"Add a data source\"</li> <li>Type : \"S3\"</li> <li>Chemin S3 : <code>s3://my-bucket/raw/</code></li> <li>Inclure les sous-dossiers (optionnel)</li> </ol> <p>\u00c9tape 3 : IAM Role</p> <ol> <li>Cr\u00e9er un nouveau r\u00f4le ou utiliser existant</li> <li>Nom : <code>AWSGlueServiceRole-default</code></li> <li>Permissions : Acc\u00e8s S3 et Glue</li> </ol> <p>\u00c9tape 4 : Sortie</p> <ol> <li>Base de donn\u00e9es : <code>data_analyst_db</code></li> <li>Pr\u00e9fixe des tables (optionnel)</li> </ol> <p>\u00c9tape 5 : Ex\u00e9cuter</p> <ol> <li>\"Run crawler now\" ou planifier</li> <li>Attendre la fin (quelques minutes)</li> <li>V\u00e9rifier les tables cr\u00e9\u00e9es</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#resultat-du-crawler","title":"R\u00e9sultat du Crawler","text":"<p>Table cr\u00e9\u00e9e automatiquement : - Colonnes d\u00e9tect\u00e9es - Types de donn\u00e9es inf\u00e9r\u00e9s - Emplacement S3 - Format de fichier</p> <p>Exemple de table cr\u00e9\u00e9e : <pre><code>Table: raw_data\nColumns:\n  - id (bigint)\n  - name (string)\n  - created_at (timestamp)\nLocation: s3://my-bucket/raw/\nFormat: csv\n</code></pre></p>"},{"location":"Cloud/AWS/FR/03-glue/#creer-un-job-etl","title":"Cr\u00e9er un job ETL","text":""},{"location":"Cloud/AWS/FR/03-glue/#types-de-jobs-glue","title":"Types de jobs Glue","text":"<ol> <li>Spark : Jobs PySpark (recommand\u00e9)</li> <li>Python shell : Scripts Python simples</li> <li>Ray : Traitement distribu\u00e9 avanc\u00e9</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#creer-un-job-spark","title":"Cr\u00e9er un job Spark","text":"<p>\u00c9tape 1 : Configuration</p> <ol> <li>Glue \u2192 \"ETL jobs\" \u2192 \"Add job\"</li> <li>Nom : <code>transform-csv-job</code></li> <li>IAM Role : <code>AWSGlueServiceRole-default</code></li> <li>Type : \"Spark\"</li> <li>Glue version : \"4.0\" (recommand\u00e9)</li> <li>DPU : 2 (minimum, ajustable)</li> </ol> <p>\u00c9tape 2 : Source de donn\u00e9es</p> <ol> <li>\"Data source\" : S\u00e9lectionner une table du Data Catalog</li> <li>Ou : Chemin S3 direct</li> </ol> <p>\u00c9tape 3 : Destination</p> <ol> <li>\"Data target\" : S3</li> <li>Format : Parquet (recommand\u00e9 pour analytics)</li> <li>Chemin : <code>s3://my-bucket/processed/</code></li> </ol> <p>\u00c9tape 4 : Script</p> <ol> <li>G\u00e9n\u00e9rer un script automatique</li> <li>Ou : \u00c9crire un script personnalis\u00e9</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#script-etl-de-base","title":"Script ETL de base","text":"<p>Script g\u00e9n\u00e9r\u00e9 automatiquement :</p> <pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Lire depuis le Data Catalog\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"raw_data\"\n)\n\n# Transformer (exemple : filtrer)\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"status\"] == \"active\"\n)\n\n# \u00c9crire vers S3\nglueContext.write_dynamic_frame.from_options(\n    frame = filtered,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://my-bucket/processed/\"},\n    format = \"parquet\"\n)\n\njob.commit()\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#transformation-de-donnees","title":"Transformation de donn\u00e9es","text":""},{"location":"Cloud/AWS/FR/03-glue/#transformations-courantes","title":"Transformations courantes","text":""},{"location":"Cloud/AWS/FR/03-glue/#1-filtrer-des-lignes","title":"1. Filtrer des lignes","text":"<pre><code>from awsglue.transforms import Filter\n\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"age\"] &gt; 18\n)\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#2-selectionner-des-colonnes","title":"2. S\u00e9lectionner des colonnes","text":"<pre><code>from awsglue.transforms import SelectFields\n\nselected = SelectFields.apply(\n    frame = datasource,\n    paths = [\"id\", \"name\", \"email\"]\n)\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#3-renommer-des-colonnes","title":"3. Renommer des colonnes","text":"<pre><code>from awsglue.transforms import RenameField\n\nrenamed = RenameField.apply(\n    frame = datasource,\n    old_name = \"old_column\",\n    new_name = \"new_column\"\n)\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#4-joindre-des-donnees","title":"4. Joindre des donn\u00e9es","text":"<pre><code>joined = Join.apply(\n    frame1 = datasource1,\n    frame2 = datasource2,\n    keys1 = [\"id\"],\n    keys2 = [\"user_id\"]\n)\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#5-agregations","title":"5. Agr\u00e9gations","text":"<pre><code># Convertir en DataFrame Spark pour agr\u00e9gations\ndf = datasource.toDF()\n\naggregated = df.groupBy(\"category\").agg({\n    \"amount\": \"sum\",\n    \"id\": \"count\"\n})\n\n# Reconvertir en DynamicFrame\nfrom awsglue.dynamicframe import DynamicFrame\nresult = DynamicFrame.fromDF(aggregated, glueContext, \"result\")\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#exemple-complet-transformation-csv-parquet","title":"Exemple complet : Transformation CSV \u2192 Parquet","text":"<pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# 1. Lire depuis S3 (via Data Catalog)\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"raw_data\"\n)\n\n# 2. Filtrer les donn\u00e9es\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"status\"] == \"active\"\n)\n\n# 3. S\u00e9lectionner colonnes\nselected = SelectFields.apply(\n    frame = filtered,\n    paths = [\"id\", \"name\", \"email\", \"created_at\"]\n)\n\n# 4. Convertir en DataFrame pour transformations avanc\u00e9es\ndf = selected.toDF()\n\n# 5. Ajouter une colonne calcul\u00e9e\nfrom pyspark.sql.functions import col, year\ndf = df.withColumn(\"year\", year(col(\"created_at\")))\n\n# 6. Reconvertir en DynamicFrame\nfrom awsglue.dynamicframe import DynamicFrame\nresult = DynamicFrame.fromDF(df, glueContext, \"result\")\n\n# 7. \u00c9crire vers S3 en Parquet (partitionn\u00e9 par ann\u00e9e)\nglueContext.write_dynamic_frame.from_options(\n    frame = result,\n    connection_type = \"s3\",\n    connection_options = {\n        \"path\": \"s3://my-bucket/processed/\",\n        \"partitionKeys\": [\"year\"]\n    },\n    format = \"parquet\"\n)\n\njob.commit()\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#orchestration-et-scheduling","title":"Orchestration et scheduling","text":""},{"location":"Cloud/AWS/FR/03-glue/#declencher-un-job-manuellement","title":"D\u00e9clencher un job manuellement","text":"<ol> <li>Glue \u2192 \"ETL jobs\"</li> <li>S\u00e9lectionner le job</li> <li>\"Run job\"</li> <li>Voir les logs en temps r\u00e9el</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#planifier-un-job-trigger","title":"Planifier un job (Trigger)","text":"<p>Cr\u00e9er un trigger :</p> <ol> <li>Glue \u2192 \"Triggers\" \u2192 \"Add trigger\"</li> <li>Nom : <code>daily-etl-trigger</code></li> <li>Type : \"Scheduled\"</li> <li>Fr\u00e9quence : \"Cron expression\"</li> <li>Exemple : <code>cron(0 2 * * ? *)</code> = Tous les jours \u00e0 2h</li> <li>Actions : S\u00e9lectionner le job \u00e0 ex\u00e9cuter</li> <li>\"Add\"</li> </ol> <p>Types de triggers : - On-demand : D\u00e9clenchement manuel - Scheduled : Planifi\u00e9 (cron) - Event-driven : D\u00e9clench\u00e9 par \u00e9v\u00e9nement (ex: nouveau fichier S3)</p>"},{"location":"Cloud/AWS/FR/03-glue/#workflows-orchestration-complexe","title":"Workflows (orchestration complexe)","text":"<p>Cr\u00e9er un workflow :</p> <ol> <li>Glue \u2192 \"Workflows\" \u2192 \"Add workflow\"</li> <li>Nom : <code>etl-pipeline-workflow</code></li> <li>Ajouter des \u00e9tapes :</li> <li>Crawler \u2192 Job ETL \u2192 Autre Job</li> <li>D\u00e9finir les d\u00e9pendances</li> <li>D\u00e9clencher le workflow</li> </ol> <p>Exemple de workflow : <pre><code>1. Crawler S3 \u2192 D\u00e9couvre nouveaux fichiers\n2. Job ETL 1 \u2192 Transforme les donn\u00e9es brutes\n3. Job ETL 2 \u2192 Agr\u00e8ge les donn\u00e9es\n4. Job ETL 3 \u2192 Charge dans Redshift\n</code></pre></p>"},{"location":"Cloud/AWS/FR/03-glue/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/AWS/FR/03-glue/#performance","title":"Performance","text":"<ol> <li>Utiliser Parquet au lieu de CSV (plus rapide)</li> <li>Partitionner les donn\u00e9es (am\u00e9liore les performances)</li> <li>Ajuster les DPU selon la taille des donn\u00e9es</li> <li>Utiliser le cache Spark pour r\u00e9utiliser des donn\u00e9es</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#couts","title":"Co\u00fbts","text":"<ol> <li>Surveiller les DPU-heures utilis\u00e9es</li> <li>Optimiser les scripts pour r\u00e9duire le temps d'ex\u00e9cution</li> <li>Utiliser les bonnes classes S3 (Standard-IA pour archives)</li> <li>Arr\u00eater les jobs qui \u00e9chouent rapidement</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#organisation","title":"Organisation","text":"<ol> <li>Nommer les jobs de mani\u00e8re coh\u00e9rente</li> <li>Documenter les transformations</li> <li>Versionner les scripts (Git)</li> <li>Tester localement avant de d\u00e9ployer</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/AWS/FR/03-glue/#exemple-1-transformer-csv-parquet","title":"Exemple 1 : Transformer CSV \u2192 Parquet","text":"<pre><code># Lire CSV depuis S3\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"raw_csv_data\"\n)\n\n# \u00c9crire en Parquet\nglueContext.write_dynamic_frame.from_options(\n    frame = datasource,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://my-bucket/parquet/\"},\n    format = \"parquet\"\n)\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#exemple-2-nettoyer-et-valider","title":"Exemple 2 : Nettoyer et valider","text":"<pre><code># Filtrer les lignes invalides\ncleaned = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"email\"] is not None and \"@\" in x[\"email\"]\n)\n\n# Supprimer les doublons (via DataFrame)\ndf = cleaned.toDF()\ndf = df.dropDuplicates([\"id\"])\n\nresult = DynamicFrame.fromDF(df, glueContext, \"result\")\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#exemple-3-joindre-plusieurs-sources","title":"Exemple 3 : Joindre plusieurs sources","text":"<pre><code># Lire deux tables\nusers = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"users\"\n)\n\norders = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"orders\"\n)\n\n# Joindre\njoined = Join.apply(\n    frame1 = users,\n    frame2 = orders,\n    keys1 = [\"id\"],\n    keys2 = [\"user_id\"]\n)\n</code></pre>"},{"location":"Cloud/AWS/FR/03-glue/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Glue = ETL serverless g\u00e9r\u00e9 par AWS</li> <li>Crawlers d\u00e9couvrent automatiquement les sch\u00e9mas</li> <li>Jobs ETL utilisent PySpark pour transformations</li> <li>Data Catalog centralise les m\u00e9tadonn\u00e9es</li> <li>Triggers permettent l'automatisation</li> </ol>"},{"location":"Cloud/AWS/FR/03-glue/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 4. Amazon Redshift - Data Warehouse pour apprendre \u00e0 utiliser Redshift pour l'analyse de donn\u00e9es.</p>"},{"location":"Cloud/AWS/FR/04-redshift/","title":"4. Amazon Redshift - Data Warehouse","text":""},{"location":"Cloud/AWS/FR/04-redshift/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Amazon Redshift et son r\u00f4le</li> <li>Cr\u00e9er un cluster Redshift (gratuit 2 mois)</li> <li>Charger des donn\u00e9es dans Redshift</li> <li>Optimiser les requ\u00eates Redshift</li> <li>Int\u00e9grer avec S3 et autres services</li> </ul>"},{"location":"Cloud/AWS/FR/04-redshift/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Redshift</li> <li>Cr\u00e9er un cluster Redshift</li> <li>Charger des donn\u00e9es</li> <li>Requ\u00eates SQL avanc\u00e9es</li> <li>Optimisation</li> <li>Int\u00e9gration avec autres services</li> </ol>"},{"location":"Cloud/AWS/FR/04-redshift/#introduction-a-redshift","title":"Introduction \u00e0 Redshift","text":""},{"location":"Cloud/AWS/FR/04-redshift/#quest-ce-quamazon-redshift","title":"Qu'est-ce qu'Amazon Redshift ?","text":"<p>Amazon Redshift = Data warehouse cloud g\u00e9r\u00e9</p> <ul> <li>OLAP : Optimis\u00e9 pour l'analyse (pas transactions)</li> <li>Colonnes : Stockage orient\u00e9 colonnes</li> <li>Massivement parall\u00e8le : Traitement distribu\u00e9</li> <li>Scalable : De quelques Go \u00e0 plusieurs Po</li> </ul>"},{"location":"Cloud/AWS/FR/04-redshift/#cas-dusage-pour-data-analyst","title":"Cas d'usage pour Data Analyst","text":"<ul> <li>Data Warehouse : Centraliser les donn\u00e9es</li> <li>Analytics : Requ\u00eates complexes sur grandes volumes</li> <li>Business Intelligence : Dashboards et rapports</li> <li>Data Mining : Analyses approfondies</li> </ul>"},{"location":"Cloud/AWS/FR/04-redshift/#free-tier-redshift","title":"Free Tier Redshift","text":"<p>Gratuit 2 mois : - 750 heures/mois de cluster <code>dc2.large</code> - 32 Go de stockage par n\u0153ud - Apr\u00e8s 2 mois : facturation normale</p> <p>\u26a0\ufe0f Important : Arr\u00eater le cluster quand non utilis\u00e9 pour \u00e9viter les co\u00fbts.</p>"},{"location":"Cloud/AWS/FR/04-redshift/#creer-un-cluster-redshift","title":"Cr\u00e9er un cluster Redshift","text":""},{"location":"Cloud/AWS/FR/04-redshift/#etape-1-acceder-a-redshift","title":"\u00c9tape 1 : Acc\u00e9der \u00e0 Redshift","text":"<ol> <li>Console AWS \u2192 Rechercher \"Redshift\"</li> <li>Cliquer sur \"Amazon Redshift\"</li> <li>\"Create cluster\"</li> </ol>"},{"location":"Cloud/AWS/FR/04-redshift/#etape-2-configuration-du-cluster","title":"\u00c9tape 2 : Configuration du cluster","text":"<p>Configuration de base :</p> <ol> <li>Cluster identifier : <code>data-analyst-cluster</code></li> <li>Node type : <code>dc2.large</code> (gratuit 2 mois)</li> <li>Number of nodes : 1 (suffisant pour d\u00e9buter)</li> <li>Database name : <code>analytics</code> (par d\u00e9faut : <code>dev</code>)</li> <li>Database port : 5439 (par d\u00e9faut)</li> <li>Master username : <code>admin</code> (ou autre)</li> <li>Master password : Mot de passe fort</li> </ol> <p>Configuration r\u00e9seau :</p> <ol> <li>VPC : Choisir un VPC existant</li> <li>Subnet group : Cr\u00e9er ou utiliser existant</li> <li>Publicly accessible : \u2705 Oui (pour acc\u00e8s facile)</li> <li>Availability zone : Choisir une zone</li> </ol> <p>S\u00e9curit\u00e9 :</p> <ol> <li>VPC security groups : Cr\u00e9er un groupe de s\u00e9curit\u00e9</li> <li>Autoriser le port 5439 depuis votre IP</li> <li>Encryption : Activer (recommand\u00e9)</li> </ol>"},{"location":"Cloud/AWS/FR/04-redshift/#etape-3-creer-le-cluster","title":"\u00c9tape 3 : Cr\u00e9er le cluster","text":"<ol> <li>Cliquer sur \"Create cluster\"</li> <li>Attendre 5-10 minutes (cr\u00e9ation)</li> <li>Cluster pr\u00eat quand status = \"Available\"</li> </ol> <p>\u26a0\ufe0f Important : Noter l'endpoint du cluster (ex: <code>data-analyst-cluster.xxxxx.eu-west-3.redshift.amazonaws.com:5439</code>)</p>"},{"location":"Cloud/AWS/FR/04-redshift/#charger-des-donnees","title":"Charger des donn\u00e9es","text":""},{"location":"Cloud/AWS/FR/04-redshift/#methode-1-copy-depuis-s3-recommande","title":"M\u00e9thode 1 : COPY depuis S3 (recommand\u00e9)","text":"<p>Le plus rapide pour grandes quantit\u00e9s :</p> <pre><code>-- Cr\u00e9er une table\nCREATE TABLE users (\n    id INTEGER,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at TIMESTAMP\n);\n\n-- Charger depuis S3\nCOPY users\nFROM 's3://my-bucket/data/users.csv'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV\nIGNOREHEADER 1;\n</code></pre> <p>Configuration IAM Role :</p> <ol> <li>IAM \u2192 \"Roles\" \u2192 \"Create role\"</li> <li>Type : \"Redshift\"</li> <li>Attacher politique : <code>AmazonS3ReadOnlyAccess</code></li> <li>Nom : <code>RedshiftS3Role</code></li> <li>Copier l'ARN pour COPY</li> </ol>"},{"location":"Cloud/AWS/FR/04-redshift/#methode-2-insert-petites-quantites","title":"M\u00e9thode 2 : INSERT (petites quantit\u00e9s)","text":"<pre><code>INSERT INTO users (id, name, email, created_at)\nVALUES (1, 'John Doe', 'john@example.com', '2024-01-01');\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#methode-3-insert-depuis-requete","title":"M\u00e9thode 3 : INSERT depuis requ\u00eate","text":"<pre><code>INSERT INTO users_aggregated\nSELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count\nFROM users\nGROUP BY DATE_TRUNC('month', created_at);\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#formats-supportes","title":"Formats support\u00e9s","text":"<ul> <li>CSV : Fichiers CSV</li> <li>JSON : Fichiers JSON</li> <li>Parquet : Format optimis\u00e9 (recommand\u00e9)</li> <li>Avro : Format Avro</li> </ul>"},{"location":"Cloud/AWS/FR/04-redshift/#requetes-sql-avancees","title":"Requ\u00eates SQL avanc\u00e9es","text":""},{"location":"Cloud/AWS/FR/04-redshift/#fonctions-analytiques","title":"Fonctions analytiques","text":"<p>Window functions :</p> <pre><code>-- ROW_NUMBER\nSELECT \n    id,\n    name,\n    ROW_NUMBER() OVER (PARTITION BY category ORDER BY created_at) AS rank\nFROM products;\n\n-- LAG/LEAD\nSELECT \n    date,\n    sales,\n    LAG(sales, 1) OVER (ORDER BY date) AS previous_sales,\n    LEAD(sales, 1) OVER (ORDER BY date) AS next_sales\nFROM daily_sales;\n\n-- RANK\nSELECT \n    user_id,\n    total_spent,\n    RANK() OVER (ORDER BY total_spent DESC) AS spending_rank\nFROM user_totals;\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#agregations-complexes","title":"Agr\u00e9gations complexes","text":"<pre><code>-- GROUP BY avec ROLLUP\nSELECT \n    category,\n    region,\n    SUM(amount) AS total\nFROM sales\nGROUP BY ROLLUP(category, region);\n\n-- GROUP BY avec CUBE\nSELECT \n    category,\n    region,\n    SUM(amount) AS total\nFROM sales\nGROUP BY CUBE(category, region);\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#jointures-optimisees","title":"Jointures optimis\u00e9es","text":"<pre><code>-- Jointure avec distribution key\nSELECT \n    u.name,\n    o.amount,\n    o.created_at\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01';\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#optimisation","title":"Optimisation","text":""},{"location":"Cloud/AWS/FR/04-redshift/#distribution-keys","title":"Distribution keys","text":"<p>Choisir la bonne distribution key :</p> <pre><code>-- Distribution par cl\u00e9 (pour jointures)\nCREATE TABLE users (\n    id INTEGER DISTKEY,\n    name VARCHAR(100),\n    email VARCHAR(100)\n);\n\n-- Distribution ALL (pour petites tables)\nCREATE TABLE categories (\n    id INTEGER,\n    name VARCHAR(100)\n) DISTSTYLE ALL;\n\n-- Distribution EVEN (par d\u00e9faut)\nCREATE TABLE logs (\n    id INTEGER,\n    message TEXT\n) DISTSTYLE EVEN;\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#sort-keys","title":"Sort keys","text":"<p>Am\u00e9liorer les performances de requ\u00eates :</p> <pre><code>-- Sort key simple\nCREATE TABLE orders (\n    id INTEGER,\n    user_id INTEGER,\n    created_at TIMESTAMP,\n    amount DECIMAL(10,2)\n) SORTKEY (created_at);\n\n-- Sort key composite\nCREATE TABLE sales (\n    date DATE,\n    region VARCHAR(50),\n    amount DECIMAL(10,2)\n) SORTKEY (date, region);\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#compression","title":"Compression","text":"<p>R\u00e9duire l'espace de stockage :</p> <pre><code>-- Compression automatique\nCREATE TABLE users (\n    id INTEGER,\n    name VARCHAR(100) ENCODE lzo,\n    email VARCHAR(100) ENCODE lzo,\n    created_at TIMESTAMP ENCODE delta\n);\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#analyze","title":"ANALYZE","text":"<p>Mettre \u00e0 jour les statistiques :</p> <pre><code>-- Analyser une table\nANALYZE users;\n\n-- Analyser toutes les tables\nANALYZE;\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#integration-avec-autres-services","title":"Int\u00e9gration avec autres services","text":""},{"location":"Cloud/AWS/FR/04-redshift/#redshift-s3","title":"Redshift + S3","text":"<p>Unload vers S3 :</p> <pre><code>UNLOAD ('SELECT * FROM users WHERE created_at &gt; ''2024-01-01''')\nTO 's3://my-bucket/exports/users/'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV\nPARALLEL OFF;\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#redshift-glue","title":"Redshift + Glue","text":"<p>Glue peut charger dans Redshift :</p> <pre><code># Dans un job Glue\nglueContext.write_dynamic_frame.from_jdbc_conf(\n    frame = transformed_data,\n    catalog_connection = \"redshift-connection\",\n    connection_options = {\n        \"dbtable\": \"users\",\n        \"database\": \"analytics\"\n    }\n)\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#redshift-quicksight","title":"Redshift + QuickSight","text":"<p>Connecter QuickSight \u00e0 Redshift :</p> <ol> <li>QuickSight \u2192 \"Data sources\"</li> <li>\"Redshift\"</li> <li>Entrer les informations de connexion</li> <li>S\u00e9lectionner les tables</li> <li>Cr\u00e9er des visualisations</li> </ol>"},{"location":"Cloud/AWS/FR/04-redshift/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/AWS/FR/04-redshift/#performance","title":"Performance","text":"<ol> <li>Utiliser COPY au lieu de INSERT pour grandes quantit\u00e9s</li> <li>Choisir les bonnes distribution keys</li> <li>Utiliser des sort keys pour requ\u00eates fr\u00e9quentes</li> <li>Compresser les colonnes pour \u00e9conomiser l'espace</li> <li>VACUUM r\u00e9guli\u00e8rement pour optimiser</li> </ol>"},{"location":"Cloud/AWS/FR/04-redshift/#couts","title":"Co\u00fbts","text":"<ol> <li>Arr\u00eater le cluster quand non utilis\u00e9</li> <li>Utiliser le bon type de n\u0153ud selon les besoins</li> <li>Surveiller l'utilisation du stockage</li> <li>Nettoyer les donn\u00e9es inutiles</li> </ol>"},{"location":"Cloud/AWS/FR/04-redshift/#securite","title":"S\u00e9curit\u00e9","text":"<ol> <li>Chiffrer les donn\u00e9es en transit et au repos</li> <li>Utiliser VPC pour isoler le cluster</li> <li>Limiter l'acc\u00e8s avec security groups</li> <li>Auditer les acc\u00e8s avec CloudTrail</li> </ol>"},{"location":"Cloud/AWS/FR/04-redshift/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/AWS/FR/04-redshift/#exemple-1-pipeline-complet-s3-redshift","title":"Exemple 1 : Pipeline complet S3 \u2192 Redshift","text":"<pre><code>-- 1. Cr\u00e9er la table\nCREATE TABLE sales (\n    id INTEGER,\n    product_id INTEGER,\n    amount DECIMAL(10,2),\n    sale_date DATE\n) DISTKEY(product_id) SORTKEY(sale_date);\n\n-- 2. Charger depuis S3\nCOPY sales\nFROM 's3://my-bucket/data/sales/'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV\nIGNOREHEADER 1;\n\n-- 3. Analyser\nANALYZE sales;\n\n-- 4. Requ\u00eates analytiques\nSELECT \n    DATE_TRUNC('month', sale_date) AS month,\n    SUM(amount) AS total_sales\nFROM sales\nGROUP BY DATE_TRUNC('month', sale_date)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#exemple-2-agregations-avec-window-functions","title":"Exemple 2 : Agr\u00e9gations avec window functions","text":"<pre><code>-- Top 10 produits par mois\nSELECT \n    product_id,\n    month,\n    total_sales,\n    RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) AS rank\nFROM (\n    SELECT \n        product_id,\n        DATE_TRUNC('month', sale_date) AS month,\n        SUM(amount) AS total_sales\n    FROM sales\n    GROUP BY product_id, DATE_TRUNC('month', sale_date)\n) monthly_sales\nWHERE RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) &lt;= 10;\n</code></pre>"},{"location":"Cloud/AWS/FR/04-redshift/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Redshift = Data warehouse pour analytics</li> <li>Free Tier : 2 mois gratuit (750 heures)</li> <li>COPY depuis S3 = m\u00e9thode la plus rapide</li> <li>Distribution et sort keys = cl\u00e9s de performance</li> <li>Arr\u00eater le cluster quand non utilis\u00e9</li> </ol>"},{"location":"Cloud/AWS/FR/04-redshift/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 5. Amazon Athena - Requ\u00eates SQL sur S3 pour apprendre \u00e0 interroger directement les fichiers S3.</p>"},{"location":"Cloud/AWS/FR/05-athena/","title":"5. Amazon Athena - Requ\u00eates SQL sur S3","text":""},{"location":"Cloud/AWS/FR/05-athena/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Amazon Athena et son utilisation</li> <li>Cr\u00e9er des tables externes pointant vers S3</li> <li>Ex\u00e9cuter des requ\u00eates SQL sur fichiers S3</li> <li>Optimiser les co\u00fbts et performances</li> <li>Int\u00e9grer avec Glue Data Catalog</li> </ul>"},{"location":"Cloud/AWS/FR/05-athena/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Athena</li> <li>Cr\u00e9er des tables externes</li> <li>Ex\u00e9cuter des requ\u00eates</li> <li>Optimisation des co\u00fbts</li> <li>Int\u00e9gration avec Glue</li> <li>Bonnes pratiques</li> </ol>"},{"location":"Cloud/AWS/FR/05-athena/#introduction-a-athena","title":"Introduction \u00e0 Athena","text":""},{"location":"Cloud/AWS/FR/05-athena/#quest-ce-quamazon-athena","title":"Qu'est-ce qu'Amazon Athena ?","text":"<p>Amazon Athena = Service de requ\u00eates SQL serverless sur S3</p> <ul> <li>Serverless : Pas d'infrastructure \u00e0 g\u00e9rer</li> <li>Pay-per-query : Payez seulement ce que vous utilisez</li> <li>Standard SQL : Syntaxe SQL standard</li> <li>S3 directement : Pas besoin de charger dans base de donn\u00e9es</li> </ul>"},{"location":"Cloud/AWS/FR/05-athena/#cas-dusage-pour-data-analyst","title":"Cas d'usage pour Data Analyst","text":"<ul> <li>Exploration de donn\u00e9es : Analyser rapidement des fichiers S3</li> <li>Data Lake queries : Requ\u00eates sur data lake</li> <li>Ad-hoc analysis : Analyses ponctuelles</li> <li>Log analysis : Analyser des logs stock\u00e9s dans S3</li> </ul>"},{"location":"Cloud/AWS/FR/05-athena/#free-tier-athena","title":"Free Tier Athena","text":"<p>Gratuit \u00e0 vie : - 10 Go de donn\u00e9es scann\u00e9es/mois - Au-del\u00e0 : 5$ par T\u00e9raoctet scann\u00e9</p> <p>\u26a0\ufe0f Important : Les co\u00fbts d\u00e9pendent de la quantit\u00e9 de donn\u00e9es scann\u00e9es. Optimiser les requ\u00eates pour r\u00e9duire les co\u00fbts.</p>"},{"location":"Cloud/AWS/FR/05-athena/#creer-des-tables-externes","title":"Cr\u00e9er des tables externes","text":""},{"location":"Cloud/AWS/FR/05-athena/#methode-1-via-lediteur-athena","title":"M\u00e9thode 1 : Via l'\u00e9diteur Athena","text":"<p>\u00c9tape 1 : Acc\u00e9der \u00e0 Athena</p> <ol> <li>Console AWS \u2192 Rechercher \"Athena\"</li> <li>Cliquer sur \"Amazon Athena\"</li> <li>Premi\u00e8re utilisation : Configurer le r\u00e9sultat S3</li> </ol> <p>\u00c9tape 2 : Configurer le r\u00e9sultat</p> <ol> <li>\"Settings\" \u2192 \"Manage\"</li> <li>\"Query result location\" : <code>s3://my-bucket/athena-results/</code></li> <li>\"Save\"</li> </ol> <p>\u00c9tape 3 : Cr\u00e9er une table</p> <pre><code>-- Table pour fichiers CSV\nCREATE EXTERNAL TABLE users (\n    id INT,\n    name STRING,\n    email STRING,\n    created_at TIMESTAMP\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nWITH SERDEPROPERTIES (\n    'serialization.format' = ',',\n    'field.delim' = ','\n)\nSTORED AS TEXTFILE\nLOCATION 's3://my-bucket/data/users/'\nTBLPROPERTIES ('skip.header.line.count'='1');\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#methode-2-via-glue-data-catalog-recommande","title":"M\u00e9thode 2 : Via Glue Data Catalog (recommand\u00e9)","text":"<p>Utiliser les tables cr\u00e9\u00e9es par Glue :</p> <ol> <li>Glue \u2192 Cr\u00e9er un crawler pour S3</li> <li>Crawler cr\u00e9e automatiquement la table</li> <li>Athena utilise directement cette table</li> </ol> <p>Avantages : - Sch\u00e9ma d\u00e9tect\u00e9 automatiquement - Pas besoin de d\u00e9finir manuellement - R\u00e9utilisable par d'autres services</p>"},{"location":"Cloud/AWS/FR/05-athena/#formats-supportes","title":"Formats support\u00e9s","text":"<p>CSV : <pre><code>CREATE EXTERNAL TABLE csv_data (\n    col1 STRING,\n    col2 INT\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/csv/';\n</code></pre></p> <p>JSON : <pre><code>CREATE EXTERNAL TABLE json_data (\n    id INT,\n    name STRING\n)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/json/';\n</code></pre></p> <p>Parquet (recommand\u00e9) : <pre><code>CREATE EXTERNAL TABLE parquet_data (\n    id INT,\n    name STRING,\n    created_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket/parquet/';\n</code></pre></p>"},{"location":"Cloud/AWS/FR/05-athena/#executer-des-requetes","title":"Ex\u00e9cuter des requ\u00eates","text":""},{"location":"Cloud/AWS/FR/05-athena/#requetes-de-base","title":"Requ\u00eates de base","text":"<p>SELECT simple :</p> <pre><code>SELECT * FROM users LIMIT 10;\n</code></pre> <p>Filtrer :</p> <pre><code>SELECT \n    id,\n    name,\n    email\nFROM users\nWHERE created_at &gt; DATE '2024-01-01'\nORDER BY created_at DESC;\n</code></pre> <p>Agr\u00e9gations :</p> <pre><code>SELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count,\n    COUNT(DISTINCT email) AS unique_emails\nFROM users\nGROUP BY DATE_TRUNC('month', created_at)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#requetes-avancees","title":"Requ\u00eates avanc\u00e9es","text":"<p>Window functions :</p> <pre><code>SELECT \n    id,\n    name,\n    created_at,\n    ROW_NUMBER() OVER (PARTITION BY DATE_TRUNC('month', created_at) ORDER BY created_at) AS rank\nFROM users;\n</code></pre> <p>Jointures :</p> <pre><code>SELECT \n    u.name,\n    o.amount,\n    o.created_at\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.created_at &gt; DATE '2024-01-01';\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#requetes-sur-partitions","title":"Requ\u00eates sur partitions","text":"<p>Si donn\u00e9es partitionn\u00e9es :</p> <pre><code>-- Table partitionn\u00e9e par date\nCREATE EXTERNAL TABLE sales (\n    id INT,\n    product_id INT,\n    amount DECIMAL(10,2)\n)\nPARTITIONED BY (sale_date DATE)\nSTORED AS PARQUET\nLOCATION 's3://bucket/sales/';\n\n-- Ajouter des partitions\nALTER TABLE sales ADD PARTITION (sale_date='2024-01-01')\nLOCATION 's3://bucket/sales/year=2024/month=01/day=01/';\n\n-- Requ\u00eate avec partition (plus rapide et moins cher)\nSELECT * FROM sales\nWHERE sale_date = DATE '2024-01-01';\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#optimisation-des-couts","title":"Optimisation des co\u00fbts","text":""},{"location":"Cloud/AWS/FR/05-athena/#reduire-les-donnees-scannees","title":"R\u00e9duire les donn\u00e9es scann\u00e9es","text":"<p>1. Utiliser WHERE pour filtrer t\u00f4t :</p> <pre><code>-- \u274c Mauvais : Scanne tout puis filtre\nSELECT * FROM large_table\nWHERE date = '2024-01-01';\n\n-- \u2705 Bon : Filtre d\u00e8s le d\u00e9but (si partitionn\u00e9)\nSELECT * FROM large_table\nWHERE date = '2024-01-01';\n</code></pre> <p>2. S\u00e9lectionner uniquement les colonnes n\u00e9cessaires :</p> <pre><code>-- \u274c Mauvais : Scanne toutes les colonnes\nSELECT * FROM large_table;\n\n-- \u2705 Bon : Scanne seulement les colonnes n\u00e9cessaires\nSELECT id, name FROM large_table;\n</code></pre> <p>3. Utiliser LIMIT :</p> <pre><code>-- Limiter le nombre de r\u00e9sultats\nSELECT * FROM large_table LIMIT 100;\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#utiliser-parquet","title":"Utiliser Parquet","text":"<p>Parquet est plus efficace que CSV :</p> <ul> <li>Compression : Moins de donn\u00e9es scann\u00e9es</li> <li>Colonnes : Scanne seulement les colonnes n\u00e9cessaires</li> <li>Co\u00fbt r\u00e9duit : Jusqu'\u00e0 90% de r\u00e9duction</li> </ul> <p>Convertir CSV \u2192 Parquet avec Glue :</p> <pre><code># Job Glue pour convertir\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"csv_data\"\n)\n\nglueContext.write_dynamic_frame.from_options(\n    frame = datasource,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bucket/parquet/\"},\n    format = \"parquet\"\n)\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#partitionner-les-donnees","title":"Partitionner les donn\u00e9es","text":"<p>Partitionner par date (recommand\u00e9) :</p> <pre><code>s3://bucket/data/\n\u251c\u2500\u2500 year=2024/\n\u2502   \u251c\u2500\u2500 month=01/\n\u2502   \u2502   \u251c\u2500\u2500 day=01/\n\u2502   \u2502   \u2514\u2500\u2500 day=02/\n\u2502   \u2514\u2500\u2500 month=02/\n</code></pre> <p>Cr\u00e9er table partitionn\u00e9e :</p> <pre><code>CREATE EXTERNAL TABLE partitioned_data (\n    id INT,\n    name STRING\n)\nPARTITIONED BY (year INT, month INT, day INT)\nSTORED AS PARQUET\nLOCATION 's3://bucket/data/';\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#integration-avec-glue","title":"Int\u00e9gration avec Glue","text":""},{"location":"Cloud/AWS/FR/05-athena/#utiliser-les-tables-glue","title":"Utiliser les tables Glue","text":"<p>Tables cr\u00e9\u00e9es par Glue sont automatiquement disponibles dans Athena :</p> <ol> <li>Glue \u2192 Crawler cr\u00e9e une table</li> <li>Athena \u2192 \"Tables\" \u2192 Voir toutes les tables Glue</li> <li>Utiliser directement dans les requ\u00eates</li> </ol> <p>Avantages : - Sch\u00e9ma automatique - Pas de d\u00e9finition manuelle - Synchronisation automatique</p>"},{"location":"Cloud/AWS/FR/05-athena/#mettre-a-jour-les-partitions","title":"Mettre \u00e0 jour les partitions","text":"<p>Si nouvelles donn\u00e9es ajout\u00e9es :</p> <pre><code>-- Mettre \u00e0 jour les partitions\nMSCK REPAIR TABLE sales;\n\n-- Ou ajouter manuellement\nALTER TABLE sales ADD PARTITION (sale_date='2024-01-02')\nLOCATION 's3://bucket/sales/year=2024/month=01/day=02/';\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/AWS/FR/05-athena/#performance","title":"Performance","text":"<ol> <li>Utiliser Parquet au lieu de CSV</li> <li>Partitionner les donn\u00e9es par date/cat\u00e9gorie</li> <li>S\u00e9lectionner uniquement les colonnes n\u00e9cessaires</li> <li>Filtrer t\u00f4t avec WHERE</li> <li>Utiliser LIMIT pour exploration</li> </ol>"},{"location":"Cloud/AWS/FR/05-athena/#couts","title":"Co\u00fbts","text":"<ol> <li>Surveiller les donn\u00e9es scann\u00e9es dans les r\u00e9sultats</li> <li>Optimiser les requ\u00eates pour r\u00e9duire le scan</li> <li>Utiliser Parquet pour compression</li> <li>Partitionner pour r\u00e9duire le scan</li> <li>Mettre en cache les r\u00e9sultats fr\u00e9quents</li> </ol>"},{"location":"Cloud/AWS/FR/05-athena/#organisation","title":"Organisation","text":"<ol> <li>Organiser S3 avec pr\u00e9fixes coh\u00e9rents</li> <li>Nommer les tables de mani\u00e8re claire</li> <li>Documenter les sch\u00e9mas</li> <li>Utiliser des bases de donn\u00e9es pour organiser</li> </ol>"},{"location":"Cloud/AWS/FR/05-athena/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/AWS/FR/05-athena/#exemple-1-analyser-des-logs","title":"Exemple 1 : Analyser des logs","text":"<pre><code>-- Table pour logs\nCREATE EXTERNAL TABLE logs (\n    timestamp TIMESTAMP,\n    level STRING,\n    message STRING,\n    user_id INT\n)\nPARTITIONED BY (date DATE)\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/logs/';\n\n-- Requ\u00eate : Erreurs par jour\nSELECT \n    date,\n    COUNT(*) AS error_count\nFROM logs\nWHERE level = 'ERROR'\nGROUP BY date\nORDER BY date DESC;\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#exemple-2-analyser-des-donnees-csv","title":"Exemple 2 : Analyser des donn\u00e9es CSV","text":"<pre><code>-- Table CSV\nCREATE EXTERNAL TABLE sales_csv (\n    id INT,\n    product_id INT,\n    amount DECIMAL(10,2),\n    sale_date DATE\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/sales/csv/'\nTBLPROPERTIES ('skip.header.line.count'='1');\n\n-- Analyse : Ventes par mois\nSELECT \n    DATE_TRUNC('month', sale_date) AS month,\n    SUM(amount) AS total_sales,\n    COUNT(*) AS transaction_count\nFROM sales_csv\nGROUP BY DATE_TRUNC('month', sale_date)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#exemple-3-jointure-de-plusieurs-tables","title":"Exemple 3 : Jointure de plusieurs tables","text":"<pre><code>-- Analyser avec jointures\nSELECT \n    p.name AS product_name,\n    c.name AS category_name,\n    SUM(s.amount) AS total_sales\nFROM sales s\nJOIN products p ON s.product_id = p.id\nJOIN categories c ON p.category_id = c.id\nWHERE s.sale_date &gt;= DATE '2024-01-01'\nGROUP BY p.name, c.name\nORDER BY total_sales DESC\nLIMIT 10;\n</code></pre>"},{"location":"Cloud/AWS/FR/05-athena/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Athena = SQL serverless sur fichiers S3</li> <li>Free Tier : 10 Go/mois de donn\u00e9es scann\u00e9es</li> <li>Parquet = format le plus efficace</li> <li>Partitionner = r\u00e9duire les co\u00fbts</li> <li>Int\u00e9gration Glue = sch\u00e9mas automatiques</li> </ol>"},{"location":"Cloud/AWS/FR/05-athena/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 6. AWS Lambda - Serverless Computing pour apprendre \u00e0 automatiser le traitement de donn\u00e9es.</p>"},{"location":"Cloud/AWS/FR/06-lambda/","title":"6. AWS Lambda - Serverless Computing","text":""},{"location":"Cloud/AWS/FR/06-lambda/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre AWS Lambda et son utilisation</li> <li>Cr\u00e9er des fonctions Lambda</li> <li>Traiter des donn\u00e9es avec Lambda</li> <li>D\u00e9clencher Lambda depuis S3</li> <li>Int\u00e9grer Lambda avec d'autres services</li> </ul>"},{"location":"Cloud/AWS/FR/06-lambda/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Lambda</li> <li>Cr\u00e9er une fonction Lambda</li> <li>Traitement de donn\u00e9es</li> <li>D\u00e9clencheurs (Triggers)</li> <li>Int\u00e9gration avec autres services</li> <li>Bonnes pratiques</li> </ol>"},{"location":"Cloud/AWS/FR/06-lambda/#introduction-a-lambda","title":"Introduction \u00e0 Lambda","text":""},{"location":"Cloud/AWS/FR/06-lambda/#quest-ce-quaws-lambda","title":"Qu'est-ce qu'AWS Lambda ?","text":"<p>AWS Lambda = Service de calcul serverless</p> <ul> <li>Serverless : Pas de serveurs \u00e0 g\u00e9rer</li> <li>Event-driven : D\u00e9clench\u00e9 par \u00e9v\u00e9nements</li> <li>Auto-scaling : S'adapte automatiquement</li> <li>Pay-per-use : Payez seulement l'ex\u00e9cution</li> </ul>"},{"location":"Cloud/AWS/FR/06-lambda/#cas-dusage-pour-data-analyst","title":"Cas d'usage pour Data Analyst","text":"<ul> <li>Traitement de fichiers : Transformer fichiers upload\u00e9s</li> <li>ETL automatis\u00e9 : D\u00e9clencher des jobs Glue</li> <li>Validation de donn\u00e9es : V\u00e9rifier les donn\u00e9es</li> <li>Notifications : Alerter sur \u00e9v\u00e9nements</li> <li>Orchestration : Coordonner plusieurs services</li> </ul>"},{"location":"Cloud/AWS/FR/06-lambda/#free-tier-lambda","title":"Free Tier Lambda","text":"<p>Gratuit \u00e0 vie : - 1 million de requ\u00eates/mois - 400 000 Go-secondes de temps de calcul/mois - Au-del\u00e0 : facturation \u00e0 l'usage</p> <p>\u26a0\ufe0f Important : Tr\u00e8s g\u00e9n\u00e9reux pour la plupart des cas d'usage.</p>"},{"location":"Cloud/AWS/FR/06-lambda/#creer-une-fonction-lambda","title":"Cr\u00e9er une fonction Lambda","text":""},{"location":"Cloud/AWS/FR/06-lambda/#etape-1-acceder-a-lambda","title":"\u00c9tape 1 : Acc\u00e9der \u00e0 Lambda","text":"<ol> <li>Console AWS \u2192 Rechercher \"Lambda\"</li> <li>Cliquer sur \"AWS Lambda\"</li> <li>\"Create function\"</li> </ol>"},{"location":"Cloud/AWS/FR/06-lambda/#etape-2-configuration-de-base","title":"\u00c9tape 2 : Configuration de base","text":"<p>Options :</p> <ol> <li>Author from scratch : Cr\u00e9er depuis z\u00e9ro</li> <li>Use a blueprint : Utiliser un template</li> <li>Browse serverless app repository : Applications pr\u00e9-construites</li> </ol> <p>Configuration :</p> <ol> <li>Function name : <code>process-data-file</code></li> <li>Runtime : Python 3.11 (ou autre)</li> <li>Architecture : x86_64 (par d\u00e9faut)</li> <li>Permissions : Cr\u00e9er un nouveau r\u00f4le avec permissions de base</li> </ol>"},{"location":"Cloud/AWS/FR/06-lambda/#etape-3-code-de-la-fonction","title":"\u00c9tape 3 : Code de la fonction","text":"<p>Exemple simple :</p> <pre><code>import json\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Fonction Lambda de base\n    \"\"\"\n    # Traitement\n    result = {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n\n    return result\n</code></pre> <p>Tester la fonction :</p> <ol> <li>Cliquer sur \"Test\"</li> <li>Cr\u00e9er un \u00e9v\u00e9nement de test</li> <li>Ex\u00e9cuter</li> <li>Voir les r\u00e9sultats</li> </ol>"},{"location":"Cloud/AWS/FR/06-lambda/#traitement-de-donnees","title":"Traitement de donn\u00e9es","text":""},{"location":"Cloud/AWS/FR/06-lambda/#exemple-1-traiter-un-fichier-csv","title":"Exemple 1 : Traiter un fichier CSV","text":"<pre><code>import json\nimport csv\nimport boto3\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # R\u00e9cup\u00e9rer le bucket et la cl\u00e9 depuis l'\u00e9v\u00e9nement\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # T\u00e9l\u00e9charger le fichier\n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response['Body'].read().decode('utf-8')\n\n    # Parser le CSV\n    reader = csv.DictReader(content.splitlines())\n    rows = list(reader)\n\n    # Traiter les donn\u00e9es\n    processed = []\n    for row in rows:\n        processed.append({\n            'id': row['id'],\n            'name': row['name'].upper(),\n            'email': row['email'].lower()\n        })\n\n    # Uploader le r\u00e9sultat\n    output_key = key.replace('raw/', 'processed/')\n    s3.put_object(\n        Bucket=bucket,\n        Key=output_key,\n        Body=json.dumps(processed)\n    )\n\n    return {\n        'statusCode': 200,\n        'body': f'Processed {len(processed)} rows'\n    }\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#exemple-2-valider-des-donnees","title":"Exemple 2 : Valider des donn\u00e9es","text":"<pre><code>import json\nimport boto3\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    bucket = event['bucket']\n    key = event['key']\n\n    # T\u00e9l\u00e9charger le fichier\n    response = s3.get_object(Bucket=bucket, Key=key)\n    data = json.loads(response['Body'].read())\n\n    # Valider\n    errors = []\n    for item in data:\n        if 'email' not in item or '@' not in item['email']:\n            errors.append(f\"Invalid email for id {item.get('id')}\")\n        if 'age' in item and item['age'] &lt; 0:\n            errors.append(f\"Invalid age for id {item.get('id')}\")\n\n    # Uploader le rapport\n    if errors:\n        s3.put_object(\n            Bucket=bucket,\n            Key=f'validation-errors/{key}',\n            Body=json.dumps(errors)\n        )\n        return {\n            'statusCode': 400,\n            'body': f'Found {len(errors)} validation errors'\n        }\n\n    return {\n        'statusCode': 200,\n        'body': 'Validation passed'\n    }\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#exemple-3-declencher-un-job-glue","title":"Exemple 3 : D\u00e9clencher un job Glue","text":"<pre><code>import boto3\n\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    # Nom du job Glue \u00e0 ex\u00e9cuter\n    job_name = 'my-etl-job'\n\n    # D\u00e9clencher le job\n    response = glue.start_job_run(\n        JobName=job_name\n    )\n\n    return {\n        'statusCode': 200,\n        'body': f'Started Glue job: {response[\"JobRunId\"]}'\n    }\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#declencheurs-triggers","title":"D\u00e9clencheurs (Triggers)","text":""},{"location":"Cloud/AWS/FR/06-lambda/#declencher-depuis-s3","title":"D\u00e9clencher depuis S3","text":"<p>Configuration :</p> <ol> <li>Lambda \u2192 Function \u2192 \"Add trigger\"</li> <li>Source : \"S3\"</li> <li>Bucket : S\u00e9lectionner le bucket</li> <li>Event type : \"All object create events\" (ou sp\u00e9cifique)</li> <li>Prefix (optionnel) : <code>raw/</code> (seulement fichiers dans ce pr\u00e9fixe)</li> <li>Suffix (optionnel) : <code>.csv</code> (seulement fichiers CSV)</li> </ol> <p>R\u00e9sultat : Lambda s'ex\u00e9cute automatiquement quand un fichier est upload\u00e9.</p>"},{"location":"Cloud/AWS/FR/06-lambda/#declencher-depuis-eventbridge-schedule","title":"D\u00e9clencher depuis EventBridge (schedule)","text":"<p>Planifier une ex\u00e9cution :</p> <ol> <li>Lambda \u2192 Function \u2192 \"Add trigger\"</li> <li>Source : \"EventBridge (CloudWatch Events)\"</li> <li>Rule : Cr\u00e9er une nouvelle r\u00e8gle</li> <li>Schedule expression : <code>cron(0 2 * * ? *)</code> (tous les jours \u00e0 2h)</li> </ol> <p>Exemple de cron : - <code>cron(0 2 * * ? *)</code> : Tous les jours \u00e0 2h - <code>cron(0 */6 * * ? *)</code> : Toutes les 6 heures - <code>cron(0 0 ? * MON *)</code> : Tous les lundis \u00e0 minuit</p>"},{"location":"Cloud/AWS/FR/06-lambda/#declencher-depuis-api-gateway","title":"D\u00e9clencher depuis API Gateway","text":"<p>Cr\u00e9er une API REST :</p> <ol> <li>API Gateway \u2192 \"Create API\"</li> <li>Type : REST API</li> <li>Cr\u00e9er une ressource et m\u00e9thode</li> <li>Int\u00e9gration : Lambda Function</li> <li>S\u00e9lectionner la fonction Lambda</li> </ol> <p>R\u00e9sultat : Appel HTTP d\u00e9clenche Lambda.</p>"},{"location":"Cloud/AWS/FR/06-lambda/#integration-avec-autres-services","title":"Int\u00e9gration avec autres services","text":""},{"location":"Cloud/AWS/FR/06-lambda/#lambda-s3","title":"Lambda + S3","text":"<p>Traitement automatique de fichiers :</p> <pre><code>import boto3\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # \u00c9v\u00e9nement S3\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = record['s3']['object']['key']\n\n        # Traiter le fichier\n        # ...\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#lambda-glue","title":"Lambda + Glue","text":"<p>D\u00e9clencher un job Glue :</p> <pre><code>import boto3\n\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    response = glue.start_job_run(\n        JobName='my-etl-job',\n        Arguments={\n            '--input-path': 's3://bucket/raw/',\n            '--output-path': 's3://bucket/processed/'\n        }\n    )\n    return response\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#lambda-sns-notifications","title":"Lambda + SNS (Notifications)","text":"<p>Envoyer une notification :</p> <pre><code>import boto3\nimport json\n\nsns = boto3.client('sns')\n\ndef lambda_handler(event, context):\n    # Traitement...\n\n    # Envoyer notification\n    sns.publish(\n        TopicArn='arn:aws:sns:region:account:topic',\n        Message=json.dumps({\n            'status': 'success',\n            'message': 'Data processing completed'\n        })\n    )\n\n    return {'statusCode': 200}\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#lambda-step-functions","title":"Lambda + Step Functions","text":"<p>Orchestrer plusieurs Lambdas :</p> <pre><code>{\n  \"Comment\": \"ETL Pipeline\",\n  \"StartAt\": \"ProcessData\",\n  \"States\": {\n    \"ProcessData\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:region:account:function:process-data\",\n      \"Next\": \"ValidateData\"\n    },\n    \"ValidateData\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:region:account:function:validate-data\",\n      \"End\": true\n    }\n  }\n}\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/AWS/FR/06-lambda/#performance","title":"Performance","text":"<ol> <li>Optimiser le code pour r\u00e9duire le temps d'ex\u00e9cution</li> <li>Utiliser les bonnes m\u00e9moires (128 MB \u00e0 10 GB)</li> <li>R\u00e9utiliser les connexions (boto3 clients)</li> <li>Utiliser des layers pour d\u00e9pendances communes</li> </ol>"},{"location":"Cloud/AWS/FR/06-lambda/#couts","title":"Co\u00fbts","text":"<ol> <li>Optimiser la dur\u00e9e d'ex\u00e9cution</li> <li>Choisir la bonne m\u00e9moire (plus de m\u00e9moire = plus rapide mais plus cher)</li> <li>\u00c9viter les timeouts inutiles</li> <li>Utiliser des r\u00e9servations si usage constant (pas dans Free Tier)</li> </ol>"},{"location":"Cloud/AWS/FR/06-lambda/#securite","title":"S\u00e9curit\u00e9","text":"<ol> <li>Utiliser IAM roles pour permissions</li> <li>Ne pas hardcoder les credentials</li> <li>Utiliser des variables d'environnement pour configuration</li> <li>Activer VPC si besoin d'acc\u00e8s priv\u00e9</li> </ol>"},{"location":"Cloud/AWS/FR/06-lambda/#gestion-derreurs","title":"Gestion d'erreurs","text":"<pre><code>import json\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ndef lambda_handler(event, context):\n    try:\n        # Traitement\n        result = process_data(event)\n        return {\n            'statusCode': 200,\n            'body': json.dumps(result)\n        }\n    except Exception as e:\n        logger.error(f'Error: {str(e)}')\n        return {\n            'statusCode': 500,\n            'body': json.dumps({'error': str(e)})\n        }\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/AWS/FR/06-lambda/#exemple-1-pipeline-etl-automatique","title":"Exemple 1 : Pipeline ETL automatique","text":"<pre><code>import boto3\nimport json\n\ns3 = boto3.client('s3')\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    \"\"\"\n    D\u00e9clenche un job Glue quand un fichier est upload\u00e9 dans S3\n    \"\"\"\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # V\u00e9rifier que c'est un fichier CSV\n    if not key.endswith('.csv'):\n        return {'statusCode': 200, 'body': 'Not a CSV file'}\n\n    # D\u00e9clencher le job Glue\n    response = glue.start_job_run(\n        JobName='csv-to-parquet-job',\n        Arguments={\n            '--input-path': f's3://{bucket}/{key}',\n            '--output-path': f's3://{bucket}/processed/'\n        }\n    )\n\n    return {\n        'statusCode': 200,\n        'body': f'Started Glue job: {response[\"JobRunId\"]}'\n    }\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#exemple-2-validation-et-notification","title":"Exemple 2 : Validation et notification","text":"<pre><code>import boto3\nimport json\nimport csv\n\ns3 = boto3.client('s3')\nsns = boto3.client('sns')\n\ndef lambda_handler(event, context):\n    bucket = event['bucket']\n    key = event['key']\n\n    # T\u00e9l\u00e9charger et valider\n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response['Body'].read().decode('utf-8')\n    reader = csv.DictReader(content.splitlines())\n\n    errors = []\n    for row in reader:\n        if not row.get('email') or '@' not in row['email']:\n            errors.append(f\"Row {row.get('id')}: Invalid email\")\n\n    # Notification\n    if errors:\n        sns.publish(\n            TopicArn='arn:aws:sns:region:account:alerts',\n            Message=f'Validation failed: {len(errors)} errors found'\n        )\n\n    return {'statusCode': 200 if not errors else 400}\n</code></pre>"},{"location":"Cloud/AWS/FR/06-lambda/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Lambda = Serverless : Pas d'infrastructure \u00e0 g\u00e9rer</li> <li>Free Tier : 1M requ\u00eates/mois : Tr\u00e8s g\u00e9n\u00e9reux</li> <li>Event-driven : D\u00e9clench\u00e9 par \u00e9v\u00e9nements</li> <li>Int\u00e9gration facile : Avec tous les services AWS</li> <li>Pay-per-use : Payez seulement l'ex\u00e9cution</li> </ol>"},{"location":"Cloud/AWS/FR/06-lambda/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 7. Projets pratiques pour cr\u00e9er des projets complets avec AWS.</p>"},{"location":"Cloud/AWS/FR/07-projets/","title":"7. Projets pratiques AWS","text":""},{"location":"Cloud/AWS/FR/07-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Appliquer les connaissances acquises</li> <li>Cr\u00e9er des pipelines ETL complets</li> <li>Construire un Data Lake sur AWS</li> <li>Cr\u00e9er des projets pour votre portfolio</li> <li>Int\u00e9grer plusieurs services AWS</li> </ul>"},{"location":"Cloud/AWS/FR/07-projets/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Projet 1 : Pipeline ETL S3 \u2192 Parquet</li> <li>Projet 2 : Data Lake sur AWS</li> <li>Projet 3 : Analytics avec Athena</li> <li>Projet 4 : Pipeline automatis\u00e9 complet</li> <li>Bonnes pratiques pour portfolio</li> </ol>"},{"location":"Cloud/AWS/FR/07-projets/#projet-1-pipeline-etl-s3-parquet","title":"Projet 1 : Pipeline ETL S3 \u2192 Parquet","text":""},{"location":"Cloud/AWS/FR/07-projets/#objectif","title":"Objectif","text":"<p>Cr\u00e9er un pipeline ETL qui transforme des fichiers CSV depuis S3 en format Parquet optimis\u00e9.</p>"},{"location":"Cloud/AWS/FR/07-projets/#architecture","title":"Architecture","text":"<pre><code>S3 (raw/) \u2192 Glue Crawler \u2192 Data Catalog \u2192 Glue Job \u2192 S3 (processed/parquet/)\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#etapes","title":"\u00c9tapes","text":""},{"location":"Cloud/AWS/FR/07-projets/#1-preparer-les-donnees","title":"1. Pr\u00e9parer les donn\u00e9es","text":"<p>Cr\u00e9er un bucket S3 : - Nom : <code>data-analyst-project-1</code> - Cr\u00e9er un dossier <code>raw/</code> - Uploader un fichier CSV de test</p> <p>Exemple de donn\u00e9es CSV : <pre><code>id,name,email,created_at,status\n1,John Doe,john@example.com,2024-01-01,active\n2,Jane Smith,jane@example.com,2024-01-02,inactive\n</code></pre></p>"},{"location":"Cloud/AWS/FR/07-projets/#2-creer-un-crawler-glue","title":"2. Cr\u00e9er un Crawler Glue","text":"<ol> <li>Glue \u2192 \"Crawlers\" \u2192 \"Add crawler\"</li> <li>Nom : <code>csv-crawler</code></li> <li>Data source : <code>s3://data-analyst-project-1/raw/</code></li> <li>IAM Role : Cr\u00e9er un r\u00f4le avec acc\u00e8s S3</li> <li>Database : <code>project1_db</code></li> <li>Ex\u00e9cuter le crawler</li> </ol>"},{"location":"Cloud/AWS/FR/07-projets/#3-creer-un-job-glue","title":"3. Cr\u00e9er un Job Glue","text":"<ol> <li>Glue \u2192 \"ETL jobs\" \u2192 \"Add job\"</li> <li>Nom : <code>csv-to-parquet-job</code></li> <li>Type : Spark</li> <li>Script :</li> </ol> <pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Lire depuis Data Catalog\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"project1_db\",\n    table_name = \"raw_data\"\n)\n\n# Filtrer les donn\u00e9es actives\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"status\"] == \"active\"\n)\n\n# \u00c9crire en Parquet\nglueContext.write_dynamic_frame.from_options(\n    frame = filtered,\n    connection_type = \"s3\",\n    connection_options = {\n        \"path\": \"s3://data-analyst-project-1/processed/parquet/\"\n    },\n    format = \"parquet\"\n)\n\njob.commit()\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#4-executer-le-job","title":"4. Ex\u00e9cuter le job","text":"<ol> <li>S\u00e9lectionner le job</li> <li>\"Run job\"</li> <li>V\u00e9rifier les logs</li> <li>V\u00e9rifier les fichiers Parquet dans S3</li> </ol>"},{"location":"Cloud/AWS/FR/07-projets/#resultat","title":"R\u00e9sultat","text":"<ul> <li>Fichiers CSV transform\u00e9s en Parquet</li> <li>Donn\u00e9es filtr\u00e9es (seulement actives)</li> <li>Pr\u00eat pour analytics avec Athena</li> </ul>"},{"location":"Cloud/AWS/FR/07-projets/#projet-2-data-lake-sur-aws","title":"Projet 2 : Data Lake sur AWS","text":""},{"location":"Cloud/AWS/FR/07-projets/#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er un Data Lake complet avec ingestion, transformation et analytics.</p>"},{"location":"Cloud/AWS/FR/07-projets/#architecture_1","title":"Architecture","text":"<pre><code>Sources \u2192 S3 (Raw) \u2192 Glue (Transform) \u2192 S3 (Processed) \u2192 Athena (Analytics)\n                \u2193\n            Lambda (Trigger)\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#etapes_1","title":"\u00c9tapes","text":""},{"location":"Cloud/AWS/FR/07-projets/#1-structure-s3","title":"1. Structure S3","text":"<pre><code>data-lake-bucket/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u251c\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#2-crawlers-pour-chaque-source","title":"2. Crawlers pour chaque source","text":"<p>Cr\u00e9er 3 crawlers : - <code>users-crawler</code> \u2192 <code>s3://bucket/raw/users/</code> - <code>orders-crawler</code> \u2192 <code>s3://bucket/raw/orders/</code> - <code>products-crawler</code> \u2192 <code>s3://bucket/raw/products/</code></p>"},{"location":"Cloud/AWS/FR/07-projets/#3-jobs-etl-pour-transformation","title":"3. Jobs ETL pour transformation","text":"<p>Job pour users : <pre><code># users-etl-job\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_lake_db\",\n    table_name = \"users\"\n)\n\n# Nettoyer et transformer\ncleaned = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"email\"] is not None\n)\n\nglueContext.write_dynamic_frame.from_options(\n    frame = cleaned,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bucket/processed/users/\"},\n    format = \"parquet\"\n)\n</code></pre></p>"},{"location":"Cloud/AWS/FR/07-projets/#4-tables-athena-pour-analytics","title":"4. Tables Athena pour analytics","text":"<pre><code>-- Table users\nCREATE EXTERNAL TABLE users_processed (\n    id INT,\n    name STRING,\n    email STRING,\n    created_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket/processed/users/';\n\n-- Table orders\nCREATE EXTERNAL TABLE orders_processed (\n    id INT,\n    user_id INT,\n    amount DECIMAL(10,2),\n    created_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket/processed/orders/';\n\n-- Requ\u00eate analytique\nSELECT \n    u.name,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_spent\nFROM users_processed u\nLEFT JOIN orders_processed o ON u.id = o.user_id\nGROUP BY u.name\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#5-automatisation-avec-lambda","title":"5. Automatisation avec Lambda","text":"<p>Lambda d\u00e9clench\u00e9 par upload S3 :</p> <pre><code>import boto3\n\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # D\u00e9terminer quel job ex\u00e9cuter selon le pr\u00e9fixe\n    if 'users' in key:\n        job_name = 'users-etl-job'\n    elif 'orders' in key:\n        job_name = 'orders-etl-job'\n    else:\n        job_name = 'products-etl-job'\n\n    # D\u00e9clencher le job\n    glue.start_job_run(JobName=job_name)\n\n    return {'statusCode': 200}\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#resultat_1","title":"R\u00e9sultat","text":"<ul> <li>Data Lake fonctionnel</li> <li>Pipeline automatis\u00e9</li> <li>Analytics avec Athena</li> <li>Projet complet pour portfolio</li> </ul>"},{"location":"Cloud/AWS/FR/07-projets/#projet-3-analytics-avec-athena","title":"Projet 3 : Analytics avec Athena","text":""},{"location":"Cloud/AWS/FR/07-projets/#objectif_2","title":"Objectif","text":"<p>Cr\u00e9er un syst\u00e8me d'analytics complet avec requ\u00eates SQL sur donn\u00e9es S3.</p>"},{"location":"Cloud/AWS/FR/07-projets/#etapes_2","title":"\u00c9tapes","text":""},{"location":"Cloud/AWS/FR/07-projets/#1-preparer-les-donnees_1","title":"1. Pr\u00e9parer les donn\u00e9es","text":"<p>Uploader des fichiers Parquet dans S3 : - <code>s3://analytics-bucket/sales/year=2024/month=01/</code> - <code>s3://analytics-bucket/sales/year=2024/month=02/</code></p>"},{"location":"Cloud/AWS/FR/07-projets/#2-creer-des-tables-partitionnees","title":"2. Cr\u00e9er des tables partitionn\u00e9es","text":"<pre><code>CREATE EXTERNAL TABLE sales (\n    id INT,\n    product_id INT,\n    amount DECIMAL(10,2),\n    sale_date TIMESTAMP\n)\nPARTITIONED BY (year INT, month INT)\nSTORED AS PARQUET\nLOCATION 's3://analytics-bucket/sales/';\n\n-- Ajouter les partitions\nALTER TABLE sales ADD PARTITION (year=2024, month=1)\nLOCATION 's3://analytics-bucket/sales/year=2024/month=01/';\n\nALTER TABLE sales ADD PARTITION (year=2024, month=2)\nLOCATION 's3://analytics-bucket/sales/year=2024/month=02/';\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#3-requetes-analytiques","title":"3. Requ\u00eates analytiques","text":"<p>Ventes par mois : <pre><code>SELECT \n    year,\n    month,\n    SUM(amount) AS total_sales,\n    COUNT(*) AS transaction_count,\n    AVG(amount) AS avg_transaction\nFROM sales\nWHERE year = 2024\nGROUP BY year, month\nORDER BY year, month;\n</code></pre></p> <p>Top produits : <pre><code>SELECT \n    product_id,\n    SUM(amount) AS total_revenue,\n    COUNT(*) AS sales_count\nFROM sales\nWHERE year = 2024\nGROUP BY product_id\nORDER BY total_revenue DESC\nLIMIT 10;\n</code></pre></p> <p>Tendances : <pre><code>SELECT \n    DATE_TRUNC('week', sale_date) AS week,\n    SUM(amount) AS weekly_sales,\n    LAG(SUM(amount), 1) OVER (ORDER BY DATE_TRUNC('week', sale_date)) AS previous_week\nFROM sales\nWHERE year = 2024\nGROUP BY DATE_TRUNC('week', sale_date)\nORDER BY week;\n</code></pre></p>"},{"location":"Cloud/AWS/FR/07-projets/#4-sauvegarder-les-resultats","title":"4. Sauvegarder les r\u00e9sultats","text":"<p>Cr\u00e9er une table pour r\u00e9sultats : <pre><code>CREATE EXTERNAL TABLE analytics_results (\n    metric_name STRING,\n    metric_value DECIMAL(10,2),\n    calculated_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://analytics-bucket/results/';\n</code></pre></p>"},{"location":"Cloud/AWS/FR/07-projets/#projet-4-pipeline-automatise-complet","title":"Projet 4 : Pipeline automatis\u00e9 complet","text":""},{"location":"Cloud/AWS/FR/07-projets/#objectif_3","title":"Objectif","text":"<p>Cr\u00e9er un pipeline ETL compl\u00e8tement automatis\u00e9 avec plusieurs services AWS.</p>"},{"location":"Cloud/AWS/FR/07-projets/#architecture-complete","title":"Architecture compl\u00e8te","text":"<pre><code>Fichier CSV upload\u00e9 \u2192 S3 (raw/)\n    \u2193 (Event)\nLambda (Validation)\n    \u2193\nS3 (validated/)\n    \u2193 (Event)\nGlue Job (Transform CSV \u2192 Parquet)\n    \u2193\nS3 (processed/parquet/)\n    \u2193\nGlue Crawler (Update Catalog)\n    \u2193\nAthena (Analytics)\n    \u2193\nS3 (results/)\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#implementation","title":"Impl\u00e9mentation","text":""},{"location":"Cloud/AWS/FR/07-projets/#1-lambda-de-validation","title":"1. Lambda de validation","text":"<pre><code>import boto3\nimport csv\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # T\u00e9l\u00e9charger et valider\n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response['Body'].read().decode('utf-8')\n    reader = csv.DictReader(content.splitlines())\n\n    valid_rows = []\n    for row in reader:\n        if row.get('email') and '@' in row['email']:\n            valid_rows.append(row)\n\n    # Uploader les donn\u00e9es valid\u00e9es\n    if valid_rows:\n        validated_key = key.replace('raw/', 'validated/')\n        # Convertir en CSV et uploader\n        # ...\n\n    return {'statusCode': 200}\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#2-glue-job-de-transformation","title":"2. Glue Job de transformation","text":"<pre><code># Transform validated CSV to Parquet\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"pipeline_db\",\n    table_name = \"validated_data\"\n)\n\n# Transformer\ntransformed = Map.apply(\n    frame = datasource,\n    f = lambda x: {\n        'id': x['id'],\n        'name': x['name'].upper(),\n        'email': x['email'].lower(),\n        'created_at': x['created_at']\n    }\n)\n\n# \u00c9crire en Parquet\nglueContext.write_dynamic_frame.from_options(\n    frame = transformed,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bucket/processed/\"},\n    format = \"parquet\"\n)\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#3-workflow-glue","title":"3. Workflow Glue","text":"<p>Cr\u00e9er un workflow : 1. Trigger : Nouveau fichier dans <code>validated/</code> 2. Action : Ex\u00e9cuter le job Glue 3. Action suivante : Mettre \u00e0 jour le crawler</p>"},{"location":"Cloud/AWS/FR/07-projets/#resultat_2","title":"R\u00e9sultat","text":"<ul> <li>Pipeline compl\u00e8tement automatis\u00e9</li> <li>Validation automatique</li> <li>Transformation automatique</li> <li>Analytics disponibles imm\u00e9diatement</li> </ul>"},{"location":"Cloud/AWS/FR/07-projets/#bonnes-pratiques-pour-portfolio","title":"Bonnes pratiques pour portfolio","text":""},{"location":"Cloud/AWS/FR/07-projets/#documentation","title":"Documentation","text":"<p>Cr\u00e9er un README pour chaque projet :</p> <pre><code># Projet : Pipeline ETL AWS\n\n## Description\nPipeline ETL automatis\u00e9 pour transformer des donn\u00e9es CSV en Parquet.\n\n## Architecture\n- S3 : Stockage\n- Glue : Transformation\n- Athena : Analytics\n\n## R\u00e9sultats\n- R\u00e9duction des co\u00fbts de 60%\n- Temps de traitement r\u00e9duit de 80%\n</code></pre>"},{"location":"Cloud/AWS/FR/07-projets/#visualisations","title":"Visualisations","text":"<p>Cr\u00e9er des diagrammes : - Architecture du syst\u00e8me - Flux de donn\u00e9es - Sch\u00e9ma de donn\u00e9es</p> <p>Outils : - Draw.io - Lucidchart - Diagrammes ASCII dans README</p>"},{"location":"Cloud/AWS/FR/07-projets/#metriques","title":"M\u00e9triques","text":"<p>Inclure des m\u00e9triques : - Temps d'ex\u00e9cution avant/apr\u00e8s - Co\u00fbts avant/apr\u00e8s - Volume de donn\u00e9es trait\u00e9es - Performance des requ\u00eates</p>"},{"location":"Cloud/AWS/FR/07-projets/#code","title":"Code","text":"<p>Bonnes pratiques : - Code comment\u00e9 - Variables d'environnement pour configuration - Gestion d'erreurs - Logging</p>"},{"location":"Cloud/AWS/FR/07-projets/#github","title":"GitHub","text":"<p>Cr\u00e9er un repository : - README avec documentation - Scripts Lambda - Scripts Glue - Configuration - Diagrammes</p>"},{"location":"Cloud/AWS/FR/07-projets/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Projets pratiques : Essentiels pour portfolio</li> <li>Documentation : Expliquer l'architecture et les r\u00e9sultats</li> <li>M\u00e9triques : Montrer l'impact (performance, co\u00fbts)</li> <li>Code propre : Comment\u00e9 et organis\u00e9</li> <li>GitHub : Partager vos projets</li> </ol>"},{"location":"Cloud/AWS/FR/07-projets/#ressources","title":"\ud83d\udd17 Ressources","text":"<ul> <li>AWS Architecture Center</li> <li>AWS Solutions</li> <li>GitHub AWS Examples</li> </ul> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation AWS pour Data Analyst. Vous pouvez maintenant cr\u00e9er des projets complets sur AWS en utilisant uniquement des ressources gratuites.</p>"},{"location":"Cloud/AWS/PL/","title":"Szkolenie AWS dla Data Analyst - Przewodnik Bezp\u0142atny","text":""},{"location":"Cloud/AWS/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>Ten kurs prowadzi Ci\u0119 przez nauk\u0119 Amazon Web Services (AWS) jako Data Analyst, u\u017cywaj\u0105c wy\u0142\u0105cznie bezp\u0142atnych zasob\u00f3w. Nauczysz si\u0119 u\u017cywa\u0107 niezb\u0119dnych us\u0142ug AWS do analizy danych bez wydawania ani grosza.</p>"},{"location":"Cloud/AWS/PL/#cele-edukacyjne","title":"\ud83c\udfaf Cele edukacyjne","text":"<ul> <li>Zrozumie\u0107 niezb\u0119dne us\u0142ugi AWS dla Data Analyst</li> <li>Tworzy\u0107 i zarz\u0105dza\u0107 bezp\u0142atnymi kontami AWS</li> <li>U\u017cywa\u0107 S3, Glue, Redshift i innych us\u0142ug danych</li> <li>Budowa\u0107 potoki ETL na AWS</li> <li>Analizowa\u0107 dane z AWS</li> <li>Tworzy\u0107 praktyczne projekty do portfolio</li> </ul>"},{"location":"Cloud/AWS/PL/#wszystko-jest-bezpatne","title":"\ud83d\udcb0 Wszystko jest bezp\u0142atne!","text":"<p>Ten kurs wykorzystuje wy\u0142\u0105cznie: - \u2705 AWS Free Tier : Bezp\u0142atne us\u0142ugi przez 12 miesi\u0119cy - \u2705 AWS Training : Bezp\u0142atne kursy i laboratoria - \u2705 Dokumentacja AWS : Kompletne bezp\u0142atne przewodniki - \u2705 AWS Workshops : Bezp\u0142atne praktyczne warsztaty</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"Cloud/AWS/PL/#struktura-kursu","title":"\ud83d\udcd6 Struktura kursu","text":""},{"location":"Cloud/AWS/PL/#1-rozpoczecie-z-aws","title":"1. Rozpocz\u0119cie z AWS","text":"<ul> <li>Utworzenie bezp\u0142atnego konta AWS</li> <li>Zrozumienie Free Tier</li> <li>Nawigacja w konsoli AWS</li> <li>Konfiguracja bezpiecze\u0144stwa (IAM)</li> </ul>"},{"location":"Cloud/AWS/PL/#2-amazon-s3-przechowywanie-danych","title":"2. Amazon S3 - Przechowywanie danych","text":"<ul> <li>Tworzenie bucket\u00f3w S3</li> <li>Przesy\u0142anie i zarz\u0105dzanie plikami</li> <li>Organizacja danych</li> <li>Integracja z innymi us\u0142ugami</li> </ul>"},{"location":"Cloud/AWS/PL/#3-aws-glue-etl-serverless","title":"3. AWS Glue - ETL Serverless","text":"<ul> <li>Tworzenie zada\u0144 ETL</li> <li>Transformacja danych</li> <li>Crawlery i katalogi danych</li> <li>Integracja z S3</li> </ul>"},{"location":"Cloud/AWS/PL/#4-amazon-redshift-hurtownia-danych","title":"4. Amazon Redshift - Hurtownia danych","text":"<ul> <li>Tworzenie klastra Redshift (bezp\u0142atnie 2 miesi\u0105ce)</li> <li>\u0141adowanie danych</li> <li>Zaawansowane zapytania SQL</li> <li>Optymalizacja</li> </ul>"},{"location":"Cloud/AWS/PL/#5-amazon-athena-zapytania-sql-na-s3","title":"5. Amazon Athena - Zapytania SQL na S3","text":"<ul> <li>Zapytania SQL na plikach S3</li> <li>Tworzenie tabel</li> <li>Optymalizacja koszt\u00f3w</li> <li>Integracja z Glue</li> </ul>"},{"location":"Cloud/AWS/PL/#6-aws-lambda-przetwarzanie-serverless","title":"6. AWS Lambda - Przetwarzanie Serverless","text":"<ul> <li>Tworzenie funkcji Lambda</li> <li>Przetwarzanie danych</li> <li>Automatyzacja przep\u0142yw\u00f3w pracy</li> <li>Integracja z innymi us\u0142ugami</li> </ul>"},{"location":"Cloud/AWS/PL/#7-projekty-praktyczne","title":"7. Projekty praktyczne","text":"<ul> <li>Kompletny potok ETL</li> <li>Data Lake na AWS</li> <li>Projekt do portfolio</li> <li>Najlepsze praktyki</li> </ul>"},{"location":"Cloud/AWS/PL/#szybki-start","title":"\ud83d\ude80 Szybki start","text":""},{"location":"Cloud/AWS/PL/#krok-1-utworzenie-bezpatnego-konta-aws","title":"Krok 1: Utworzenie bezp\u0142atnego konta AWS","text":"<ol> <li>Przejd\u017a do: https://aws.amazon.com/pl/free/</li> <li>Kliknij \"Utw\u00f3rz bezp\u0142atne konto\"</li> <li>Wype\u0142nij formularz (wymagana karta kredytowa, ale nie obci\u0105\u017cana)</li> <li>Zweryfikuj to\u017csamo\u015b\u0107 telefonicznie</li> <li>Wybierz plan wsparcia (bezp\u0142atny)</li> </ol> <p>Wa\u017cne: AWS nie obci\u0105\u017cy Ci\u0119, dop\u00f3ki pozostajesz w limitach Free Tier.</p>"},{"location":"Cloud/AWS/PL/#krok-2-eksploracja-free-tier","title":"Krok 2: Eksploracja Free Tier","text":"<p>Bezp\u0142atne us\u0142ugi przydatne dla Data Analyst:</p> <ul> <li>Amazon S3 : 5 GB przechowywania (zawsze bezp\u0142atne)</li> <li>AWS Glue : 10 000 obiekt\u00f3w/miesi\u0105c (zawsze bezp\u0142atne)</li> <li>Amazon Redshift : 2 miesi\u0105ce bezp\u0142atnie (750 godzin)</li> <li>AWS Lambda : 1 milion \u017c\u0105da\u0144/miesi\u0105c (zawsze bezp\u0142atne)</li> <li>Amazon Athena : 10 GB przeskanowanych danych/miesi\u0105c (zawsze bezp\u0142atne)</li> </ul>"},{"location":"Cloud/AWS/PL/#krok-3-przejscie-przez-kurs","title":"Krok 3: Przej\u015bcie przez kurs","text":"<ol> <li>Zacznij od modu\u0142u 1 (Rozpocz\u0119cie)</li> <li>Post\u0119puj zgodnie z kolejno\u015bci\u0105 modu\u0142\u00f3w</li> <li>\u0106wicz z projektami z modu\u0142u 7</li> <li>Monitoruj u\u017cycie w AWS Billing</li> </ol>"},{"location":"Cloud/AWS/PL/#niezbedne-usugi-aws-dla-data-analyst","title":"\ud83d\udcca Niezb\u0119dne us\u0142ugi AWS dla Data Analyst","text":"Us\u0142uga Zastosowanie Free Tier S3 Przechowywanie danych 5 GB (zawsze) Glue ETL serverless 10K obiekt\u00f3w/miesi\u0105c Redshift Hurtownia danych 2 miesi\u0105ce bezp\u0142atnie Athena Zapytania SQL na S3 10 GB/miesi\u0105c Lambda Przetwarzanie serverless 1M \u017c\u0105da\u0144/miesi\u0105c QuickSight Wizualizacja 1 bezp\u0142atny u\u017cytkownik"},{"location":"Cloud/AWS/PL/#zarzadzanie-kosztami","title":"\u26a0\ufe0f Zarz\u0105dzanie kosztami","text":""},{"location":"Cloud/AWS/PL/#wskazowki-aby-pozostac-bezpatnym","title":"Wskaz\u00f3wki, aby pozosta\u0107 bezp\u0142atnym","text":"<ol> <li>Monitorowanie rozlicze\u0144</li> <li>W\u0142\u0105cz alerty rozliczeniowe</li> <li>Regularnie sprawdzaj AWS Cost Explorer</li> <li> <p>Zalecany limit: alert 5 USD</p> </li> <li> <p>Przestrzeganie limit\u00f3w Free Tier</p> </li> <li>Uwa\u017cnie czytaj warunki</li> <li>Niekt\u00f3re us\u0142ugi bezp\u0142atne przez 12 miesi\u0119cy</li> <li> <p>Inne zawsze bezp\u0142atne (z limitami)</p> </li> <li> <p>Usuwanie nieu\u017cywanych zasob\u00f3w</p> </li> <li>Zatrzymaj nieu\u017cywane instancje</li> <li>Usu\u0144 puste buckety S3</li> <li> <p>Regularne czyszczenie</p> </li> <li> <p>U\u017cywanie bezp\u0142atnych region\u00f3w</p> </li> <li>Niekt\u00f3re regiony oferuj\u0105 wi\u0119cej bezp\u0142atnych us\u0142ug</li> <li>Sprawd\u017a dost\u0119pno\u015b\u0107 wed\u0142ug regionu</li> </ol>"},{"location":"Cloud/AWS/PL/#bezpatne-zasoby","title":"\ud83d\udcda Bezp\u0142atne zasoby","text":""},{"location":"Cloud/AWS/PL/#oficjalne-szkolenie-aws","title":"Oficjalne szkolenie AWS","text":"<ul> <li>AWS Training : https://aws.amazon.com/pl/training/</li> <li>Bezp\u0142atne kursy</li> <li>Praktyczne laboratoria</li> <li> <p>Przygotowanie do certyfikacji</p> </li> <li> <p>AWS Workshops : https://workshops.aws/</p> </li> <li>Prowadzone warsztaty</li> <li>Praktyczne laboratoria</li> <li> <p>Kompletne projekty</p> </li> <li> <p>Dokumentacja AWS : https://docs.aws.amazon.com/</p> </li> <li>Kompletne przewodniki</li> <li>Przyk\u0142ady kodu</li> <li>Referencja API</li> </ul>"},{"location":"Cloud/AWS/PL/#zewnetrzne-bezpatne-zasoby","title":"Zewn\u0119trzne bezp\u0142atne zasoby","text":"<ul> <li>YouTube : Oficjalny kana\u0142 AWS</li> <li>GitHub : Przyk\u0142ady AWS</li> <li>Blog AWS : Artyku\u0142y i samouczki</li> </ul>"},{"location":"Cloud/AWS/PL/#certyfikacje-opcjonalne","title":"\ud83c\udf93 Certyfikacje (opcjonalne)","text":""},{"location":"Cloud/AWS/PL/#aws-certified-cloud-practitioner","title":"AWS Certified Cloud Practitioner","text":"<ul> <li>Koszt : ~100 USD</li> <li>Przygotowanie : Bezp\u0142atne (AWS Training)</li> <li>Czas trwania : 2-3 tygodnie</li> <li>Poziom : Pocz\u0105tkuj\u0105cy</li> </ul>"},{"location":"Cloud/AWS/PL/#aws-certified-data-analytics-specialty","title":"AWS Certified Data Analytics - Specialty","text":"<ul> <li>Koszt : ~300 USD</li> <li>Przygotowanie : Bezp\u0142atne (AWS Training)</li> <li>Czas trwania : 2-3 miesi\u0105ce</li> <li>Poziom : Zaawansowany</li> </ul>"},{"location":"Cloud/AWS/PL/#konwencje","title":"\ud83d\udcdd Konwencje","text":"<ul> <li>Wszystkie przyk\u0142ady u\u017cywaj\u0105 Free Tier</li> <li>Koszty wskazane, je\u015bli przekroczone</li> <li>Polecenia testowane na konsoli AWS</li> <li>Czasy mog\u0105 si\u0119 r\u00f3\u017cni\u0107 w zale\u017cno\u015bci od regionu</li> </ul>"},{"location":"Cloud/AWS/PL/#wkad","title":"\ud83e\udd1d Wk\u0142ad","text":"<p>Ten kurs jest zaprojektowany tak, aby by\u0142 rozwijany. Nie wahaj si\u0119 proponowa\u0107 ulepsze\u0144 lub dodatkowych przypadk\u00f3w u\u017cycia.</p>"},{"location":"Cloud/AWS/PL/#dodatkowe-zasoby","title":"\ud83d\udcda Dodatkowe zasoby","text":"<ul> <li>AWS Free Tier</li> <li>AWS Training</li> <li>Dokumentacja AWS</li> <li>AWS Workshops</li> </ul>"},{"location":"Cloud/AWS/PL/01-getting-started/","title":"1. Rozpocz\u0119cie z AWS","text":""},{"location":"Cloud/AWS/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Utworzenie bezp\u0142atnego konta AWS</li> <li>Zrozumienie AWS Free Tier</li> <li>Nawigacja w konsoli AWS</li> <li>Konfiguracja podstawowego bezpiecze\u0144stwa (IAM)</li> <li>Monitorowanie koszt\u00f3w</li> </ul>"},{"location":"Cloud/AWS/PL/01-getting-started/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Utworzenie bezp\u0142atnego konta AWS</li> <li>Zrozumienie Free Tier</li> <li>Nawigacja w konsoli AWS</li> <li>Konfiguracja IAM (bezpiecze\u0144stwo)</li> <li>Monitorowanie koszt\u00f3w</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#utworzenie-bezpatnego-konta-aws","title":"Utworzenie bezp\u0142atnego konta AWS","text":""},{"location":"Cloud/AWS/PL/01-getting-started/#krok-1-rejestracja","title":"Krok 1: Rejestracja","text":"<ol> <li>Przej\u015b\u0107 na stron\u0119 AWS</li> <li>URL: https://aws.amazon.com/pl/free/</li> <li> <p>Klikn\u0105\u0107 \"Utw\u00f3rz bezp\u0142atne konto\"</p> </li> <li> <p>Wype\u0142ni\u0107 formularz</p> </li> <li>Email</li> <li>Silne has\u0142o</li> <li> <p>Nazwa konta AWS</p> </li> <li> <p>Informacje kontaktowe</p> </li> <li>Imi\u0119 i nazwisko</li> <li>Numer telefonu</li> <li> <p>Kraj</p> </li> <li> <p>Weryfikacja</p> </li> <li>Kod otrzymany SMS</li> <li> <p>Wprowadzi\u0107 kod weryfikacyjny</p> </li> <li> <p>Metoda p\u0142atno\u015bci</p> </li> <li>Wa\u017cne: Wymagana karta kredytowa, ale nie obci\u0105\u017cana</li> <li>AWS nie obci\u0105\u017cy Ci\u0119, dop\u00f3ki pozostajesz w limitach Free Tier</li> <li> <p>Mo\u017cesz usun\u0105\u0107 kart\u0119 p\u00f3\u017aniej (nie zalecane)</p> </li> <li> <p>Weryfikacja to\u017csamo\u015bci</p> </li> <li>Automatyczne po\u0142\u0105czenie</li> <li> <p>Wprowadzi\u0107 4-cyfrowy kod</p> </li> <li> <p>Plan wsparcia</p> </li> <li>Wybra\u0107 \"Plan podstawowy\" (bezp\u0142atny)</li> <li>Inne plany s\u0105 p\u0142atne</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#krok-2-potwierdzenie","title":"Krok 2: Potwierdzenie","text":"<ul> <li>Email potwierdzaj\u0105cy otrzymany</li> <li>Konto AWS aktywne natychmiast</li> <li>Dost\u0119p do konsoli AWS</li> </ul> <p>\u26a0\ufe0f Wa\u017cne: Nie tworzy\u0107 wielu kont z t\u0105 sam\u0105 kart\u0105 kredytow\u0105 (ryzyko zawieszenia).</p>"},{"location":"Cloud/AWS/PL/01-getting-started/#zrozumienie-free-tier","title":"Zrozumienie Free Tier","text":""},{"location":"Cloud/AWS/PL/01-getting-started/#typy-free-tier","title":"Typy Free Tier","text":"<p>AWS oferuje 3 typy bezp\u0142atnych us\u0142ug:</p>"},{"location":"Cloud/AWS/PL/01-getting-started/#1-bezpatne-usugi-przez-12-miesiecy","title":"1. Bezp\u0142atne us\u0142ugi przez 12 miesi\u0119cy","text":"<p>Us\u0142ugi przydatne dla Data Analyst:</p> <ul> <li>Amazon EC2: 750 godzin/miesi\u0105c (t2.micro)</li> <li>Amazon RDS: 750 godzin/miesi\u0105c</li> <li>Amazon Redshift: 750 godzin/miesi\u0105c (tylko 2 miesi\u0105ce)</li> <li>Amazon Elasticsearch: 750 godzin/miesi\u0105c</li> </ul> <p>Warunki: - Bezp\u0142atne przez 12 miesi\u0119cy po rejestracji - Limity miesi\u0119czne - Poza limitami: normalne rozliczanie</p>"},{"location":"Cloud/AWS/PL/01-getting-started/#2-zawsze-bezpatne-usugi-z-limitami","title":"2. Zawsze bezp\u0142atne us\u0142ugi (z limitami)","text":"<p>Us\u0142ugi przydatne dla Data Analyst:</p> <ul> <li>Amazon S3: 5 GB przechowywania (zawsze bezp\u0142atne)</li> <li>AWS Lambda: 1 milion \u017c\u0105da\u0144/miesi\u0105c (zawsze bezp\u0142atne)</li> <li>AWS Glue: 10 000 obiekt\u00f3w/miesi\u0105c (zawsze bezp\u0142atne)</li> <li>Amazon Athena: 10 GB przeskanowanych danych/miesi\u0105c (zawsze bezp\u0142atne)</li> <li>Amazon CloudWatch: 10 niestandardowych metryk (zawsze bezp\u0142atne)</li> </ul> <p>Warunki: - Bezp\u0142atne w niesko\u0144czono\u015b\u0107 - Limity miesi\u0119czne - Poza limitami: rozliczanie poza limitem</p>"},{"location":"Cloud/AWS/PL/01-getting-started/#3-krotkoterminowe-bezpatne-proby","title":"3. Kr\u00f3tkoterminowe bezp\u0142atne pr\u00f3by","text":"<ul> <li>Amazon Redshift: 2 miesi\u0105ce bezp\u0142atnie</li> <li>Amazon QuickSight: 1 bezp\u0142atny u\u017cytkownik</li> </ul>"},{"location":"Cloud/AWS/PL/01-getting-started/#sprawdzenie-free-tier","title":"Sprawdzenie Free Tier","text":"<ol> <li>Przej\u015b\u0107 do konsoli AWS</li> <li>Menu \"Us\u0142ugi\" \u2192 \"Rozliczenia\"</li> <li>Klikn\u0105\u0107 \"Free Tier\"</li> <li>Zobaczy\u0107 u\u017cycie wed\u0142ug us\u0142ugi</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#nawigacja-w-konsoli-aws","title":"Nawigacja w konsoli AWS","text":""},{"location":"Cloud/AWS/PL/01-getting-started/#interfejs-gowny","title":"Interfejs g\u0142\u00f3wny","text":"<p>Kluczowe elementy:</p> <ol> <li>Pasek wyszukiwania (u g\u00f3ry)</li> <li>Szybkie wyszukiwanie us\u0142ug</li> <li> <p>Przyk\u0142ad: wpisa\u0107 \"S3\" aby uzyska\u0107 dost\u0119p do Amazon S3</p> </li> <li> <p>Menu Us\u0142ugi (u g\u00f3ry po lewej)</p> </li> <li>Wszystkie us\u0142ugi AWS</li> <li> <p>Zorganizowane wed\u0142ug kategorii</p> </li> <li> <p>Region (u g\u00f3ry po prawej)</p> </li> <li>Wybra\u0107 region AWS</li> <li>Zalecenie: Wybra\u0107 najbli\u017cszy region</li> <li> <p>Przyk\u0142ad: <code>eu-west-3</code> (Pary\u017c) dla Francji</p> </li> <li> <p>Nazwa konta (u g\u00f3ry po prawej)</p> </li> <li>Ustawienia konta</li> <li>Rozliczenia</li> <li>Wsparcie</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#niezbedne-usugi-dla-data-analyst","title":"Niezb\u0119dne us\u0142ugi dla Data Analyst","text":"<p>W menu Us\u0142ugi, szuka\u0107:</p> <ul> <li>S3: Przechowywanie danych</li> <li>Glue: ETL serverless</li> <li>Redshift: Hurtownia danych</li> <li>Athena: Zapytania SQL na S3</li> <li>Lambda: Przetwarzanie serverless</li> <li>IAM: Zarz\u0105dzanie dost\u0119pem</li> </ul>"},{"location":"Cloud/AWS/PL/01-getting-started/#pierwsze-poaczenie","title":"Pierwsze po\u0142\u0105czenie","text":"<ol> <li>Zalogowa\u0107 si\u0119: https://console.aws.amazon.com/</li> <li>Eksplorowa\u0107 pulpit nawigacyjny</li> <li>Klikn\u0105\u0107 \"Us\u0142ugi\" aby zobaczy\u0107 wszystkie us\u0142ugi</li> <li>U\u017cy\u0107 paska wyszukiwania aby znale\u017a\u0107 us\u0142ug\u0119</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#konfiguracja-iam-bezpieczenstwo","title":"Konfiguracja IAM (bezpiecze\u0144stwo)","text":""},{"location":"Cloud/AWS/PL/01-getting-started/#czym-jest-iam","title":"Czym jest IAM?","text":"<p>IAM (Identity and Access Management) = Zarz\u0105dzanie dost\u0119pem i to\u017csamo\u015bci\u0105</p> <ul> <li>Tworzenie u\u017cytkownik\u00f3w</li> <li>Zarz\u0105dzanie uprawnieniami</li> <li>Zabezpieczanie dost\u0119pu do us\u0142ug</li> </ul>"},{"location":"Cloud/AWS/PL/01-getting-started/#najlepsze-praktyki-bezpieczenstwa","title":"Najlepsze praktyki bezpiecze\u0144stwa","text":""},{"location":"Cloud/AWS/PL/01-getting-started/#1-waczenie-uwierzytelniania-dwuskadnikowego-mfa","title":"1. W\u0142\u0105czenie uwierzytelniania dwusk\u0142adnikowego (MFA)","text":"<p>Dla konta root:</p> <ol> <li>Przej\u015b\u0107 do IAM</li> <li>Klikn\u0105\u0107 \"Aktywuj MFA\"</li> <li>Wybra\u0107 urz\u0105dzenie (telefon)</li> <li>Zeskanowa\u0107 kod QR aplikacj\u0105 MFA</li> <li>Wprowadzi\u0107 kody weryfikacyjne</li> </ol> <p>\u26a0\ufe0f Wa\u017cne: Zawsze w\u0142\u0105cza\u0107 MFA dla konta root.</p>"},{"location":"Cloud/AWS/PL/01-getting-started/#2-utworzenie-uzytkownika-iam-zalecane","title":"2. Utworzenie u\u017cytkownika IAM (zalecane)","text":"<p>Nie u\u017cywa\u0107 konta root do codziennej pracy.</p> <ol> <li>Przej\u015b\u0107 do IAM</li> <li>Klikn\u0105\u0107 \"U\u017cytkownicy\" \u2192 \"Dodaj u\u017cytkownik\u00f3w\"</li> <li>Nazwa u\u017cytkownika: <code>data-analyst</code></li> <li>Typ dost\u0119pu: \"Dost\u0119p programistyczny\" + \"Dost\u0119p do konsoli zarz\u0105dzania AWS\"</li> <li>Uprawnienia: \"Do\u0142\u0105cz istniej\u0105ce zasady bezpo\u015brednio\"</li> <li>Wybra\u0107: <code>PowerUserAccess</code> (na pocz\u0105tek)</li> <li>Lub utworzy\u0107 niestandardowe uprawnienia</li> <li>Utworzy\u0107 u\u017cytkownika</li> <li>Zapisa\u0107 dane dost\u0119powe (klucz dost\u0119pu + sekret)</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#3-grupy-iam-opcjonalne","title":"3. Grupy IAM (opcjonalne)","text":"<p>Tworzenie grup do organizowania u\u017cytkownik\u00f3w:</p> <ol> <li>IAM \u2192 \"Grupy\" \u2192 \"Utw\u00f3rz grup\u0119\"</li> <li>Nazwa: <code>DataAnalystGroup</code></li> <li>Do\u0142\u0105czy\u0107 zasady</li> <li>Doda\u0107 u\u017cytkownik\u00f3w do grupy</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#zalecane-zasady-iam-dla-data-analyst","title":"Zalecane zasady IAM dla Data Analyst","text":"<p>Niezb\u0119dne zasady:</p> <ul> <li><code>AmazonS3FullAccess</code>: Pe\u0142ny dost\u0119p do S3</li> <li><code>AWSGlueServiceRole</code>: Dost\u0119p do Glue</li> <li><code>AmazonRedshiftFullAccess</code>: Dost\u0119p do Redshift</li> <li><code>AmazonAthenaFullAccess</code>: Dost\u0119p do Athena</li> <li><code>AWSLambdaFullAccess</code>: Dost\u0119p do Lambda</li> </ul> <p>\u26a0\ufe0f Zasada najmniejszych uprawnie\u0144: Dawa\u0107 tylko niezb\u0119dne uprawnienia.</p>"},{"location":"Cloud/AWS/PL/01-getting-started/#monitorowanie-kosztow","title":"Monitorowanie koszt\u00f3w","text":""},{"location":"Cloud/AWS/PL/01-getting-started/#waczenie-alertow-rozliczeniowych","title":"W\u0142\u0105czenie alert\u00f3w rozliczeniowych","text":"<p>Krok 1: W\u0142\u0105czenie alert\u00f3w</p> <ol> <li>Przej\u015b\u0107 do \"Rozliczenia\" \u2192 \"Preferencje\"</li> <li>W\u0142\u0105czy\u0107 \"Otrzymuj alerty rozliczeniowe\"</li> <li>W\u0142\u0105czy\u0107 \"Otrzymuj alerty u\u017cycia Free Tier\"</li> </ol> <p>Krok 2: Utworzenie alertu CloudWatch</p> <ol> <li>Przej\u015b\u0107 do CloudWatch</li> <li>\"Alarmy\" \u2192 \"Utw\u00f3rz alarm\"</li> <li>Metryka: \"EstimatedCharges\"</li> <li>Pr\u00f3g: 5 USD (zalecane)</li> <li>Powiadomienie: Email</li> </ol> <p>Wynik: Email otrzymany, je\u015bli koszty przekrocz\u0105 5 USD.</p>"},{"location":"Cloud/AWS/PL/01-getting-started/#sprawdzenie-uzycia-free-tier","title":"Sprawdzenie u\u017cycia Free Tier","text":"<ol> <li>\"Rozliczenia\" \u2192 \"Free Tier\"</li> <li>Zobaczy\u0107 u\u017cycie wed\u0142ug us\u0142ugi</li> <li>Sprawdzi\u0107 pozosta\u0142e limity</li> <li>Monitorowa\u0107 daty wyga\u015bni\u0119cia (12 miesi\u0119cy)</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#aws-cost-explorer","title":"AWS Cost Explorer","text":"<ol> <li>\"Rozliczenia\" \u2192 \"Cost Explorer\"</li> <li>Zobaczy\u0107 koszty wed\u0142ug us\u0142ugi</li> <li>Filtrowa\u0107 wed\u0142ug okresu</li> <li>Eksportowa\u0107 raporty</li> </ol> <p>\u26a0\ufe0f Wa\u017cne: Sprawdza\u0107 regularnie (zalecane cotygodniowo).</p>"},{"location":"Cloud/AWS/PL/01-getting-started/#wskazowki-aby-pozostac-bezpatnym","title":"Wskaz\u00f3wki, aby pozosta\u0107 bezp\u0142atnym","text":"<ol> <li>Usuwanie nieu\u017cywanych zasob\u00f3w</li> <li>Zatrzyma\u0107 nieu\u017cywane instancje EC2</li> <li>Usun\u0105\u0107 puste buckety S3</li> <li> <p>Wyczy\u015bci\u0107 migawki</p> </li> <li> <p>Przestrzeganie limit\u00f3w Free Tier</p> </li> <li>Uwa\u017cnie czyta\u0107 warunki</li> <li>Monitorowa\u0107 u\u017cycie</li> <li> <p>Ustawia\u0107 alerty</p> </li> <li> <p>U\u017cywanie bezp\u0142atnych region\u00f3w</p> </li> <li>Niekt\u00f3re regiony oferuj\u0105 wi\u0119cej bezp\u0142atnych us\u0142ug</li> <li> <p>Sprawdzi\u0107 dost\u0119pno\u015b\u0107</p> </li> <li> <p>Zatrzymywanie nieu\u017cywanych us\u0142ug</p> </li> <li>Redshift: zatrzyma\u0107 klaster, gdy nieu\u017cywany</li> <li>EC2: zatrzyma\u0107 instancje</li> <li>RDS: zatrzyma\u0107 bazy danych</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Bezp\u0142atne konto AWS: 200 USD kredytu + Free Tier</li> <li>Free Tier: 3 typy (12 miesi\u0119cy, zawsze bezp\u0142atne, pr\u00f3by)</li> <li>Bezpiecze\u0144stwo IAM: W\u0142\u0105czy\u0107 MFA, tworzy\u0107 u\u017cytkownik\u00f3w</li> <li>Monitorowanie: Alerty rozliczeniowe niezb\u0119dne</li> <li>Pozosta\u0107 bezp\u0142atnym: Usuwa\u0107 nieu\u017cywane zasoby</li> </ol>"},{"location":"Cloud/AWS/PL/01-getting-started/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 2. Amazon S3 - Przechowywanie danych, aby nauczy\u0107 si\u0119 przechowywa\u0107 dane na AWS.</p>"},{"location":"Cloud/AWS/PL/02-s3/","title":"2. Amazon S3 - Przechowywanie danych","text":""},{"location":"Cloud/AWS/PL/02-s3/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Amazon S3 i jego u\u017cycie</li> <li>Tworzy\u0107 i zarz\u0105dza\u0107 bucketami S3</li> <li>Przesy\u0142a\u0107 i organizowa\u0107 pliki</li> <li>Zrozumie\u0107 klasy przechowywania</li> <li>Integrowa\u0107 S3 z innymi us\u0142ugami AWS</li> </ul>"},{"location":"Cloud/AWS/PL/02-s3/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do S3</li> <li>Utworzy\u0107 bucket S3</li> <li>Przesy\u0142a\u0107 i zarz\u0105dza\u0107 plikami</li> <li>Klasy przechowywania</li> <li>Organizacja danych</li> <li>Integracja z innymi us\u0142ugami</li> </ol>"},{"location":"Cloud/AWS/PL/02-s3/#wprowadzenie-do-s3","title":"Wprowadzenie do S3","text":""},{"location":"Cloud/AWS/PL/02-s3/#czym-jest-amazon-s3","title":"Czym jest Amazon S3?","text":"<p>Amazon S3 (Simple Storage Service) = Us\u0142uga przechowywania obiekt\u00f3w</p> <ul> <li>Nieograniczone przechowywanie</li> <li>Wysoka dost\u0119pno\u015b\u0107 (99.99%)</li> <li>Bezpieczne domy\u015blnie</li> <li>Integracja ze wszystkimi us\u0142ugami AWS</li> </ul>"},{"location":"Cloud/AWS/PL/02-s3/#przypadki-uzycia-dla-data-analyst","title":"Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Data Lake : Przechowywa\u0107 surowe dane</li> <li>Backup : Tworzy\u0107 kopie zapasowe danych</li> <li>ETL : \u0179r\u00f3d\u0142o/destynacja dla pipeline'\u00f3w</li> <li>Analytics : Dane dla Athena, Redshift</li> <li>Archiwizacja : Dane historyczne</li> </ul>"},{"location":"Cloud/AWS/PL/02-s3/#free-tier-s3","title":"Free Tier S3","text":"<p>Darmowe na zawsze : - 5 GB standardowego przechowywania - 20 000 \u017c\u0105da\u0144 GET - 2 000 \u017c\u0105da\u0144 PUT - 15 GB transferu danych wychodz\u0105cych</p> <p>\u26a0\ufe0f Wa\u017cne : Poza tymi limitami, normalne rozliczanie.</p>"},{"location":"Cloud/AWS/PL/02-s3/#utworzyc-bucket-s3","title":"Utworzy\u0107 bucket S3","text":""},{"location":"Cloud/AWS/PL/02-s3/#krok-1-dostep-do-s3","title":"Krok 1 : Dost\u0119p do S3","text":"<ol> <li>Konsola AWS \u2192 Szuka\u0107 \"S3\"</li> <li>Klikn\u0105\u0107 \"Amazon S3\"</li> <li>Klikn\u0105\u0107 \"Create bucket\"</li> </ol>"},{"location":"Cloud/AWS/PL/02-s3/#krok-2-konfiguracja-bucketa","title":"Krok 2 : Konfiguracja bucketa","text":"<p>Podstawowe informacje : - Bucket name : Nazwa unikalna globalnie (np. <code>my-data-analyst-bucket</code>) - Region : Wybra\u0107 najbli\u017cszy region (np. <code>eu-west-3</code> Pary\u017c)</p> <p>Opcje konfiguracji :</p> <ol> <li>Object Ownership</li> <li>\"ACLs disabled\" (zalecane)</li> <li> <p>\"Bucket owner enforced\"</p> </li> <li> <p>Block Public Access</p> </li> <li>\u2705 Wszystko w\u0142\u0105czy\u0107 (bezpiecze\u0144stwo domy\u015blne)</li> <li> <p>Wy\u0142\u0105czy\u0107 tylko w przypadku konkretnej potrzeby</p> </li> <li> <p>Versioning</p> </li> <li>Wy\u0142\u0105czone domy\u015blnie (darmowe)</li> <li> <p>W\u0142\u0105czy\u0107 je\u015bli potrzeba wielu wersji</p> </li> <li> <p>Tags (opcjonalne)</p> </li> <li>Doda\u0107 tagi do organizacji</li> <li> <p>Np. <code>Project: Data-Analyst-Training</code></p> </li> <li> <p>Default encryption</p> </li> <li>\u2705 W\u0142\u0105czy\u0107 (zalecane)</li> <li>\"Amazon S3 managed keys (SSE-S3)\" (darmowe)</li> </ol>"},{"location":"Cloud/AWS/PL/02-s3/#krok-3-utworzyc-bucket","title":"Krok 3 : Utworzy\u0107 bucket","text":"<ol> <li>Klikn\u0105\u0107 \"Create bucket\"</li> <li>Bucket utworzony i widoczny na li\u015bcie</li> <li>Gotowy do u\u017cycia</li> </ol> <p>\u26a0\ufe0f Wa\u017cne : Nazwa bucketa musi by\u0107 unikalna globalnie w AWS.</p>"},{"location":"Cloud/AWS/PL/02-s3/#przesyac-i-zarzadzac-plikami","title":"Przesy\u0142a\u0107 i zarz\u0105dza\u0107 plikami","text":""},{"location":"Cloud/AWS/PL/02-s3/#przesac-plik","title":"Przes\u0142a\u0107 plik","text":"<p>Metoda 1 : Interfejs web</p> <ol> <li>Klikn\u0105\u0107 nazw\u0119 bucketa</li> <li>Klikn\u0105\u0107 \"Upload\"</li> <li>\"Add files\" lub \"Add folder\"</li> <li>Wybra\u0107 pliki</li> <li>Klikn\u0105\u0107 \"Upload\"</li> </ol> <p>Metoda 2 : AWS CLI</p> <pre><code># Zainstalowa\u0107 AWS CLI (je\u015bli jeszcze nie)\n# Windows: https://aws.amazon.com/cli/\n# Linux/Mac: pip install awscli\n\n# Skonfigurowa\u0107 credentials\naws configure\n\n# Przes\u0142a\u0107 plik\naws s3 cp local-file.csv s3://my-data-analyst-bucket/data/\n</code></pre> <p>Metoda 3 : SDK Python (boto3)</p> <pre><code>import boto3\n\n# Utworzy\u0107 klienta S3\ns3 = boto3.client('s3')\n\n# Przes\u0142a\u0107 plik\ns3.upload_file('local-file.csv', 'my-data-analyst-bucket', 'data/file.csv')\n</code></pre>"},{"location":"Cloud/AWS/PL/02-s3/#pobrac-plik","title":"Pobra\u0107 plik","text":"<p>Interfejs web : 1. Klikn\u0105\u0107 plik 2. Klikn\u0105\u0107 \"Download\"</p> <p>AWS CLI : <pre><code>aws s3 cp s3://my-data-analyst-bucket/data/file.csv local-file.csv\n</code></pre></p> <p>Python : <pre><code>s3.download_file('my-data-analyst-bucket', 'data/file.csv', 'local-file.csv')\n</code></pre></p>"},{"location":"Cloud/AWS/PL/02-s3/#zarzadzac-plikami","title":"Zarz\u0105dza\u0107 plikami","text":"<p>Dost\u0119pne akcje : - Download : Pobiera\u0107 - Open : Otwiera\u0107 w przegl\u0105darce - Copy : Kopiowa\u0107 do innej lokalizacji - Move : Przenosi\u0107 - Delete : Usuwa\u0107 - Make public : Uczyni\u0107 publicznym (uwaga bezpiecze\u0144stwo)</p>"},{"location":"Cloud/AWS/PL/02-s3/#klasy-przechowywania","title":"Klasy przechowywania","text":""},{"location":"Cloud/AWS/PL/02-s3/#s3-standard-domyslne","title":"S3 Standard (domy\u015blne)","text":"<p>U\u017cycie : - Dane cz\u0119sto dost\u0119pne - Aplikacje produkcyjne</p> <p>Charakterystyka : - Szybki dost\u0119p - 99.99% dost\u0119pno\u015bci - Koszt : ~0.023$ za GB/miesi\u0105c</p> <p>Free Tier : 5 GB darmowe</p>"},{"location":"Cloud/AWS/PL/02-s3/#s3-intelligent-tiering","title":"S3 Intelligent-Tiering","text":"<p>U\u017cycie : - Dane ze zmiennym dost\u0119pem - Automatyczna optymalizacja koszt\u00f3w</p> <p>Charakterystyka : - Automatycznie przenosi mi\u0119dzy klasami - Brak op\u0142at za odzyskiwanie - Koszt : ~0.023$ za GB/miesi\u0105c</p>"},{"location":"Cloud/AWS/PL/02-s3/#s3-standard-ia-infrequent-access","title":"S3 Standard-IA (Infrequent Access)","text":"<p>U\u017cycie : - Dane rzadko dost\u0119pne - Backup, archiwa</p> <p>Charakterystyka : - Szybki dost\u0119p gdy potrzebny - Koszt przechowywania : ~0.0125$ za GB/miesi\u0105c - Koszt odzyskiwania : ~0.01$ za GB</p>"},{"location":"Cloud/AWS/PL/02-s3/#s3-one-zone-ia","title":"S3 One Zone-IA","text":"<p>U\u017cycie : - Dane reprodukowalne - Backup drugorz\u0119dny</p> <p>Charakterystyka : - Przechowywanie w jednej strefie - Koszt : ~0.01$ za GB/miesi\u0105c - \u26a0\ufe0f Ryzyko utraty je\u015bli strefa ulegnie awarii</p>"},{"location":"Cloud/AWS/PL/02-s3/#s3-glacier","title":"S3 Glacier","text":"<p>U\u017cycie : - Archiwizacja d\u0142ugoterminowa - Dane rzadko potrzebne</p> <p>Charakterystyka : - Odzyskiwanie : 1-5 minut do kilku godzin - Koszt : ~0.004$ za GB/miesi\u0105c - Op\u0142aty za odzyskiwanie wed\u0142ug pr\u0119dko\u015bci</p>"},{"location":"Cloud/AWS/PL/02-s3/#wybrac-klase-przechowywania","title":"Wybra\u0107 klas\u0119 przechowywania","text":"<p>Dla Data Analyst : - S3 Standard : Dane aktywne (cz\u0119ste analizy) - S3 Standard-IA : Dane historyczne (okazjonalne analizy) - S3 Glacier : Archiwa (rzadko u\u017cywane)</p> <p>Automatyczne przej\u015bcie : - Skonfigurowa\u0107 regu\u0142y przej\u015bcia - Przyk\u0142ad : Standard \u2192 Standard-IA po 30 dniach</p>"},{"location":"Cloud/AWS/PL/02-s3/#organizacja-danych","title":"Organizacja danych","text":""},{"location":"Cloud/AWS/PL/02-s3/#zalecana-struktura","title":"Zalecana struktura","text":"<p>Organizacja wed\u0142ug projektu : <pre><code>bucket-name/\n\u251c\u2500\u2500 raw/              # Dane surowe\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2502   \u251c\u2500\u2500 01/\n\u2502   \u2502   \u251c\u2500\u2500 02/\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 processed/        # Dane przekszta\u0142cone\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 analytics/        # Dane do analizy\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 archive/          # Archiwa\n    \u2514\u2500\u2500 ...\n</code></pre></p> <p>Organizacja wed\u0142ug typu : <pre><code>bucket-name/\n\u251c\u2500\u2500 csv/\n\u251c\u2500\u2500 json/\n\u251c\u2500\u2500 parquet/\n\u2514\u2500\u2500 logs/\n</code></pre></p>"},{"location":"Cloud/AWS/PL/02-s3/#prefiksy-i-foldery","title":"Prefiksy i foldery","text":"<p>S3 nie ma \"prawdziwych\" folder\u00f3w, ale u\u017cywa prefiks\u00f3w :</p> <ul> <li><code>data/2024/01/file.csv</code> = Prefiks <code>data/2024/01/</code></li> <li>Interfejs web symuluje foldery</li> <li>U\u017cywa\u0107 <code>/</code> do organizacji</li> </ul> <p>Dobre praktyki : - U\u017cywa\u0107 sp\u00f3jnych prefiks\u00f3w - Uwzgl\u0119dnia\u0107 dat\u0119 w \u015bcie\u017cce - Rozdziela\u0107 wed\u0142ug typu danych</p>"},{"location":"Cloud/AWS/PL/02-s3/#integracja-z-innymi-usugami","title":"Integracja z innymi us\u0142ugami","text":""},{"location":"Cloud/AWS/PL/02-s3/#s3-aws-glue","title":"S3 + AWS Glue","text":"<p>U\u017cycie : - S3 jako \u017ar\u00f3d\u0142o danych - Glue przekszta\u0142ca dane - Wynik do S3 lub innej destynacji</p> <p>Przyk\u0142ad : <pre><code># Job Glue czyta z S3\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"my_database\",\n    table_name = \"s3_data\"\n)\n</code></pre></p>"},{"location":"Cloud/AWS/PL/02-s3/#s3-amazon-athena","title":"S3 + Amazon Athena","text":"<p>U\u017cycie : - Zapytania SQL bezpo\u015brednio na plikach S3 - Nie potrzeba \u0142adowa\u0107 do bazy danych - Pay-per-query</p> <p>Przyk\u0142ad : <pre><code>-- Utworzy\u0107 tabel\u0119 zewn\u0119trzn\u0105 wskazuj\u0105c\u0105 na S3\nCREATE EXTERNAL TABLE my_table (\n    id INT,\n    name STRING\n)\nSTORED AS PARQUET\nLOCATION 's3://my-bucket/data/';\n</code></pre></p>"},{"location":"Cloud/AWS/PL/02-s3/#s3-amazon-redshift","title":"S3 + Amazon Redshift","text":"<p>U\u017cycie : - S3 jako \u017ar\u00f3d\u0142o dla COPY - Redshift jako data warehouse - Szybkie \u0142adowanie du\u017cych ilo\u015bci</p> <p>Przyk\u0142ad : <pre><code>COPY my_table\nFROM 's3://my-bucket/data/file.csv'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV;\n</code></pre></p>"},{"location":"Cloud/AWS/PL/02-s3/#s3-aws-lambda","title":"S3 + AWS Lambda","text":"<p>U\u017cycie : - Wyzwala\u0107 Lambda przy przes\u0142aniu - Automatyczne przetwarzanie plik\u00f3w - Przekszta\u0142canie, walidacja, etc.</p> <p>Konfiguracja : 1. S3 \u2192 Properties \u2192 Event notifications 2. Utworzy\u0107 powiadomienie 3. Wyzwalacz : \"All object create events\" 4. Destynacja : Funkcja Lambda</p>"},{"location":"Cloud/AWS/PL/02-s3/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/AWS/PL/02-s3/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":"<ol> <li>Nigdy nie czyni\u0107 bucket\u00f3w publicznymi (chyba \u017ce konkretna potrzeba)</li> <li>U\u017cywa\u0107 IAM do kontroli dost\u0119pu</li> <li>W\u0142\u0105czy\u0107 szyfrowanie domy\u015blnie</li> <li>U\u017cywa\u0107 bucket policies dla szczeg\u00f3\u0142owych uprawnie\u0144</li> </ol>"},{"location":"Cloud/AWS/PL/02-s3/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>U\u017cywa\u0107 prefiks\u00f3w do roz\u0142o\u017cenia obci\u0105\u017cenia</li> <li>Unika\u0107 nazw sekwencyjnych (np. file1, file2, file3)</li> <li>U\u017cywa\u0107 Multipart Upload dla du\u017cych plik\u00f3w (&gt;100MB)</li> <li>W\u0142\u0105czy\u0107 Transfer Acceleration je\u015bli potrzeba (p\u0142atne)</li> </ol>"},{"location":"Cloud/AWS/PL/02-s3/#koszty","title":"Koszty","text":"<ol> <li>Monitorowa\u0107 u\u017cycie regularnie</li> <li>U\u017cywa\u0107 odpowiednich klas przechowywania</li> <li>Usuwa\u0107 niepotrzebne pliki</li> <li>Konfigurowa\u0107 automatyczne przej\u015bcia</li> <li>U\u017cywa\u0107 S3 Lifecycle do automatyzacji</li> </ol>"},{"location":"Cloud/AWS/PL/02-s3/#organizacja","title":"Organizacja","text":"<ol> <li>Nazywa\u0107 buckety sp\u00f3jnie</li> <li>U\u017cywa\u0107 tag\u00f3w do organizacji</li> <li>Dokumentowa\u0107 struktur\u0119 danych</li> <li>Tworzy\u0107 konwencje nazewnictwa</li> </ol>"},{"location":"Cloud/AWS/PL/02-s3/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/AWS/PL/02-s3/#przykad-1-przesac-plik-csv","title":"Przyk\u0142ad 1 : Przes\u0142a\u0107 plik CSV","text":"<pre><code>import boto3\nimport pandas as pd\n\n# Utworzy\u0107 klienta S3\ns3 = boto3.client('s3')\n\n# Czyta\u0107 plik lokalny\ndf = pd.read_csv('data.csv')\n\n# Przes\u0142a\u0107 do S3\ns3.upload_file('data.csv', 'my-bucket', 'raw/2024/data.csv')\n</code></pre>"},{"location":"Cloud/AWS/PL/02-s3/#przykad-2-listowac-pliki-z-prefiksu","title":"Przyk\u0142ad 2 : Listowa\u0107 pliki z prefiksu","text":"<pre><code># Listowa\u0107 wszystkie pliki w prefiksie\nresponse = s3.list_objects_v2(\n    Bucket='my-bucket',\n    Prefix='raw/2024/'\n)\n\nfor obj in response.get('Contents', []):\n    print(obj['Key'], obj['Size'])\n</code></pre>"},{"location":"Cloud/AWS/PL/02-s3/#przykad-3-pobrac-i-przetworzyc","title":"Przyk\u0142ad 3 : Pobra\u0107 i przetworzy\u0107","text":"<pre><code># Pobra\u0107 z S3\ns3.download_file('my-bucket', 'raw/data.csv', 'local-data.csv')\n\n# Przetworzy\u0107\ndf = pd.read_csv('local-data.csv')\n# ... przetwarzanie ...\n\n# Przes\u0142a\u0107 wynik\ndf.to_csv('processed-data.csv', index=False)\ns3.upload_file('processed-data.csv', 'my-bucket', 'processed/data.csv')\n</code></pre>"},{"location":"Cloud/AWS/PL/02-s3/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>S3 = Nieograniczone przechowywanie i wysoka dost\u0119pno\u015b\u0107</li> <li>Free Tier : 5 GB zawsze darmowe</li> <li>Organizowa\u0107 z prefiksami dla lepszej wydajno\u015bci</li> <li>Wybra\u0107 odpowiedni\u0105 klas\u0119 wed\u0142ug u\u017cycia</li> <li>S3 integruje si\u0119 ze wszystkimi us\u0142ugami danych AWS</li> </ol>"},{"location":"Cloud/AWS/PL/02-s3/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 3. AWS Glue - ETL Serverless, aby nauczy\u0107 si\u0119 przekszta\u0142ca\u0107 dane z AWS Glue.</p>"},{"location":"Cloud/AWS/PL/03-glue/","title":"3. AWS Glue - ETL Serverless","text":""},{"location":"Cloud/AWS/PL/03-glue/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 AWS Glue i jego rol\u0119 w ETL</li> <li>Tworzy\u0107 crawlery do odkrywania danych</li> <li>Tworzy\u0107 joby ETL z Glue</li> <li>Przekszta\u0142ca\u0107 dane z PySpark</li> <li>Integrowa\u0107 Glue z S3 i innymi us\u0142ugami</li> </ul>"},{"location":"Cloud/AWS/PL/03-glue/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do AWS Glue</li> <li>Utworzy\u0107 Data Catalog</li> <li>Crawlery - Odkrywa\u0107 dane</li> <li>Utworzy\u0107 job ETL</li> <li>Przekszta\u0142canie danych</li> <li>Orkiestracja i planowanie</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#wprowadzenie-do-aws-glue","title":"Wprowadzenie do AWS Glue","text":""},{"location":"Cloud/AWS/PL/03-glue/#czym-jest-aws-glue","title":"Czym jest AWS Glue?","text":"<p>AWS Glue = Zarz\u0105dzana us\u0142uga ETL serverless</p> <ul> <li>ETL : Extract, Transform, Load</li> <li>Serverless : Brak serwer\u00f3w do zarz\u0105dzania</li> <li>Zarz\u0105dzane : AWS zarz\u0105dza infrastruktur\u0105</li> <li>Skalowalne : Automatycznie dostosowuje si\u0119</li> </ul>"},{"location":"Cloud/AWS/PL/03-glue/#komponenty-glue","title":"Komponenty Glue","text":"<ol> <li>Data Catalog : Katalog metadanych</li> <li>Crawlery : Automatycznie odkrywaj\u0105 schematy</li> <li>ETL Jobs : Skrypty przekszta\u0142cania (Python/PySpark)</li> <li>Triggers : Automatyczne wyzwalanie</li> <li>Workflows : Orkiestracja wielu job\u00f3w</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#free-tier-glue","title":"Free Tier Glue","text":"<p>Darmowe na zawsze : - 10 000 obiekt\u00f3w/miesi\u0105c w Data Catalog - 1 milion zapyta\u0144/miesi\u0105c do Data Catalog - 0.44$ za DPU-godzin\u0119 (pierwszy milion darmowy)</p> <p>\u26a0\ufe0f Wa\u017cne : Joby Glue zu\u017cywaj\u0105 DPU (Data Processing Units). Monitorowa\u0107 koszty.</p>"},{"location":"Cloud/AWS/PL/03-glue/#utworzyc-data-catalog","title":"Utworzy\u0107 Data Catalog","text":""},{"location":"Cloud/AWS/PL/03-glue/#czym-jest-data-catalog","title":"Czym jest Data Catalog?","text":"<p>Data Catalog = Scentralizowany katalog metadanych</p> <ul> <li>Schematy danych</li> <li>Lokalizacje (S3, bazy danych)</li> <li>Typy danych</li> <li>Partycje</li> </ul>"},{"location":"Cloud/AWS/PL/03-glue/#struktura-data-catalog","title":"Struktura Data Catalog","text":"<ul> <li>Databases : Grupy tabel</li> <li>Tables : Metadane danych</li> <li>Partitions : Organizacja danych</li> </ul>"},{"location":"Cloud/AWS/PL/03-glue/#utworzyc-baze-danych","title":"Utworzy\u0107 baz\u0119 danych","text":"<ol> <li>Konsola AWS \u2192 Glue \u2192 \"Databases\"</li> <li>\"Add database\"</li> <li>Nazwa : <code>data_analyst_db</code></li> <li>Opis (opcjonalne)</li> <li>\"Create\"</li> </ol> <p>U\u017cycie : - Organizowa\u0107 tabele wed\u0142ug projektu - Przyk\u0142ad : <code>raw_data_db</code>, <code>processed_data_db</code></p>"},{"location":"Cloud/AWS/PL/03-glue/#crawlery-odkrywac-dane","title":"Crawlery - Odkrywa\u0107 dane","text":""},{"location":"Cloud/AWS/PL/03-glue/#czym-jest-crawler","title":"Czym jest Crawler?","text":"<p>Crawler = Us\u0142uga skanuj\u0105ca dane i automatycznie tworz\u0105ca tabele</p> <ul> <li>Analizuje pliki w S3</li> <li>Automatycznie wykrywa schemat</li> <li>Tworzy tabele w Data Catalog</li> <li>Obs\u0142uguje : CSV, JSON, Parquet, etc.</li> </ul>"},{"location":"Cloud/AWS/PL/03-glue/#utworzyc-crawler","title":"Utworzy\u0107 Crawler","text":"<p>Krok 1 : Podstawowa konfiguracja</p> <ol> <li>Glue \u2192 \"Crawlers\" \u2192 \"Add crawler\"</li> <li>Nazwa : <code>s3-csv-crawler</code></li> <li>Opis (opcjonalne)</li> </ol> <p>Krok 2 : \u0179r\u00f3d\u0142o danych</p> <ol> <li>\"Add a data source\"</li> <li>Typ : \"S3\"</li> <li>\u015acie\u017cka S3 : <code>s3://my-bucket/raw/</code></li> <li>Uwzgl\u0119dni\u0107 podfoldery (opcjonalne)</li> </ol> <p>Krok 3 : Rola IAM</p> <ol> <li>Utworzy\u0107 now\u0105 rol\u0119 lub u\u017cy\u0107 istniej\u0105cej</li> <li>Nazwa : <code>AWSGlueServiceRole-default</code></li> <li>Uprawnienia : Dost\u0119p S3 i Glue</li> </ol> <p>Krok 4 : Wyj\u015bcie</p> <ol> <li>Baza danych : <code>data_analyst_db</code></li> <li>Prefiks tabel (opcjonalne)</li> </ol> <p>Krok 5 : Wykona\u0107</p> <ol> <li>\"Run crawler now\" lub zaplanowa\u0107</li> <li>Czeka\u0107 na zako\u0144czenie (kilka minut)</li> <li>Sprawdzi\u0107 utworzone tabele</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#wynik-crawlera","title":"Wynik Crawlera","text":"<p>Automatycznie utworzona tabela : - Wykryte kolumny - Wnioskowane typy danych - Lokalizacja S3 - Format pliku</p> <p>Przyk\u0142ad utworzonej tabeli : <pre><code>Table: raw_data\nColumns:\n  - id (bigint)\n  - name (string)\n  - created_at (timestamp)\nLocation: s3://my-bucket/raw/\nFormat: csv\n</code></pre></p>"},{"location":"Cloud/AWS/PL/03-glue/#utworzyc-job-etl","title":"Utworzy\u0107 job ETL","text":""},{"location":"Cloud/AWS/PL/03-glue/#typy-jobow-glue","title":"Typy job\u00f3w Glue","text":"<ol> <li>Spark : Joby PySpark (zalecane)</li> <li>Python shell : Proste skrypty Python</li> <li>Ray : Zaawansowane przetwarzanie rozproszone</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#utworzyc-job-spark","title":"Utworzy\u0107 job Spark","text":"<p>Krok 1 : Konfiguracja</p> <ol> <li>Glue \u2192 \"ETL jobs\" \u2192 \"Add job\"</li> <li>Nazwa : <code>transform-csv-job</code></li> <li>Rola IAM : <code>AWSGlueServiceRole-default</code></li> <li>Typ : \"Spark\"</li> <li>Wersja Glue : \"4.0\" (zalecane)</li> <li>DPU : 2 (minimum, regulowane)</li> </ol> <p>Krok 2 : \u0179r\u00f3d\u0142o danych</p> <ol> <li>\"Data source\" : Wybra\u0107 tabel\u0119 z Data Catalog</li> <li>Lub : Bezpo\u015brednia \u015bcie\u017cka S3</li> </ol> <p>Krok 3 : Destynacja</p> <ol> <li>\"Data target\" : S3</li> <li>Format : Parquet (zalecane dla analytics)</li> <li>\u015acie\u017cka : <code>s3://my-bucket/processed/</code></li> </ol> <p>Krok 4 : Skrypt</p> <ol> <li>Wygenerowa\u0107 skrypt automatyczny</li> <li>Lub : Napisa\u0107 skrypt niestandardowy</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#podstawowy-skrypt-etl","title":"Podstawowy skrypt ETL","text":"<p>Automatycznie wygenerowany skrypt :</p> <pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Czyta\u0107 z Data Catalog\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"raw_data\"\n)\n\n# Przekszta\u0142ci\u0107 (przyk\u0142ad : filtrowa\u0107)\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"status\"] == \"active\"\n)\n\n# Zapisa\u0107 do S3\nglueContext.write_dynamic_frame.from_options(\n    frame = filtered,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://my-bucket/processed/\"},\n    format = \"parquet\"\n)\n\njob.commit()\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#przeksztacanie-danych","title":"Przekszta\u0142canie danych","text":""},{"location":"Cloud/AWS/PL/03-glue/#czeste-przeksztacenia","title":"Cz\u0119ste przekszta\u0142cenia","text":""},{"location":"Cloud/AWS/PL/03-glue/#1-filtrowac-wiersze","title":"1. Filtrowa\u0107 wiersze","text":"<pre><code>from awsglue.transforms import Filter\n\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"age\"] &gt; 18\n)\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#2-wybierac-kolumny","title":"2. Wybiera\u0107 kolumny","text":"<pre><code>from awsglue.transforms import SelectFields\n\nselected = SelectFields.apply(\n    frame = datasource,\n    paths = [\"id\", \"name\", \"email\"]\n)\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#3-zmieniac-nazwy-kolumn","title":"3. Zmienia\u0107 nazwy kolumn","text":"<pre><code>from awsglue.transforms import RenameField\n\nrenamed = RenameField.apply(\n    frame = datasource,\n    old_name = \"old_column\",\n    new_name = \"new_column\"\n)\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#4-aczyc-dane","title":"4. \u0141\u0105czy\u0107 dane","text":"<pre><code>joined = Join.apply(\n    frame1 = datasource1,\n    frame2 = datasource2,\n    keys1 = [\"id\"],\n    keys2 = [\"user_id\"]\n)\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#5-agregacje","title":"5. Agregacje","text":"<pre><code># Konwertowa\u0107 na DataFrame Spark dla agregacji\ndf = datasource.toDF()\n\naggregated = df.groupBy(\"category\").agg({\n    \"amount\": \"sum\",\n    \"id\": \"count\"\n})\n\n# Konwertowa\u0107 z powrotem na DynamicFrame\nfrom awsglue.dynamicframe import DynamicFrame\nresult = DynamicFrame.fromDF(aggregated, glueContext, \"result\")\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#kompletny-przykad-przeksztacenie-csv-parquet","title":"Kompletny przyk\u0142ad : Przekszta\u0142cenie CSV \u2192 Parquet","text":"<pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# 1. Czyta\u0107 z S3 (przez Data Catalog)\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"raw_data\"\n)\n\n# 2. Filtrowa\u0107 dane\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"status\"] == \"active\"\n)\n\n# 3. Wybiera\u0107 kolumny\nselected = SelectFields.apply(\n    frame = filtered,\n    paths = [\"id\", \"name\", \"email\", \"created_at\"]\n)\n\n# 4. Konwertowa\u0107 na DataFrame dla zaawansowanych przekszta\u0142ce\u0144\ndf = selected.toDF()\n\n# 5. Doda\u0107 kolumn\u0119 obliczon\u0105\nfrom pyspark.sql.functions import col, year\ndf = df.withColumn(\"year\", year(col(\"created_at\")))\n\n# 6. Konwertowa\u0107 z powrotem na DynamicFrame\nfrom awsglue.dynamicframe import DynamicFrame\nresult = DynamicFrame.fromDF(df, glueContext, \"result\")\n\n# 7. Zapisa\u0107 do S3 w Parquet (partycjonowane wed\u0142ug roku)\nglueContext.write_dynamic_frame.from_options(\n    frame = result,\n    connection_type = \"s3\",\n    connection_options = {\n        \"path\": \"s3://my-bucket/processed/\",\n        \"partitionKeys\": [\"year\"]\n    },\n    format = \"parquet\"\n)\n\njob.commit()\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#orkiestracja-i-planowanie","title":"Orkiestracja i planowanie","text":""},{"location":"Cloud/AWS/PL/03-glue/#wyzwolic-job-recznie","title":"Wyzwoli\u0107 job r\u0119cznie","text":"<ol> <li>Glue \u2192 \"ETL jobs\"</li> <li>Wybra\u0107 job</li> <li>\"Run job\"</li> <li>Zobaczy\u0107 logi w czasie rzeczywistym</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#zaplanowac-job-trigger","title":"Zaplanowa\u0107 job (Trigger)","text":"<p>Utworzy\u0107 trigger :</p> <ol> <li>Glue \u2192 \"Triggers\" \u2192 \"Add trigger\"</li> <li>Nazwa : <code>daily-etl-trigger</code></li> <li>Typ : \"Scheduled\"</li> <li>Cz\u0119stotliwo\u015b\u0107 : \"Cron expression\"</li> <li>Przyk\u0142ad : <code>cron(0 2 * * ? *)</code> = Codziennie o 2h</li> <li>Akcje : Wybra\u0107 job do wykonania</li> <li>\"Add\"</li> </ol> <p>Typy trigger\u00f3w : - On-demand : R\u0119czne wyzwalanie - Scheduled : Zaplanowane (cron) - Event-driven : Wyzwalane przez zdarzenie (np. nowy plik S3)</p>"},{"location":"Cloud/AWS/PL/03-glue/#workflows-zozona-orkiestracja","title":"Workflows (z\u0142o\u017cona orkiestracja)","text":"<p>Utworzy\u0107 workflow :</p> <ol> <li>Glue \u2192 \"Workflows\" \u2192 \"Add workflow\"</li> <li>Nazwa : <code>etl-pipeline-workflow</code></li> <li>Doda\u0107 kroki :</li> <li>Crawler \u2192 Job ETL \u2192 Inny Job</li> <li>Zdefiniowa\u0107 zale\u017cno\u015bci</li> <li>Wyzwoli\u0107 workflow</li> </ol> <p>Przyk\u0142ad workflow : <pre><code>1. Crawler S3 \u2192 Odkrywa nowe pliki\n2. Job ETL 1 \u2192 Przekszta\u0142ca surowe dane\n3. Job ETL 2 \u2192 Agreguje dane\n4. Job ETL 3 \u2192 \u0141aduje do Redshift\n</code></pre></p>"},{"location":"Cloud/AWS/PL/03-glue/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/AWS/PL/03-glue/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>U\u017cywa\u0107 Parquet zamiast CSV (szybsze)</li> <li>Partycjonowa\u0107 dane (poprawia wydajno\u015b\u0107)</li> <li>Dostosowa\u0107 DPU wed\u0142ug rozmiaru danych</li> <li>U\u017cywa\u0107 cache Spark do ponownego u\u017cycia danych</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#koszty","title":"Koszty","text":"<ol> <li>Monitorowa\u0107 DPU-godziny u\u017cywane</li> <li>Optymalizowa\u0107 skrypty do zmniejszenia czasu wykonania</li> <li>U\u017cywa\u0107 odpowiednich klas S3 (Standard-IA dla archiw\u00f3w)</li> <li>Zatrzymywa\u0107 joby kt\u00f3re szybko ko\u0144cz\u0105 si\u0119 niepowodzeniem</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#organizacja","title":"Organizacja","text":"<ol> <li>Nazywa\u0107 joby sp\u00f3jnie</li> <li>Dokumentowa\u0107 przekszta\u0142cenia</li> <li>Wersjonowa\u0107 skrypty (Git)</li> <li>Testowa\u0107 lokalnie przed wdro\u017ceniem</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/AWS/PL/03-glue/#przykad-1-przeksztacic-csv-parquet","title":"Przyk\u0142ad 1 : Przekszta\u0142ci\u0107 CSV \u2192 Parquet","text":"<pre><code># Czyta\u0107 CSV z S3\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"raw_csv_data\"\n)\n\n# Zapisa\u0107 w Parquet\nglueContext.write_dynamic_frame.from_options(\n    frame = datasource,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://my-bucket/parquet/\"},\n    format = \"parquet\"\n)\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#przykad-2-czyscic-i-walidowac","title":"Przyk\u0142ad 2 : Czy\u015bci\u0107 i walidowa\u0107","text":"<pre><code># Filtrowa\u0107 nieprawid\u0142owe wiersze\ncleaned = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"email\"] is not None and \"@\" in x[\"email\"]\n)\n\n# Usun\u0105\u0107 duplikaty (przez DataFrame)\ndf = cleaned.toDF()\ndf = df.dropDuplicates([\"id\"])\n\nresult = DynamicFrame.fromDF(df, glueContext, \"result\")\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#przykad-3-aczyc-wiele-zrode","title":"Przyk\u0142ad 3 : \u0141\u0105czy\u0107 wiele \u017ar\u00f3de\u0142","text":"<pre><code># Czyta\u0107 dwie tabele\nusers = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"users\"\n)\n\norders = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"orders\"\n)\n\n# \u0141\u0105czy\u0107\njoined = Join.apply(\n    frame1 = users,\n    frame2 = orders,\n    keys1 = [\"id\"],\n    keys2 = [\"user_id\"]\n)\n</code></pre>"},{"location":"Cloud/AWS/PL/03-glue/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Glue = ETL serverless zarz\u0105dzane przez AWS</li> <li>Crawlery automatycznie odkrywaj\u0105 schematy</li> <li>Joby ETL u\u017cywaj\u0105 PySpark do przekszta\u0142ce\u0144</li> <li>Data Catalog centralizuje metadane</li> <li>Triggers umo\u017cliwiaj\u0105 automatyzacj\u0119</li> </ol>"},{"location":"Cloud/AWS/PL/03-glue/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 4. Amazon Redshift - Data Warehouse, aby nauczy\u0107 si\u0119 u\u017cywa\u0107 Redshift do analizy danych.</p>"},{"location":"Cloud/AWS/PL/04-redshift/","title":"4. Amazon Redshift - Data Warehouse","text":""},{"location":"Cloud/AWS/PL/04-redshift/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Amazon Redshift i jego rol\u0119</li> <li>Utworzy\u0107 klaster Redshift (darmowy 2 miesi\u0105ce)</li> <li>\u0141adowa\u0107 dane do Redshift</li> <li>Optymalizowa\u0107 zapytania Redshift</li> <li>Integrowa\u0107 z S3 i innymi us\u0142ugami</li> </ul>"},{"location":"Cloud/AWS/PL/04-redshift/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Redshift</li> <li>Utworzy\u0107 klaster Redshift</li> <li>\u0141adowa\u0107 dane</li> <li>Zaawansowane zapytania SQL</li> <li>Optymalizacja</li> <li>Integracja z innymi us\u0142ugami</li> </ol>"},{"location":"Cloud/AWS/PL/04-redshift/#wprowadzenie-do-redshift","title":"Wprowadzenie do Redshift","text":""},{"location":"Cloud/AWS/PL/04-redshift/#czym-jest-amazon-redshift","title":"Czym jest Amazon Redshift?","text":"<p>Amazon Redshift = Zarz\u0105dzany data warehouse w chmurze</p> <ul> <li>OLAP : Zoptymalizowany do analizy (nie transakcje)</li> <li>Kolumnowy : Przechowywanie zorientowane na kolumny</li> <li>Masowo r\u00f3wnoleg\u0142y : Przetwarzanie rozproszone</li> <li>Skalowalny : Od kilku GB do kilku PB</li> </ul>"},{"location":"Cloud/AWS/PL/04-redshift/#przypadki-uzycia-dla-data-analyst","title":"Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Data Warehouse : Centralizowa\u0107 dane</li> <li>Analytics : Z\u0142o\u017cone zapytania na du\u017cych wolumenach</li> <li>Business Intelligence : Pulpity nawigacyjne i raporty</li> <li>Data Mining : Dog\u0142\u0119bne analizy</li> </ul>"},{"location":"Cloud/AWS/PL/04-redshift/#free-tier-redshift","title":"Free Tier Redshift","text":"<p>Darmowy 2 miesi\u0105ce : - 750 godzin/miesi\u0105c klastra <code>dc2.large</code> - 32 GB przechowywania na w\u0119ze\u0142 - Po 2 miesi\u0105cach : normalne rozliczanie</p> <p>\u26a0\ufe0f Wa\u017cne : Zatrzyma\u0107 klaster gdy nieu\u017cywany, aby unikn\u0105\u0107 koszt\u00f3w.</p>"},{"location":"Cloud/AWS/PL/04-redshift/#utworzyc-klaster-redshift","title":"Utworzy\u0107 klaster Redshift","text":""},{"location":"Cloud/AWS/PL/04-redshift/#krok-1-dostep-do-redshift","title":"Krok 1 : Dost\u0119p do Redshift","text":"<ol> <li>Konsola AWS \u2192 Szuka\u0107 \"Redshift\"</li> <li>Klikn\u0105\u0107 \"Amazon Redshift\"</li> <li>\"Create cluster\"</li> </ol>"},{"location":"Cloud/AWS/PL/04-redshift/#krok-2-konfiguracja-klastra","title":"Krok 2 : Konfiguracja klastra","text":"<p>Podstawowa konfiguracja :</p> <ol> <li>Cluster identifier : <code>data-analyst-cluster</code></li> <li>Node type : <code>dc2.large</code> (darmowy 2 miesi\u0105ce)</li> <li>Number of nodes : 1 (wystarczaj\u0105ce do rozpocz\u0119cia)</li> <li>Database name : <code>analytics</code> (domy\u015blnie : <code>dev</code>)</li> <li>Database port : 5439 (domy\u015blnie)</li> <li>Master username : <code>admin</code> (lub inny)</li> <li>Master password : Silne has\u0142o</li> </ol> <p>Konfiguracja sieci :</p> <ol> <li>VPC : Wybra\u0107 istniej\u0105cy VPC</li> <li>Subnet group : Utworzy\u0107 lub u\u017cy\u0107 istniej\u0105cego</li> <li>Publicly accessible : \u2705 Tak (dla \u0142atwego dost\u0119pu)</li> <li>Availability zone : Wybra\u0107 stref\u0119</li> </ol> <p>Bezpiecze\u0144stwo :</p> <ol> <li>VPC security groups : Utworzy\u0107 grup\u0119 bezpiecze\u0144stwa</li> <li>Zezwoli\u0107 port 5439 z Twojego IP</li> <li>Encryption : W\u0142\u0105czy\u0107 (zalecane)</li> </ol>"},{"location":"Cloud/AWS/PL/04-redshift/#krok-3-utworzyc-klaster","title":"Krok 3 : Utworzy\u0107 klaster","text":"<ol> <li>Klikn\u0105\u0107 \"Create cluster\"</li> <li>Czeka\u0107 5-10 minut (tworzenie)</li> <li>Klaster gotowy gdy status = \"Available\"</li> </ol> <p>\u26a0\ufe0f Wa\u017cne : Zanotowa\u0107 endpoint klastra (np. <code>data-analyst-cluster.xxxxx.eu-west-3.redshift.amazonaws.com:5439</code>)</p>"},{"location":"Cloud/AWS/PL/04-redshift/#adowac-dane","title":"\u0141adowa\u0107 dane","text":""},{"location":"Cloud/AWS/PL/04-redshift/#metoda-1-copy-z-s3-zalecane","title":"Metoda 1 : COPY z S3 (zalecane)","text":"<p>Najszybsze dla du\u017cych ilo\u015bci :</p> <pre><code>-- Utworzy\u0107 tabel\u0119\nCREATE TABLE users (\n    id INTEGER,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at TIMESTAMP\n);\n\n-- \u0141adowa\u0107 z S3\nCOPY users\nFROM 's3://my-bucket/data/users.csv'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV\nIGNOREHEADER 1;\n</code></pre> <p>Konfiguracja roli IAM :</p> <ol> <li>IAM \u2192 \"Roles\" \u2192 \"Create role\"</li> <li>Typ : \"Redshift\"</li> <li>Do\u0142\u0105czy\u0107 polityk\u0119 : <code>AmazonS3ReadOnlyAccess</code></li> <li>Nazwa : <code>RedshiftS3Role</code></li> <li>Skopiowa\u0107 ARN dla COPY</li> </ol>"},{"location":"Cloud/AWS/PL/04-redshift/#metoda-2-insert-mae-ilosci","title":"Metoda 2 : INSERT (ma\u0142e ilo\u015bci)","text":"<pre><code>INSERT INTO users (id, name, email, created_at)\nVALUES (1, 'John Doe', 'john@example.com', '2024-01-01');\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#metoda-3-insert-z-zapytania","title":"Metoda 3 : INSERT z zapytania","text":"<pre><code>INSERT INTO users_aggregated\nSELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count\nFROM users\nGROUP BY DATE_TRUNC('month', created_at);\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#obsugiwane-formaty","title":"Obs\u0142ugiwane formaty","text":"<ul> <li>CSV : Pliki CSV</li> <li>JSON : Pliki JSON</li> <li>Parquet : Format zoptymalizowany (zalecane)</li> <li>Avro : Format Avro</li> </ul>"},{"location":"Cloud/AWS/PL/04-redshift/#zaawansowane-zapytania-sql","title":"Zaawansowane zapytania SQL","text":""},{"location":"Cloud/AWS/PL/04-redshift/#funkcje-analityczne","title":"Funkcje analityczne","text":"<p>Funkcje okna :</p> <pre><code>-- ROW_NUMBER\nSELECT \n    id,\n    name,\n    ROW_NUMBER() OVER (PARTITION BY category ORDER BY created_at) AS rank\nFROM products;\n\n-- LAG/LEAD\nSELECT \n    date,\n    sales,\n    LAG(sales, 1) OVER (ORDER BY date) AS previous_sales,\n    LEAD(sales, 1) OVER (ORDER BY date) AS next_sales\nFROM daily_sales;\n\n-- RANK\nSELECT \n    user_id,\n    total_spent,\n    RANK() OVER (ORDER BY total_spent DESC) AS spending_rank\nFROM user_totals;\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#zozone-agregacje","title":"Z\u0142o\u017cone agregacje","text":"<pre><code>-- GROUP BY z ROLLUP\nSELECT \n    category,\n    region,\n    SUM(amount) AS total\nFROM sales\nGROUP BY ROLLUP(category, region);\n\n-- GROUP BY z CUBE\nSELECT \n    category,\n    region,\n    SUM(amount) AS total\nFROM sales\nGROUP BY CUBE(category, region);\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#zoptymalizowane-zaczenia","title":"Zoptymalizowane z\u0142\u0105czenia","text":"<pre><code>-- Z\u0142\u0105czenie z kluczem dystrybucji\nSELECT \n    u.name,\n    o.amount,\n    o.created_at\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01';\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#optymalizacja","title":"Optymalizacja","text":""},{"location":"Cloud/AWS/PL/04-redshift/#klucze-dystrybucji","title":"Klucze dystrybucji","text":"<p>Wybra\u0107 odpowiedni klucz dystrybucji :</p> <pre><code>-- Dystrybucja wed\u0142ug klucza (dla z\u0142\u0105cze\u0144)\nCREATE TABLE users (\n    id INTEGER DISTKEY,\n    name VARCHAR(100),\n    email VARCHAR(100)\n);\n\n-- Dystrybucja ALL (dla ma\u0142ych tabel)\nCREATE TABLE categories (\n    id INTEGER,\n    name VARCHAR(100)\n) DISTSTYLE ALL;\n\n-- Dystrybucja EVEN (domy\u015blnie)\nCREATE TABLE logs (\n    id INTEGER,\n    message TEXT\n) DISTSTYLE EVEN;\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#klucze-sortowania","title":"Klucze sortowania","text":"<p>Poprawi\u0107 wydajno\u015b\u0107 zapyta\u0144 :</p> <pre><code>-- Prosty klucz sortowania\nCREATE TABLE orders (\n    id INTEGER,\n    user_id INTEGER,\n    created_at TIMESTAMP,\n    amount DECIMAL(10,2)\n) SORTKEY (created_at);\n\n-- Z\u0142o\u017cony klucz sortowania\nCREATE TABLE sales (\n    date DATE,\n    region VARCHAR(50),\n    amount DECIMAL(10,2)\n) SORTKEY (date, region);\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#kompresja","title":"Kompresja","text":"<p>Zmniejszy\u0107 przestrze\u0144 przechowywania :</p> <pre><code>-- Automatyczna kompresja\nCREATE TABLE users (\n    id INTEGER,\n    name VARCHAR(100) ENCODE lzo,\n    email VARCHAR(100) ENCODE lzo,\n    created_at TIMESTAMP ENCODE delta\n);\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#analyze","title":"ANALYZE","text":"<p>Aktualizowa\u0107 statystyki :</p> <pre><code>-- Analizowa\u0107 tabel\u0119\nANALYZE users;\n\n-- Analizowa\u0107 wszystkie tabele\nANALYZE;\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#integracja-z-innymi-usugami","title":"Integracja z innymi us\u0142ugami","text":""},{"location":"Cloud/AWS/PL/04-redshift/#redshift-s3","title":"Redshift + S3","text":"<p>Unload do S3 :</p> <pre><code>UNLOAD ('SELECT * FROM users WHERE created_at &gt; ''2024-01-01''')\nTO 's3://my-bucket/exports/users/'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV\nPARALLEL OFF;\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#redshift-glue","title":"Redshift + Glue","text":"<p>Glue mo\u017ce \u0142adowa\u0107 do Redshift :</p> <pre><code># W jobie Glue\nglueContext.write_dynamic_frame.from_jdbc_conf(\n    frame = transformed_data,\n    catalog_connection = \"redshift-connection\",\n    connection_options = {\n        \"dbtable\": \"users\",\n        \"database\": \"analytics\"\n    }\n)\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#redshift-quicksight","title":"Redshift + QuickSight","text":"<p>Po\u0142\u0105czy\u0107 QuickSight z Redshift :</p> <ol> <li>QuickSight \u2192 \"Data sources\"</li> <li>\"Redshift\"</li> <li>Wprowadzi\u0107 informacje po\u0142\u0105czenia</li> <li>Wybra\u0107 tabele</li> <li>Tworzy\u0107 wizualizacje</li> </ol>"},{"location":"Cloud/AWS/PL/04-redshift/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/AWS/PL/04-redshift/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>U\u017cywa\u0107 COPY zamiast INSERT dla du\u017cych ilo\u015bci</li> <li>Wybra\u0107 odpowiednie klucze dystrybucji</li> <li>U\u017cywa\u0107 kluczy sortowania dla cz\u0119stych zapyta\u0144</li> <li>Kompresowa\u0107 kolumny aby oszcz\u0119dzi\u0107 przestrze\u0144</li> <li>VACUUM regularnie aby zoptymalizowa\u0107</li> </ol>"},{"location":"Cloud/AWS/PL/04-redshift/#koszty","title":"Koszty","text":"<ol> <li>Zatrzyma\u0107 klaster gdy nieu\u017cywany</li> <li>U\u017cywa\u0107 odpowiedniego typu w\u0119z\u0142a wed\u0142ug potrzeb</li> <li>Monitorowa\u0107 u\u017cycie przechowywania</li> <li>Czy\u015bci\u0107 dane niepotrzebne</li> </ol>"},{"location":"Cloud/AWS/PL/04-redshift/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":"<ol> <li>Szyfrowa\u0107 dane w tranzycie i w spoczynku</li> <li>U\u017cywa\u0107 VPC aby izolowa\u0107 klaster</li> <li>Ogranicza\u0107 dost\u0119p z grupami bezpiecze\u0144stwa</li> <li>Audytowa\u0107 dost\u0119p z CloudTrail</li> </ol>"},{"location":"Cloud/AWS/PL/04-redshift/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/AWS/PL/04-redshift/#przykad-1-kompletny-pipeline-s3-redshift","title":"Przyk\u0142ad 1 : Kompletny pipeline S3 \u2192 Redshift","text":"<pre><code>-- 1. Utworzy\u0107 tabel\u0119\nCREATE TABLE sales (\n    id INTEGER,\n    product_id INTEGER,\n    amount DECIMAL(10,2),\n    sale_date DATE\n) DISTKEY(product_id) SORTKEY(sale_date);\n\n-- 2. \u0141adowa\u0107 z S3\nCOPY sales\nFROM 's3://my-bucket/data/sales/'\nIAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'\nCSV\nIGNOREHEADER 1;\n\n-- 3. Analizowa\u0107\nANALYZE sales;\n\n-- 4. Zapytania analityczne\nSELECT \n    DATE_TRUNC('month', sale_date) AS month,\n    SUM(amount) AS total_sales\nFROM sales\nGROUP BY DATE_TRUNC('month', sale_date)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#przykad-2-agregacje-z-funkcjami-okna","title":"Przyk\u0142ad 2 : Agregacje z funkcjami okna","text":"<pre><code>-- Top 10 produkt\u00f3w na miesi\u0105c\nSELECT \n    product_id,\n    month,\n    total_sales,\n    RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) AS rank\nFROM (\n    SELECT \n        product_id,\n        DATE_TRUNC('month', sale_date) AS month,\n        SUM(amount) AS total_sales\n    FROM sales\n    GROUP BY product_id, DATE_TRUNC('month', sale_date)\n) monthly_sales\nWHERE RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) &lt;= 10;\n</code></pre>"},{"location":"Cloud/AWS/PL/04-redshift/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Redshift = Data warehouse dla analytics</li> <li>Free Tier : 2 miesi\u0105ce darmowe (750 godzin)</li> <li>COPY z S3 = najszybsza metoda</li> <li>Klucze dystrybucji i sortowania = klucze wydajno\u015bci</li> <li>Zatrzyma\u0107 klaster gdy nieu\u017cywany</li> </ol>"},{"location":"Cloud/AWS/PL/04-redshift/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 5. Amazon Athena - Zapytania SQL na S3, aby nauczy\u0107 si\u0119 bezpo\u015brednio odpytywa\u0107 pliki S3.</p>"},{"location":"Cloud/AWS/PL/05-athena/","title":"5. Amazon Athena - Zapytania SQL na S3","text":""},{"location":"Cloud/AWS/PL/05-athena/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Amazon Athena i jego u\u017cycie</li> <li>Tworzy\u0107 tabele zewn\u0119trzne wskazuj\u0105ce na S3</li> <li>Wykonywa\u0107 zapytania SQL na plikach S3</li> <li>Optymalizowa\u0107 koszty i wydajno\u015b\u0107</li> <li>Integrowa\u0107 z Glue Data Catalog</li> </ul>"},{"location":"Cloud/AWS/PL/05-athena/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Athena</li> <li>Tworzy\u0107 tabele zewn\u0119trzne</li> <li>Wykonywa\u0107 zapytania</li> <li>Optymalizacja koszt\u00f3w</li> <li>Integracja z Glue</li> <li>Dobre praktyki</li> </ol>"},{"location":"Cloud/AWS/PL/05-athena/#wprowadzenie-do-athena","title":"Wprowadzenie do Athena","text":""},{"location":"Cloud/AWS/PL/05-athena/#czym-jest-amazon-athena","title":"Czym jest Amazon Athena?","text":"<p>Amazon Athena = Us\u0142uga zapyta\u0144 SQL serverless na S3</p> <ul> <li>Serverless : Brak infrastruktury do zarz\u0105dzania</li> <li>Pay-per-query : P\u0142acisz tylko za u\u017cycie</li> <li>Standard SQL : Standardowa sk\u0142adnia SQL</li> <li>Bezpo\u015brednio na S3 : Nie potrzeba \u0142adowa\u0107 do bazy danych</li> </ul>"},{"location":"Cloud/AWS/PL/05-athena/#przypadki-uzycia-dla-data-analyst","title":"Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Eksploracja danych : Szybko analizowa\u0107 pliki S3</li> <li>Zapytania Data Lake : Zapytania na data lake</li> <li>Analizy ad-hoc : Analizy jednorazowe</li> <li>Analiza log\u00f3w : Analizowa\u0107 logi przechowywane w S3</li> </ul>"},{"location":"Cloud/AWS/PL/05-athena/#free-tier-athena","title":"Free Tier Athena","text":"<p>Darmowe na zawsze : - 10 GB danych przeskanowanych/miesi\u0105c - Poza tym : 5$ za Terabajt przeskanowany</p> <p>\u26a0\ufe0f Wa\u017cne : Koszty zale\u017c\u0105 od ilo\u015bci przeskanowanych danych. Optymalizowa\u0107 zapytania aby zmniejszy\u0107 koszty.</p>"},{"location":"Cloud/AWS/PL/05-athena/#tworzyc-tabele-zewnetrzne","title":"Tworzy\u0107 tabele zewn\u0119trzne","text":""},{"location":"Cloud/AWS/PL/05-athena/#metoda-1-przez-edytor-athena","title":"Metoda 1 : Przez edytor Athena","text":"<p>Krok 1 : Dost\u0119p do Athena</p> <ol> <li>Konsola AWS \u2192 Szuka\u0107 \"Athena\"</li> <li>Klikn\u0105\u0107 \"Amazon Athena\"</li> <li>Pierwsze u\u017cycie : Skonfigurowa\u0107 wynik S3</li> </ol> <p>Krok 2 : Skonfigurowa\u0107 wynik</p> <ol> <li>\"Settings\" \u2192 \"Manage\"</li> <li>\"Query result location\" : <code>s3://my-bucket/athena-results/</code></li> <li>\"Save\"</li> </ol> <p>Krok 3 : Utworzy\u0107 tabel\u0119</p> <pre><code>-- Tabela dla plik\u00f3w CSV\nCREATE EXTERNAL TABLE users (\n    id INT,\n    name STRING,\n    email STRING,\n    created_at TIMESTAMP\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nWITH SERDEPROPERTIES (\n    'serialization.format' = ',',\n    'field.delim' = ','\n)\nSTORED AS TEXTFILE\nLOCATION 's3://my-bucket/data/users/'\nTBLPROPERTIES ('skip.header.line.count'='1');\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#metoda-2-przez-glue-data-catalog-zalecane","title":"Metoda 2 : Przez Glue Data Catalog (zalecane)","text":"<p>U\u017cywa\u0107 tabel utworzonych przez Glue :</p> <ol> <li>Glue \u2192 Utworzy\u0107 crawler dla S3</li> <li>Crawler automatycznie tworzy tabel\u0119</li> <li>Athena u\u017cywa bezpo\u015brednio tej tabeli</li> </ol> <p>Zalety : - Automatycznie wykryty schemat - Nie potrzeba definiowa\u0107 r\u0119cznie - Wykorzystywane przez inne us\u0142ugi</p>"},{"location":"Cloud/AWS/PL/05-athena/#obsugiwane-formaty","title":"Obs\u0142ugiwane formaty","text":"<p>CSV : <pre><code>CREATE EXTERNAL TABLE csv_data (\n    col1 STRING,\n    col2 INT\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/csv/';\n</code></pre></p> <p>JSON : <pre><code>CREATE EXTERNAL TABLE json_data (\n    id INT,\n    name STRING\n)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/json/';\n</code></pre></p> <p>Parquet (zalecane) : <pre><code>CREATE EXTERNAL TABLE parquet_data (\n    id INT,\n    name STRING,\n    created_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket/parquet/';\n</code></pre></p>"},{"location":"Cloud/AWS/PL/05-athena/#wykonywac-zapytania","title":"Wykonywa\u0107 zapytania","text":""},{"location":"Cloud/AWS/PL/05-athena/#podstawowe-zapytania","title":"Podstawowe zapytania","text":"<p>Prosty SELECT :</p> <pre><code>SELECT * FROM users LIMIT 10;\n</code></pre> <p>Filtrowa\u0107 :</p> <pre><code>SELECT \n    id,\n    name,\n    email\nFROM users\nWHERE created_at &gt; DATE '2024-01-01'\nORDER BY created_at DESC;\n</code></pre> <p>Agregacje :</p> <pre><code>SELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count,\n    COUNT(DISTINCT email) AS unique_emails\nFROM users\nGROUP BY DATE_TRUNC('month', created_at)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#zaawansowane-zapytania","title":"Zaawansowane zapytania","text":"<p>Funkcje okna :</p> <pre><code>SELECT \n    id,\n    name,\n    created_at,\n    ROW_NUMBER() OVER (PARTITION BY DATE_TRUNC('month', created_at) ORDER BY created_at) AS rank\nFROM users;\n</code></pre> <p>Z\u0142\u0105czenia :</p> <pre><code>SELECT \n    u.name,\n    o.amount,\n    o.created_at\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.created_at &gt; DATE '2024-01-01';\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#zapytania-na-partycjach","title":"Zapytania na partycjach","text":"<p>Je\u015bli dane s\u0105 partycjonowane :</p> <pre><code>-- Tabela partycjonowana wed\u0142ug daty\nCREATE EXTERNAL TABLE sales (\n    id INT,\n    product_id INT,\n    amount DECIMAL(10,2)\n)\nPARTITIONED BY (sale_date DATE)\nSTORED AS PARQUET\nLOCATION 's3://bucket/sales/';\n\n-- Doda\u0107 partycje\nALTER TABLE sales ADD PARTITION (sale_date='2024-01-01')\nLOCATION 's3://bucket/sales/year=2024/month=01/day=01/';\n\n-- Zapytanie z partycj\u0105 (szybsze i ta\u0144sze)\nSELECT * FROM sales\nWHERE sale_date = DATE '2024-01-01';\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#optymalizacja-kosztow","title":"Optymalizacja koszt\u00f3w","text":""},{"location":"Cloud/AWS/PL/05-athena/#zmniejszyc-przeskanowane-dane","title":"Zmniejszy\u0107 przeskanowane dane","text":"<p>1. U\u017cywa\u0107 WHERE aby filtrowa\u0107 wcze\u015bnie :</p> <pre><code>-- \u274c Z\u0142e : Skanuje wszystko potem filtruje\nSELECT * FROM large_table\nWHERE date = '2024-01-01';\n\n-- \u2705 Dobre : Filtruje od pocz\u0105tku (je\u015bli partycjonowane)\nSELECT * FROM large_table\nWHERE date = '2024-01-01';\n</code></pre> <p>2. Wybiera\u0107 tylko potrzebne kolumny :</p> <pre><code>-- \u274c Z\u0142e : Skanuje wszystkie kolumny\nSELECT * FROM large_table;\n\n-- \u2705 Dobre : Skanuje tylko potrzebne kolumny\nSELECT id, name FROM large_table;\n</code></pre> <p>3. U\u017cywa\u0107 LIMIT :</p> <pre><code>-- Ograniczy\u0107 liczb\u0119 wynik\u00f3w\nSELECT * FROM large_table LIMIT 100;\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#uzywac-parquet","title":"U\u017cywa\u0107 Parquet","text":"<p>Parquet jest bardziej efektywny ni\u017c CSV :</p> <ul> <li>Kompresja : Mniej przeskanowanych danych</li> <li>Kolumny : Skanuje tylko potrzebne kolumny</li> <li>Zmniejszone koszty : Do 90% redukcji</li> </ul> <p>Konwertowa\u0107 CSV \u2192 Parquet z Glue :</p> <pre><code># Job Glue do konwersji\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_analyst_db\",\n    table_name = \"csv_data\"\n)\n\nglueContext.write_dynamic_frame.from_options(\n    frame = datasource,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bucket/parquet/\"},\n    format = \"parquet\"\n)\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#partycjonowac-dane","title":"Partycjonowa\u0107 dane","text":"<p>Partycjonowa\u0107 wed\u0142ug daty (zalecane) :</p> <pre><code>s3://bucket/data/\n\u251c\u2500\u2500 year=2024/\n\u2502   \u251c\u2500\u2500 month=01/\n\u2502   \u2502   \u251c\u2500\u2500 day=01/\n\u2502   \u2502   \u2514\u2500\u2500 day=02/\n\u2502   \u2514\u2500\u2500 month=02/\n</code></pre> <p>Utworzy\u0107 tabel\u0119 partycjonowan\u0105 :</p> <pre><code>CREATE EXTERNAL TABLE partitioned_data (\n    id INT,\n    name STRING\n)\nPARTITIONED BY (year INT, month INT, day INT)\nSTORED AS PARQUET\nLOCATION 's3://bucket/data/';\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#integracja-z-glue","title":"Integracja z Glue","text":""},{"location":"Cloud/AWS/PL/05-athena/#uzywac-tabel-glue","title":"U\u017cywa\u0107 tabel Glue","text":"<p>Tabele utworzone przez Glue s\u0105 automatycznie dost\u0119pne w Athena :</p> <ol> <li>Glue \u2192 Crawler tworzy tabel\u0119</li> <li>Athena \u2192 \"Tables\" \u2192 Zobaczy\u0107 wszystkie tabele Glue</li> <li>U\u017cywa\u0107 bezpo\u015brednio w zapytaniach</li> </ol> <p>Zalety : - Automatyczny schemat - Brak r\u0119cznej definicji - Automatyczna synchronizacja</p>"},{"location":"Cloud/AWS/PL/05-athena/#aktualizowac-partycje","title":"Aktualizowa\u0107 partycje","text":"<p>Je\u015bli dodane nowe dane :</p> <pre><code>-- Aktualizowa\u0107 partycje\nMSCK REPAIR TABLE sales;\n\n-- Lub doda\u0107 r\u0119cznie\nALTER TABLE sales ADD PARTITION (sale_date='2024-01-02')\nLOCATION 's3://bucket/sales/year=2024/month=01/day=02/';\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/AWS/PL/05-athena/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>U\u017cywa\u0107 Parquet zamiast CSV</li> <li>Partycjonowa\u0107 dane wed\u0142ug daty/kategorii</li> <li>Wybiera\u0107 tylko potrzebne kolumny</li> <li>Filtrowa\u0107 wcze\u015bnie z WHERE</li> <li>U\u017cywa\u0107 LIMIT do eksploracji</li> </ol>"},{"location":"Cloud/AWS/PL/05-athena/#koszty","title":"Koszty","text":"<ol> <li>Monitorowa\u0107 przeskanowane dane w wynikach</li> <li>Optymalizowa\u0107 zapytania aby zmniejszy\u0107 skanowanie</li> <li>U\u017cywa\u0107 Parquet do kompresji</li> <li>Partycjonowa\u0107 aby zmniejszy\u0107 skanowanie</li> <li>Buforowa\u0107 cz\u0119ste wyniki</li> </ol>"},{"location":"Cloud/AWS/PL/05-athena/#organizacja","title":"Organizacja","text":"<ol> <li>Organizowa\u0107 S3 ze sp\u00f3jnymi prefiksami</li> <li>Nazywa\u0107 tabele jasno</li> <li>Dokumentowa\u0107 schematy</li> <li>U\u017cywa\u0107 baz danych do organizacji</li> </ol>"},{"location":"Cloud/AWS/PL/05-athena/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/AWS/PL/05-athena/#przykad-1-analizowac-logi","title":"Przyk\u0142ad 1 : Analizowa\u0107 logi","text":"<pre><code>-- Tabela dla log\u00f3w\nCREATE EXTERNAL TABLE logs (\n    timestamp TIMESTAMP,\n    level STRING,\n    message STRING,\n    user_id INT\n)\nPARTITIONED BY (date DATE)\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/logs/';\n\n-- Zapytanie : B\u0142\u0119dy na dzie\u0144\nSELECT \n    date,\n    COUNT(*) AS error_count\nFROM logs\nWHERE level = 'ERROR'\nGROUP BY date\nORDER BY date DESC;\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#przykad-2-analizowac-dane-csv","title":"Przyk\u0142ad 2 : Analizowa\u0107 dane CSV","text":"<pre><code>-- Tabela CSV\nCREATE EXTERNAL TABLE sales_csv (\n    id INT,\n    product_id INT,\n    amount DECIMAL(10,2),\n    sale_date DATE\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\nSTORED AS TEXTFILE\nLOCATION 's3://bucket/sales/csv/'\nTBLPROPERTIES ('skip.header.line.count'='1');\n\n-- Analiza : Sprzeda\u017c na miesi\u0105c\nSELECT \n    DATE_TRUNC('month', sale_date) AS month,\n    SUM(amount) AS total_sales,\n    COUNT(*) AS transaction_count\nFROM sales_csv\nGROUP BY DATE_TRUNC('month', sale_date)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#przykad-3-zaczenie-wielu-tabel","title":"Przyk\u0142ad 3 : Z\u0142\u0105czenie wielu tabel","text":"<pre><code>-- Analizowa\u0107 z z\u0142\u0105czeniami\nSELECT \n    p.name AS product_name,\n    c.name AS category_name,\n    SUM(s.amount) AS total_sales\nFROM sales s\nJOIN products p ON s.product_id = p.id\nJOIN categories c ON p.category_id = c.id\nWHERE s.sale_date &gt;= DATE '2024-01-01'\nGROUP BY p.name, c.name\nORDER BY total_sales DESC\nLIMIT 10;\n</code></pre>"},{"location":"Cloud/AWS/PL/05-athena/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Athena = SQL serverless na plikach S3</li> <li>Free Tier : 10 GB/miesi\u0105c przeskanowanych danych</li> <li>Parquet = najbardziej efektywny format</li> <li>Partycjonowa\u0107 = zmniejszy\u0107 koszty</li> <li>Integracja Glue = automatyczne schematy</li> </ol>"},{"location":"Cloud/AWS/PL/05-athena/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 6. AWS Lambda - Serverless Computing, aby nauczy\u0107 si\u0119 automatyzowa\u0107 przetwarzanie danych.</p>"},{"location":"Cloud/AWS/PL/06-lambda/","title":"6. AWS Lambda - Serverless Computing","text":""},{"location":"Cloud/AWS/PL/06-lambda/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 AWS Lambda i jego u\u017cycie</li> <li>Tworzy\u0107 funkcje Lambda</li> <li>Przetwarza\u0107 dane z Lambda</li> <li>Wyzwala\u0107 Lambda z S3</li> <li>Integrowa\u0107 Lambda z innymi us\u0142ugami</li> </ul>"},{"location":"Cloud/AWS/PL/06-lambda/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Lambda</li> <li>Utworzy\u0107 funkcj\u0119 Lambda</li> <li>Przetwarzanie danych</li> <li>Wyzwalacze (Triggers)</li> <li>Integracja z innymi us\u0142ugami</li> <li>Dobre praktyki</li> </ol>"},{"location":"Cloud/AWS/PL/06-lambda/#wprowadzenie-do-lambda","title":"Wprowadzenie do Lambda","text":""},{"location":"Cloud/AWS/PL/06-lambda/#czym-jest-aws-lambda","title":"Czym jest AWS Lambda?","text":"<p>AWS Lambda = Us\u0142uga oblicze\u0144 serverless</p> <ul> <li>Serverless : Brak serwer\u00f3w do zarz\u0105dzania</li> <li>Event-driven : Wyzwalane przez zdarzenia</li> <li>Auto-scaling : Automatycznie dostosowuje si\u0119</li> <li>Pay-per-use : P\u0142acisz tylko za wykonanie</li> </ul>"},{"location":"Cloud/AWS/PL/06-lambda/#przypadki-uzycia-dla-data-analyst","title":"Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Przetwarzanie plik\u00f3w : Przekszta\u0142ca\u0107 przes\u0142ane pliki</li> <li>Automatyzowany ETL : Wyzwala\u0107 joby Glue</li> <li>Walidacja danych : Weryfikowa\u0107 dane</li> <li>Powiadomienia : Alertowa\u0107 o zdarzeniach</li> <li>Orkiestracja : Koordynowa\u0107 wiele us\u0142ug</li> </ul>"},{"location":"Cloud/AWS/PL/06-lambda/#free-tier-lambda","title":"Free Tier Lambda","text":"<p>Darmowe na zawsze : - 1 milion \u017c\u0105da\u0144/miesi\u0105c - 400 000 GB-sekund czasu oblicze\u0144/miesi\u0105c - Poza tym : rozliczanie wed\u0142ug u\u017cycia</p> <p>\u26a0\ufe0f Wa\u017cne : Bardzo hojne dla wi\u0119kszo\u015bci przypadk\u00f3w u\u017cycia.</p>"},{"location":"Cloud/AWS/PL/06-lambda/#utworzyc-funkcje-lambda","title":"Utworzy\u0107 funkcj\u0119 Lambda","text":""},{"location":"Cloud/AWS/PL/06-lambda/#krok-1-dostep-do-lambda","title":"Krok 1 : Dost\u0119p do Lambda","text":"<ol> <li>Konsola AWS \u2192 Szuka\u0107 \"Lambda\"</li> <li>Klikn\u0105\u0107 \"AWS Lambda\"</li> <li>\"Create function\"</li> </ol>"},{"location":"Cloud/AWS/PL/06-lambda/#krok-2-podstawowa-konfiguracja","title":"Krok 2 : Podstawowa konfiguracja","text":"<p>Opcje :</p> <ol> <li>Author from scratch : Utworzy\u0107 od zera</li> <li>Use a blueprint : U\u017cywa\u0107 szablonu</li> <li>Browse serverless app repository : Aplikacje wst\u0119pnie zbudowane</li> </ol> <p>Konfiguracja :</p> <ol> <li>Function name : <code>process-data-file</code></li> <li>Runtime : Python 3.11 (lub inny)</li> <li>Architecture : x86_64 (domy\u015blnie)</li> <li>Permissions : Utworzy\u0107 now\u0105 rol\u0119 z podstawowymi uprawnieniami</li> </ol>"},{"location":"Cloud/AWS/PL/06-lambda/#krok-3-kod-funkcji","title":"Krok 3 : Kod funkcji","text":"<p>Prosty przyk\u0142ad :</p> <pre><code>import json\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Podstawowa funkcja Lambda\n    \"\"\"\n    # Przetwarzanie\n    result = {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n\n    return result\n</code></pre> <p>Testowa\u0107 funkcj\u0119 :</p> <ol> <li>Klikn\u0105\u0107 \"Test\"</li> <li>Utworzy\u0107 zdarzenie testowe</li> <li>Wykona\u0107</li> <li>Zobaczy\u0107 wyniki</li> </ol>"},{"location":"Cloud/AWS/PL/06-lambda/#przetwarzanie-danych","title":"Przetwarzanie danych","text":""},{"location":"Cloud/AWS/PL/06-lambda/#przykad-1-przetwarzac-plik-csv","title":"Przyk\u0142ad 1 : Przetwarza\u0107 plik CSV","text":"<pre><code>import json\nimport csv\nimport boto3\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # Pobra\u0107 bucket i klucz ze zdarzenia\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # Pobra\u0107 plik\n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response['Body'].read().decode('utf-8')\n\n    # Parsowa\u0107 CSV\n    reader = csv.DictReader(content.splitlines())\n    rows = list(reader)\n\n    # Przetwarza\u0107 dane\n    processed = []\n    for row in rows:\n        processed.append({\n            'id': row['id'],\n            'name': row['name'].upper(),\n            'email': row['email'].lower()\n        })\n\n    # Przes\u0142a\u0107 wynik\n    output_key = key.replace('raw/', 'processed/')\n    s3.put_object(\n        Bucket=bucket,\n        Key=output_key,\n        Body=json.dumps(processed)\n    )\n\n    return {\n        'statusCode': 200,\n        'body': f'Processed {len(processed)} rows'\n    }\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#przykad-2-walidowac-dane","title":"Przyk\u0142ad 2 : Walidowa\u0107 dane","text":"<pre><code>import json\nimport boto3\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    bucket = event['bucket']\n    key = event['key']\n\n    # Pobra\u0107 plik\n    response = s3.get_object(Bucket=bucket, Key=key)\n    data = json.loads(response['Body'].read())\n\n    # Walidowa\u0107\n    errors = []\n    for item in data:\n        if 'email' not in item or '@' not in item['email']:\n            errors.append(f\"Invalid email for id {item.get('id')}\")\n        if 'age' in item and item['age'] &lt; 0:\n            errors.append(f\"Invalid age for id {item.get('id')}\")\n\n    # Przes\u0142a\u0107 raport\n    if errors:\n        s3.put_object(\n            Bucket=bucket,\n            Key=f'validation-errors/{key}',\n            Body=json.dumps(errors)\n        )\n        return {\n            'statusCode': 400,\n            'body': f'Found {len(errors)} validation errors'\n        }\n\n    return {\n        'statusCode': 200,\n        'body': 'Validation passed'\n    }\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#przykad-3-wyzwolic-job-glue","title":"Przyk\u0142ad 3 : Wyzwoli\u0107 job Glue","text":"<pre><code>import boto3\n\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    # Nazwa joba Glue do wykonania\n    job_name = 'my-etl-job'\n\n    # Wyzwoli\u0107 job\n    response = glue.start_job_run(\n        JobName=job_name\n    )\n\n    return {\n        'statusCode': 200,\n        'body': f'Started Glue job: {response[\"JobRunId\"]}'\n    }\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#wyzwalacze-triggers","title":"Wyzwalacze (Triggers)","text":""},{"location":"Cloud/AWS/PL/06-lambda/#wyzwalac-z-s3","title":"Wyzwala\u0107 z S3","text":"<p>Konfiguracja :</p> <ol> <li>Lambda \u2192 Function \u2192 \"Add trigger\"</li> <li>\u0179r\u00f3d\u0142o : \"S3\"</li> <li>Bucket : Wybra\u0107 bucket</li> <li>Typ zdarzenia : \"All object create events\" (lub konkretny)</li> <li>Prefiks (opcjonalne) : <code>raw/</code> (tylko pliki w tym prefiksie)</li> <li>Sufiks (opcjonalne) : <code>.csv</code> (tylko pliki CSV)</li> </ol> <p>Wynik : Lambda wykonuje si\u0119 automatycznie gdy plik jest przes\u0142any.</p>"},{"location":"Cloud/AWS/PL/06-lambda/#wyzwalac-z-eventbridge-harmonogram","title":"Wyzwala\u0107 z EventBridge (harmonogram)","text":"<p>Zaplanowa\u0107 wykonanie :</p> <ol> <li>Lambda \u2192 Function \u2192 \"Add trigger\"</li> <li>\u0179r\u00f3d\u0142o : \"EventBridge (CloudWatch Events)\"</li> <li>Regu\u0142a : Utworzy\u0107 now\u0105 regu\u0142\u0119</li> <li>Wyra\u017cenie harmonogramu : <code>cron(0 2 * * ? *)</code> (codziennie o 2h)</li> </ol> <p>Przyk\u0142ady cron : - <code>cron(0 2 * * ? *)</code> : Codziennie o 2h - <code>cron(0 */6 * * ? *)</code> : Co 6 godzin - <code>cron(0 0 ? * MON *)</code> : W ka\u017cdy poniedzia\u0142ek o p\u00f3\u0142nocy</p>"},{"location":"Cloud/AWS/PL/06-lambda/#wyzwalac-z-api-gateway","title":"Wyzwala\u0107 z API Gateway","text":"<p>Utworzy\u0107 API REST :</p> <ol> <li>API Gateway \u2192 \"Create API\"</li> <li>Typ : REST API</li> <li>Utworzy\u0107 zas\u00f3b i metod\u0119</li> <li>Integracja : Lambda Function</li> <li>Wybra\u0107 funkcj\u0119 Lambda</li> </ol> <p>Wynik : Wywo\u0142anie HTTP wyzwala Lambda.</p>"},{"location":"Cloud/AWS/PL/06-lambda/#integracja-z-innymi-usugami","title":"Integracja z innymi us\u0142ugami","text":""},{"location":"Cloud/AWS/PL/06-lambda/#lambda-s3","title":"Lambda + S3","text":"<p>Automatyczne przetwarzanie plik\u00f3w :</p> <pre><code>import boto3\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # Zdarzenie S3\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = record['s3']['object']['key']\n\n        # Przetwarza\u0107 plik\n        # ...\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#lambda-glue","title":"Lambda + Glue","text":"<p>Wyzwoli\u0107 job Glue :</p> <pre><code>import boto3\n\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    response = glue.start_job_run(\n        JobName='my-etl-job',\n        Arguments={\n            '--input-path': 's3://bucket/raw/',\n            '--output-path': 's3://bucket/processed/'\n        }\n    )\n    return response\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#lambda-sns-powiadomienia","title":"Lambda + SNS (Powiadomienia)","text":"<p>Wys\u0142a\u0107 powiadomienie :</p> <pre><code>import boto3\nimport json\n\nsns = boto3.client('sns')\n\ndef lambda_handler(event, context):\n    # Przetwarzanie...\n\n    # Wys\u0142a\u0107 powiadomienie\n    sns.publish(\n        TopicArn='arn:aws:sns:region:account:topic',\n        Message=json.dumps({\n            'status': 'success',\n            'message': 'Data processing completed'\n        })\n    )\n\n    return {'statusCode': 200}\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#lambda-step-functions","title":"Lambda + Step Functions","text":"<p>Orkiestrowa\u0107 wiele Lambd :</p> <pre><code>{\n  \"Comment\": \"ETL Pipeline\",\n  \"StartAt\": \"ProcessData\",\n  \"States\": {\n    \"ProcessData\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:region:account:function:process-data\",\n      \"Next\": \"ValidateData\"\n    },\n    \"ValidateData\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:region:account:function:validate-data\",\n      \"End\": true\n    }\n  }\n}\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/AWS/PL/06-lambda/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>Optymalizowa\u0107 kod aby zmniejszy\u0107 czas wykonania</li> <li>U\u017cywa\u0107 odpowiedniej pami\u0119ci (128 MB do 10 GB)</li> <li>Ponownie u\u017cywa\u0107 po\u0142\u0105cze\u0144 (klienci boto3)</li> <li>U\u017cywa\u0107 layers dla wsp\u00f3lnych zale\u017cno\u015bci</li> </ol>"},{"location":"Cloud/AWS/PL/06-lambda/#koszty","title":"Koszty","text":"<ol> <li>Optymalizowa\u0107 czas trwania wykonania</li> <li>Wybra\u0107 odpowiedni\u0105 pami\u0119\u0107 (wi\u0119cej pami\u0119ci = szybciej ale dro\u017cej)</li> <li>Unika\u0107 niepotrzebnych timeout\u00f3w</li> <li>U\u017cywa\u0107 rezerwacji je\u015bli sta\u0142e u\u017cycie (nie w Free Tier)</li> </ol>"},{"location":"Cloud/AWS/PL/06-lambda/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":"<ol> <li>U\u017cywa\u0107 r\u00f3l IAM dla uprawnie\u0144</li> <li>Nie hardcodowa\u0107 credentials</li> <li>U\u017cywa\u0107 zmiennych \u015brodowiskowych do konfiguracji</li> <li>W\u0142\u0105czy\u0107 VPC je\u015bli potrzeba prywatnego dost\u0119pu</li> </ol>"},{"location":"Cloud/AWS/PL/06-lambda/#obsuga-bedow","title":"Obs\u0142uga b\u0142\u0119d\u00f3w","text":"<pre><code>import json\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ndef lambda_handler(event, context):\n    try:\n        # Przetwarzanie\n        result = process_data(event)\n        return {\n            'statusCode': 200,\n            'body': json.dumps(result)\n        }\n    except Exception as e:\n        logger.error(f'Error: {str(e)}')\n        return {\n            'statusCode': 500,\n            'body': json.dumps({'error': str(e)})\n        }\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/AWS/PL/06-lambda/#przykad-1-automatyczny-pipeline-etl","title":"Przyk\u0142ad 1 : Automatyczny pipeline ETL","text":"<pre><code>import boto3\nimport json\n\ns3 = boto3.client('s3')\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Wyzwala job Glue gdy plik jest przes\u0142any do S3\n    \"\"\"\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # Sprawdzi\u0107 czy to plik CSV\n    if not key.endswith('.csv'):\n        return {'statusCode': 200, 'body': 'Not a CSV file'}\n\n    # Wyzwoli\u0107 job Glue\n    response = glue.start_job_run(\n        JobName='csv-to-parquet-job',\n        Arguments={\n            '--input-path': f's3://{bucket}/{key}',\n            '--output-path': f's3://{bucket}/processed/'\n        }\n    )\n\n    return {\n        'statusCode': 200,\n        'body': f'Started Glue job: {response[\"JobRunId\"]}'\n    }\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#przykad-2-walidacja-i-powiadomienie","title":"Przyk\u0142ad 2 : Walidacja i powiadomienie","text":"<pre><code>import boto3\nimport json\nimport csv\n\ns3 = boto3.client('s3')\nsns = boto3.client('sns')\n\ndef lambda_handler(event, context):\n    bucket = event['bucket']\n    key = event['key']\n\n    # Pobra\u0107 i walidowa\u0107\n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response['Body'].read().decode('utf-8')\n    reader = csv.DictReader(content.splitlines())\n\n    errors = []\n    for row in reader:\n        if not row.get('email') or '@' not in row['email']:\n            errors.append(f\"Row {row.get('id')}: Invalid email\")\n\n    # Powiadomienie\n    if errors:\n        sns.publish(\n            TopicArn='arn:aws:sns:region:account:alerts',\n            Message=f'Validation failed: {len(errors)} errors found'\n        )\n\n    return {'statusCode': 200 if not errors else 400}\n</code></pre>"},{"location":"Cloud/AWS/PL/06-lambda/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Lambda = Serverless : Brak infrastruktury do zarz\u0105dzania</li> <li>Free Tier : 1M \u017c\u0105da\u0144/miesi\u0105c : Bardzo hojne</li> <li>Event-driven : Wyzwalane przez zdarzenia</li> <li>\u0141atwa integracja : Ze wszystkimi us\u0142ugami AWS</li> <li>Pay-per-use : P\u0142acisz tylko za wykonanie</li> </ol>"},{"location":"Cloud/AWS/PL/06-lambda/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 7. Projekty praktyczne, aby tworzy\u0107 kompletne projekty z AWS.</p>"},{"location":"Cloud/AWS/PL/07-projets/","title":"7. Projekty praktyczne AWS","text":""},{"location":"Cloud/AWS/PL/07-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Stosowa\u0107 zdobyt\u0105 wiedz\u0119</li> <li>Tworzy\u0107 kompletne pipeline'y ETL</li> <li>Budowa\u0107 Data Lake na AWS</li> <li>Tworzy\u0107 projekty dla portfolio</li> <li>Integrowa\u0107 wiele us\u0142ug AWS</li> </ul>"},{"location":"Cloud/AWS/PL/07-projets/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Projekt 1 : Pipeline ETL S3 \u2192 Parquet</li> <li>Projekt 2 : Data Lake na AWS</li> <li>Projekt 3 : Analytics z Athena</li> <li>Projekt 4 : Kompletny zautomatyzowany pipeline</li> <li>Dobre praktyki dla portfolio</li> </ol>"},{"location":"Cloud/AWS/PL/07-projets/#projekt-1-pipeline-etl-s3-parquet","title":"Projekt 1 : Pipeline ETL S3 \u2192 Parquet","text":""},{"location":"Cloud/AWS/PL/07-projets/#cel","title":"Cel","text":"<p>Utworzy\u0107 pipeline ETL kt\u00f3ry przekszta\u0142ca pliki CSV z S3 w zoptymalizowany format Parquet.</p>"},{"location":"Cloud/AWS/PL/07-projets/#architektura","title":"Architektura","text":"<pre><code>S3 (raw/) \u2192 Glue Crawler \u2192 Data Catalog \u2192 Glue Job \u2192 S3 (processed/parquet/)\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#kroki","title":"Kroki","text":""},{"location":"Cloud/AWS/PL/07-projets/#1-przygotowac-dane","title":"1. Przygotowa\u0107 dane","text":"<p>Utworzy\u0107 bucket S3 : - Nazwa : <code>data-analyst-project-1</code> - Utworzy\u0107 folder <code>raw/</code> - Przes\u0142a\u0107 plik CSV testowy</p> <p>Przyk\u0142ad danych CSV : <pre><code>id,name,email,created_at,status\n1,John Doe,john@example.com,2024-01-01,active\n2,Jane Smith,jane@example.com,2024-01-02,inactive\n</code></pre></p>"},{"location":"Cloud/AWS/PL/07-projets/#2-utworzyc-crawler-glue","title":"2. Utworzy\u0107 Crawler Glue","text":"<ol> <li>Glue \u2192 \"Crawlers\" \u2192 \"Add crawler\"</li> <li>Nazwa : <code>csv-crawler</code></li> <li>\u0179r\u00f3d\u0142o danych : <code>s3://data-analyst-project-1/raw/</code></li> <li>Rola IAM : Utworzy\u0107 rol\u0119 z dost\u0119pem S3</li> <li>Baza danych : <code>project1_db</code></li> <li>Wykona\u0107 crawler</li> </ol>"},{"location":"Cloud/AWS/PL/07-projets/#3-utworzyc-job-glue","title":"3. Utworzy\u0107 Job Glue","text":"<ol> <li>Glue \u2192 \"ETL jobs\" \u2192 \"Add job\"</li> <li>Nazwa : <code>csv-to-parquet-job</code></li> <li>Typ : Spark</li> <li>Skrypt :</li> </ol> <pre><code>import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Czyta\u0107 z Data Catalog\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"project1_db\",\n    table_name = \"raw_data\"\n)\n\n# Filtrowa\u0107 aktywne dane\nfiltered = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"status\"] == \"active\"\n)\n\n# Zapisa\u0107 w Parquet\nglueContext.write_dynamic_frame.from_options(\n    frame = filtered,\n    connection_type = \"s3\",\n    connection_options = {\n        \"path\": \"s3://data-analyst-project-1/processed/parquet/\"\n    },\n    format = \"parquet\"\n)\n\njob.commit()\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#4-wykonac-job","title":"4. Wykona\u0107 job","text":"<ol> <li>Wybra\u0107 job</li> <li>\"Run job\"</li> <li>Sprawdzi\u0107 logi</li> <li>Sprawdzi\u0107 pliki Parquet w S3</li> </ol>"},{"location":"Cloud/AWS/PL/07-projets/#wynik","title":"Wynik","text":"<ul> <li>Pliki CSV przekszta\u0142cone w Parquet</li> <li>Dane przefiltrowane (tylko aktywne)</li> <li>Gotowe do analytics z Athena</li> </ul>"},{"location":"Cloud/AWS/PL/07-projets/#projekt-2-data-lake-na-aws","title":"Projekt 2 : Data Lake na AWS","text":""},{"location":"Cloud/AWS/PL/07-projets/#cel_1","title":"Cel","text":"<p>Utworzy\u0107 kompletny Data Lake z ingerencj\u0105, przekszta\u0142caniem i analytics.</p>"},{"location":"Cloud/AWS/PL/07-projets/#architektura_1","title":"Architektura","text":"<pre><code>\u0179r\u00f3d\u0142a \u2192 S3 (Raw) \u2192 Glue (Transform) \u2192 S3 (Processed) \u2192 Athena (Analytics)\n                \u2193\n            Lambda (Trigger)\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#kroki_1","title":"Kroki","text":""},{"location":"Cloud/AWS/PL/07-projets/#1-struktura-s3","title":"1. Struktura S3","text":"<pre><code>data-lake-bucket/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u251c\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#2-crawlery-dla-kazdego-zroda","title":"2. Crawlery dla ka\u017cdego \u017ar\u00f3d\u0142a","text":"<p>Utworzy\u0107 3 crawlery : - <code>users-crawler</code> \u2192 <code>s3://bucket/raw/users/</code> - <code>orders-crawler</code> \u2192 <code>s3://bucket/raw/orders/</code> - <code>products-crawler</code> \u2192 <code>s3://bucket/raw/products/</code></p>"},{"location":"Cloud/AWS/PL/07-projets/#3-joby-etl-do-przeksztacania","title":"3. Joby ETL do przekszta\u0142cania","text":"<p>Job dla users : <pre><code># users-etl-job\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"data_lake_db\",\n    table_name = \"users\"\n)\n\n# Czy\u015bci\u0107 i przekszta\u0142ca\u0107\ncleaned = Filter.apply(\n    frame = datasource,\n    f = lambda x: x[\"email\"] is not None\n)\n\nglueContext.write_dynamic_frame.from_options(\n    frame = cleaned,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bucket/processed/users/\"},\n    format = \"parquet\"\n)\n</code></pre></p>"},{"location":"Cloud/AWS/PL/07-projets/#4-tabele-athena-do-analytics","title":"4. Tabele Athena do analytics","text":"<pre><code>-- Tabela users\nCREATE EXTERNAL TABLE users_processed (\n    id INT,\n    name STRING,\n    email STRING,\n    created_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket/processed/users/';\n\n-- Tabela orders\nCREATE EXTERNAL TABLE orders_processed (\n    id INT,\n    user_id INT,\n    amount DECIMAL(10,2),\n    created_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket/processed/orders/';\n\n-- Zapytanie analityczne\nSELECT \n    u.name,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_spent\nFROM users_processed u\nLEFT JOIN orders_processed o ON u.id = o.user_id\nGROUP BY u.name\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#5-automatyzacja-z-lambda","title":"5. Automatyzacja z Lambda","text":"<p>Lambda wyzwalana przez przes\u0142anie S3 :</p> <pre><code>import boto3\n\nglue = boto3.client('glue')\n\ndef lambda_handler(event, context):\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # Okre\u015bli\u0107 kt\u00f3ry job wykona\u0107 wed\u0142ug prefiksu\n    if 'users' in key:\n        job_name = 'users-etl-job'\n    elif 'orders' in key:\n        job_name = 'orders-etl-job'\n    else:\n        job_name = 'products-etl-job'\n\n    # Wyzwoli\u0107 job\n    glue.start_job_run(JobName=job_name)\n\n    return {'statusCode': 200}\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#wynik_1","title":"Wynik","text":"<ul> <li>Funkcjonalny Data Lake</li> <li>Zautomatyzowany pipeline</li> <li>Analytics z Athena</li> <li>Kompletny projekt dla portfolio</li> </ul>"},{"location":"Cloud/AWS/PL/07-projets/#projekt-3-analytics-z-athena","title":"Projekt 3 : Analytics z Athena","text":""},{"location":"Cloud/AWS/PL/07-projets/#cel_2","title":"Cel","text":"<p>Utworzy\u0107 kompletny system analytics z zapytaniami SQL na danych S3.</p>"},{"location":"Cloud/AWS/PL/07-projets/#kroki_2","title":"Kroki","text":""},{"location":"Cloud/AWS/PL/07-projets/#1-przygotowac-dane_1","title":"1. Przygotowa\u0107 dane","text":"<p>Przes\u0142a\u0107 pliki Parquet do S3 : - <code>s3://analytics-bucket/sales/year=2024/month=01/</code> - <code>s3://analytics-bucket/sales/year=2024/month=02/</code></p>"},{"location":"Cloud/AWS/PL/07-projets/#2-utworzyc-tabele-partycjonowane","title":"2. Utworzy\u0107 tabele partycjonowane","text":"<pre><code>CREATE EXTERNAL TABLE sales (\n    id INT,\n    product_id INT,\n    amount DECIMAL(10,2),\n    sale_date TIMESTAMP\n)\nPARTITIONED BY (year INT, month INT)\nSTORED AS PARQUET\nLOCATION 's3://analytics-bucket/sales/';\n\n-- Doda\u0107 partycje\nALTER TABLE sales ADD PARTITION (year=2024, month=1)\nLOCATION 's3://analytics-bucket/sales/year=2024/month=01/';\n\nALTER TABLE sales ADD PARTITION (year=2024, month=2)\nLOCATION 's3://analytics-bucket/sales/year=2024/month=02/';\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#3-zapytania-analityczne","title":"3. Zapytania analityczne","text":"<p>Sprzeda\u017c na miesi\u0105c : <pre><code>SELECT \n    year,\n    month,\n    SUM(amount) AS total_sales,\n    COUNT(*) AS transaction_count,\n    AVG(amount) AS avg_transaction\nFROM sales\nWHERE year = 2024\nGROUP BY year, month\nORDER BY year, month;\n</code></pre></p> <p>Top produkty : <pre><code>SELECT \n    product_id,\n    SUM(amount) AS total_revenue,\n    COUNT(*) AS sales_count\nFROM sales\nWHERE year = 2024\nGROUP BY product_id\nORDER BY total_revenue DESC\nLIMIT 10;\n</code></pre></p> <p>Trendy : <pre><code>SELECT \n    DATE_TRUNC('week', sale_date) AS week,\n    SUM(amount) AS weekly_sales,\n    LAG(SUM(amount), 1) OVER (ORDER BY DATE_TRUNC('week', sale_date)) AS previous_week\nFROM sales\nWHERE year = 2024\nGROUP BY DATE_TRUNC('week', sale_date)\nORDER BY week;\n</code></pre></p>"},{"location":"Cloud/AWS/PL/07-projets/#4-zapisac-wyniki","title":"4. Zapisa\u0107 wyniki","text":"<p>Utworzy\u0107 tabel\u0119 dla wynik\u00f3w : <pre><code>CREATE EXTERNAL TABLE analytics_results (\n    metric_name STRING,\n    metric_value DECIMAL(10,2),\n    calculated_at TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://analytics-bucket/results/';\n</code></pre></p>"},{"location":"Cloud/AWS/PL/07-projets/#projekt-4-kompletny-zautomatyzowany-pipeline","title":"Projekt 4 : Kompletny zautomatyzowany pipeline","text":""},{"location":"Cloud/AWS/PL/07-projets/#cel_3","title":"Cel","text":"<p>Utworzy\u0107 kompletnie zautomatyzowany pipeline ETL z wieloma us\u0142ugami AWS.</p>"},{"location":"Cloud/AWS/PL/07-projets/#kompletna-architektura","title":"Kompletna architektura","text":"<pre><code>Plik CSV przes\u0142any \u2192 S3 (raw/)\n    \u2193 (Event)\nLambda (Walidacja)\n    \u2193\nS3 (validated/)\n    \u2193 (Event)\nGlue Job (Przekszta\u0142\u0107 CSV \u2192 Parquet)\n    \u2193\nS3 (processed/parquet/)\n    \u2193\nGlue Crawler (Aktualizuj Catalog)\n    \u2193\nAthena (Analytics)\n    \u2193\nS3 (results/)\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#implementacja","title":"Implementacja","text":""},{"location":"Cloud/AWS/PL/07-projets/#1-lambda-walidacji","title":"1. Lambda walidacji","text":"<pre><code>import boto3\nimport csv\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = event['Records'][0]['s3']['object']['key']\n\n    # Pobra\u0107 i walidowa\u0107\n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response['Body'].read().decode('utf-8')\n    reader = csv.DictReader(content.splitlines())\n\n    valid_rows = []\n    for row in reader:\n        if row.get('email') and '@' in row['email']:\n            valid_rows.append(row)\n\n    # Przes\u0142a\u0107 zwalidowane dane\n    if valid_rows:\n        validated_key = key.replace('raw/', 'validated/')\n        # Konwertowa\u0107 w CSV i przes\u0142a\u0107\n        # ...\n\n    return {'statusCode': 200}\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#2-glue-job-przeksztacania","title":"2. Glue Job przekszta\u0142cania","text":"<pre><code># Przekszta\u0142\u0107 zwalidowany CSV w Parquet\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database = \"pipeline_db\",\n    table_name = \"validated_data\"\n)\n\n# Przekszta\u0142ci\u0107\ntransformed = Map.apply(\n    frame = datasource,\n    f = lambda x: {\n        'id': x['id'],\n        'name': x['name'].upper(),\n        'email': x['email'].lower(),\n        'created_at': x['created_at']\n    }\n)\n\n# Zapisa\u0107 w Parquet\nglueContext.write_dynamic_frame.from_options(\n    frame = transformed,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bucket/processed/\"},\n    format = \"parquet\"\n)\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#3-workflow-glue","title":"3. Workflow Glue","text":"<p>Utworzy\u0107 workflow : 1. Wyzwalacz : Nowy plik w <code>validated/</code> 2. Akcja : Wykona\u0107 job Glue 3. Nast\u0119pna akcja : Aktualizowa\u0107 crawler</p>"},{"location":"Cloud/AWS/PL/07-projets/#wynik_2","title":"Wynik","text":"<ul> <li>Kompletnie zautomatyzowany pipeline</li> <li>Automatyczna walidacja</li> <li>Automatyczne przekszta\u0142canie</li> <li>Analytics dost\u0119pne natychmiast</li> </ul>"},{"location":"Cloud/AWS/PL/07-projets/#dobre-praktyki-dla-portfolio","title":"Dobre praktyki dla portfolio","text":""},{"location":"Cloud/AWS/PL/07-projets/#dokumentacja","title":"Dokumentacja","text":"<p>Utworzy\u0107 README dla ka\u017cdego projektu :</p> <pre><code># Projekt : Pipeline ETL AWS\n\n## Opis\nZautomatyzowany pipeline ETL do przekszta\u0142cania danych CSV w Parquet.\n\n## Architektura\n- S3 : Przechowywanie\n- Glue : Przekszta\u0142canie\n- Athena : Analytics\n\n## Wyniki\n- Redukcja koszt\u00f3w o 60%\n- Czas przetwarzania zmniejszony o 80%\n</code></pre>"},{"location":"Cloud/AWS/PL/07-projets/#wizualizacje","title":"Wizualizacje","text":"<p>Tworzy\u0107 diagramy : - Architektura systemu - Przep\u0142yw danych - Schemat danych</p> <p>Narz\u0119dzia : - Draw.io - Lucidchart - Diagramy ASCII w README</p>"},{"location":"Cloud/AWS/PL/07-projets/#metryki","title":"Metryki","text":"<p>Uwzgl\u0119dnia\u0107 metryki : - Czas wykonania przed/po - Koszty przed/po - Wolumen przetworzonych danych - Wydajno\u015b\u0107 zapyta\u0144</p>"},{"location":"Cloud/AWS/PL/07-projets/#kod","title":"Kod","text":"<p>Dobre praktyki : - Kod skomentowany - Zmienne \u015brodowiskowe do konfiguracji - Obs\u0142uga b\u0142\u0119d\u00f3w - Logowanie</p>"},{"location":"Cloud/AWS/PL/07-projets/#github","title":"GitHub","text":"<p>Utworzy\u0107 repozytorium : - README z dokumentacj\u0105 - Skrypty Lambda - Skrypty Glue - Konfiguracja - Diagramy</p>"},{"location":"Cloud/AWS/PL/07-projets/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Projekty praktyczne : Niezb\u0119dne dla portfolio</li> <li>Dokumentacja : Wyja\u015bnia\u0107 architektur\u0119 i wyniki</li> <li>Metryki : Pokazywa\u0107 wp\u0142yw (wydajno\u015b\u0107, koszty)</li> <li>Czysty kod : Skomentowany i zorganizowany</li> <li>GitHub : Dzieli\u0107 si\u0119 projektami</li> </ol>"},{"location":"Cloud/AWS/PL/07-projets/#zasoby","title":"\ud83d\udd17 Zasoby","text":"<ul> <li>AWS Architecture Center</li> <li>AWS Solutions</li> <li>GitHub AWS Examples</li> </ul> <p>Gratulacje ! Uko\u0144czy\u0142e\u015b formacj\u0119 AWS dla Data Analyst. Mo\u017cesz teraz tworzy\u0107 kompletne projekty na AWS u\u017cywaj\u0105c wy\u0142\u0105cznie darmowych zasob\u00f3w.</p>"},{"location":"Cloud/Azure/EN/","title":"Azure Training for Data Analyst - Free Guide","text":""},{"location":"Cloud/Azure/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through learning Microsoft Azure as a Data Analyst, using only free resources. You'll learn to use essential Azure services for data analysis without spending a penny.</p>"},{"location":"Cloud/Azure/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand essential Azure services for Data Analyst</li> <li>Create and manage free Azure accounts</li> <li>Use Azure Data Factory, SQL Database, and other data services</li> <li>Build ETL pipelines on Azure</li> <li>Analyze data with Azure</li> <li>Integrate with PowerBI</li> <li>Create practical projects for your portfolio</li> </ul>"},{"location":"Cloud/Azure/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 Free Azure Account : $200 credit (30 days) + free services - \u2705 Microsoft Learn : Free interactive courses - \u2705 Azure Documentation : Complete free guides - \u2705 Azure Labs : Free practical labs</p> <p>Total Budget: $0</p>"},{"location":"Cloud/Azure/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"Cloud/Azure/EN/#1-azure-getting-started","title":"1. Azure Getting Started","text":"<ul> <li>Create a free Azure account</li> <li>Understand free credits</li> <li>Navigate Azure Portal</li> <li>Security configuration (Azure AD)</li> </ul>"},{"location":"Cloud/Azure/EN/#2-azure-storage-data-storage","title":"2. Azure Storage - Data Storage","text":"<ul> <li>Create storage accounts</li> <li>Blob Storage, Data Lake Storage</li> <li>Upload and manage files</li> <li>Data organization</li> </ul>"},{"location":"Cloud/Azure/EN/#3-azure-data-factory-cloud-etl","title":"3. Azure Data Factory - Cloud ETL","text":"<ul> <li>Create ETL pipelines</li> <li>Transformation activities</li> <li>Integration with data sources</li> <li>Workflow orchestration</li> </ul>"},{"location":"Cloud/Azure/EN/#4-azure-sql-database-database","title":"4. Azure SQL Database - Database","text":"<ul> <li>Create SQL Database (free up to 32 GB)</li> <li>Migrate data</li> <li>Query optimization</li> <li>PowerBI integration</li> </ul>"},{"location":"Cloud/Azure/EN/#5-azure-synapse-analytics-data-warehouse","title":"5. Azure Synapse Analytics - Data Warehouse","text":"<ul> <li>Create Synapse workspace</li> <li>Load data</li> <li>Advanced SQL queries</li> <li>PowerBI integration</li> </ul>"},{"location":"Cloud/Azure/EN/#6-azure-databricks-big-data-analytics","title":"6. Azure Databricks - Big Data Analytics","text":"<ul> <li>Create Databricks cluster</li> <li>Data processing with Spark</li> <li>Python/SQL notebooks</li> <li>Integration with other services</li> </ul>"},{"location":"Cloud/Azure/EN/#7-practical-projects","title":"7. Practical Projects","text":"<ul> <li>Complete Azure ETL pipeline</li> <li>PowerBI integration</li> <li>Portfolio project</li> <li>Best practices</li> </ul>"},{"location":"Cloud/Azure/EN/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"Cloud/Azure/EN/#step-1-create-a-free-azure-account","title":"Step 1: Create a Free Azure Account","text":"<ol> <li>Go to: https://azure.microsoft.com/free/</li> <li>Click \"Start free\"</li> <li>Sign in with Microsoft account</li> <li>Verify your identity by phone</li> <li>Enter credit card information (not charged)</li> </ol> <p>Important: Azure gives you $200 credit for 30 days, then permanent free services.</p>"},{"location":"Cloud/Azure/EN/#step-2-explore-free-services","title":"Step 2: Explore Free Services","text":"<p>Free services useful for Data Analyst:</p> <ul> <li>Azure SQL Database : Free up to 32 GB (12 months)</li> <li>Azure Storage : 5 GB (12 months)</li> <li>Azure Data Factory : Free up to 5 pipelines</li> <li>Azure Functions : 1 million executions/month (always free)</li> <li>Azure Cosmos DB : 400 RU/s (always free)</li> </ul>"},{"location":"Cloud/Azure/EN/#step-3-follow-the-training","title":"Step 3: Follow the Training","text":"<ol> <li>Start with module 1 (Getting Started)</li> <li>Follow module order</li> <li>Use Microsoft Learn for labs</li> <li>Practice with module 7 projects</li> <li>Monitor your usage in Azure Cost Management</li> </ol>"},{"location":"Cloud/Azure/EN/#essential-azure-services-for-data-analyst","title":"\ud83d\udcca Essential Azure Services for Data Analyst","text":"Service Usage Free Tier Storage Data storage 5 GB (12 months) Data Factory Cloud ETL 5 free pipelines SQL Database Database 32 GB (12 months) Synapse Data warehouse Free credit Databricks Big Data Free credit Functions Serverless 1M executions/month PowerBI Visualization 1 free user"},{"location":"Cloud/Azure/EN/#cost-management","title":"\u26a0\ufe0f Cost Management","text":""},{"location":"Cloud/Azure/EN/#tips-to-stay-free","title":"Tips to Stay Free","text":"<ol> <li>Monitor Billing</li> <li>Enable cost alerts</li> <li>Regularly check Azure Cost Management</li> <li> <p>Recommended limit: $5 alert</p> </li> <li> <p>Use Free Services</p> </li> <li>Prioritize always-free services</li> <li>Use free credits wisely</li> <li> <p>Stop unused resources</p> </li> <li> <p>Delete Unused Resources</p> </li> <li>Stop virtual machines</li> <li>Delete empty storage accounts</li> <li> <p>Regular cleanup</p> </li> <li> <p>Use Free Regions</p> </li> <li>Some regions offer more free services</li> <li>Check availability by region</li> </ol>"},{"location":"Cloud/Azure/EN/#free-resources","title":"\ud83d\udcda Free Resources","text":""},{"location":"Cloud/Azure/EN/#official-microsoft-training","title":"Official Microsoft Training","text":"<ul> <li>Microsoft Learn : https://learn.microsoft.com/</li> <li>Free interactive paths</li> <li>Integrated Azure labs</li> <li>Certification preparation</li> <li> <p>Recommended paths:</p> <ul> <li>\"Azure Data Fundamentals\"</li> <li>\"Azure Data Factory\"</li> <li>\"Azure SQL Database\"</li> </ul> </li> <li> <p>Azure Documentation : https://learn.microsoft.com/azure/</p> </li> <li>Complete guides</li> <li>Step-by-step tutorials</li> <li> <p>Code examples</p> </li> <li> <p>Azure Labs : Free practical labs</p> </li> <li>Access via Microsoft Learn</li> <li>Temporary Azure environments</li> </ul>"},{"location":"Cloud/Azure/EN/#external-free-resources","title":"External Free Resources","text":"<ul> <li>YouTube : Official Microsoft Azure channel</li> <li>GitHub : Microsoft Azure examples</li> <li>Azure Blog : Articles and tutorials</li> </ul>"},{"location":"Cloud/Azure/EN/#certifications-optional","title":"\ud83c\udf93 Certifications (Optional)","text":""},{"location":"Cloud/Azure/EN/#az-900-azure-fundamentals","title":"AZ-900 : Azure Fundamentals","text":"<ul> <li>Cost : ~$100</li> <li>Preparation : Free (Microsoft Learn)</li> <li>Duration : 2-3 weeks</li> <li>Level : Beginner</li> </ul>"},{"location":"Cloud/Azure/EN/#dp-900-azure-data-fundamentals","title":"DP-900 : Azure Data Fundamentals","text":"<ul> <li>Cost : ~$100</li> <li>Preparation : Free (Microsoft Learn)</li> <li>Duration : 3-4 weeks</li> <li>Level : Data Analyst</li> </ul>"},{"location":"Cloud/Azure/EN/#dp-203-azure-data-engineer-associate","title":"DP-203 : Azure Data Engineer Associate","text":"<ul> <li>Cost : ~$165</li> <li>Preparation : Free (Microsoft Learn)</li> <li>Duration : 2-3 months</li> <li>Level : Advanced</li> </ul>"},{"location":"Cloud/Azure/EN/#powerbi-integration","title":"\ud83d\udd17 PowerBI Integration","text":"<p>Azure integrates perfectly with PowerBI:</p> <ul> <li>Azure SQL Database \u2192 PowerBI (direct connection)</li> <li>Azure Data Factory \u2192 PowerBI (data pipeline)</li> <li>Azure Synapse \u2192 PowerBI (data warehouse)</li> <li>Azure Storage \u2192 PowerBI (files)</li> </ul> <p>This integration is a major advantage for Data Analyst using PowerBI.</p>"},{"location":"Cloud/Azure/EN/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>All examples use Free Tier</li> <li>Costs indicated if exceeded</li> <li>Commands tested on Azure Portal</li> <li>Times may vary by region</li> </ul>"},{"location":"Cloud/Azure/EN/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>This training is designed to be evolving. Feel free to suggest improvements or additional use cases.</p>"},{"location":"Cloud/Azure/EN/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Free Azure Account</li> <li>Microsoft Learn</li> <li>Azure Documentation</li> <li>Azure Pricing Calculator</li> </ul>"},{"location":"Cloud/Azure/EN/01-getting-started/","title":"1. Azure Getting Started","text":""},{"location":"Cloud/Azure/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create a free Azure account</li> <li>Understand Azure free credits</li> <li>Navigate Azure Portal</li> <li>Configure basic security (Azure AD)</li> <li>Monitor costs</li> </ul>"},{"location":"Cloud/Azure/EN/01-getting-started/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Create a Free Azure Account</li> <li>Understand Free Credits</li> <li>Navigate Azure Portal</li> <li>Azure AD Configuration (Security)</li> <li>Cost Monitoring</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#create-a-free-azure-account","title":"Create a Free Azure Account","text":""},{"location":"Cloud/Azure/EN/01-getting-started/#step-1-registration","title":"Step 1: Registration","text":"<ol> <li>Go to Azure website</li> <li>URL: https://azure.microsoft.com/free/</li> <li> <p>Click \"Start free\"</p> </li> <li> <p>Sign in with Microsoft</p> </li> <li>Use existing Microsoft account</li> <li> <p>Or create new Microsoft account</p> </li> <li> <p>Identity verification</p> </li> <li>Code received by SMS or email</li> <li> <p>Enter verification code</p> </li> <li> <p>Personal information</p> </li> <li>Name</li> <li>First name</li> <li>Phone number</li> <li> <p>Country</p> </li> <li> <p>Phone verification</p> </li> <li>Automatic call or SMS</li> <li> <p>Enter verification code</p> </li> <li> <p>Payment method</p> </li> <li>Important: Credit card required but not charged</li> <li>Azure gives you $200 credit for 30 days</li> <li>After 30 days: permanent free services</li> <li> <p>You can remove the card later (not recommended)</p> </li> <li> <p>Final identity verification</p> </li> <li>Verification by SMS or call</li> <li>Account confirmation</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#step-2-confirmation","title":"Step 2: Confirmation","text":"<ul> <li>Confirmation email received</li> <li>Azure account active immediately</li> <li>Access to Azure Portal</li> <li>$200 credit available for 30 days</li> </ul> <p>\u26a0\ufe0f Important: Don't create multiple accounts with the same credit card (risk of suspension).</p>"},{"location":"Cloud/Azure/EN/01-getting-started/#understand-free-credits","title":"Understand Free Credits","text":""},{"location":"Cloud/Azure/EN/01-getting-started/#azure-free-offer","title":"Azure Free Offer","text":"<p>Azure offers 3 types of free services:</p>"},{"location":"Cloud/Azure/EN/01-getting-started/#1-200-credit-30-days","title":"1. $200 Credit (30 days)","text":"<p>What you can do: - Test any Azure service - Create virtual machines - Use paid services - Experiment freely</p> <p>Conditions: - Valid 30 days after registration - If credit exhausted before 30 days: services stopped - After 30 days: switch to permanent free services</p>"},{"location":"Cloud/Azure/EN/01-getting-started/#2-free-services-for-12-months","title":"2. Free services for 12 months","text":"<p>Services useful for Data Analyst:</p> <ul> <li>Azure SQL Database: Free up to 32 GB (12 months)</li> <li>Azure Storage: 5 GB (12 months)</li> <li>Azure App Service: 60 minutes/day (12 months)</li> <li>Azure Functions: 1 million executions/month (always free)</li> </ul> <p>Conditions: - Free for 12 months after registration - Monthly limits - Beyond: normal billing</p>"},{"location":"Cloud/Azure/EN/01-getting-started/#3-always-free-services","title":"3. Always free services","text":"<p>Services useful for Data Analyst:</p> <ul> <li>Azure Functions: 1 million executions/month (always free)</li> <li>Azure Cosmos DB: 400 RU/s (always free)</li> <li>Azure Active Directory: 50,000 objects (always free)</li> <li>Azure DevOps: 5 users (always free)</li> </ul> <p>Conditions: - Free indefinitely - Monthly limits - Beyond: billing beyond the limit</p>"},{"location":"Cloud/Azure/EN/01-getting-started/#check-your-credits","title":"Check Your Credits","text":"<ol> <li>Go to Azure Portal</li> <li>\"Cost Management + Billing\"</li> <li>View remaining credits</li> <li>View usage by service</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#navigate-azure-portal","title":"Navigate Azure Portal","text":""},{"location":"Cloud/Azure/EN/01-getting-started/#main-interface","title":"Main Interface","text":"<p>Key elements:</p> <ol> <li>Search bar (top)</li> <li>Quickly search for services</li> <li> <p>Example: type \"SQL\" to find SQL Database</p> </li> <li> <p>Azure menu (\u2630 icon top left)</p> </li> <li>All Azure services</li> <li>Organized by category</li> <li> <p>Customizable favorites</p> </li> <li> <p>Notifications (top right)</p> </li> <li>Alerts and notifications</li> <li> <p>Deployment status</p> </li> <li> <p>Settings (top right)</p> </li> <li>Account settings</li> <li>Theme (light/dark)</li> <li> <p>Language</p> </li> <li> <p>Cloud Shell (&gt;_ icon top)</p> </li> <li>Terminal in browser</li> <li>PowerShell or Bash</li> <li>Very useful for commands</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#essential-services-for-data-analyst","title":"Essential Services for Data Analyst","text":"<p>In Azure menu, search for:</p> <ul> <li>Storage accounts: Data storage</li> <li>Data Factory: Cloud ETL</li> <li>SQL databases: SQL databases</li> <li>Synapse Analytics: Data warehouse</li> <li>Databricks: Big Data analytics</li> <li>Functions: Serverless computing</li> </ul>"},{"location":"Cloud/Azure/EN/01-getting-started/#first-connection","title":"First Connection","text":"<ol> <li>Sign in: https://portal.azure.com/</li> <li>Explore the dashboard</li> <li>Click \"All services\" to see all services</li> <li>Use search bar to find a service</li> <li>Pin frequent services to dashboard</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#azure-ad-configuration-security","title":"Azure AD Configuration (Security)","text":""},{"location":"Cloud/Azure/EN/01-getting-started/#what-is-azure-ad","title":"What is Azure AD?","text":"<p>Azure AD (Azure Active Directory) = Identity and access management</p> <ul> <li>Manage users</li> <li>Manage permissions</li> <li>Secure access to services</li> <li>Multi-factor authentication (MFA)</li> </ul>"},{"location":"Cloud/Azure/EN/01-getting-started/#security-best-practices","title":"Security Best Practices","text":""},{"location":"Cloud/Azure/EN/01-getting-started/#1-enable-multi-factor-authentication-mfa","title":"1. Enable Multi-Factor Authentication (MFA)","text":"<p>For administrator account:</p> <ol> <li>Go to Azure AD</li> <li>\"Users\" \u2192 Select your account</li> <li>\"Multi-factor authentication\"</li> <li>Click \"Enable\"</li> <li>Follow instructions</li> </ol> <p>\u26a0\ufe0f Important: Always enable MFA for administrator accounts.</p>"},{"location":"Cloud/Azure/EN/01-getting-started/#2-create-azure-ad-users-recommended","title":"2. Create Azure AD Users (Recommended)","text":"<p>For team work:</p> <ol> <li>Go to Azure AD</li> <li>\"Users\" \u2192 \"New user\"</li> <li>Username: <code>data-analyst@yourdomain.onmicrosoft.com</code></li> <li>Temporary password</li> <li>Roles: \"User\" (default)</li> <li>Create user</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#3-azure-roles-rbac","title":"3. Azure Roles (RBAC)","text":"<p>Roles useful for Data Analyst:</p> <ul> <li>Contributor: Can create and manage resources</li> <li>Reader: Can only read</li> <li>Storage Account Contributor: Access to storage accounts</li> <li>SQL DB Contributor: Access to SQL databases</li> </ul> <p>Assign a role:</p> <ol> <li>Go to resource (ex: Storage Account)</li> <li>\"Access control (IAM)\"</li> <li>\"Add\" \u2192 \"Add role assignment\"</li> <li>Select role</li> <li>Select user</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#recommended-security-policies","title":"Recommended Security Policies","text":"<ol> <li>Strong passwords</li> <li>Minimum 12 characters</li> <li> <p>Complexity required</p> </li> <li> <p>Password expiration</p> </li> <li> <p>90 days (recommended)</p> </li> <li> <p>Account lockout</p> </li> <li>After 5 failed attempts</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#cost-monitoring","title":"Cost Monitoring","text":""},{"location":"Cloud/Azure/EN/01-getting-started/#enable-cost-alerts","title":"Enable Cost Alerts","text":"<p>Step 1: Configure alerts</p> <ol> <li>Go to \"Cost Management + Billing\"</li> <li>\"Cost alerts\"</li> <li>\"New cost alert\"</li> <li>Threshold: $5 (recommended)</li> <li>Email notification</li> </ol> <p>Result: Email received if costs exceed $5.</p>"},{"location":"Cloud/Azure/EN/01-getting-started/#check-credit-usage","title":"Check Credit Usage","text":"<ol> <li>\"Cost Management + Billing\"</li> <li>\"Azure credits\"</li> <li>View remaining credits</li> <li>View usage by service</li> <li>View expiration date (30 days)</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#azure-cost-management","title":"Azure Cost Management","text":"<ol> <li>\"Cost Management + Billing\" \u2192 \"Cost Management\"</li> <li>View costs by service</li> <li>Filter by period</li> <li>Export reports</li> <li>Create budgets</li> </ol> <p>\u26a0\ufe0f Important: Check regularly (weekly recommended).</p>"},{"location":"Cloud/Azure/EN/01-getting-started/#tips-to-stay-free","title":"Tips to Stay Free","text":"<ol> <li>Delete unused resources</li> <li>Stop unused virtual machines</li> <li>Delete empty storage accounts</li> <li> <p>Clean up resource groups</p> </li> <li> <p>Use free services</p> </li> <li>Prioritize always-free services</li> <li>Use credits wisely</li> <li> <p>Stop unused services</p> </li> <li> <p>Create budgets</p> </li> <li>\"Cost Management\" \u2192 \"Budgets\"</li> <li>Create $5 budget</li> <li> <p>Automatic alerts</p> </li> <li> <p>Stop unused services</p> </li> <li>Virtual machines: stop when not used</li> <li>Databases: stop or pause</li> <li>Storage accounts: delete if empty</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#resource-groups","title":"Resource Groups","text":"<p>Organize your resources:</p> <ol> <li>Create resource group: <code>rg-data-analyst-training</code></li> <li>All training resources in this group</li> <li>Facilitates one-time deletion</li> <li>Facilitates cost management</li> </ol> <p>Create a resource group:</p> <ol> <li>\"Resource groups\" \u2192 \"Add\"</li> <li>Name: <code>rg-data-analyst-training</code></li> <li>Region: Choose closest region</li> <li>Create</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#key-points-to-remember","title":"\ud83d\udcca Key Points to Remember","text":"<ol> <li>Free Azure account: $200 credit (30 days) + free services</li> <li>Free credits: 3 types ($200, 12 months, always free)</li> <li>Azure AD Security: Enable MFA, create users</li> <li>Monitoring: Cost alerts essential</li> <li>Stay free: Delete unused resources, use resource groups</li> </ol>"},{"location":"Cloud/Azure/EN/01-getting-started/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Go to module 2. Azure Storage - Data Storage to learn how to store data on Azure.</p>"},{"location":"Cloud/Azure/EN/02-storage/","title":"2. Azure Storage - Data Storage","text":""},{"location":"Cloud/Azure/EN/02-storage/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Azure Storage and its usage</li> <li>Create storage accounts</li> <li>Use Blob Storage and Data Lake Storage</li> <li>Upload and manage files</li> <li>Organize data</li> </ul>"},{"location":"Cloud/Azure/EN/02-storage/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Azure Storage</li> <li>Create a Storage Account</li> <li>Blob Storage</li> <li>Data Lake Storage Gen2</li> <li>Upload and Manage Files</li> <li>Integration with Other Services</li> </ol>"},{"location":"Cloud/Azure/EN/02-storage/#introduction-to-azure-storage","title":"Introduction to Azure Storage","text":""},{"location":"Cloud/Azure/EN/02-storage/#what-is-azure-storage","title":"What is Azure Storage?","text":"<p>Azure Storage = Managed cloud storage service</p> <ul> <li>Unlimited storage : Scalable according to needs</li> <li>High availability : 99.99% availability</li> <li>Secure : Encryption by default</li> <li>Integration : With all Azure services</li> </ul>"},{"location":"Cloud/Azure/EN/02-storage/#storage-types","title":"Storage Types","text":"<ol> <li>Blob Storage : Files (CSV, JSON, Parquet, etc.)</li> <li>Data Lake Storage Gen2 : Data Lake with hierarchical file system</li> <li>File Storage : File shares</li> <li>Queue Storage : Queues</li> <li>Table Storage : NoSQL storage</li> </ol>"},{"location":"Cloud/Azure/EN/02-storage/#azure-storage-free-tier","title":"Azure Storage Free Tier","text":"<p>Free for 12 months: - 5 GB Blob storage - 5 GB File storage - 5 GB Table storage - 5 GB Queue storage</p> <p>Free forever: - 200 GB outbound data transfer/month</p> <p>\u26a0\ufe0f Important: Beyond these limits, normal billing.</p>"},{"location":"Cloud/Azure/EN/02-storage/#create-a-storage-account","title":"Create a Storage Account","text":""},{"location":"Cloud/Azure/EN/02-storage/#step-1-access-azure-storage","title":"Step 1: Access Azure Storage","text":"<ol> <li>Azure Portal \u2192 Search \"Storage accounts\"</li> <li>Click on \"Storage accounts\"</li> <li>Click on \"Create\"</li> </ol>"},{"location":"Cloud/Azure/EN/02-storage/#step-2-basic-configuration","title":"Step 2: Basic Configuration","text":"<p>Basic Information: - Subscription : Choose your subscription - Resource group : Create or use existing - Storage account name : Globally unique name (e.g., <code>mydataanalyststorage</code>) - Region : Choose the closest region (e.g., <code>France Central</code>)</p> <p>Performance Options: - Performance : Standard (recommended to start) - Redundancy : LRS (Locally Redundant Storage) - cheapest</p>"},{"location":"Cloud/Azure/EN/02-storage/#step-3-advanced-options","title":"Step 3: Advanced Options","text":"<p>Security: - Secure transfer required : \u2705 Enable (recommended) - Allow Blob public access : \u274c Disable (security)</p> <p>Data Lake Storage Gen2: - Hierarchical namespace : \u2705 Enable if Data Lake needed</p>"},{"location":"Cloud/Azure/EN/02-storage/#step-4-create-the-account","title":"Step 4: Create the Account","text":"<ol> <li>Click on \"Review + create\"</li> <li>Verify configuration</li> <li>Click on \"Create\"</li> <li>Wait for creation (1-2 minutes)</li> </ol> <p>\u26a0\ufe0f Important: Note the storage account name.</p>"},{"location":"Cloud/Azure/EN/02-storage/#blob-storage","title":"Blob Storage","text":""},{"location":"Cloud/Azure/EN/02-storage/#what-is-blob-storage","title":"What is Blob Storage?","text":"<p>Blob Storage = Object storage for files</p> <ul> <li>Containers : Organize files (like folders)</li> <li>Blobs : Individual files</li> <li>Types : Block blobs, Page blobs, Append blobs</li> </ul>"},{"location":"Cloud/Azure/EN/02-storage/#create-a-container","title":"Create a Container","text":"<p>Via Azure Portal:</p> <ol> <li>Storage account \u2192 \"Containers\"</li> <li>Click on \"+ Container\"</li> <li>Name: <code>raw-data</code> (or other)</li> <li>Public access level: Private (recommended)</li> <li>Click on \"Create\"</li> </ol> <p>Via Azure CLI:</p> <pre><code>az storage container create \\\n  --name raw-data \\\n  --account-name mydataanalyststorage \\\n  --auth-mode login\n</code></pre> <p>Via Python:</p> <pre><code>from azure.storage.blob import BlobServiceClient\n\n# Connection\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=...\"\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\n# Create a container\ncontainer_client = blob_service_client.create_container(\"raw-data\")\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#blob-types","title":"Blob Types","text":"<p>Block Blobs: - Files (CSV, JSON, Parquet, images, etc.) - Up to 4.75 TB per blob - Recommended for most cases</p> <p>Page Blobs: - Virtual disks - Up to 8 TB</p> <p>Append Blobs: - Logs - Append-only data</p>"},{"location":"Cloud/Azure/EN/02-storage/#data-lake-storage-gen2","title":"Data Lake Storage Gen2","text":""},{"location":"Cloud/Azure/EN/02-storage/#what-is-data-lake-storage-gen2","title":"What is Data Lake Storage Gen2?","text":"<p>Data Lake Storage Gen2 = Blob Storage + hierarchical file system</p> <ul> <li>Blob Storage compatible : Uses the same APIs</li> <li>File system : Hierarchical organization</li> <li>Big Data optimized : For analytics and ML</li> <li>Integration : With Azure Synapse, Databricks, etc.</li> </ul>"},{"location":"Cloud/Azure/EN/02-storage/#enable-data-lake-storage-gen2","title":"Enable Data Lake Storage Gen2","text":"<p>When creating the account: 1. In \"Advanced\" \u2192 Enable \"Hierarchical namespace\" 2. Create the account</p> <p>\u26a0\ufe0f Important: Cannot be enabled after creation.</p>"},{"location":"Cloud/Azure/EN/02-storage/#data-lake-structure","title":"Data Lake Structure","text":"<pre><code>data-lake/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2502   \u251c\u2500\u2500 01/\n\u2502   \u2502   \u2514\u2500\u2500 02/\n\u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 2024/\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#create-files-and-folders","title":"Create Files and Folders","text":"<p>Via Azure Portal:</p> <ol> <li>Storage account \u2192 \"Data Lake\"</li> <li>Navigate the structure</li> <li>Upload files</li> <li>Create folders</li> </ol> <p>Via Python:</p> <pre><code>from azure.storage.filedatalake import DataLakeServiceClient\n\n# Connection\naccount_name = \"mydataanalyststorage\"\naccount_key = \"...\"\ndatalake_service_client = DataLakeServiceClient(\n    account_url=f\"https://{account_name}.dfs.core.windows.net\",\n    credential=account_key\n)\n\n# Create a file system\nfile_system_client = datalake_service_client.create_file_system(\"data-lake\")\n\n# Create a directory\ndirectory_client = file_system_client.create_directory(\"raw/2024\")\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#upload-and-manage-files","title":"Upload and Manage Files","text":""},{"location":"Cloud/Azure/EN/02-storage/#upload-a-file","title":"Upload a File","text":"<p>Via Azure Portal:</p> <ol> <li>Container \u2192 \"Upload\"</li> <li>Select the file</li> <li>Click on \"Upload\"</li> </ol> <p>Via Azure CLI:</p> <pre><code>az storage blob upload \\\n  --account-name mydataanalyststorage \\\n  --container-name raw-data \\\n  --name data.csv \\\n  --file ./local-data.csv \\\n  --auth-mode login\n</code></pre> <p>Via Python:</p> <pre><code>from azure.storage.blob import BlobServiceClient\n\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\ncontainer_client = blob_service_client.get_container_client(\"raw-data\")\n\n# Upload a file\nwith open(\"local-data.csv\", \"rb\") as data:\n    container_client.upload_blob(name=\"data.csv\", data=data)\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#download-a-file","title":"Download a File","text":"<p>Via Python:</p> <pre><code># Download a blob\nblob_client = container_client.get_blob_client(\"data.csv\")\nwith open(\"downloaded-data.csv\", \"wb\") as download_file:\n    download_file.write(blob_client.download_blob().readall())\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#list-files","title":"List Files","text":"<p>Via Python:</p> <pre><code># List all blobs in a container\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    print(f\"Name: {blob.name}, Size: {blob.size}\")\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#delete-a-file","title":"Delete a File","text":"<p>Via Python:</p> <pre><code># Delete a blob\nblob_client = container_client.get_blob_client(\"data.csv\")\nblob_client.delete_blob()\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#integration-with-other-services","title":"Integration with Other Services","text":""},{"location":"Cloud/Azure/EN/02-storage/#azure-storage-data-factory","title":"Azure Storage + Data Factory","text":"<p>Usage: - Data source for ETL pipelines - Destination for transformed data</p> <p>Example: <pre><code>{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"connectionString\": \"...\",\n    \"container\": \"raw-data\"\n  }\n}\n</code></pre></p>"},{"location":"Cloud/Azure/EN/02-storage/#azure-storage-azure-sql-database","title":"Azure Storage + Azure SQL Database","text":"<p>Usage: - Import data from Blob Storage - Export data to Blob Storage</p> <p>SQL Example: <pre><code>-- Import from Blob Storage\nBULK INSERT my_table\nFROM 'https://mystorageaccount.blob.core.windows.net/raw-data/data.csv'\nWITH (\n    FORMAT = 'CSV',\n    FIRSTROW = 2\n);\n</code></pre></p>"},{"location":"Cloud/Azure/EN/02-storage/#azure-storage-powerbi","title":"Azure Storage + PowerBI","text":"<p>Usage: - Connect PowerBI to Blob Storage - Analyze files directly</p> <p>Configuration: 1. PowerBI \u2192 \"Get Data\" 2. \"Azure Blob Storage\" 3. Enter container URL 4. Select files</p>"},{"location":"Cloud/Azure/EN/02-storage/#azure-storage-azure-functions","title":"Azure Storage + Azure Functions","text":"<p>Usage: - Trigger Functions on upload - Automatically process files</p> <p>Configuration: 1. Function \u2192 \"Add trigger\" 2. \"Azure Blob Storage trigger\" 3. Configure container and path</p>"},{"location":"Cloud/Azure/EN/02-storage/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/Azure/EN/02-storage/#organization","title":"Organization","text":"<ol> <li>Use containers to organize by project</li> <li>Name clearly files and containers</li> <li>Organize by date : <code>raw/2024/01/data.csv</code></li> <li>Separate by type : <code>raw/</code>, <code>processed/</code>, <code>analytics/</code></li> </ol>"},{"location":"Cloud/Azure/EN/02-storage/#performance","title":"Performance","text":"<ol> <li>Use random names for blobs (avoid sequences)</li> <li>Enable CDN if global distribution needed (paid)</li> <li>Use block blobs for most cases</li> <li>Partition data to improve performance</li> </ol>"},{"location":"Cloud/Azure/EN/02-storage/#costs","title":"Costs","text":"<ol> <li>Monitor usage in Azure Cost Management</li> <li>Delete unnecessary files</li> <li>Use appropriate storage classes</li> <li>Configure lifecycle rules to automate</li> </ol>"},{"location":"Cloud/Azure/EN/02-storage/#security","title":"Security","text":"<ol> <li>Never make containers public (except specific need)</li> <li>Use SAS (Shared Access Signature) for temporary access</li> <li>Enable encryption by default</li> <li>Use Azure AD for authentication</li> </ol>"},{"location":"Cloud/Azure/EN/02-storage/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/Azure/EN/02-storage/#example-1-upload-a-csv-file","title":"Example 1: Upload a CSV File","text":"<pre><code>from azure.storage.blob import BlobServiceClient\nimport pandas as pd\n\n# Connection\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=...\"\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\ncontainer_client = blob_service_client.get_container_client(\"raw-data\")\n\n# Read a local file\ndf = pd.read_csv(\"local-data.csv\")\n\n# Upload to Azure Storage\nwith open(\"local-data.csv\", \"rb\") as data:\n    container_client.upload_blob(name=\"2024/01/data.csv\", data=data)\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#example-2-download-and-process","title":"Example 2: Download and Process","text":"<pre><code># Download from Azure Storage\nblob_client = container_client.get_blob_client(\"2024/01/data.csv\")\nwith open(\"downloaded-data.csv\", \"wb\") as download_file:\n    download_file.write(blob_client.download_blob().readall())\n\n# Process\ndf = pd.read_csv(\"downloaded-data.csv\")\n# ... processing ...\n\n# Upload result\ndf.to_csv(\"processed-data.csv\", index=False)\nwith open(\"processed-data.csv\", \"rb\") as data:\n    container_client.upload_blob(name=\"processed/2024/01/data.csv\", data=data)\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#example-3-list-and-filter","title":"Example 3: List and Filter","text":"<pre><code># List all files in a prefix\nblob_list = container_client.list_blobs(name_starts_with=\"2024/01/\")\nfor blob in blob_list:\n    print(f\"File: {blob.name}, Size: {blob.size} bytes, Modified: {blob.last_modified}\")\n</code></pre>"},{"location":"Cloud/Azure/EN/02-storage/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Azure Storage = Unlimited storage and highly available</li> <li>Free Tier: 5 GB for 12 months</li> <li>Blob Storage for files, Data Lake Gen2 for Big Data</li> <li>Organize with containers and prefixes</li> <li>Native integration with all Azure data services</li> </ol>"},{"location":"Cloud/Azure/EN/02-storage/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 3. Azure Data Factory - Cloud ETL to learn how to create ETL pipelines on Azure.</p>"},{"location":"Cloud/Azure/EN/03-data-factory/","title":"3. Azure Data Factory - Cloud ETL","text":""},{"location":"Cloud/Azure/EN/03-data-factory/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Azure Data Factory and its role</li> <li>Create ETL pipelines</li> <li>Use transformation activities</li> <li>Integrate with data sources</li> <li>Orchestrate workflows</li> </ul>"},{"location":"Cloud/Azure/EN/03-data-factory/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Data Factory</li> <li>Create a Data Factory</li> <li>Create a Pipeline</li> <li>Transformation Activities</li> <li>Integration with Data Sources</li> <li>Orchestration and Scheduling</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#introduction-to-data-factory","title":"Introduction to Data Factory","text":""},{"location":"Cloud/Azure/EN/03-data-factory/#what-is-azure-data-factory","title":"What is Azure Data Factory?","text":"<p>Azure Data Factory = Managed cloud ETL service</p> <ul> <li>ETL : Extract, Transform, Load</li> <li>Cloud : No infrastructure to manage</li> <li>Managed : Microsoft manages the infrastructure</li> <li>Scalable : Automatically adapts</li> </ul>"},{"location":"Cloud/Azure/EN/03-data-factory/#data-factory-components","title":"Data Factory Components","text":"<ol> <li>Pipelines : ETL workflows</li> <li>Activities : Steps in a pipeline</li> <li>Datasets : Data representations</li> <li>Linked Services : Connections to sources</li> <li>Triggers : Automatic triggering</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#data-factory-free-tier","title":"Data Factory Free Tier","text":"<p>Free forever: - 5 free pipelines - Limited activities - Beyond: usage-based billing</p> <p>\u26a0\ufe0f Important: Monitor costs, especially for transformation activities.</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#create-a-data-factory","title":"Create a Data Factory","text":""},{"location":"Cloud/Azure/EN/03-data-factory/#step-1-access-data-factory","title":"Step 1: Access Data Factory","text":"<ol> <li>Azure Portal \u2192 Search \"Data Factory\"</li> <li>Click on \"Data factories\"</li> <li>Click on \"Create\"</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#step-2-basic-configuration","title":"Step 2: Basic Configuration","text":"<p>Basic Information: - Subscription : Choose your subscription - Resource group : Create or use existing - Name : <code>my-data-factory</code> (globally unique) - Version : V2 (recommended) - Region : Choose the closest region</p> <p>Git Configuration (optional): - Configure Git later : To start quickly - Or configure Git/GitHub for versioning</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#step-3-create-the-data-factory","title":"Step 3: Create the Data Factory","text":"<ol> <li>Click on \"Review + create\"</li> <li>Verify configuration</li> <li>Click on \"Create\"</li> <li>Wait for creation (2-3 minutes)</li> </ol> <p>\u26a0\ufe0f Important: Note the Data Factory name.</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#step-4-open-data-factory-studio","title":"Step 4: Open Data Factory Studio","text":"<ol> <li>Once created, click on \"Open Azure Data Factory Studio\"</li> <li>Web interface to create pipelines</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#create-a-pipeline","title":"Create a Pipeline","text":""},{"location":"Cloud/Azure/EN/03-data-factory/#step-1-create-a-linked-service","title":"Step 1: Create a Linked Service","text":"<p>Linked Service = Connection to a data source</p> <p>Example: Azure Blob Storage</p> <ol> <li>Data Factory Studio \u2192 \"Manage\" \u2192 \"Linked services\"</li> <li>Click on \"+ New\"</li> <li>Search \"Azure Blob Storage\"</li> <li>Configuration:</li> <li>Name : <code>AzureBlobStorage1</code></li> <li>Storage account name : Select your account</li> <li>Authentication method : Account key (or other)</li> <li>Click on \"Create\"</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#step-2-create-a-dataset","title":"Step 2: Create a Dataset","text":"<p>Dataset = Data representation</p> <ol> <li>Data Factory Studio \u2192 \"Author\" \u2192 \"Datasets\"</li> <li>Click on \"+ New\"</li> <li>Choose \"Azure Blob Storage\"</li> <li>Configuration:</li> <li>Name : <code>CSVData</code></li> <li>Linked service : <code>AzureBlobStorage1</code></li> <li>File path : <code>raw-data/</code></li> <li>File format : DelimitedText (CSV)</li> <li>Click on \"Create\"</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#step-3-create-a-pipeline","title":"Step 3: Create a Pipeline","text":"<ol> <li>Data Factory Studio \u2192 \"Author\" \u2192 \"Pipelines\"</li> <li>Click on \"+ New pipeline\"</li> <li>Name the pipeline: <code>CopyCSVToParquet</code></li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#step-4-add-an-activity","title":"Step 4: Add an Activity","text":"<p>Example: Copy Data</p> <ol> <li>In the pipeline, drag \"Copy Data\" from \"Move &amp; transform\"</li> <li>Configure:</li> <li>Source : Dataset <code>CSVData</code></li> <li>Sink (Destination) : Create a new Parquet dataset</li> <li>Click on \"Publish\" to save</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#transformation-activities","title":"Transformation Activities","text":""},{"location":"Cloud/Azure/EN/03-data-factory/#copy-data","title":"Copy Data","text":"<p>Copy data from a source to a destination</p> <p>Configuration: - Source : Source dataset - Sink : Destination dataset - Mapping : Column mapping</p> <p>Example: CSV \u2192 Parquet</p> <pre><code>{\n  \"name\": \"CopyCSVToParquet\",\n  \"type\": \"Copy\",\n  \"inputs\": [{\"referenceName\": \"CSVData\"}],\n  \"outputs\": [{\"referenceName\": \"ParquetData\"}],\n  \"typeProperties\": {\n    \"source\": {\"type\": \"DelimitedTextSource\"},\n    \"sink\": {\"type\": \"ParquetSink\"}\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/EN/03-data-factory/#data-flow","title":"Data Flow","text":"<p>Data transformation with graphical interface</p> <p>Steps: 1. Create a Data Flow 2. Add a source 3. Add transformations:    - Select : Select columns    - Filter : Filter rows    - Derived Column : Create calculated columns    - Aggregate : Aggregations    - Join : Join data 4. Add a sink</p> <p>Example transformations:</p> <pre><code>Source (CSV) \n  \u2192 Select (columns)\n  \u2192 Filter (status = 'active')\n  \u2192 Derived Column (new column)\n  \u2192 Aggregate (SUM, COUNT)\n  \u2192 Sink (Parquet)\n</code></pre>"},{"location":"Cloud/Azure/EN/03-data-factory/#lookup","title":"Lookup","text":"<p>Look up values in another source</p> <p>Usage: - Validate data - Enrich data - Check references</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#stored-procedure","title":"Stored Procedure","text":"<p>Execute a SQL stored procedure</p> <p>Usage: - Processing in SQL Database - Complex business logic - Database-side optimization</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#integration-with-data-sources","title":"Integration with Data Sources","text":""},{"location":"Cloud/Azure/EN/03-data-factory/#azure-blob-storage","title":"Azure Blob Storage","text":"<p>Data source:</p> <pre><code>{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"connectionString\": \"...\",\n    \"container\": \"raw-data\"\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/EN/03-data-factory/#azure-sql-database","title":"Azure SQL Database","text":"<p>Data source:</p> <pre><code>{\n  \"type\": \"AzureSqlDatabase\",\n  \"typeProperties\": {\n    \"connectionString\": \"...\",\n    \"tableName\": \"users\"\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/EN/03-data-factory/#azure-data-lake-storage-gen2","title":"Azure Data Lake Storage Gen2","text":"<p>Data source:</p> <pre><code>{\n  \"type\": \"AzureBlobFS\",\n  \"typeProperties\": {\n    \"url\": \"https://account.dfs.core.windows.net\",\n    \"fileSystem\": \"data-lake\"\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/EN/03-data-factory/#local-files-via-self-hosted-ir","title":"Local Files (via Self-hosted IR)","text":"<p>Integration Runtime: - Self-hosted IR for local file access - Install on a local machine - Connect to Data Factory</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#orchestration-and-scheduling","title":"Orchestration and Scheduling","text":""},{"location":"Cloud/Azure/EN/03-data-factory/#trigger-manually","title":"Trigger Manually","text":"<ol> <li>Data Factory Studio \u2192 \"Monitor\"</li> <li>Select the pipeline</li> <li>Click on \"Trigger now\"</li> <li>View execution in real-time</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#schedule-a-pipeline-trigger","title":"Schedule a Pipeline (Trigger)","text":"<p>Create a trigger:</p> <ol> <li>Pipeline \u2192 \"Add trigger\" \u2192 \"New/Edit\"</li> <li>Type: \"Schedule\"</li> <li>Configuration:</li> <li>Name : <code>DailyTrigger</code></li> <li>Type : Schedule</li> <li>Recurrence : Daily</li> <li>Start time : 02:00</li> <li>Click on \"OK\"</li> </ol> <p>Trigger types: - Schedule : Scheduled (cron) - Event : Triggered by event - Tumbling window : Sliding window</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#trigger-by-event","title":"Trigger by Event","text":"<p>Example: New file in Blob Storage</p> <ol> <li>Create a \"Storage event\" trigger</li> <li>Configure:</li> <li>Storage account : Your account</li> <li>Container : <code>raw-data</code></li> <li>Event type : Blob created</li> <li>Associate with pipeline</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/Azure/EN/03-data-factory/#performance","title":"Performance","text":"<ol> <li>Use Data Flow for complex transformations</li> <li>Optimize activities to reduce time</li> <li>Use parallelism when possible</li> <li>Choose the right regions to reduce latency</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#costs","title":"Costs","text":"<ol> <li>Monitor executions in Monitor</li> <li>Use the 5 free pipelines wisely</li> <li>Optimize Data Flows (expensive)</li> <li>Stop unused pipelines</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#organization","title":"Organization","text":"<ol> <li>Name clearly pipelines and activities</li> <li>Document transformations</li> <li>Version with Git</li> <li>Test before publishing</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#security","title":"Security","text":"<ol> <li>Use Key Vault for secrets</li> <li>Limit permissions of Linked Services</li> <li>Audit executions</li> <li>Encrypt data in transit</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/Azure/EN/03-data-factory/#example-1-simple-csv-parquet-pipeline","title":"Example 1: Simple CSV \u2192 Parquet Pipeline","text":"<p>Pipeline: 1. Source: Azure Blob Storage (CSV) 2. Activity: Copy Data 3. Sink: Azure Blob Storage (Parquet)</p> <p>Configuration: - Source: <code>raw-data/data.csv</code> - Sink: <code>processed-data/data.parquet</code> - Format: DelimitedText \u2192 Parquet</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#example-2-pipeline-with-transformation","title":"Example 2: Pipeline with Transformation","text":"<p>Pipeline: 1. Source: Azure SQL Database 2. Data Flow:    - Select columns    - Filter rows    - Aggregate 3. Sink: Azure Blob Storage (Parquet)</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#example-3-orchestrated-pipeline","title":"Example 3: Orchestrated Pipeline","text":"<p>Pipeline: 1. Lookup: Check if new data 2. If Condition: If new data 3. Copy Data: Copy to staging 4. Data Flow: Transform 5. Copy Data: Load into destination</p>"},{"location":"Cloud/Azure/EN/03-data-factory/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Data Factory = Cloud ETL managed by Microsoft</li> <li>Free Tier: 5 pipelines free</li> <li>Pipelines orchestrate activities</li> <li>Data Flows for complex transformations</li> <li>Triggers enable automation</li> </ol>"},{"location":"Cloud/Azure/EN/03-data-factory/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 4. Azure SQL Database - Database to learn how to use SQL Database on Azure.</p>"},{"location":"Cloud/Azure/EN/04-sql-database/","title":"4. Azure SQL Database - Database","text":""},{"location":"Cloud/Azure/EN/04-sql-database/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Azure SQL Database</li> <li>Create a SQL Database (free up to 32 GB)</li> <li>Migrate data</li> <li>Optimize queries</li> <li>Integrate with PowerBI</li> </ul>"},{"location":"Cloud/Azure/EN/04-sql-database/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to SQL Database</li> <li>Create a SQL Database</li> <li>Connect to the Database</li> <li>Load Data</li> <li>SQL Queries</li> <li>Integration with PowerBI</li> </ol>"},{"location":"Cloud/Azure/EN/04-sql-database/#introduction-to-sql-database","title":"Introduction to SQL Database","text":""},{"location":"Cloud/Azure/EN/04-sql-database/#what-is-azure-sql-database","title":"What is Azure SQL Database?","text":"<p>Azure SQL Database = Managed cloud SQL database</p> <ul> <li>SQL Server compatible : Standard SQL syntax</li> <li>Managed : Microsoft manages the infrastructure</li> <li>Scalable : From a few GB to several TB</li> <li>High availability : 99.99% availability</li> </ul>"},{"location":"Cloud/Azure/EN/04-sql-database/#use-cases-for-data-analyst","title":"Use Cases for Data Analyst","text":"<ul> <li>Data Warehouse : Centralize data</li> <li>Analytics : Complex queries</li> <li>Business Intelligence : Source for PowerBI</li> <li>Data Integration : Central point for ETL</li> </ul>"},{"location":"Cloud/Azure/EN/04-sql-database/#sql-database-free-tier","title":"SQL Database Free Tier","text":"<p>Free for 12 months: - Basic tier : Up to 32 GB - DTU : 5 DTU (Database Transaction Units) - Backup : Automatic (7 days)</p> <p>\u26a0\ufe0f Important: After 12 months, normal billing. Monitor costs.</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#create-a-sql-database","title":"Create a SQL Database","text":""},{"location":"Cloud/Azure/EN/04-sql-database/#step-1-access-sql-database","title":"Step 1: Access SQL Database","text":"<ol> <li>Azure Portal \u2192 Search \"SQL databases\"</li> <li>Click on \"SQL databases\"</li> <li>Click on \"Create\"</li> </ol>"},{"location":"Cloud/Azure/EN/04-sql-database/#step-2-basic-configuration","title":"Step 2: Basic Configuration","text":"<p>Basic Information: - Subscription : Choose your subscription - Resource group : Create or use existing - Database name : <code>analytics-db</code> - Server : Create a new server or use existing</p> <p>Create a SQL Server: - Server name : <code>my-sql-server-xxxxx</code> (globally unique) - Location : Choose the region - Authentication method : SQL authentication (or Azure AD) - Server admin login : <code>sqladmin</code> (or other) - Password : Strong password - Allow Azure services : \u2705 Yes (for Data Factory)</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#step-3-database-configuration","title":"Step 3: Database Configuration","text":"<p>Compute + storage: - Service tier : Basic (for Free Tier) - Compute tier : Serverless (or Provisioned) - Storage : 2 GB (free, extensible up to 32 GB)</p> <p>\u26a0\ufe0f Important: Basic tier = 5 DTU, sufficient to start.</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#step-4-network-configuration","title":"Step 4: Network Configuration","text":"<p>Networking: - Public endpoint : \u2705 Enable - Firewall rules :   - \u2705 Allow Azure services and resources   - Add your IP for local access</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#step-5-create-the-database","title":"Step 5: Create the Database","text":"<ol> <li>Click on \"Review + create\"</li> <li>Verify configuration</li> <li>Click on \"Create\"</li> <li>Wait for creation (2-3 minutes)</li> </ol> <p>\u26a0\ufe0f Important: Note the server name and credentials.</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#connect-to-the-database","title":"Connect to the Database","text":""},{"location":"Cloud/Azure/EN/04-sql-database/#via-azure-portal-query-editor","title":"Via Azure Portal (Query Editor)","text":"<ol> <li>SQL Database \u2192 \"Query editor\"</li> <li>Enter credentials</li> <li>Execute SQL queries</li> </ol>"},{"location":"Cloud/Azure/EN/04-sql-database/#via-sql-server-management-studio-ssms","title":"Via SQL Server Management Studio (SSMS)","text":"<p>Download SSMS: - https://aka.ms/ssmsfullsetup</p> <p>Connection: - Server name : <code>my-sql-server-xxxxx.database.windows.net</code> - Authentication : SQL Server Authentication - Login : <code>sqladmin</code> - Password : Your password</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#via-azure-data-studio","title":"Via Azure Data Studio","text":"<p>Download Azure Data Studio: - https://aka.ms/azuredatastudio</p> <p>Advantages: - Free and open-source - Modern interface - Notebook support - Git integration</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#via-python-pyodbc","title":"Via Python (pyodbc)","text":"<pre><code>import pyodbc\n\n# Connection\nserver = 'my-sql-server-xxxxx.database.windows.net'\ndatabase = 'analytics-db'\nusername = 'sqladmin'\npassword = 'your-password'\ndriver = '{ODBC Driver 17 for SQL Server}'\n\nconn = pyodbc.connect(\n    f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n)\n\n# Execute a query\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM users\")\nrows = cursor.fetchall()\nfor row in rows:\n    print(row)\n</code></pre>"},{"location":"Cloud/Azure/EN/04-sql-database/#load-data","title":"Load Data","text":""},{"location":"Cloud/Azure/EN/04-sql-database/#method-1-insert-small-quantities","title":"Method 1: INSERT (Small Quantities)","text":"<pre><code>INSERT INTO users (id, name, email, created_at)\nVALUES (1, 'John Doe', 'john@example.com', '2024-01-01');\n</code></pre>"},{"location":"Cloud/Azure/EN/04-sql-database/#method-2-bulk-insert-from-blob-storage","title":"Method 2: BULK INSERT from Blob Storage","text":"<p>Prerequisites: - Create a SAS key for Blob Storage - Create a credential in SQL Database</p> <p>Example:</p> <pre><code>-- Create a credential\nCREATE DATABASE SCOPED CREDENTIAL BlobCredential\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'your-sas-token';\n\n-- Create an external data source\nCREATE EXTERNAL DATA SOURCE BlobStorage\nWITH (\n    TYPE = BLOB_STORAGE,\n    LOCATION = 'https://mystorageaccount.blob.core.windows.net',\n    CREDENTIAL = BlobCredential\n);\n\n-- Import from Blob Storage\nBULK INSERT users\nFROM 'raw-data/users.csv'\nWITH (\n    DATA_SOURCE = 'BlobStorage',\n    FORMAT = 'CSV',\n    FIRSTROW = 2,\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n'\n);\n</code></pre>"},{"location":"Cloud/Azure/EN/04-sql-database/#method-3-via-data-factory","title":"Method 3: Via Data Factory","text":"<p>Pipeline: 1. Source: Azure Blob Storage (CSV) 2. Activity: Copy Data 3. Sink: Azure SQL Database</p> <p>Configuration: - Source: <code>raw-data/users.csv</code> - Sink: Table <code>users</code> in SQL Database - Mapping: Automatic or manual columns</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#method-4-via-python-pandas","title":"Method 4: Via Python (pandas)","text":"<pre><code>import pandas as pd\nimport pyodbc\n\n# Read a CSV file\ndf = pd.read_csv('users.csv')\n\n# Connection\nconn = pyodbc.connect(connection_string)\n\n# Write to SQL Database\ndf.to_sql('users', conn, if_exists='append', index=False)\n</code></pre>"},{"location":"Cloud/Azure/EN/04-sql-database/#sql-queries","title":"SQL Queries","text":""},{"location":"Cloud/Azure/EN/04-sql-database/#basic-queries","title":"Basic Queries","text":"<p>Simple SELECT:</p> <pre><code>SELECT * FROM users LIMIT 10;\n</code></pre> <p>Filter:</p> <pre><code>SELECT id, name, email\nFROM users\nWHERE created_at &gt; '2024-01-01'\nORDER BY created_at DESC;\n</code></pre> <p>Aggregations:</p> <pre><code>SELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count,\n    COUNT(DISTINCT email) AS unique_emails\nFROM users\nGROUP BY DATE_TRUNC('month', created_at)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/Azure/EN/04-sql-database/#advanced-queries","title":"Advanced Queries","text":"<p>Window functions:</p> <pre><code>SELECT \n    id,\n    name,\n    created_at,\n    ROW_NUMBER() OVER (PARTITION BY DATE_TRUNC('month', created_at) ORDER BY created_at) AS rank\nFROM users;\n</code></pre> <p>Joins:</p> <pre><code>SELECT \n    u.name,\n    o.amount,\n    o.created_at\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.created_at &gt; '2024-01-01';\n</code></pre> <p>CTE (Common Table Expressions):</p> <pre><code>WITH monthly_users AS (\n    SELECT \n        DATE_TRUNC('month', created_at) AS month,\n        COUNT(*) AS user_count\n    FROM users\n    GROUP BY DATE_TRUNC('month', created_at)\n)\nSELECT \n    month,\n    user_count,\n    LAG(user_count, 1) OVER (ORDER BY month) AS previous_month\nFROM monthly_users;\n</code></pre>"},{"location":"Cloud/Azure/EN/04-sql-database/#integration-with-powerbi","title":"Integration with PowerBI","text":""},{"location":"Cloud/Azure/EN/04-sql-database/#direct-connection","title":"Direct Connection","text":"<p>Step 1: In PowerBI Desktop</p> <ol> <li>\"Get Data\" \u2192 \"Azure\" \u2192 \"Azure SQL Database\"</li> <li>Enter information:</li> <li>Server : <code>my-sql-server-xxxxx.database.windows.net</code></li> <li>Database : <code>analytics-db</code></li> <li>Data connectivity mode : Import (or DirectQuery)</li> </ol> <p>Step 2: Authentication</p> <ul> <li>Authentication method : Database</li> <li>Username : <code>sqladmin</code></li> <li>Password : Your password</li> </ul> <p>Step 3: Select Tables</p> <ul> <li>Choose tables to import</li> <li>Click on \"Load\"</li> </ul>"},{"location":"Cloud/Azure/EN/04-sql-database/#directquery-vs-import","title":"DirectQuery vs Import","text":"<p>Import: - \u2705 Fast for visualizations - \u2705 Works offline - \u274c Static data (requires refresh)</p> <p>DirectQuery: - \u2705 Real-time data - \u2705 No size limit - \u274c Slower (queries on each interaction)</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#create-visualizations","title":"Create Visualizations","text":"<p>Example: 1. Import the <code>users</code> table 2. Create a chart: Number of users per month 3. Add filters 4. Publish to PowerBI Service</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/Azure/EN/04-sql-database/#performance","title":"Performance","text":"<ol> <li>Create indexes on frequently used columns</li> <li>Optimize queries with EXPLAIN</li> <li>Use views to simplify</li> <li>Partition large tables</li> </ol>"},{"location":"Cloud/Azure/EN/04-sql-database/#costs","title":"Costs","text":"<ol> <li>Monitor usage in Azure Cost Management</li> <li>Use Basic tier to start</li> <li>Stop the database if unused (Serverless)</li> <li>Clean up unnecessary data</li> </ol>"},{"location":"Cloud/Azure/EN/04-sql-database/#security","title":"Security","text":"<ol> <li>Use Azure AD for authentication</li> <li>Limit access with firewall rules</li> <li>Encrypt data (enabled by default)</li> <li>Audit access with SQL Auditing</li> </ol>"},{"location":"Cloud/Azure/EN/04-sql-database/#organization","title":"Organization","text":"<ol> <li>Name clearly tables and columns</li> <li>Document schemas</li> <li>Use schemas to organize</li> <li>Version SQL scripts (Git)</li> </ol>"},{"location":"Cloud/Azure/EN/04-sql-database/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/Azure/EN/04-sql-database/#example-1-complete-blob-sql-database-pipeline","title":"Example 1: Complete Blob \u2192 SQL Database Pipeline","text":"<p>Via Data Factory: 1. Source: Azure Blob Storage (CSV) 2. Activity: Copy Data 3. Sink: Azure SQL Database 4. Trigger: Schedule (daily)</p>"},{"location":"Cloud/Azure/EN/04-sql-database/#example-2-analytical-queries","title":"Example 2: Analytical Queries","text":"<pre><code>-- Top 10 users by spending\nSELECT TOP 10\n    u.name,\n    SUM(o.amount) AS total_spent,\n    COUNT(o.id) AS order_count\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.created_at &gt;= DATEADD(month, -3, GETDATE())\nGROUP BY u.name\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"Cloud/Azure/EN/04-sql-database/#example-3-export-to-powerbi","title":"Example 3: Export to PowerBI","text":"<ol> <li>Create a view for PowerBI</li> <li>Connect PowerBI to the view</li> <li>Create visualizations</li> <li>Publish the report</li> </ol>"},{"location":"Cloud/Azure/EN/04-sql-database/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>SQL Database = Cloud SQL database managed by Microsoft</li> <li>Free Tier: 32 GB for 12 months (Basic tier)</li> <li>SQL Server compatible : Standard syntax</li> <li>PowerBI integration : Direct connection</li> <li>Scalable : From Basic to Premium</li> </ol>"},{"location":"Cloud/Azure/EN/04-sql-database/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 5. Azure Synapse Analytics - Data Warehouse to learn how to use Synapse for data analysis.</p>"},{"location":"Cloud/Azure/EN/05-synapse/","title":"5. Azure Synapse Analytics - Data Warehouse","text":""},{"location":"Cloud/Azure/EN/05-synapse/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Azure Synapse Analytics</li> <li>Create a Synapse workspace</li> <li>Load data</li> <li>Execute advanced SQL queries</li> <li>Integrate with PowerBI</li> </ul>"},{"location":"Cloud/Azure/EN/05-synapse/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Synapse</li> <li>Create a Synapse Workspace</li> <li>Load Data</li> <li>Advanced SQL Queries</li> <li>Integration with PowerBI</li> <li>Best Practices</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#introduction-to-synapse","title":"Introduction to Synapse","text":""},{"location":"Cloud/Azure/EN/05-synapse/#what-is-azure-synapse-analytics","title":"What is Azure Synapse Analytics?","text":"<p>Azure Synapse Analytics = Unified analytics platform</p> <ul> <li>Data Warehouse : Data storage and analysis</li> <li>Big Data : Large-scale processing</li> <li>SQL : Standard SQL queries</li> <li>Spark : Distributed processing</li> <li>Integration : With all Azure services</li> </ul>"},{"location":"Cloud/Azure/EN/05-synapse/#synapse-components","title":"Synapse Components","text":"<ol> <li>SQL Pool : SQL data warehouse (formerly SQL Data Warehouse)</li> <li>Spark Pool : Spark clusters for Big Data</li> <li>Synapse Studio : Unified web interface</li> <li>Pipelines : Integrated ETL</li> <li>Notebooks : Python, SQL, Scala</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#synapse-free-tier","title":"Synapse Free Tier","text":"<p>Free with Azure credit: - Use the 200$ free credit (30 days) - After: normal billing</p> <p>\u26a0\ufe0f Important: Synapse can be expensive. Monitor costs carefully.</p>"},{"location":"Cloud/Azure/EN/05-synapse/#create-a-synapse-workspace","title":"Create a Synapse Workspace","text":""},{"location":"Cloud/Azure/EN/05-synapse/#step-1-access-synapse","title":"Step 1: Access Synapse","text":"<ol> <li>Azure Portal \u2192 Search \"Azure Synapse Analytics\"</li> <li>Click on \"Azure Synapse Analytics\"</li> <li>Click on \"Create\"</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#step-2-basic-configuration","title":"Step 2: Basic Configuration","text":"<p>Basic Information: - Subscription : Choose your subscription - Resource group : Create or use existing - Workspace name : <code>my-synapse-workspace</code> - Region : Choose the region - Data Lake Storage Gen2 : Create new or use existing</p> <p>SQL Administrator: - SQL admin name : <code>sqladmin</code> - Password : Strong password</p>"},{"location":"Cloud/Azure/EN/05-synapse/#step-3-sql-pool-configuration","title":"Step 3: SQL Pool Configuration","text":"<p>SQL Pool: - Create a SQL pool : \u2705 Yes (to start) - Performance level : DW100c (cheapest) - Or : Create later (Serverless SQL)</p> <p>\u26a0\ufe0f Important: Serverless SQL = pay-per-query, more economical to start.</p>"},{"location":"Cloud/Azure/EN/05-synapse/#step-4-create-the-workspace","title":"Step 4: Create the Workspace","text":"<ol> <li>Click on \"Review + create\"</li> <li>Verify configuration</li> <li>Click on \"Create\"</li> <li>Wait for creation (5-10 minutes)</li> </ol> <p>\u26a0\ufe0f Important: Note SQL credentials.</p>"},{"location":"Cloud/Azure/EN/05-synapse/#load-data","title":"Load Data","text":""},{"location":"Cloud/Azure/EN/05-synapse/#method-1-copy-from-data-lake-storage","title":"Method 1: COPY from Data Lake Storage","text":"<p>Fastest for large quantities:</p> <pre><code>-- Create a table\nCREATE TABLE users (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at DATETIME2\n)\nWITH (\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n-- Load from Data Lake Storage\nCOPY INTO users\nFROM 'https://mystorageaccount.dfs.core.windows.net/data-lake/raw/users.csv'\nWITH (\n    FILE_TYPE = 'CSV',\n    FIRSTROW = 2,\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n'\n);\n</code></pre>"},{"location":"Cloud/Azure/EN/05-synapse/#method-2-via-synapse-pipelines","title":"Method 2: Via Synapse Pipelines","text":"<p>Integrated pipeline:</p> <ol> <li>Synapse Studio \u2192 \"Integrate\" \u2192 \"Pipelines\"</li> <li>Create a new pipeline</li> <li>Add \"Copy Data\" activity</li> <li>Source: Azure Blob Storage or Data Lake</li> <li>Sink: SQL Pool</li> <li>Execute the pipeline</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#method-3-insert-small-quantities","title":"Method 3: INSERT (Small Quantities)","text":"<pre><code>INSERT INTO users (id, name, email, created_at)\nVALUES (1, 'John Doe', 'john@example.com', '2024-01-01');\n</code></pre>"},{"location":"Cloud/Azure/EN/05-synapse/#method-4-via-polybase-external-tables","title":"Method 4: Via PolyBase (External Tables)","text":"<p>Create an external table:</p> <pre><code>-- Create a credential\nCREATE DATABASE SCOPED CREDENTIAL BlobCredential\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'your-sas-token';\n\n-- Create an external data source\nCREATE EXTERNAL DATA SOURCE BlobStorage\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'wasbs://container@account.blob.core.windows.net',\n    CREDENTIAL = BlobCredential\n);\n\n-- Create an external file format\nCREATE EXTERNAL FILE FORMAT CSVFormat\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (FIELD_TERMINATOR = ',')\n);\n\n-- Create an external table\nCREATE EXTERNAL TABLE users_external (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(100)\n)\nWITH (\n    LOCATION = 'raw/users.csv',\n    DATA_SOURCE = BlobStorage,\n    FILE_FORMAT = CSVFormat\n);\n\n-- Load into internal table\nINSERT INTO users\nSELECT * FROM users_external;\n</code></pre>"},{"location":"Cloud/Azure/EN/05-synapse/#advanced-sql-queries","title":"Advanced SQL Queries","text":""},{"location":"Cloud/Azure/EN/05-synapse/#basic-queries","title":"Basic Queries","text":"<p>Simple SELECT:</p> <pre><code>SELECT TOP 100 * FROM users;\n</code></pre> <p>Aggregations:</p> <pre><code>SELECT \n    YEAR(created_at) AS year,\n    MONTH(created_at) AS month,\n    COUNT(*) AS user_count\nFROM users\nGROUP BY YEAR(created_at), MONTH(created_at)\nORDER BY year, month;\n</code></pre>"},{"location":"Cloud/Azure/EN/05-synapse/#window-functions","title":"Window Functions","text":"<p>ROW_NUMBER:</p> <pre><code>SELECT \n    id,\n    name,\n    created_at,\n    ROW_NUMBER() OVER (PARTITION BY YEAR(created_at) ORDER BY created_at) AS rank\nFROM users;\n</code></pre> <p>LAG/LEAD:</p> <pre><code>SELECT \n    date,\n    sales,\n    LAG(sales, 1) OVER (ORDER BY date) AS previous_sales,\n    LEAD(sales, 1) OVER (ORDER BY date) AS next_sales\nFROM daily_sales;\n</code></pre>"},{"location":"Cloud/Azure/EN/05-synapse/#distribution-and-performance","title":"Distribution and Performance","text":"<p>Distribution keys:</p> <pre><code>-- HASH distribution (for joins)\nCREATE TABLE users (\n    id INT,\n    name VARCHAR(100)\n)\nWITH (\n    DISTRIBUTION = HASH(id),\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n-- ROUND_ROBIN distribution (default)\nCREATE TABLE logs (\n    id INT,\n    message VARCHAR(MAX)\n)\nWITH (\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n</code></pre> <p>Clustered Columnstore Index: - Optimized for analytics - High compression - Fast queries on large tables</p>"},{"location":"Cloud/Azure/EN/05-synapse/#integration-with-powerbi","title":"Integration with PowerBI","text":""},{"location":"Cloud/Azure/EN/05-synapse/#direct-connection","title":"Direct Connection","text":"<p>Step 1: In PowerBI Desktop</p> <ol> <li>\"Get Data\" \u2192 \"Azure\" \u2192 \"Azure Synapse Analytics SQL\"</li> <li>Enter information:</li> <li>Server : <code>my-synapse-workspace-ondemand.sql.azuresynapse.net</code> (Serverless)</li> <li>Database : Database name</li> <li>Data connectivity mode : DirectQuery (recommended)</li> </ol> <p>Step 2: Authentication</p> <ul> <li>Authentication method : Database</li> <li>Username : <code>sqladmin</code></li> <li>Password : Your password</li> </ul> <p>Step 3: Select Tables</p> <ul> <li>Choose tables or views</li> <li>Click on \"Load\"</li> </ul>"},{"location":"Cloud/Azure/EN/05-synapse/#create-views-for-powerbi","title":"Create Views for PowerBI","text":"<p>Optimized view:</p> <pre><code>CREATE VIEW vw_user_analytics AS\nSELECT \n    u.id,\n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name, u.email;\n</code></pre> <p>Use the view in PowerBI: - Simpler for users - Centralized business logic - Optimized performance</p>"},{"location":"Cloud/Azure/EN/05-synapse/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/Azure/EN/05-synapse/#performance","title":"Performance","text":"<ol> <li>Use Columnstore Index for analytics</li> <li>Choose the right distribution keys</li> <li>Partition large tables</li> <li>Optimize queries with EXPLAIN</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#costs","title":"Costs","text":"<ol> <li>Use Serverless SQL to start (pay-per-query)</li> <li>Pause the SQL Pool when unused</li> <li>Monitor costs in Azure Cost Management</li> <li>Use the right pool sizes</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#organization","title":"Organization","text":"<ol> <li>Create schemas to organize</li> <li>Name clearly tables and views</li> <li>Document schemas</li> <li>Use views to simplify</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#security","title":"Security","text":"<ol> <li>Use Azure AD for authentication</li> <li>Limit access with firewall rules</li> <li>Encrypt data (enabled by default)</li> <li>Audit access</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/Azure/EN/05-synapse/#example-1-complete-data-lake-synapse-pipeline","title":"Example 1: Complete Data Lake \u2192 Synapse Pipeline","text":"<p>Synapse Pipeline: 1. Source: Data Lake Storage (Parquet) 2. Activity: Copy Data 3. Sink: SQL Pool 4. Trigger: Schedule (daily)</p>"},{"location":"Cloud/Azure/EN/05-synapse/#example-2-complex-analytical-queries","title":"Example 2: Complex Analytical Queries","text":"<pre><code>-- Sales analysis with window functions\nWITH monthly_sales AS (\n    SELECT \n        YEAR(sale_date) AS year,\n        MONTH(sale_date) AS month,\n        SUM(amount) AS total_sales\n    FROM sales\n    GROUP BY YEAR(sale_date), MONTH(sale_date)\n)\nSELECT \n    year,\n    month,\n    total_sales,\n    LAG(total_sales, 1) OVER (ORDER BY year, month) AS previous_month,\n    (total_sales - LAG(total_sales, 1) OVER (ORDER BY year, month)) / \n        LAG(total_sales, 1) OVER (ORDER BY year, month) * 100 AS growth_percent\nFROM monthly_sales\nORDER BY year, month;\n</code></pre>"},{"location":"Cloud/Azure/EN/05-synapse/#example-3-export-to-powerbi","title":"Example 3: Export to PowerBI","text":"<ol> <li>Create an analytical view</li> <li>Connect PowerBI to the view</li> <li>Create visualizations</li> <li>Publish the report</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Synapse = Unified analytics platform</li> <li>SQL Pool for data warehouse</li> <li>Serverless SQL for pay-per-query</li> <li>Native PowerBI integration</li> <li>Scalable from a few GB to several PB</li> </ol>"},{"location":"Cloud/Azure/EN/05-synapse/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 6. Azure Databricks - Big Data Analytics to learn how to use Databricks for Big Data.</p>"},{"location":"Cloud/Azure/EN/06-databricks/","title":"6. Azure Databricks - Big Data Analytics","text":""},{"location":"Cloud/Azure/EN/06-databricks/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Azure Databricks</li> <li>Create a Databricks workspace</li> <li>Use Python/SQL notebooks</li> <li>Process data with Spark</li> <li>Integrate with other Azure services</li> </ul>"},{"location":"Cloud/Azure/EN/06-databricks/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Databricks</li> <li>Create a Databricks Workspace</li> <li>Create a Cluster</li> <li>Python/SQL Notebooks</li> <li>Data Processing with Spark</li> <li>Integration with Other Services</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#introduction-to-databricks","title":"Introduction to Databricks","text":""},{"location":"Cloud/Azure/EN/06-databricks/#what-is-azure-databricks","title":"What is Azure Databricks?","text":"<p>Azure Databricks = Big Data platform based on Apache Spark</p> <ul> <li>Apache Spark : Distributed processing engine</li> <li>Notebooks : Python, SQL, Scala, R</li> <li>Managed : Microsoft manages the infrastructure</li> <li>Scalable : Auto-scaling clusters</li> </ul>"},{"location":"Cloud/Azure/EN/06-databricks/#use-cases-for-data-analyst","title":"Use Cases for Data Analyst","text":"<ul> <li>Big Data processing : Process large quantities</li> <li>ETL : Complex transformations</li> <li>Machine Learning : Integrated MLlib</li> <li>Data Science : Interactive notebooks</li> </ul>"},{"location":"Cloud/Azure/EN/06-databricks/#databricks-free-tier","title":"Databricks Free Tier","text":"<p>Free with Azure credit: - Use the 200$ free credit (30 days) - After: normal billing</p> <p>\u26a0\ufe0f Important: Databricks can be expensive. Monitor costs carefully.</p>"},{"location":"Cloud/Azure/EN/06-databricks/#create-a-databricks-workspace","title":"Create a Databricks Workspace","text":""},{"location":"Cloud/Azure/EN/06-databricks/#step-1-access-databricks","title":"Step 1: Access Databricks","text":"<ol> <li>Azure Portal \u2192 Search \"Azure Databricks\"</li> <li>Click on \"Azure Databricks\"</li> <li>Click on \"Create\"</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#step-2-basic-configuration","title":"Step 2: Basic Configuration","text":"<p>Basic Information: - Subscription : Choose your subscription - Resource group : Create or use existing - Workspace name : <code>my-databricks-workspace</code> - Region : Choose the region - Pricing tier : Standard (or Premium)</p> <p>Networking: - Virtual network : Create new or use existing - Public IP : \u2705 Enable (for easy access)</p>"},{"location":"Cloud/Azure/EN/06-databricks/#step-3-create-the-workspace","title":"Step 3: Create the Workspace","text":"<ol> <li>Click on \"Review + create\"</li> <li>Verify configuration</li> <li>Click on \"Create\"</li> <li>Wait for creation (5-10 minutes)</li> </ol> <p>\u26a0\ufe0f Important: Note the workspace URL.</p>"},{"location":"Cloud/Azure/EN/06-databricks/#step-4-open-databricks","title":"Step 4: Open Databricks","text":"<ol> <li>Once created, click on \"Launch Workspace\"</li> <li>Databricks web interface</li> <li>Sign in with Azure AD</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#create-a-cluster","title":"Create a Cluster","text":""},{"location":"Cloud/Azure/EN/06-databricks/#step-1-access-clusters","title":"Step 1: Access Clusters","text":"<ol> <li>Databricks Workspace \u2192 \"Compute\"</li> <li>Click on \"Create Cluster\"</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#step-2-cluster-configuration","title":"Step 2: Cluster Configuration","text":"<p>Basic Configuration: - Cluster name : <code>my-cluster</code> - Cluster mode : Standard (or Single Node for tests) - Databricks runtime version : Latest LTS (recommended) - Python version : 3.11</p> <p>Node Type: - Worker type : Standard_DS3_v2 (to start) - Driver type : Standard_DS3_v2 - Min workers : 0 (to save) - Max workers : 2 (to start)</p> <p>\u26a0\ufe0f Important: Min workers = 0 allows auto-termination when inactive.</p>"},{"location":"Cloud/Azure/EN/06-databricks/#step-3-advanced-options","title":"Step 3: Advanced Options","text":"<p>Auto-termination: - \u2705 Enable (stops cluster after inactivity) - Terminate after : 30 minutes</p> <p>Tags: - Add tags for organization</p>"},{"location":"Cloud/Azure/EN/06-databricks/#step-4-create-the-cluster","title":"Step 4: Create the Cluster","text":"<ol> <li>Click on \"Create Cluster\"</li> <li>Wait for startup (3-5 minutes)</li> <li>Cluster ready when status = \"Running\"</li> </ol> <p>\u26a0\ufe0f Important: Cluster consumes resources even when inactive. Stop when unused.</p>"},{"location":"Cloud/Azure/EN/06-databricks/#pythonsql-notebooks","title":"Python/SQL Notebooks","text":""},{"location":"Cloud/Azure/EN/06-databricks/#create-a-notebook","title":"Create a Notebook","text":"<p>Step 1: Create a Notebook</p> <ol> <li>Databricks Workspace \u2192 \"Workspace\"</li> <li>Right-click \u2192 \"Create\" \u2192 \"Notebook\"</li> <li>Name: <code>data-processing</code></li> <li>Language: Python (or SQL)</li> <li>Cluster: Attach to created cluster</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#step-2-use-the-notebook","title":"Step 2: Use the Notebook","text":"<p>Python Cells:</p> <pre><code># Cell 1: Import libraries\nimport pandas as pd\nfrom pyspark.sql import SparkSession\n\n# Cell 2: Create a Spark session\nspark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()\n\n# Cell 3: Read data\ndf = spark.read.csv(\"dbfs:/FileStore/data/users.csv\", header=True, inferSchema=True)\n\n# Cell 4: Display data\ndf.show()\n\n# Cell 5: Transform\ndf_filtered = df.filter(df[\"status\"] == \"active\")\ndf_filtered.show()\n</code></pre> <p>SQL Cells:</p> <pre><code>-- SQL Cell: Create a temporary view\nCREATE OR REPLACE TEMPORARY VIEW users AS\nSELECT * FROM csv.`dbfs:/FileStore/data/users.csv`\n\n-- SQL Query\nSELECT \n    YEAR(created_at) AS year,\n    COUNT(*) AS user_count\nFROM users\nGROUP BY YEAR(created_at)\nORDER BY year;\n</code></pre>"},{"location":"Cloud/Azure/EN/06-databricks/#execute-a-notebook","title":"Execute a Notebook","text":"<ul> <li>Run cell : Execute a cell</li> <li>Run all : Execute all cells</li> <li>Run all above : Execute all cells above</li> </ul>"},{"location":"Cloud/Azure/EN/06-databricks/#data-processing-with-spark","title":"Data Processing with Spark","text":""},{"location":"Cloud/Azure/EN/06-databricks/#read-data","title":"Read Data","text":"<p>From Data Lake Storage:</p> <pre><code># Read CSV\ndf = spark.read.csv(\n    \"abfss://container@account.dfs.core.windows.net/data/users.csv\",\n    header=True,\n    inferSchema=True\n)\n\n# Read Parquet\ndf = spark.read.parquet(\n    \"abfss://container@account.dfs.core.windows.net/data/users.parquet\"\n)\n\n# Read JSON\ndf = spark.read.json(\n    \"abfss://container@account.dfs.core.windows.net/data/users.json\"\n)\n</code></pre> <p>From Azure Blob Storage:</p> <pre><code># Configure access\nspark.conf.set(\n    \"fs.azure.account.key.accountname.blob.core.windows.net\",\n    \"your-account-key\"\n)\n\n# Read\ndf = spark.read.csv(\n    \"wasbs://container@accountname.blob.core.windows.net/data/users.csv\",\n    header=True\n)\n</code></pre>"},{"location":"Cloud/Azure/EN/06-databricks/#transform-data","title":"Transform Data","text":"<p>Filter:</p> <pre><code>df_filtered = df.filter(df[\"age\"] &gt; 18)\n</code></pre> <p>Select Columns:</p> <pre><code>df_selected = df.select(\"id\", \"name\", \"email\")\n</code></pre> <p>Aggregations:</p> <pre><code>df_aggregated = df.groupBy(\"category\").agg({\n    \"amount\": \"sum\",\n    \"id\": \"count\"\n})\n</code></pre> <p>Join:</p> <pre><code>df_joined = df1.join(df2, df1.id == df2.user_id, \"inner\")\n</code></pre>"},{"location":"Cloud/Azure/EN/06-databricks/#write-data","title":"Write Data","text":"<p>To Data Lake Storage:</p> <pre><code># Write in Parquet\ndf.write.mode(\"overwrite\").parquet(\n    \"abfss://container@account.dfs.core.windows.net/processed/users.parquet\"\n)\n\n# Write in CSV\ndf.write.mode(\"overwrite\").csv(\n    \"abfss://container@account.dfs.core.windows.net/processed/users.csv\"\n)\n</code></pre>"},{"location":"Cloud/Azure/EN/06-databricks/#integration-with-other-services","title":"Integration with Other Services","text":""},{"location":"Cloud/Azure/EN/06-databricks/#databricks-data-lake-storage","title":"Databricks + Data Lake Storage","text":"<p>Direct Access:</p> <pre><code># Configure access\nspark.conf.set(\n    \"fs.azure.account.auth.type.account.dfs.core.windows.net\",\n    \"OAuth\"\n)\nspark.conf.set(\n    \"fs.azure.account.oauth.provider.type.account.dfs.core.windows.net\",\n    \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\"\n)\n\n# Read\ndf = spark.read.parquet(\n    \"abfss://container@account.dfs.core.windows.net/data/users.parquet\"\n)\n</code></pre>"},{"location":"Cloud/Azure/EN/06-databricks/#databricks-azure-sql-database","title":"Databricks + Azure SQL Database","text":"<p>Read from SQL Database:</p> <pre><code>df = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://server.database.windows.net:1433;database=db\") \\\n    .option(\"dbtable\", \"users\") \\\n    .option(\"user\", \"sqladmin\") \\\n    .option(\"password\", \"password\") \\\n    .load()\n</code></pre> <p>Write to SQL Database:</p> <pre><code>df.write \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://server.database.windows.net:1433;database=db\") \\\n    .option(\"dbtable\", \"users_processed\") \\\n    .option(\"user\", \"sqladmin\") \\\n    .option(\"password\", \"password\") \\\n    .mode(\"overwrite\") \\\n    .save()\n</code></pre>"},{"location":"Cloud/Azure/EN/06-databricks/#databricks-data-factory","title":"Databricks + Data Factory","text":"<p>Data Factory Pipeline: 1. Source: Azure Blob Storage 2. Activity: Databricks Notebook 3. Sink: Azure SQL Database</p> <p>Configuration: - Notebook path: <code>/Workspace/path/to/notebook</code> - Parameters: Pass parameters</p>"},{"location":"Cloud/Azure/EN/06-databricks/#best-practices","title":"Best Practices","text":""},{"location":"Cloud/Azure/EN/06-databricks/#performance","title":"Performance","text":"<ol> <li>Use cache to reuse data</li> <li>Partition data to improve performance</li> <li>Optimize transformations to reduce time</li> <li>Use the right number of workers</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#costs","title":"Costs","text":"<ol> <li>Stop clusters when unused</li> <li>Use auto-termination to save</li> <li>Monitor costs in Azure Cost Management</li> <li>Use smaller clusters to start</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#organization","title":"Organization","text":"<ol> <li>Organize notebooks in folders</li> <li>Name clearly notebooks and clusters</li> <li>Document code</li> <li>Version with Git</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#security","title":"Security","text":"<ol> <li>Use Azure AD for authentication</li> <li>Limit access with RBAC</li> <li>Encrypt data in transit and at rest</li> <li>Audit access</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#practical-examples","title":"Practical Examples","text":""},{"location":"Cloud/Azure/EN/06-databricks/#example-1-complete-etl-pipeline","title":"Example 1: Complete ETL Pipeline","text":"<p>Databricks Notebook:</p> <pre><code># 1. Read from Data Lake\ndf = spark.read.parquet(\n    \"abfss://container@account.dfs.core.windows.net/raw/users.parquet\"\n)\n\n# 2. Transform\ndf_processed = df \\\n    .filter(df[\"status\"] == \"active\") \\\n    .select(\"id\", \"name\", \"email\", \"created_at\") \\\n    .withColumn(\"year\", year(col(\"created_at\")))\n\n# 3. Write to Data Lake\ndf_processed.write.mode(\"overwrite\").parquet(\n    \"abfss://container@account.dfs.core.windows.net/processed/users.parquet\"\n)\n</code></pre>"},{"location":"Cloud/Azure/EN/06-databricks/#example-2-analysis-with-spark-sql","title":"Example 2: Analysis with Spark SQL","text":"<pre><code># Create a temporary view\ndf.createOrReplaceTempView(\"users\")\n\n# SQL Query\nresult = spark.sql(\"\"\"\n    SELECT \n        YEAR(created_at) AS year,\n        COUNT(*) AS user_count,\n        COUNT(DISTINCT email) AS unique_emails\n    FROM users\n    GROUP BY YEAR(created_at)\n    ORDER BY year\n\"\"\")\n\nresult.show()\n</code></pre>"},{"location":"Cloud/Azure/EN/06-databricks/#example-3-integration-with-data-factory","title":"Example 3: Integration with Data Factory","text":"<ol> <li>Create a Databricks notebook</li> <li>In Data Factory, add \"Databricks Notebook\" activity</li> <li>Configure the notebook</li> <li>Execute the pipeline</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Databricks = Big Data with Apache Spark</li> <li>Notebooks Python/SQL for development</li> <li>Auto-scaling clusters for performance</li> <li>Native integration with Azure services</li> <li>Paid : Monitor costs</li> </ol>"},{"location":"Cloud/Azure/EN/06-databricks/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 7. Practical Projects to create complete projects with Azure.</p>"},{"location":"Cloud/Azure/EN/07-projets/","title":"7. Azure Practical Projects","text":""},{"location":"Cloud/Azure/EN/07-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Apply acquired knowledge</li> <li>Create complete ETL pipelines</li> <li>Integrate with PowerBI</li> <li>Create projects for your portfolio</li> <li>Use multiple Azure services</li> </ul>"},{"location":"Cloud/Azure/EN/07-projets/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Project 1: ETL Pipeline Blob \u2192 SQL Database</li> <li>Project 2: Data Lake with Synapse</li> <li>Project 3: Analytics with PowerBI</li> <li>Project 4: Complete Automated Pipeline</li> <li>Best Practices for Portfolio</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#project-1-etl-pipeline-blob-sql-database","title":"Project 1: ETL Pipeline Blob \u2192 SQL Database","text":""},{"location":"Cloud/Azure/EN/07-projets/#objective","title":"Objective","text":"<p>Create an ETL pipeline that loads CSV files from Blob Storage to SQL Database.</p>"},{"location":"Cloud/Azure/EN/07-projets/#architecture","title":"Architecture","text":"<pre><code>Blob Storage (CSV) \u2192 Data Factory \u2192 SQL Database \u2192 PowerBI\n</code></pre>"},{"location":"Cloud/Azure/EN/07-projets/#steps","title":"Steps","text":""},{"location":"Cloud/Azure/EN/07-projets/#1-prepare-data","title":"1. Prepare Data","text":"<p>Create a Blob Storage container: - Name: <code>raw-data</code> - Upload a test CSV file</p> <p>Example CSV data: <pre><code>id,name,email,created_at,status\n1,John Doe,john@example.com,2024-01-01,active\n2,Jane Smith,jane@example.com,2024-01-02,inactive\n</code></pre></p>"},{"location":"Cloud/Azure/EN/07-projets/#2-create-a-sql-database","title":"2. Create a SQL Database","text":"<ol> <li>Azure Portal \u2192 Create SQL Database</li> <li>Configuration:</li> <li>Name: <code>analytics-db</code></li> <li>Server: Create new server</li> <li>Service tier: Basic (free 12 months)</li> <li>Create the database</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#3-create-the-table-in-sql-database","title":"3. Create the Table in SQL Database","text":"<pre><code>CREATE TABLE users (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at DATETIME2,\n    status VARCHAR(20)\n);\n</code></pre>"},{"location":"Cloud/Azure/EN/07-projets/#4-create-a-data-factory-pipeline","title":"4. Create a Data Factory Pipeline","text":"<ol> <li>Data Factory Studio \u2192 \"Author\" \u2192 \"Pipelines\"</li> <li>Create a new pipeline: <code>LoadCSVToSQL</code></li> <li>Add \"Copy Data\" activity</li> <li>Configuration:</li> <li>Source : Azure Blob Storage (CSV)</li> <li>Sink : Azure SQL Database (table users)</li> <li>Publish the pipeline</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#5-execute-the-pipeline","title":"5. Execute the Pipeline","text":"<ol> <li>Click on \"Trigger now\"</li> <li>Check execution in \"Monitor\"</li> <li>Check data in SQL Database</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#result","title":"Result","text":"<ul> <li>CSV files loaded into SQL Database</li> <li>Functional ETL pipeline</li> <li>Ready for analytics with PowerBI</li> </ul>"},{"location":"Cloud/Azure/EN/07-projets/#project-2-data-lake-with-synapse","title":"Project 2: Data Lake with Synapse","text":""},{"location":"Cloud/Azure/EN/07-projets/#objective_1","title":"Objective","text":"<p>Create a complete Data Lake with ingestion, transformation, and analytics.</p>"},{"location":"Cloud/Azure/EN/07-projets/#architecture_1","title":"Architecture","text":"<pre><code>Sources \u2192 Data Lake Storage (Raw) \u2192 Synapse (Transform) \u2192 Data Lake (Processed) \u2192 PowerBI\n                \u2193\n        Data Factory (Orchestration)\n</code></pre>"},{"location":"Cloud/Azure/EN/07-projets/#steps_1","title":"Steps","text":""},{"location":"Cloud/Azure/EN/07-projets/#1-data-lake-storage-structure","title":"1. Data Lake Storage Structure","text":"<pre><code>data-lake/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u251c\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"Cloud/Azure/EN/07-projets/#2-create-a-synapse-workspace","title":"2. Create a Synapse Workspace","text":"<ol> <li>Azure Portal \u2192 Create Azure Synapse Analytics</li> <li>Configuration:</li> <li>Workspace name: <code>my-synapse-workspace</code></li> <li>Data Lake Storage: Create new</li> <li>Create the workspace</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#3-data-factory-pipelines-for-transformation","title":"3. Data Factory Pipelines for Transformation","text":"<p>Pipeline for users:</p> <ol> <li>Synapse Studio \u2192 \"Integrate\" \u2192 \"Pipelines\"</li> <li>Create pipeline: <code>TransformUsers</code></li> <li>Activities:</li> <li>Source: Data Lake Storage (raw/users/)</li> <li>Data Flow: Transform (filter, clean)</li> <li>Sink: Data Lake Storage (processed/users/)</li> <li>Publish</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#4-synapse-tables-for-analytics","title":"4. Synapse Tables for Analytics","text":"<pre><code>-- Create an external table\nCREATE EXTERNAL TABLE users_processed (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at DATETIME2\n)\nWITH (\n    LOCATION = 'processed/users/',\n    DATA_SOURCE = DataLakeStorage,\n    FILE_FORMAT = ParquetFormat\n);\n\n-- Analytical query\nSELECT \n    YEAR(created_at) AS year,\n    COUNT(*) AS user_count\nFROM users_processed\nGROUP BY YEAR(created_at);\n</code></pre>"},{"location":"Cloud/Azure/EN/07-projets/#5-automation-with-triggers","title":"5. Automation with Triggers","text":"<ol> <li>Pipeline \u2192 \"Add trigger\" \u2192 \"New/Edit\"</li> <li>Type: Schedule</li> <li>Recurrence: Daily</li> <li>Start time: 02:00</li> <li>Save</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#result_1","title":"Result","text":"<ul> <li>Functional Data Lake</li> <li>Automated pipeline</li> <li>Analytics with Synapse</li> <li>Complete project for portfolio</li> </ul>"},{"location":"Cloud/Azure/EN/07-projets/#project-3-analytics-with-powerbi","title":"Project 3: Analytics with PowerBI","text":""},{"location":"Cloud/Azure/EN/07-projets/#objective_2","title":"Objective","text":"<p>Create a complete analytics system with PowerBI connected to Azure.</p>"},{"location":"Cloud/Azure/EN/07-projets/#steps_2","title":"Steps","text":""},{"location":"Cloud/Azure/EN/07-projets/#1-prepare-data_1","title":"1. Prepare Data","text":"<p>In SQL Database or Synapse: - Load data - Create analytical views</p>"},{"location":"Cloud/Azure/EN/07-projets/#2-connect-powerbi-to-azure-sql-database","title":"2. Connect PowerBI to Azure SQL Database","text":"<ol> <li>PowerBI Desktop \u2192 \"Get Data\"</li> <li>\"Azure\" \u2192 \"Azure SQL Database\"</li> <li>Configuration:</li> <li>Server: <code>my-sql-server.database.windows.net</code></li> <li>Database: <code>analytics-db</code></li> <li>Authentication: Database</li> <li>Select tables or views</li> <li>Click on \"Load\"</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#3-create-visualizations","title":"3. Create Visualizations","text":"<p>Example: 1. Import the <code>users</code> table 2. Create a chart: Number of users per month 3. Add filters 4. Create a dashboard</p>"},{"location":"Cloud/Azure/EN/07-projets/#4-publish-to-powerbi-service","title":"4. Publish to PowerBI Service","text":"<ol> <li>PowerBI Desktop \u2192 \"Publish\"</li> <li>Select the workspace</li> <li>Publish</li> <li>Access the report on powerbi.com</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#5-refresh-data","title":"5. Refresh Data","text":"<ol> <li>PowerBI Service \u2192 Dataset \u2192 \"Schedule refresh\"</li> <li>Configuration:</li> <li>Frequency: Daily</li> <li>Time: 03:00</li> <li>Save</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#result_2","title":"Result","text":"<ul> <li>Analytics with PowerBI</li> <li>Interactive visualizations</li> <li>Automatic refresh</li> <li>Complete project for portfolio</li> </ul>"},{"location":"Cloud/Azure/EN/07-projets/#project-4-complete-automated-pipeline","title":"Project 4: Complete Automated Pipeline","text":""},{"location":"Cloud/Azure/EN/07-projets/#objective_3","title":"Objective","text":"<p>Create a completely automated ETL pipeline with multiple Azure services.</p>"},{"location":"Cloud/Azure/EN/07-projets/#complete-architecture","title":"Complete Architecture","text":"<pre><code>CSV file uploaded \u2192 Blob Storage (raw/)\n    \u2193 (Event)\nAzure Function (Validation)\n    \u2193\nBlob Storage (validated/)\n    \u2193 (Trigger)\nData Factory Pipeline (Transform CSV \u2192 Parquet)\n    \u2193\nData Lake Storage (processed/)\n    \u2193\nSynapse (Analytics)\n    \u2193\nSQL Database (Results)\n    \u2193\nPowerBI (Visualization)\n</code></pre>"},{"location":"Cloud/Azure/EN/07-projets/#implementation","title":"Implementation","text":""},{"location":"Cloud/Azure/EN/07-projets/#1-azure-function-for-validation","title":"1. Azure Function for Validation","text":"<pre><code>import azure.functions as func\nimport logging\nimport csv\nfrom azure.storage.blob import BlobServiceClient\n\ndef main(blob: func.InputStream):\n    logging.info(f'Processing blob: {blob.name}')\n\n    # Read the blob\n    content = blob.read().decode('utf-8')\n    reader = csv.DictReader(content.splitlines())\n\n    # Validate\n    valid_rows = []\n    for row in reader:\n        if row.get('email') and '@' in row['email']:\n            valid_rows.append(row)\n\n    # Upload validated data\n    if valid_rows:\n        # Upload to validated/\n        # ...\n\n    logging.info(f'Validated {len(valid_rows)} rows')\n</code></pre>"},{"location":"Cloud/Azure/EN/07-projets/#2-data-factory-transformation-pipeline","title":"2. Data Factory Transformation Pipeline","text":"<p>Pipeline: 1. Source: Blob Storage (validated/) 2. Data Flow: Transform (clean, enrich) 3. Sink: Data Lake Storage (processed/parquet/)</p>"},{"location":"Cloud/Azure/EN/07-projets/#3-synapse-for-analytics","title":"3. Synapse for Analytics","text":"<pre><code>-- Create an analytical view\nCREATE VIEW vw_user_analytics AS\nSELECT \n    u.id,\n    u.name,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n</code></pre>"},{"location":"Cloud/Azure/EN/07-projets/#4-powerbi-for-visualization","title":"4. PowerBI for Visualization","text":"<ol> <li>Connect PowerBI to Synapse</li> <li>Use the view <code>vw_user_analytics</code></li> <li>Create visualizations</li> <li>Publish the report</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#result_3","title":"Result","text":"<ul> <li>Completely automated pipeline</li> <li>Automatic validation</li> <li>Automatic transformation</li> <li>Analytics available immediately</li> <li>PowerBI visualizations</li> </ul>"},{"location":"Cloud/Azure/EN/07-projets/#best-practices-for-portfolio","title":"Best Practices for Portfolio","text":""},{"location":"Cloud/Azure/EN/07-projets/#documentation","title":"Documentation","text":"<p>Create a README for each project:</p> <pre><code># Project: Azure ETL Pipeline\n\n## Description\nAutomated ETL pipeline to transform CSV data to Parquet.\n\n## Architecture\n- Blob Storage: Storage\n- Data Factory: Transformation\n- SQL Database: Database\n- PowerBI: Visualization\n\n## Results\n- 50% cost reduction\n- 70% processing time reduction\n</code></pre>"},{"location":"Cloud/Azure/EN/07-projets/#visualizations","title":"Visualizations","text":"<p>Create diagrams: - System architecture - Data flow - Data schema</p> <p>Tools: - Draw.io - Lucidchart - ASCII diagrams in README</p>"},{"location":"Cloud/Azure/EN/07-projets/#metrics","title":"Metrics","text":"<p>Include metrics: - Execution time before/after - Costs before/after - Volume of processed data - Query performance</p>"},{"location":"Cloud/Azure/EN/07-projets/#code","title":"Code","text":"<p>Best practices: - Commented code - Environment variables for configuration - Error handling - Logging</p>"},{"location":"Cloud/Azure/EN/07-projets/#github","title":"GitHub","text":"<p>Create a repository: - README with documentation - Data Factory scripts (JSON) - SQL scripts - Configuration - Diagrams</p>"},{"location":"Cloud/Azure/EN/07-projets/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Practical projects : Essential for portfolio</li> <li>Documentation : Explain architecture and results</li> <li>Metrics : Show impact (performance, costs)</li> <li>Clean code : Commented and organized</li> <li>GitHub : Share your projects</li> </ol>"},{"location":"Cloud/Azure/EN/07-projets/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>Azure Architecture Center</li> <li>Azure Solutions</li> <li>GitHub Azure Examples</li> </ul> <p>Congratulations! You have completed the Azure training for Data Analyst. You can now create complete projects on Azure using the available free resources.</p>"},{"location":"Cloud/Azure/FR/","title":"Formation Azure pour Data Analyst - Guide Gratuit","text":""},{"location":"Cloud/Azure/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'apprentissage de Microsoft Azure en tant que Data Analyst, en utilisant uniquement des ressources gratuites. Vous apprendrez \u00e0 utiliser les services Azure essentiels pour l'analyse de donn\u00e9es sans d\u00e9penser un centime.</p>"},{"location":"Cloud/Azure/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre les services Azure essentiels pour Data Analyst</li> <li>Cr\u00e9er et g\u00e9rer des comptes Azure gratuits</li> <li>Utiliser Azure Data Factory, SQL Database, et autres services data</li> <li>Construire des pipelines ETL sur Azure</li> <li>Analyser des donn\u00e9es avec Azure</li> <li>Int\u00e9grer avec PowerBI</li> <li>Cr\u00e9er des projets pratiques pour votre portfolio</li> </ul>"},{"location":"Cloud/Azure/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 Compte Azure gratuit : 200$ de cr\u00e9dit (30 jours) + services gratuits - \u2705 Microsoft Learn : Cours interactifs gratuits - \u2705 Documentation Azure : Guides complets gratuits - \u2705 Labs Azure : Labs pratiques gratuits</p> <p>Budget total : 0\u20ac</p>"},{"location":"Cloud/Azure/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"Cloud/Azure/FR/#1-prise-en-main-azure","title":"1. Prise en main Azure","text":"<ul> <li>Cr\u00e9er un compte Azure gratuit</li> <li>Comprendre les cr\u00e9dits gratuits</li> <li>Naviguer dans le portail Azure</li> <li>Configuration de s\u00e9curit\u00e9 (Azure AD)</li> </ul>"},{"location":"Cloud/Azure/FR/#2-azure-storage-stockage-de-donnees","title":"2. Azure Storage - Stockage de donn\u00e9es","text":"<ul> <li>Cr\u00e9er des comptes de stockage</li> <li>Blob Storage, Data Lake Storage</li> <li>Uploader et g\u00e9rer des fichiers</li> <li>Organisation des donn\u00e9es</li> </ul>"},{"location":"Cloud/Azure/FR/#3-azure-data-factory-etl-cloud","title":"3. Azure Data Factory - ETL Cloud","text":"<ul> <li>Cr\u00e9er des pipelines ETL</li> <li>Activit\u00e9s de transformation</li> <li>Int\u00e9gration avec sources de donn\u00e9es</li> <li>Orchestration de workflows</li> </ul>"},{"location":"Cloud/Azure/FR/#4-azure-sql-database-base-de-donnees","title":"4. Azure SQL Database - Base de donn\u00e9es","text":"<ul> <li>Cr\u00e9er une base SQL Database (gratuit jusqu'\u00e0 32 Go)</li> <li>Migrer des donn\u00e9es</li> <li>Optimisation de requ\u00eates</li> <li>Int\u00e9gration avec PowerBI</li> </ul>"},{"location":"Cloud/Azure/FR/#5-azure-synapse-analytics-data-warehouse","title":"5. Azure Synapse Analytics - Data Warehouse","text":"<ul> <li>Cr\u00e9er un workspace Synapse</li> <li>Charger des donn\u00e9es</li> <li>Requ\u00eates SQL avanc\u00e9es</li> <li>Int\u00e9gration avec PowerBI</li> </ul>"},{"location":"Cloud/Azure/FR/#6-azure-databricks-big-data-analytics","title":"6. Azure Databricks - Big Data Analytics","text":"<ul> <li>Cr\u00e9er un cluster Databricks</li> <li>Traitement de donn\u00e9es avec Spark</li> <li>Notebooks Python/SQL</li> <li>Int\u00e9gration avec autres services</li> </ul>"},{"location":"Cloud/Azure/FR/#7-projets-pratiques","title":"7. Projets pratiques","text":"<ul> <li>Pipeline ETL complet Azure</li> <li>Int\u00e9gration PowerBI</li> <li>Projet pour portfolio</li> <li>Bonnes pratiques</li> </ul>"},{"location":"Cloud/Azure/FR/#demarrage-rapide","title":"\ud83d\ude80 D\u00e9marrage rapide","text":""},{"location":"Cloud/Azure/FR/#etape-1-creer-un-compte-azure-gratuit","title":"\u00c9tape 1 : Cr\u00e9er un compte Azure gratuit","text":"<ol> <li>Aller sur : https://azure.microsoft.com/fr-fr/free/</li> <li>Cliquer sur \"D\u00e9marrer gratuitement\"</li> <li>Se connecter avec un compte Microsoft</li> <li>V\u00e9rifier votre identit\u00e9 par t\u00e9l\u00e9phone</li> <li>Entrer les informations de carte bancaire (non d\u00e9bit\u00e9e)</li> </ol> <p>Important : Azure vous donne 200$ de cr\u00e9dit pour 30 jours, puis des services gratuits permanents.</p>"},{"location":"Cloud/Azure/FR/#etape-2-explorer-les-services-gratuits","title":"\u00c9tape 2 : Explorer les services gratuits","text":"<p>Services gratuits utiles pour Data Analyst :</p> <ul> <li>Azure SQL Database : Gratuit jusqu'\u00e0 32 Go (12 mois)</li> <li>Azure Storage : 5 Go (12 mois)</li> <li>Azure Data Factory : Gratuit jusqu'\u00e0 5 pipelines</li> <li>Azure Functions : 1 million d'ex\u00e9cutions/mois (toujours gratuit)</li> <li>Azure Cosmos DB : 400 RU/s (toujours gratuit)</li> </ul>"},{"location":"Cloud/Azure/FR/#etape-3-suivre-la-formation","title":"\u00c9tape 3 : Suivre la formation","text":"<ol> <li>Commencez par le module 1 (Prise en main)</li> <li>Suivez l'ordre des modules</li> <li>Utilisez Microsoft Learn pour les labs</li> <li>Pratiquez avec les projets du module 7</li> <li>Surveillez votre utilisation dans Azure Cost Management</li> </ol>"},{"location":"Cloud/Azure/FR/#services-azure-essentiels-pour-data-analyst","title":"\ud83d\udcca Services Azure essentiels pour Data Analyst","text":"Service Usage Free Tier Storage Stockage de donn\u00e9es 5 Go (12 mois) Data Factory ETL cloud 5 pipelines gratuits SQL Database Base de donn\u00e9es 32 Go (12 mois) Synapse Data warehouse Cr\u00e9dit gratuit Databricks Big Data Cr\u00e9dit gratuit Functions Serverless 1M ex\u00e9cutions/mois PowerBI Visualisation 1 utilisateur gratuit"},{"location":"Cloud/Azure/FR/#gestion-des-couts","title":"\u26a0\ufe0f Gestion des co\u00fbts","text":""},{"location":"Cloud/Azure/FR/#conseils-pour-rester-gratuit","title":"Conseils pour rester gratuit","text":"<ol> <li>Surveiller la facturation</li> <li>Activer les alertes de co\u00fbt</li> <li>V\u00e9rifier r\u00e9guli\u00e8rement Azure Cost Management</li> <li> <p>Limite recommand\u00e9e : 5\u20ac d'alerte</p> </li> <li> <p>Utiliser les services gratuits</p> </li> <li>Privil\u00e9gier les services toujours gratuits</li> <li>Utiliser les cr\u00e9dits gratuits intelligemment</li> <li> <p>Arr\u00eater les ressources non utilis\u00e9es</p> </li> <li> <p>Supprimer les ressources inutilis\u00e9es</p> </li> <li>Arr\u00eater les machines virtuelles</li> <li>Supprimer les comptes de stockage vides</li> <li> <p>Nettoyer r\u00e9guli\u00e8rement</p> </li> <li> <p>Utiliser les r\u00e9gions gratuites</p> </li> <li>Certaines r\u00e9gions offrent plus de services gratuits</li> <li>V\u00e9rifier la disponibilit\u00e9 par r\u00e9gion</li> </ol>"},{"location":"Cloud/Azure/FR/#ressources-gratuites","title":"\ud83d\udcda Ressources gratuites","text":""},{"location":"Cloud/Azure/FR/#formation-officielle-microsoft","title":"Formation officielle Microsoft","text":"<ul> <li>Microsoft Learn : https://learn.microsoft.com/fr-fr/</li> <li>Parcours interactifs gratuits</li> <li>Labs Azure int\u00e9gr\u00e9s</li> <li>Certifications pr\u00e9par\u00e9es</li> <li> <p>Parcours recommand\u00e9s :</p> <ul> <li>\"Azure Data Fundamentals\"</li> <li>\"Azure Data Factory\"</li> <li>\"Azure SQL Database\"</li> </ul> </li> <li> <p>Documentation Azure : https://learn.microsoft.com/fr-fr/azure/</p> </li> <li>Guides complets</li> <li>Tutoriels pas \u00e0 pas</li> <li> <p>Exemples de code</p> </li> <li> <p>Azure Labs : Labs pratiques gratuits</p> </li> <li>Acc\u00e8s via Microsoft Learn</li> <li>Environnements Azure temporaires</li> </ul>"},{"location":"Cloud/Azure/FR/#ressources-externes-gratuites","title":"Ressources externes gratuites","text":"<ul> <li>YouTube : Cha\u00eene officielle Microsoft Azure</li> <li>GitHub : Exemples Azure Microsoft</li> <li>Azure Blog : Articles et tutoriels</li> </ul>"},{"location":"Cloud/Azure/FR/#certifications-optionnel","title":"\ud83c\udf93 Certifications (optionnel)","text":""},{"location":"Cloud/Azure/FR/#az-900-azure-fundamentals","title":"AZ-900 : Azure Fundamentals","text":"<ul> <li>Co\u00fbt : ~100\u20ac</li> <li>Pr\u00e9paration : Gratuite (Microsoft Learn)</li> <li>Dur\u00e9e : 2-3 semaines</li> <li>Niveau : D\u00e9butant</li> </ul>"},{"location":"Cloud/Azure/FR/#dp-900-azure-data-fundamentals","title":"DP-900 : Azure Data Fundamentals","text":"<ul> <li>Co\u00fbt : ~100\u20ac</li> <li>Pr\u00e9paration : Gratuite (Microsoft Learn)</li> <li>Dur\u00e9e : 3-4 semaines</li> <li>Niveau : Data Analyst</li> </ul>"},{"location":"Cloud/Azure/FR/#dp-203-azure-data-engineer-associate","title":"DP-203 : Azure Data Engineer Associate","text":"<ul> <li>Co\u00fbt : ~165\u20ac</li> <li>Pr\u00e9paration : Gratuite (Microsoft Learn)</li> <li>Dur\u00e9e : 2-3 mois</li> <li>Niveau : Avanc\u00e9</li> </ul>"},{"location":"Cloud/Azure/FR/#integration-avec-powerbi","title":"\ud83d\udd17 Int\u00e9gration avec PowerBI","text":"<p>Azure s'int\u00e8gre parfaitement avec PowerBI :</p> <ul> <li>Azure SQL Database \u2192 PowerBI (connexion directe)</li> <li>Azure Data Factory \u2192 PowerBI (pipeline de donn\u00e9es)</li> <li>Azure Synapse \u2192 PowerBI (data warehouse)</li> <li>Azure Storage \u2192 PowerBI (fichiers)</li> </ul> <p>Cette int\u00e9gration est un avantage majeur pour Data Analyst utilisant PowerBI.</p>"},{"location":"Cloud/Azure/FR/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>Tous les exemples utilisent le Free Tier</li> <li>Les co\u00fbts sont indiqu\u00e9s si d\u00e9passement</li> <li>Les commandes sont test\u00e9es sur Azure Portal</li> <li>Les temps peuvent varier selon la r\u00e9gion</li> </ul>"},{"location":"Cloud/Azure/FR/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>Cette formation est con\u00e7ue pour \u00eatre \u00e9volutive. N'h\u00e9sitez pas \u00e0 proposer des am\u00e9liorations ou des cas d'usage suppl\u00e9mentaires.</p>"},{"location":"Cloud/Azure/FR/#ressources-complementaires","title":"\ud83d\udcda Ressources compl\u00e9mentaires","text":"<ul> <li>Compte Azure gratuit</li> <li>Microsoft Learn</li> <li>Documentation Azure</li> <li>Azure Pricing Calculator</li> </ul>"},{"location":"Cloud/Azure/FR/01-getting-started/","title":"1. Prise en main Azure","text":""},{"location":"Cloud/Azure/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er un compte Azure gratuit</li> <li>Comprendre les cr\u00e9dits gratuits Azure</li> <li>Naviguer dans le portail Azure</li> <li>Configurer la s\u00e9curit\u00e9 de base (Azure AD)</li> <li>Surveiller les co\u00fbts</li> </ul>"},{"location":"Cloud/Azure/FR/01-getting-started/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Cr\u00e9er un compte Azure gratuit</li> <li>Comprendre les cr\u00e9dits gratuits</li> <li>Naviguer dans le portail Azure</li> <li>Configuration Azure AD (s\u00e9curit\u00e9)</li> <li>Surveillance des co\u00fbts</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#creer-un-compte-azure-gratuit","title":"Cr\u00e9er un compte Azure gratuit","text":""},{"location":"Cloud/Azure/FR/01-getting-started/#etape-1-inscription","title":"\u00c9tape 1 : Inscription","text":"<ol> <li>Aller sur le site Azure</li> <li>URL : https://azure.microsoft.com/fr-fr/free/</li> <li> <p>Cliquer sur \"D\u00e9marrer gratuitement\"</p> </li> <li> <p>Se connecter avec Microsoft</p> </li> <li>Utiliser un compte Microsoft existant</li> <li> <p>Ou cr\u00e9er un nouveau compte Microsoft</p> </li> <li> <p>V\u00e9rification d'identit\u00e9</p> </li> <li>Code re\u00e7u par SMS ou email</li> <li> <p>Entrer le code de v\u00e9rification</p> </li> <li> <p>Informations personnelles</p> </li> <li>Nom</li> <li>Pr\u00e9nom</li> <li>Num\u00e9ro de t\u00e9l\u00e9phone</li> <li> <p>Pays</p> </li> <li> <p>V\u00e9rification par t\u00e9l\u00e9phone</p> </li> <li>Appel automatique ou SMS</li> <li> <p>Entrer le code de v\u00e9rification</p> </li> <li> <p>M\u00e9thode de paiement</p> </li> <li>Important : Carte bancaire requise mais non d\u00e9bit\u00e9e</li> <li>Azure vous donne 200$ de cr\u00e9dit pour 30 jours</li> <li>Apr\u00e8s 30 jours : services gratuits permanents</li> <li> <p>Vous pouvez supprimer la carte apr\u00e8s (non recommand\u00e9)</p> </li> <li> <p>V\u00e9rification d'identit\u00e9 finale</p> </li> <li>V\u00e9rification par SMS ou appel</li> <li>Confirmation du compte</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#etape-2-confirmation","title":"\u00c9tape 2 : Confirmation","text":"<ul> <li>Email de confirmation re\u00e7u</li> <li>Compte Azure actif imm\u00e9diatement</li> <li>Acc\u00e8s au portail Azure</li> <li>200$ de cr\u00e9dit disponibles pour 30 jours</li> </ul> <p>\u26a0\ufe0f Important : Ne pas cr\u00e9er plusieurs comptes avec la m\u00eame carte bancaire (risque de suspension).</p>"},{"location":"Cloud/Azure/FR/01-getting-started/#comprendre-les-credits-gratuits","title":"Comprendre les cr\u00e9dits gratuits","text":""},{"location":"Cloud/Azure/FR/01-getting-started/#offre-azure-gratuit","title":"Offre Azure gratuit","text":"<p>Azure offre 3 types de services gratuits :</p>"},{"location":"Cloud/Azure/FR/01-getting-started/#1-credit-de-200-30-jours","title":"1. Cr\u00e9dit de 200$ (30 jours)","text":"<p>Ce que vous pouvez faire : - Tester n'importe quel service Azure - Cr\u00e9er des machines virtuelles - Utiliser des services payants - Exp\u00e9rimenter librement</p> <p>Conditions : - Valable 30 jours apr\u00e8s inscription - Si cr\u00e9dit \u00e9puis\u00e9 avant 30 jours : services arr\u00eat\u00e9s - Apr\u00e8s 30 jours : passage aux services gratuits permanents</p>"},{"location":"Cloud/Azure/FR/01-getting-started/#2-services-gratuits-pendant-12-mois","title":"2. Services gratuits pendant 12 mois","text":"<p>Services utiles pour Data Analyst :</p> <ul> <li>Azure SQL Database : Gratuit jusqu'\u00e0 32 Go (12 mois)</li> <li>Azure Storage : 5 Go (12 mois)</li> <li>Azure App Service : 60 minutes/jour (12 mois)</li> <li>Azure Functions : 1 million d'ex\u00e9cutions/mois (toujours gratuit)</li> </ul> <p>Conditions : - Gratuit pendant 12 mois apr\u00e8s inscription - Limites par mois - Au-del\u00e0 : facturation normale</p>"},{"location":"Cloud/Azure/FR/01-getting-started/#3-services-toujours-gratuits","title":"3. Services toujours gratuits","text":"<p>Services utiles pour Data Analyst :</p> <ul> <li>Azure Functions : 1 million d'ex\u00e9cutions/mois (toujours gratuit)</li> <li>Azure Cosmos DB : 400 RU/s (toujours gratuit)</li> <li>Azure Active Directory : 50 000 objets (toujours gratuit)</li> <li>Azure DevOps : 5 utilisateurs (toujours gratuit)</li> </ul> <p>Conditions : - Gratuit ind\u00e9finiment - Limites par mois - Au-del\u00e0 : facturation au-del\u00e0 de la limite</p>"},{"location":"Cloud/Azure/FR/01-getting-started/#verifier-vos-credits","title":"V\u00e9rifier vos cr\u00e9dits","text":"<ol> <li>Aller dans le portail Azure</li> <li>\"Cost Management + Billing\"</li> <li>Voir les cr\u00e9dits restants</li> <li>Voir l'utilisation par service</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#naviguer-dans-le-portail-azure","title":"Naviguer dans le portail Azure","text":""},{"location":"Cloud/Azure/FR/01-getting-started/#interface-principale","title":"Interface principale","text":"<p>\u00c9l\u00e9ments cl\u00e9s :</p> <ol> <li>Barre de recherche (en haut)</li> <li>Rechercher des services rapidement</li> <li> <p>Exemple : taper \"SQL\" pour trouver SQL Database</p> </li> <li> <p>Menu Azure (ic\u00f4ne \u2630 en haut \u00e0 gauche)</p> </li> <li>Tous les services Azure</li> <li>Organis\u00e9s par cat\u00e9gorie</li> <li> <p>Favoris personnalisables</p> </li> <li> <p>Notifications (en haut \u00e0 droite)</p> </li> <li>Alertes et notifications</li> <li> <p>Statut des d\u00e9ploiements</p> </li> <li> <p>Param\u00e8tres (en haut \u00e0 droite)</p> </li> <li>Param\u00e8tres du compte</li> <li>Th\u00e8me (clair/sombre)</li> <li> <p>Langue</p> </li> <li> <p>Cloud Shell (ic\u00f4ne &gt;_ en haut)</p> </li> <li>Terminal dans le navigateur</li> <li>PowerShell ou Bash</li> <li>Tr\u00e8s utile pour les commandes</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#services-essentiels-pour-data-analyst","title":"Services essentiels pour Data Analyst","text":"<p>Dans le menu Azure, chercher :</p> <ul> <li>Storage accounts : Stockage de donn\u00e9es</li> <li>Data Factory : ETL cloud</li> <li>SQL databases : Bases de donn\u00e9es SQL</li> <li>Synapse Analytics : Data warehouse</li> <li>Databricks : Big Data analytics</li> <li>Functions : Serverless computing</li> </ul>"},{"location":"Cloud/Azure/FR/01-getting-started/#premiere-connexion","title":"Premi\u00e8re connexion","text":"<ol> <li>Se connecter : https://portal.azure.com/</li> <li>Explorer le tableau de bord</li> <li>Cliquer sur \"Tous les services\" pour voir tous les services</li> <li>Utiliser la barre de recherche pour trouver un service</li> <li>\u00c9pingler les services fr\u00e9quents au tableau de bord</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#configuration-azure-ad-securite","title":"Configuration Azure AD (s\u00e9curit\u00e9)","text":""},{"location":"Cloud/Azure/FR/01-getting-started/#quest-ce-quazure-ad","title":"Qu'est-ce qu'Azure AD ?","text":"<p>Azure AD (Azure Active Directory) = Gestion des identit\u00e9s et acc\u00e8s</p> <ul> <li>G\u00e9rer les utilisateurs</li> <li>G\u00e9rer les permissions</li> <li>S\u00e9curiser l'acc\u00e8s aux services</li> <li>Authentification multi-facteurs (MFA)</li> </ul>"},{"location":"Cloud/Azure/FR/01-getting-started/#bonnes-pratiques-de-securite","title":"Bonnes pratiques de s\u00e9curit\u00e9","text":""},{"location":"Cloud/Azure/FR/01-getting-started/#1-activer-lauthentification-multi-facteurs-mfa","title":"1. Activer l'authentification multi-facteurs (MFA)","text":"<p>Pour le compte administrateur :</p> <ol> <li>Aller dans Azure AD</li> <li>\"Utilisateurs\" \u2192 S\u00e9lectionner votre compte</li> <li>\"Authentification multifacteur\"</li> <li>Cliquer sur \"Activer\"</li> <li>Suivre les instructions</li> </ol> <p>\u26a0\ufe0f Important : Toujours activer MFA pour les comptes administrateurs.</p>"},{"location":"Cloud/Azure/FR/01-getting-started/#2-creer-des-utilisateurs-azure-ad-recommande","title":"2. Cr\u00e9er des utilisateurs Azure AD (recommand\u00e9)","text":"<p>Pour le travail en \u00e9quipe :</p> <ol> <li>Aller dans Azure AD</li> <li>\"Utilisateurs\" \u2192 \"Nouvel utilisateur\"</li> <li>Nom d'utilisateur : <code>data-analyst@votredomaine.onmicrosoft.com</code></li> <li>Mot de passe temporaire</li> <li>R\u00f4les : \"Utilisateur\" (par d\u00e9faut)</li> <li>Cr\u00e9er l'utilisateur</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#3-roles-azure-rbac","title":"3. R\u00f4les Azure (RBAC)","text":"<p>R\u00f4les utiles pour Data Analyst :</p> <ul> <li>Contributeur : Peut cr\u00e9er et g\u00e9rer des ressources</li> <li>Lecteur : Peut seulement lire</li> <li>Contributeur de compte de stockage : Acc\u00e8s aux comptes de stockage</li> <li>Contributeur SQL DB : Acc\u00e8s aux bases SQL</li> </ul> <p>Attribuer un r\u00f4le :</p> <ol> <li>Aller \u00e0 la ressource (ex: Storage Account)</li> <li>\"Contr\u00f4le d'acc\u00e8s (IAM)\"</li> <li>\"Ajouter\" \u2192 \"Ajouter une attribution de r\u00f4le\"</li> <li>S\u00e9lectionner le r\u00f4le</li> <li>S\u00e9lectionner l'utilisateur</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#politiques-de-securite-recommandees","title":"Politiques de s\u00e9curit\u00e9 recommand\u00e9es","text":"<ol> <li>Mots de passe forts</li> <li>Minimum 12 caract\u00e8res</li> <li> <p>Complexit\u00e9 requise</p> </li> <li> <p>Expiration des mots de passe</p> </li> <li> <p>90 jours (recommand\u00e9)</p> </li> <li> <p>Blocage de compte</p> </li> <li>Apr\u00e8s 5 tentatives \u00e9chou\u00e9es</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#surveillance-des-couts","title":"Surveillance des co\u00fbts","text":""},{"location":"Cloud/Azure/FR/01-getting-started/#activer-les-alertes-de-cout","title":"Activer les alertes de co\u00fbt","text":"<p>\u00c9tape 1 : Configurer les alertes</p> <ol> <li>Aller dans \"Cost Management + Billing\"</li> <li>\"Alertes de co\u00fbt\"</li> <li>\"Nouvelle alerte de co\u00fbt\"</li> <li>Seuil : 5\u20ac (recommand\u00e9)</li> <li>Email de notification</li> </ol> <p>R\u00e9sultat : Email re\u00e7u si les co\u00fbts d\u00e9passent 5\u20ac.</p>"},{"location":"Cloud/Azure/FR/01-getting-started/#verifier-lutilisation-des-credits","title":"V\u00e9rifier l'utilisation des cr\u00e9dits","text":"<ol> <li>\"Cost Management + Billing\"</li> <li>\"Cr\u00e9dits Azure\"</li> <li>Voir les cr\u00e9dits restants</li> <li>Voir l'utilisation par service</li> <li>Voir la date d'expiration (30 jours)</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#azure-cost-management","title":"Azure Cost Management","text":"<ol> <li>\"Cost Management + Billing\" \u2192 \"Cost Management\"</li> <li>Voir les co\u00fbts par service</li> <li>Filtrer par p\u00e9riode</li> <li>Exporter les rapports</li> <li>Cr\u00e9er des budgets</li> </ol> <p>\u26a0\ufe0f Important : V\u00e9rifier r\u00e9guli\u00e8rement (hebdomadaire recommand\u00e9).</p>"},{"location":"Cloud/Azure/FR/01-getting-started/#conseils-pour-rester-gratuit","title":"Conseils pour rester gratuit","text":"<ol> <li>Supprimer les ressources inutilis\u00e9es</li> <li>Arr\u00eater les machines virtuelles non utilis\u00e9es</li> <li>Supprimer les comptes de stockage vides</li> <li> <p>Nettoyer les groupes de ressources</p> </li> <li> <p>Utiliser les services gratuits</p> </li> <li>Privil\u00e9gier les services toujours gratuits</li> <li>Utiliser les cr\u00e9dits intelligemment</li> <li> <p>Arr\u00eater les services non utilis\u00e9s</p> </li> <li> <p>Cr\u00e9er des budgets</p> </li> <li>\"Cost Management\" \u2192 \"Budgets\"</li> <li>Cr\u00e9er un budget de 5\u20ac</li> <li> <p>Alertes automatiques</p> </li> <li> <p>Arr\u00eater les services non utilis\u00e9s</p> </li> <li>Machines virtuelles : arr\u00eater quand non utilis\u00e9es</li> <li>Bases de donn\u00e9es : arr\u00eater ou mettre en pause</li> <li>Comptes de stockage : supprimer si vides</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#groupes-de-ressources","title":"Groupes de ressources","text":"<p>Organiser vos ressources :</p> <ol> <li>Cr\u00e9er un groupe de ressources : <code>rg-data-analyst-training</code></li> <li>Toutes les ressources de formation dans ce groupe</li> <li>Facilite la suppression en une fois</li> <li>Facilite la gestion des co\u00fbts</li> </ol> <p>Cr\u00e9er un groupe de ressources :</p> <ol> <li>\"Groupes de ressources\" \u2192 \"Ajouter\"</li> <li>Nom : <code>rg-data-analyst-training</code></li> <li>R\u00e9gion : Choisir la r\u00e9gion la plus proche</li> <li>Cr\u00e9er</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Compte Azure gratuit : 200$ de cr\u00e9dit (30 jours) + services gratuits</li> <li>Cr\u00e9dits gratuits : 3 types (200$, 12 mois, toujours gratuit)</li> <li>S\u00e9curit\u00e9 Azure AD : Activer MFA, cr\u00e9er utilisateurs</li> <li>Surveillance : Alertes de co\u00fbt essentielles</li> <li>Rester gratuit : Supprimer ressources inutilis\u00e9es, utiliser groupes de ressources</li> </ol>"},{"location":"Cloud/Azure/FR/01-getting-started/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 2. Azure Storage - Stockage de donn\u00e9es pour apprendre \u00e0 stocker des donn\u00e9es sur Azure.</p>"},{"location":"Cloud/Azure/FR/02-storage/","title":"2. Azure Storage - Stockage de donn\u00e9es","text":""},{"location":"Cloud/Azure/FR/02-storage/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Azure Storage et son utilisation</li> <li>Cr\u00e9er des comptes de stockage</li> <li>Utiliser Blob Storage et Data Lake Storage</li> <li>Uploader et g\u00e9rer des fichiers</li> <li>Organiser les donn\u00e9es</li> </ul>"},{"location":"Cloud/Azure/FR/02-storage/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Azure Storage</li> <li>Cr\u00e9er un compte de stockage</li> <li>Blob Storage</li> <li>Data Lake Storage Gen2</li> <li>Uploader et g\u00e9rer des fichiers</li> <li>Int\u00e9gration avec autres services</li> </ol>"},{"location":"Cloud/Azure/FR/02-storage/#introduction-a-azure-storage","title":"Introduction \u00e0 Azure Storage","text":""},{"location":"Cloud/Azure/FR/02-storage/#quest-ce-quazure-storage","title":"Qu'est-ce qu'Azure Storage ?","text":"<p>Azure Storage = Service de stockage cloud g\u00e9r\u00e9</p> <ul> <li>Stockage illimit\u00e9 : \u00c9volutif selon les besoins</li> <li>Haute disponibilit\u00e9 : 99.99% de disponibilit\u00e9</li> <li>S\u00e9curis\u00e9 : Chiffrement par d\u00e9faut</li> <li>Int\u00e9gration : Avec tous les services Azure</li> </ul>"},{"location":"Cloud/Azure/FR/02-storage/#types-de-stockage","title":"Types de stockage","text":"<ol> <li>Blob Storage : Fichiers (CSV, JSON, Parquet, etc.)</li> <li>Data Lake Storage Gen2 : Data Lake avec syst\u00e8me de fichiers hi\u00e9rarchique</li> <li>File Storage : Partages de fichiers</li> <li>Queue Storage : Files d'attente</li> <li>Table Storage : Stockage NoSQL</li> </ol>"},{"location":"Cloud/Azure/FR/02-storage/#free-tier-azure-storage","title":"Free Tier Azure Storage","text":"<p>Gratuit 12 mois : - 5 Go de stockage Blob - 5 Go de stockage File - 5 Go de stockage Table - 5 Go de stockage Queue</p> <p>Gratuit \u00e0 vie : - 200 Go de transfert de donn\u00e9es sortantes/mois</p> <p>\u26a0\ufe0f Important : Au-del\u00e0 de ces limites, facturation normale.</p>"},{"location":"Cloud/Azure/FR/02-storage/#creer-un-compte-de-stockage","title":"Cr\u00e9er un compte de stockage","text":""},{"location":"Cloud/Azure/FR/02-storage/#etape-1-acceder-a-azure-storage","title":"\u00c9tape 1 : Acc\u00e9der \u00e0 Azure Storage","text":"<ol> <li>Portail Azure \u2192 Rechercher \"Storage accounts\"</li> <li>Cliquer sur \"Storage accounts\"</li> <li>Cliquer sur \"Create\"</li> </ol>"},{"location":"Cloud/Azure/FR/02-storage/#etape-2-configuration-de-base","title":"\u00c9tape 2 : Configuration de base","text":"<p>Informations de base : - Subscription : Choisir votre abonnement - Resource group : Cr\u00e9er ou utiliser existant - Storage account name : Nom unique globalement (ex: <code>mydataanalyststorage</code>) - Region : Choisir la r\u00e9gion la plus proche (ex: <code>France Central</code>)</p> <p>Options de performance : - Performance : Standard (recommand\u00e9 pour d\u00e9buter) - Redundancy : LRS (Local Redundant Storage) - le moins cher</p>"},{"location":"Cloud/Azure/FR/02-storage/#etape-3-options-avancees","title":"\u00c9tape 3 : Options avanc\u00e9es","text":"<p>S\u00e9curit\u00e9 : - Secure transfer required : \u2705 Activer (recommand\u00e9) - Allow Blob public access : \u274c D\u00e9sactiver (s\u00e9curit\u00e9)</p> <p>Data Lake Storage Gen2 : - Hierarchical namespace : \u2705 Activer si besoin de Data Lake</p>"},{"location":"Cloud/Azure/FR/02-storage/#etape-4-creer-le-compte","title":"\u00c9tape 4 : Cr\u00e9er le compte","text":"<ol> <li>Cliquer sur \"Review + create\"</li> <li>V\u00e9rifier la configuration</li> <li>Cliquer sur \"Create\"</li> <li>Attendre la cr\u00e9ation (1-2 minutes)</li> </ol> <p>\u26a0\ufe0f Important : Noter le nom du compte de stockage.</p>"},{"location":"Cloud/Azure/FR/02-storage/#blob-storage","title":"Blob Storage","text":""},{"location":"Cloud/Azure/FR/02-storage/#quest-ce-que-blob-storage","title":"Qu'est-ce que Blob Storage ?","text":"<p>Blob Storage = Stockage d'objets pour fichiers</p> <ul> <li>Containers : Organisent les fichiers (comme des dossiers)</li> <li>Blobs : Fichiers individuels</li> <li>Types : Block blobs, Page blobs, Append blobs</li> </ul>"},{"location":"Cloud/Azure/FR/02-storage/#creer-un-container","title":"Cr\u00e9er un container","text":"<p>Via le portail Azure :</p> <ol> <li>Storage account \u2192 \"Containers\"</li> <li>Cliquer sur \"+ Container\"</li> <li>Nom : <code>raw-data</code> (ou autre)</li> <li>Public access level : Private (recommand\u00e9)</li> <li>Cliquer sur \"Create\"</li> </ol> <p>Via Azure CLI :</p> <pre><code>az storage container create \\\n  --name raw-data \\\n  --account-name mydataanalyststorage \\\n  --auth-mode login\n</code></pre> <p>Via Python :</p> <pre><code>from azure.storage.blob import BlobServiceClient\n\n# Connexion\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=...\"\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\n# Cr\u00e9er un container\ncontainer_client = blob_service_client.create_container(\"raw-data\")\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#types-de-blobs","title":"Types de blobs","text":"<p>Block Blobs : - Fichiers (CSV, JSON, Parquet, images, etc.) - Jusqu'\u00e0 4.75 To par blob - Recommand\u00e9 pour la plupart des cas</p> <p>Page Blobs : - Disques virtuels - Jusqu'\u00e0 8 To</p> <p>Append Blobs : - Logs - Donn\u00e9es d'ajout uniquement</p>"},{"location":"Cloud/Azure/FR/02-storage/#data-lake-storage-gen2","title":"Data Lake Storage Gen2","text":""},{"location":"Cloud/Azure/FR/02-storage/#quest-ce-que-data-lake-storage-gen2","title":"Qu'est-ce que Data Lake Storage Gen2 ?","text":"<p>Data Lake Storage Gen2 = Blob Storage + syst\u00e8me de fichiers hi\u00e9rarchique</p> <ul> <li>Compatible Blob Storage : Utilise les m\u00eames APIs</li> <li>Syst\u00e8me de fichiers : Organisation hi\u00e9rarchique</li> <li>Optimis\u00e9 Big Data : Pour analytics et ML</li> <li>Int\u00e9gration : Avec Azure Synapse, Databricks, etc.</li> </ul>"},{"location":"Cloud/Azure/FR/02-storage/#activer-data-lake-storage-gen2","title":"Activer Data Lake Storage Gen2","text":"<p>Lors de la cr\u00e9ation du compte : 1. Dans \"Advanced\" \u2192 Activer \"Hierarchical namespace\" 2. Cr\u00e9er le compte</p> <p>\u26a0\ufe0f Important : Ne peut pas \u00eatre activ\u00e9 apr\u00e8s cr\u00e9ation.</p>"},{"location":"Cloud/Azure/FR/02-storage/#structure-data-lake","title":"Structure Data Lake","text":"<pre><code>data-lake/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2502   \u251c\u2500\u2500 01/\n\u2502   \u2502   \u2514\u2500\u2500 02/\n\u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 2024/\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#creer-des-fichiers-et-dossiers","title":"Cr\u00e9er des fichiers et dossiers","text":"<p>Via le portail Azure :</p> <ol> <li>Storage account \u2192 \"Data Lake\"</li> <li>Naviguer dans la structure</li> <li>Uploader des fichiers</li> <li>Cr\u00e9er des dossiers</li> </ol> <p>Via Python :</p> <pre><code>from azure.storage.filedatalake import DataLakeServiceClient\n\n# Connexion\naccount_name = \"mydataanalyststorage\"\naccount_key = \"...\"\ndatalake_service_client = DataLakeServiceClient(\n    account_url=f\"https://{account_name}.dfs.core.windows.net\",\n    credential=account_key\n)\n\n# Cr\u00e9er un syst\u00e8me de fichiers\nfile_system_client = datalake_service_client.create_file_system(\"data-lake\")\n\n# Cr\u00e9er un r\u00e9pertoire\ndirectory_client = file_system_client.create_directory(\"raw/2024\")\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#uploader-et-gerer-des-fichiers","title":"Uploader et g\u00e9rer des fichiers","text":""},{"location":"Cloud/Azure/FR/02-storage/#uploader-un-fichier","title":"Uploader un fichier","text":"<p>Via le portail Azure :</p> <ol> <li>Container \u2192 \"Upload\"</li> <li>S\u00e9lectionner le fichier</li> <li>Cliquer sur \"Upload\"</li> </ol> <p>Via Azure CLI :</p> <pre><code>az storage blob upload \\\n  --account-name mydataanalyststorage \\\n  --container-name raw-data \\\n  --name data.csv \\\n  --file ./local-data.csv \\\n  --auth-mode login\n</code></pre> <p>Via Python :</p> <pre><code>from azure.storage.blob import BlobServiceClient\n\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\ncontainer_client = blob_service_client.get_container_client(\"raw-data\")\n\n# Uploader un fichier\nwith open(\"local-data.csv\", \"rb\") as data:\n    container_client.upload_blob(name=\"data.csv\", data=data)\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#telecharger-un-fichier","title":"T\u00e9l\u00e9charger un fichier","text":"<p>Via Python :</p> <pre><code># T\u00e9l\u00e9charger un blob\nblob_client = container_client.get_blob_client(\"data.csv\")\nwith open(\"downloaded-data.csv\", \"wb\") as download_file:\n    download_file.write(blob_client.download_blob().readall())\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#lister-les-fichiers","title":"Lister les fichiers","text":"<p>Via Python :</p> <pre><code># Lister tous les blobs dans un container\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    print(f\"Name: {blob.name}, Size: {blob.size}\")\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#supprimer-un-fichier","title":"Supprimer un fichier","text":"<p>Via Python :</p> <pre><code># Supprimer un blob\nblob_client = container_client.get_blob_client(\"data.csv\")\nblob_client.delete_blob()\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#integration-avec-autres-services","title":"Int\u00e9gration avec autres services","text":""},{"location":"Cloud/Azure/FR/02-storage/#azure-storage-data-factory","title":"Azure Storage + Data Factory","text":"<p>Utilisation : - Source de donn\u00e9es pour pipelines ETL - Destination pour donn\u00e9es transform\u00e9es</p> <p>Exemple : <pre><code>{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"connectionString\": \"...\",\n    \"container\": \"raw-data\"\n  }\n}\n</code></pre></p>"},{"location":"Cloud/Azure/FR/02-storage/#azure-storage-azure-sql-database","title":"Azure Storage + Azure SQL Database","text":"<p>Utilisation : - Importer des donn\u00e9es depuis Blob Storage - Exporter des donn\u00e9es vers Blob Storage</p> <p>Exemple SQL : <pre><code>-- Importer depuis Blob Storage\nBULK INSERT my_table\nFROM 'https://mystorageaccount.blob.core.windows.net/raw-data/data.csv'\nWITH (\n    FORMAT = 'CSV',\n    FIRSTROW = 2\n);\n</code></pre></p>"},{"location":"Cloud/Azure/FR/02-storage/#azure-storage-powerbi","title":"Azure Storage + PowerBI","text":"<p>Utilisation : - Connecter PowerBI \u00e0 Blob Storage - Analyser des fichiers directement</p> <p>Configuration : 1. PowerBI \u2192 \"Get Data\" 2. \"Azure Blob Storage\" 3. Entrer l'URL du container 4. S\u00e9lectionner les fichiers</p>"},{"location":"Cloud/Azure/FR/02-storage/#azure-storage-azure-functions","title":"Azure Storage + Azure Functions","text":"<p>Utilisation : - D\u00e9clencher Functions lors d'upload - Traiter automatiquement les fichiers</p> <p>Configuration : 1. Function \u2192 \"Add trigger\" 2. \"Azure Blob Storage trigger\" 3. Configurer le container et le chemin</p>"},{"location":"Cloud/Azure/FR/02-storage/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/Azure/FR/02-storage/#organisation","title":"Organisation","text":"<ol> <li>Utiliser des containers pour organiser par projet</li> <li>Nommer clairement les fichiers et containers</li> <li>Organiser par date : <code>raw/2024/01/data.csv</code></li> <li>S\u00e9parer par type : <code>raw/</code>, <code>processed/</code>, <code>analytics/</code></li> </ol>"},{"location":"Cloud/Azure/FR/02-storage/#performance","title":"Performance","text":"<ol> <li>Utiliser des noms al\u00e9atoires pour les blobs (\u00e9viter les s\u00e9quences)</li> <li>Activer CDN si besoin de distribution globale (payant)</li> <li>Utiliser des blobs de blocs pour la plupart des cas</li> <li>Partitionner les donn\u00e9es pour am\u00e9liorer les performances</li> </ol>"},{"location":"Cloud/Azure/FR/02-storage/#couts","title":"Co\u00fbts","text":"<ol> <li>Surveiller l'utilisation dans Azure Cost Management</li> <li>Supprimer les fichiers inutiles</li> <li>Utiliser les bonnes classes de stockage</li> <li>Configurer des r\u00e8gles de cycle de vie pour automatiser</li> </ol>"},{"location":"Cloud/Azure/FR/02-storage/#securite","title":"S\u00e9curit\u00e9","text":"<ol> <li>Ne jamais rendre publics les containers (sauf besoin sp\u00e9cifique)</li> <li>Utiliser SAS (Shared Access Signature) pour acc\u00e8s temporaire</li> <li>Activer le chiffrement par d\u00e9faut</li> <li>Utiliser Azure AD pour authentification</li> </ol>"},{"location":"Cloud/Azure/FR/02-storage/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/Azure/FR/02-storage/#exemple-1-uploader-un-fichier-csv","title":"Exemple 1 : Uploader un fichier CSV","text":"<pre><code>from azure.storage.blob import BlobServiceClient\nimport pandas as pd\n\n# Connexion\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=...\"\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\ncontainer_client = blob_service_client.get_container_client(\"raw-data\")\n\n# Lire un fichier local\ndf = pd.read_csv(\"local-data.csv\")\n\n# Uploader vers Azure Storage\nwith open(\"local-data.csv\", \"rb\") as data:\n    container_client.upload_blob(name=\"2024/01/data.csv\", data=data)\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#exemple-2-telecharger-et-traiter","title":"Exemple 2 : T\u00e9l\u00e9charger et traiter","text":"<pre><code># T\u00e9l\u00e9charger depuis Azure Storage\nblob_client = container_client.get_blob_client(\"2024/01/data.csv\")\nwith open(\"downloaded-data.csv\", \"wb\") as download_file:\n    download_file.write(blob_client.download_blob().readall())\n\n# Traiter\ndf = pd.read_csv(\"downloaded-data.csv\")\n# ... traitement ...\n\n# Uploader le r\u00e9sultat\ndf.to_csv(\"processed-data.csv\", index=False)\nwith open(\"processed-data.csv\", \"rb\") as data:\n    container_client.upload_blob(name=\"processed/2024/01/data.csv\", data=data)\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#exemple-3-lister-et-filtrer","title":"Exemple 3 : Lister et filtrer","text":"<pre><code># Lister tous les fichiers dans un pr\u00e9fixe\nblob_list = container_client.list_blobs(name_starts_with=\"2024/01/\")\nfor blob in blob_list:\n    print(f\"File: {blob.name}, Size: {blob.size} bytes, Modified: {blob.last_modified}\")\n</code></pre>"},{"location":"Cloud/Azure/FR/02-storage/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Azure Storage = Stockage illimit\u00e9 et hautement disponible</li> <li>Free Tier : 5 Go pendant 12 mois</li> <li>Blob Storage pour fichiers, Data Lake Gen2 pour Big Data</li> <li>Organiser avec containers et pr\u00e9fixes</li> <li>Int\u00e9gration native avec tous les services Azure data</li> </ol>"},{"location":"Cloud/Azure/FR/02-storage/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 3. Azure Data Factory - ETL Cloud pour apprendre \u00e0 cr\u00e9er des pipelines ETL sur Azure.</p>"},{"location":"Cloud/Azure/FR/03-data-factory/","title":"3. Azure Data Factory - ETL Cloud","text":""},{"location":"Cloud/Azure/FR/03-data-factory/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Azure Data Factory et son r\u00f4le</li> <li>Cr\u00e9er des pipelines ETL</li> <li>Utiliser des activit\u00e9s de transformation</li> <li>Int\u00e9grer avec des sources de donn\u00e9es</li> <li>Orchestrer des workflows</li> </ul>"},{"location":"Cloud/Azure/FR/03-data-factory/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Data Factory</li> <li>Cr\u00e9er un Data Factory</li> <li>Cr\u00e9er un pipeline</li> <li>Activit\u00e9s de transformation</li> <li>Int\u00e9gration avec sources de donn\u00e9es</li> <li>Orchestration et scheduling</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#introduction-a-data-factory","title":"Introduction \u00e0 Data Factory","text":""},{"location":"Cloud/Azure/FR/03-data-factory/#quest-ce-quazure-data-factory","title":"Qu'est-ce qu'Azure Data Factory ?","text":"<p>Azure Data Factory = Service ETL cloud g\u00e9r\u00e9</p> <ul> <li>ETL : Extract, Transform, Load</li> <li>Cloud : Pas d'infrastructure \u00e0 g\u00e9rer</li> <li>G\u00e9r\u00e9 : Microsoft g\u00e8re l'infrastructure</li> <li>Scalable : S'adapte automatiquement</li> </ul>"},{"location":"Cloud/Azure/FR/03-data-factory/#composants-data-factory","title":"Composants Data Factory","text":"<ol> <li>Pipelines : Flux de travail ETL</li> <li>Activities : \u00c9tapes dans un pipeline</li> <li>Datasets : Repr\u00e9sentations des donn\u00e9es</li> <li>Linked Services : Connexions aux sources</li> <li>Triggers : D\u00e9clenchement automatique</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#free-tier-data-factory","title":"Free Tier Data Factory","text":"<p>Gratuit \u00e0 vie : - 5 pipelines gratuits - Activit\u00e9s limit\u00e9es - Au-del\u00e0 : facturation \u00e0 l'usage</p> <p>\u26a0\ufe0f Important : Surveiller les co\u00fbts, surtout pour les activit\u00e9s de transformation.</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#creer-un-data-factory","title":"Cr\u00e9er un Data Factory","text":""},{"location":"Cloud/Azure/FR/03-data-factory/#etape-1-acceder-a-data-factory","title":"\u00c9tape 1 : Acc\u00e9der \u00e0 Data Factory","text":"<ol> <li>Portail Azure \u2192 Rechercher \"Data Factory\"</li> <li>Cliquer sur \"Data factories\"</li> <li>Cliquer sur \"Create\"</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#etape-2-configuration-de-base","title":"\u00c9tape 2 : Configuration de base","text":"<p>Informations de base : - Subscription : Choisir votre abonnement - Resource group : Cr\u00e9er ou utiliser existant - Name : <code>my-data-factory</code> (unique globalement) - Version : V2 (recommand\u00e9) - Region : Choisir la r\u00e9gion la plus proche</p> <p>Git configuration (optionnel) : - Configure Git later : Pour d\u00e9buter rapidement - Ou configurer Git/GitHub pour versioning</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#etape-3-creer-le-data-factory","title":"\u00c9tape 3 : Cr\u00e9er le Data Factory","text":"<ol> <li>Cliquer sur \"Review + create\"</li> <li>V\u00e9rifier la configuration</li> <li>Cliquer sur \"Create\"</li> <li>Attendre la cr\u00e9ation (2-3 minutes)</li> </ol> <p>\u26a0\ufe0f Important : Noter le nom du Data Factory.</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#etape-4-ouvrir-data-factory-studio","title":"\u00c9tape 4 : Ouvrir Data Factory Studio","text":"<ol> <li>Une fois cr\u00e9\u00e9, cliquer sur \"Open Azure Data Factory Studio\"</li> <li>Interface web pour cr\u00e9er des pipelines</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#creer-un-pipeline","title":"Cr\u00e9er un pipeline","text":""},{"location":"Cloud/Azure/FR/03-data-factory/#etape-1-creer-un-linked-service","title":"\u00c9tape 1 : Cr\u00e9er un Linked Service","text":"<p>Linked Service = Connexion \u00e0 une source de donn\u00e9es</p> <p>Exemple : Azure Blob Storage</p> <ol> <li>Data Factory Studio \u2192 \"Manage\" \u2192 \"Linked services\"</li> <li>Cliquer sur \"+ New\"</li> <li>Rechercher \"Azure Blob Storage\"</li> <li>Configuration :</li> <li>Name : <code>AzureBlobStorage1</code></li> <li>Storage account name : S\u00e9lectionner votre compte</li> <li>Authentication method : Account key (ou autre)</li> <li>Cliquer sur \"Create\"</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#etape-2-creer-un-dataset","title":"\u00c9tape 2 : Cr\u00e9er un Dataset","text":"<p>Dataset = Repr\u00e9sentation des donn\u00e9es</p> <ol> <li>Data Factory Studio \u2192 \"Author\" \u2192 \"Datasets\"</li> <li>Cliquer sur \"+ New\"</li> <li>Choisir \"Azure Blob Storage\"</li> <li>Configuration :</li> <li>Name : <code>CSVData</code></li> <li>Linked service : <code>AzureBlobStorage1</code></li> <li>File path : <code>raw-data/</code></li> <li>File format : DelimitedText (CSV)</li> <li>Cliquer sur \"Create\"</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#etape-3-creer-un-pipeline","title":"\u00c9tape 3 : Cr\u00e9er un pipeline","text":"<ol> <li>Data Factory Studio \u2192 \"Author\" \u2192 \"Pipelines\"</li> <li>Cliquer sur \"+ New pipeline\"</li> <li>Nommer le pipeline : <code>CopyCSVToParquet</code></li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#etape-4-ajouter-une-activite","title":"\u00c9tape 4 : Ajouter une activit\u00e9","text":"<p>Exemple : Copy Data</p> <ol> <li>Dans le pipeline, glisser \"Copy Data\" depuis \"Move &amp; transform\"</li> <li>Configurer :</li> <li>Source : Dataset <code>CSVData</code></li> <li>Sink (Destination) : Cr\u00e9er un nouveau dataset Parquet</li> <li>Cliquer sur \"Publish\" pour sauvegarder</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#activites-de-transformation","title":"Activit\u00e9s de transformation","text":""},{"location":"Cloud/Azure/FR/03-data-factory/#copy-data","title":"Copy Data","text":"<p>Copier des donn\u00e9es d'une source \u00e0 une destination</p> <p>Configuration : - Source : Dataset source - Sink : Dataset destination - Mapping : Mapping des colonnes</p> <p>Exemple : CSV \u2192 Parquet</p> <pre><code>{\n  \"name\": \"CopyCSVToParquet\",\n  \"type\": \"Copy\",\n  \"inputs\": [{\"referenceName\": \"CSVData\"}],\n  \"outputs\": [{\"referenceName\": \"ParquetData\"}],\n  \"typeProperties\": {\n    \"source\": {\"type\": \"DelimitedTextSource\"},\n    \"sink\": {\"type\": \"ParquetSink\"}\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/FR/03-data-factory/#data-flow","title":"Data Flow","text":"<p>Transformation de donn\u00e9es avec interface graphique</p> <p>\u00c9tapes : 1. Cr\u00e9er un Data Flow 2. Ajouter une source 3. Ajouter des transformations :    - Select : S\u00e9lectionner colonnes    - Filter : Filtrer des lignes    - Derived Column : Cr\u00e9er des colonnes calcul\u00e9es    - Aggregate : Agr\u00e9gations    - Join : Joindre des donn\u00e9es 4. Ajouter un sink</p> <p>Exemple de transformations :</p> <pre><code>Source (CSV) \n  \u2192 Select (colonnes)\n  \u2192 Filter (status = 'active')\n  \u2192 Derived Column (nouvelle colonne)\n  \u2192 Aggregate (SUM, COUNT)\n  \u2192 Sink (Parquet)\n</code></pre>"},{"location":"Cloud/Azure/FR/03-data-factory/#lookup","title":"Lookup","text":"<p>Rechercher des valeurs dans une autre source</p> <p>Utilisation : - Valider des donn\u00e9es - Enrichir des donn\u00e9es - V\u00e9rifier des r\u00e9f\u00e9rences</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#stored-procedure","title":"Stored Procedure","text":"<p>Ex\u00e9cuter une proc\u00e9dure stock\u00e9e SQL</p> <p>Utilisation : - Traitement dans SQL Database - Logique m\u00e9tier complexe - Optimisation c\u00f4t\u00e9 base</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#integration-avec-sources-de-donnees","title":"Int\u00e9gration avec sources de donn\u00e9es","text":""},{"location":"Cloud/Azure/FR/03-data-factory/#azure-blob-storage","title":"Azure Blob Storage","text":"<p>Source de donn\u00e9es :</p> <pre><code>{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"connectionString\": \"...\",\n    \"container\": \"raw-data\"\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/FR/03-data-factory/#azure-sql-database","title":"Azure SQL Database","text":"<p>Source de donn\u00e9es :</p> <pre><code>{\n  \"type\": \"AzureSqlDatabase\",\n  \"typeProperties\": {\n    \"connectionString\": \"...\",\n    \"tableName\": \"users\"\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/FR/03-data-factory/#azure-data-lake-storage-gen2","title":"Azure Data Lake Storage Gen2","text":"<p>Source de donn\u00e9es :</p> <pre><code>{\n  \"type\": \"AzureBlobFS\",\n  \"typeProperties\": {\n    \"url\": \"https://account.dfs.core.windows.net\",\n    \"fileSystem\": \"data-lake\"\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/FR/03-data-factory/#fichiers-locaux-via-self-hosted-ir","title":"Fichiers locaux (via Self-hosted IR)","text":"<p>Integration Runtime : - Self-hosted IR pour acc\u00e8s aux fichiers locaux - Installer sur une machine locale - Connecter au Data Factory</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#orchestration-et-scheduling","title":"Orchestration et scheduling","text":""},{"location":"Cloud/Azure/FR/03-data-factory/#declencher-manuellement","title":"D\u00e9clencher manuellement","text":"<ol> <li>Data Factory Studio \u2192 \"Monitor\"</li> <li>S\u00e9lectionner le pipeline</li> <li>Cliquer sur \"Trigger now\"</li> <li>Voir l'ex\u00e9cution en temps r\u00e9el</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#planifier-un-pipeline-trigger","title":"Planifier un pipeline (Trigger)","text":"<p>Cr\u00e9er un trigger :</p> <ol> <li>Pipeline \u2192 \"Add trigger\" \u2192 \"New/Edit\"</li> <li>Type : \"Schedule\"</li> <li>Configuration :</li> <li>Name : <code>DailyTrigger</code></li> <li>Type : Schedule</li> <li>Recurrence : Daily</li> <li>Start time : 02:00</li> <li>Cliquer sur \"OK\"</li> </ol> <p>Types de triggers : - Schedule : Planifi\u00e9 (cron) - Event : D\u00e9clench\u00e9 par \u00e9v\u00e9nement - Tumbling window : Fen\u00eatre glissante</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#declencher-par-evenement","title":"D\u00e9clencher par \u00e9v\u00e9nement","text":"<p>Exemple : Nouveau fichier dans Blob Storage</p> <ol> <li>Cr\u00e9er un trigger \"Storage event\"</li> <li>Configurer :</li> <li>Storage account : Votre compte</li> <li>Container : <code>raw-data</code></li> <li>Event type : Blob created</li> <li>Associer au pipeline</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/Azure/FR/03-data-factory/#performance","title":"Performance","text":"<ol> <li>Utiliser Data Flow pour transformations complexes</li> <li>Optimiser les activit\u00e9s pour r\u00e9duire le temps</li> <li>Utiliser le parall\u00e9lisme quand possible</li> <li>Choisir les bonnes r\u00e9gions pour r\u00e9duire la latence</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#couts","title":"Co\u00fbts","text":"<ol> <li>Surveiller les ex\u00e9cutions dans Monitor</li> <li>Utiliser les 5 pipelines gratuits intelligemment</li> <li>Optimiser les Data Flows (co\u00fbteux)</li> <li>Arr\u00eater les pipelines non utilis\u00e9s</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#organisation","title":"Organisation","text":"<ol> <li>Nommer clairement les pipelines et activit\u00e9s</li> <li>Documenter les transformations</li> <li>Versionner avec Git</li> <li>Tester avant de publier</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#securite","title":"S\u00e9curit\u00e9","text":"<ol> <li>Utiliser Key Vault pour les secrets</li> <li>Limiter les permissions des Linked Services</li> <li>Auditer les ex\u00e9cutions</li> <li>Chiffrer les donn\u00e9es en transit</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/Azure/FR/03-data-factory/#exemple-1-pipeline-simple-csv-parquet","title":"Exemple 1 : Pipeline simple CSV \u2192 Parquet","text":"<p>Pipeline : 1. Source : Azure Blob Storage (CSV) 2. Activity : Copy Data 3. Sink : Azure Blob Storage (Parquet)</p> <p>Configuration : - Source : <code>raw-data/data.csv</code> - Sink : <code>processed-data/data.parquet</code> - Format : DelimitedText \u2192 Parquet</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#exemple-2-pipeline-avec-transformation","title":"Exemple 2 : Pipeline avec transformation","text":"<p>Pipeline : 1. Source : Azure SQL Database 2. Data Flow :    - Select colonnes    - Filter lignes    - Aggregate 3. Sink : Azure Blob Storage (Parquet)</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#exemple-3-pipeline-orchestre","title":"Exemple 3 : Pipeline orchestr\u00e9","text":"<p>Pipeline : 1. Lookup : V\u00e9rifier si nouvelles donn\u00e9es 2. If Condition : Si nouvelles donn\u00e9es 3. Copy Data : Copier vers staging 4. Data Flow : Transformer 5. Copy Data : Charger dans destination</p>"},{"location":"Cloud/Azure/FR/03-data-factory/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Data Factory = ETL cloud g\u00e9r\u00e9 par Microsoft</li> <li>Free Tier : 5 pipelines gratuits</li> <li>Pipelines orchestrent les activit\u00e9s</li> <li>Data Flows pour transformations complexes</li> <li>Triggers permettent l'automatisation</li> </ol>"},{"location":"Cloud/Azure/FR/03-data-factory/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 4. Azure SQL Database - Base de donn\u00e9es pour apprendre \u00e0 utiliser SQL Database sur Azure.</p>"},{"location":"Cloud/Azure/FR/04-sql-database/","title":"4. Azure SQL Database - Base de donn\u00e9es","text":""},{"location":"Cloud/Azure/FR/04-sql-database/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Azure SQL Database</li> <li>Cr\u00e9er une base SQL Database (gratuit jusqu'\u00e0 32 Go)</li> <li>Migrer des donn\u00e9es</li> <li>Optimiser les requ\u00eates</li> <li>Int\u00e9grer avec PowerBI</li> </ul>"},{"location":"Cloud/Azure/FR/04-sql-database/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 SQL Database</li> <li>Cr\u00e9er une base SQL Database</li> <li>Se connecter \u00e0 la base</li> <li>Charger des donn\u00e9es</li> <li>Requ\u00eates SQL</li> <li>Int\u00e9gration avec PowerBI</li> </ol>"},{"location":"Cloud/Azure/FR/04-sql-database/#introduction-a-sql-database","title":"Introduction \u00e0 SQL Database","text":""},{"location":"Cloud/Azure/FR/04-sql-database/#quest-ce-quazure-sql-database","title":"Qu'est-ce qu'Azure SQL Database ?","text":"<p>Azure SQL Database = Base de donn\u00e9es SQL cloud g\u00e9r\u00e9e</p> <ul> <li>SQL Server compatible : Syntaxe SQL standard</li> <li>G\u00e9r\u00e9 : Microsoft g\u00e8re l'infrastructure</li> <li>Scalable : De quelques Go \u00e0 plusieurs To</li> <li>Haute disponibilit\u00e9 : 99.99% de disponibilit\u00e9</li> </ul>"},{"location":"Cloud/Azure/FR/04-sql-database/#cas-dusage-pour-data-analyst","title":"Cas d'usage pour Data Analyst","text":"<ul> <li>Data Warehouse : Centraliser les donn\u00e9es</li> <li>Analytics : Requ\u00eates complexes</li> <li>Business Intelligence : Source pour PowerBI</li> <li>Data Integration : Point central pour ETL</li> </ul>"},{"location":"Cloud/Azure/FR/04-sql-database/#free-tier-sql-database","title":"Free Tier SQL Database","text":"<p>Gratuit 12 mois : - Basic tier : Jusqu'\u00e0 32 Go - DTU : 5 DTU (Database Transaction Units) - Backup : Automatique (7 jours)</p> <p>\u26a0\ufe0f Important : Apr\u00e8s 12 mois, facturation normale. Surveiller les co\u00fbts.</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#creer-une-base-sql-database","title":"Cr\u00e9er une base SQL Database","text":""},{"location":"Cloud/Azure/FR/04-sql-database/#etape-1-acceder-a-sql-database","title":"\u00c9tape 1 : Acc\u00e9der \u00e0 SQL Database","text":"<ol> <li>Portail Azure \u2192 Rechercher \"SQL databases\"</li> <li>Cliquer sur \"SQL databases\"</li> <li>Cliquer sur \"Create\"</li> </ol>"},{"location":"Cloud/Azure/FR/04-sql-database/#etape-2-configuration-de-base","title":"\u00c9tape 2 : Configuration de base","text":"<p>Informations de base : - Subscription : Choisir votre abonnement - Resource group : Cr\u00e9er ou utiliser existant - Database name : <code>analytics-db</code> - Server : Cr\u00e9er un nouveau serveur ou utiliser existant</p> <p>Cr\u00e9er un serveur SQL : - Server name : <code>my-sql-server-xxxxx</code> (unique globalement) - Location : Choisir la r\u00e9gion - Authentication method : SQL authentication (ou Azure AD) - Server admin login : <code>sqladmin</code> (ou autre) - Password : Mot de passe fort - Allow Azure services : \u2705 Oui (pour Data Factory)</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#etape-3-configuration-de-la-base","title":"\u00c9tape 3 : Configuration de la base","text":"<p>Compute + storage : - Service tier : Basic (pour Free Tier) - Compute tier : Serverless (ou Provisioned) - Storage : 2 Go (gratuit, extensible jusqu'\u00e0 32 Go)</p> <p>\u26a0\ufe0f Important : Basic tier = 5 DTU, suffisant pour d\u00e9buter.</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#etape-4-configuration-reseau","title":"\u00c9tape 4 : Configuration r\u00e9seau","text":"<p>Networking : - Public endpoint : \u2705 Enable - Firewall rules :   - \u2705 Allow Azure services and resources   - Ajouter votre IP pour acc\u00e8s local</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#etape-5-creer-la-base","title":"\u00c9tape 5 : Cr\u00e9er la base","text":"<ol> <li>Cliquer sur \"Review + create\"</li> <li>V\u00e9rifier la configuration</li> <li>Cliquer sur \"Create\"</li> <li>Attendre la cr\u00e9ation (2-3 minutes)</li> </ol> <p>\u26a0\ufe0f Important : Noter le nom du serveur et les credentials.</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#se-connecter-a-la-base","title":"Se connecter \u00e0 la base","text":""},{"location":"Cloud/Azure/FR/04-sql-database/#via-azure-portal-query-editor","title":"Via Azure Portal (Query Editor)","text":"<ol> <li>SQL Database \u2192 \"Query editor\"</li> <li>Entrer les credentials</li> <li>Ex\u00e9cuter des requ\u00eates SQL</li> </ol>"},{"location":"Cloud/Azure/FR/04-sql-database/#via-sql-server-management-studio-ssms","title":"Via SQL Server Management Studio (SSMS)","text":"<p>T\u00e9l\u00e9charger SSMS : - https://aka.ms/ssmsfullsetup</p> <p>Connexion : - Server name : <code>my-sql-server-xxxxx.database.windows.net</code> - Authentication : SQL Server Authentication - Login : <code>sqladmin</code> - Password : Votre mot de passe</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#via-azure-data-studio","title":"Via Azure Data Studio","text":"<p>T\u00e9l\u00e9charger Azure Data Studio : - https://aka.ms/azuredatastudio</p> <p>Avantages : - Gratuit et open-source - Interface moderne - Support notebooks - Int\u00e9gration Git</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#via-python-pyodbc","title":"Via Python (pyodbc)","text":"<pre><code>import pyodbc\n\n# Connexion\nserver = 'my-sql-server-xxxxx.database.windows.net'\ndatabase = 'analytics-db'\nusername = 'sqladmin'\npassword = 'your-password'\ndriver = '{ODBC Driver 17 for SQL Server}'\n\nconn = pyodbc.connect(\n    f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n)\n\n# Ex\u00e9cuter une requ\u00eate\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM users\")\nrows = cursor.fetchall()\nfor row in rows:\n    print(row)\n</code></pre>"},{"location":"Cloud/Azure/FR/04-sql-database/#charger-des-donnees","title":"Charger des donn\u00e9es","text":""},{"location":"Cloud/Azure/FR/04-sql-database/#methode-1-insert-petites-quantites","title":"M\u00e9thode 1 : INSERT (petites quantit\u00e9s)","text":"<pre><code>INSERT INTO users (id, name, email, created_at)\nVALUES (1, 'John Doe', 'john@example.com', '2024-01-01');\n</code></pre>"},{"location":"Cloud/Azure/FR/04-sql-database/#methode-2-bulk-insert-depuis-blob-storage","title":"M\u00e9thode 2 : BULK INSERT depuis Blob Storage","text":"<p>Pr\u00e9requis : - Cr\u00e9er une cl\u00e9 SAS pour Blob Storage - Cr\u00e9er un credential dans SQL Database</p> <p>Exemple :</p> <pre><code>-- Cr\u00e9er un credential\nCREATE DATABASE SCOPED CREDENTIAL BlobCredential\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'your-sas-token';\n\n-- Cr\u00e9er une source de donn\u00e9es externe\nCREATE EXTERNAL DATA SOURCE BlobStorage\nWITH (\n    TYPE = BLOB_STORAGE,\n    LOCATION = 'https://mystorageaccount.blob.core.windows.net',\n    CREDENTIAL = BlobCredential\n);\n\n-- Importer depuis Blob Storage\nBULK INSERT users\nFROM 'raw-data/users.csv'\nWITH (\n    DATA_SOURCE = 'BlobStorage',\n    FORMAT = 'CSV',\n    FIRSTROW = 2,\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n'\n);\n</code></pre>"},{"location":"Cloud/Azure/FR/04-sql-database/#methode-3-via-data-factory","title":"M\u00e9thode 3 : Via Data Factory","text":"<p>Pipeline : 1. Source : Azure Blob Storage (CSV) 2. Activity : Copy Data 3. Sink : Azure SQL Database</p> <p>Configuration : - Source : <code>raw-data/users.csv</code> - Sink : Table <code>users</code> dans SQL Database - Mapping : Colonnes automatique ou manuel</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#methode-4-via-python-pandas","title":"M\u00e9thode 4 : Via Python (pandas)","text":"<pre><code>import pandas as pd\nimport pyodbc\n\n# Lire un fichier CSV\ndf = pd.read_csv('users.csv')\n\n# Connexion\nconn = pyodbc.connect(connection_string)\n\n# \u00c9crire dans SQL Database\ndf.to_sql('users', conn, if_exists='append', index=False)\n</code></pre>"},{"location":"Cloud/Azure/FR/04-sql-database/#requetes-sql","title":"Requ\u00eates SQL","text":""},{"location":"Cloud/Azure/FR/04-sql-database/#requetes-de-base","title":"Requ\u00eates de base","text":"<p>SELECT simple :</p> <pre><code>SELECT * FROM users LIMIT 10;\n</code></pre> <p>Filtrer :</p> <pre><code>SELECT id, name, email\nFROM users\nWHERE created_at &gt; '2024-01-01'\nORDER BY created_at DESC;\n</code></pre> <p>Agr\u00e9gations :</p> <pre><code>SELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count,\n    COUNT(DISTINCT email) AS unique_emails\nFROM users\nGROUP BY DATE_TRUNC('month', created_at)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/Azure/FR/04-sql-database/#requetes-avancees","title":"Requ\u00eates avanc\u00e9es","text":"<p>Window functions :</p> <pre><code>SELECT \n    id,\n    name,\n    created_at,\n    ROW_NUMBER() OVER (PARTITION BY DATE_TRUNC('month', created_at) ORDER BY created_at) AS rank\nFROM users;\n</code></pre> <p>Jointures :</p> <pre><code>SELECT \n    u.name,\n    o.amount,\n    o.created_at\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.created_at &gt; '2024-01-01';\n</code></pre> <p>CTE (Common Table Expressions) :</p> <pre><code>WITH monthly_users AS (\n    SELECT \n        DATE_TRUNC('month', created_at) AS month,\n        COUNT(*) AS user_count\n    FROM users\n    GROUP BY DATE_TRUNC('month', created_at)\n)\nSELECT \n    month,\n    user_count,\n    LAG(user_count, 1) OVER (ORDER BY month) AS previous_month\nFROM monthly_users;\n</code></pre>"},{"location":"Cloud/Azure/FR/04-sql-database/#integration-avec-powerbi","title":"Int\u00e9gration avec PowerBI","text":""},{"location":"Cloud/Azure/FR/04-sql-database/#connexion-directe","title":"Connexion directe","text":"<p>\u00c9tape 1 : Dans PowerBI Desktop</p> <ol> <li>\"Get Data\" \u2192 \"Azure\" \u2192 \"Azure SQL Database\"</li> <li>Entrer les informations :</li> <li>Server : <code>my-sql-server-xxxxx.database.windows.net</code></li> <li>Database : <code>analytics-db</code></li> <li>Data connectivity mode : Import (ou DirectQuery)</li> </ol> <p>\u00c9tape 2 : Authentification</p> <ul> <li>Authentication method : Database</li> <li>Username : <code>sqladmin</code></li> <li>Password : Votre mot de passe</li> </ul> <p>\u00c9tape 3 : S\u00e9lectionner les tables</p> <ul> <li>Choisir les tables \u00e0 importer</li> <li>Cliquer sur \"Load\"</li> </ul>"},{"location":"Cloud/Azure/FR/04-sql-database/#directquery-vs-import","title":"DirectQuery vs Import","text":"<p>Import : - \u2705 Rapide pour visualisations - \u2705 Fonctionne hors ligne - \u274c Donn\u00e9es statiques (n\u00e9cessite refresh)</p> <p>DirectQuery : - \u2705 Donn\u00e9es en temps r\u00e9el - \u2705 Pas de limite de taille - \u274c Plus lent (requ\u00eates \u00e0 chaque interaction)</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#creer-des-visualisations","title":"Cr\u00e9er des visualisations","text":"<p>Exemple : 1. Importer la table <code>users</code> 2. Cr\u00e9er un graphique : Nombre d'utilisateurs par mois 3. Ajouter des filtres 4. Publier sur PowerBI Service</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/Azure/FR/04-sql-database/#performance","title":"Performance","text":"<ol> <li>Cr\u00e9er des index sur colonnes fr\u00e9quemment utilis\u00e9es</li> <li>Optimiser les requ\u00eates avec EXPLAIN</li> <li>Utiliser des vues pour simplifier</li> <li>Partitionner les grandes tables</li> </ol>"},{"location":"Cloud/Azure/FR/04-sql-database/#couts","title":"Co\u00fbts","text":"<ol> <li>Surveiller l'utilisation dans Azure Cost Management</li> <li>Utiliser Basic tier pour d\u00e9buter</li> <li>Arr\u00eater la base si non utilis\u00e9e (Serverless)</li> <li>Nettoyer les donn\u00e9es inutiles</li> </ol>"},{"location":"Cloud/Azure/FR/04-sql-database/#securite","title":"S\u00e9curit\u00e9","text":"<ol> <li>Utiliser Azure AD pour authentification</li> <li>Limiter les acc\u00e8s avec firewall rules</li> <li>Chiffrer les donn\u00e9es (activ\u00e9 par d\u00e9faut)</li> <li>Auditer les acc\u00e8s avec SQL Auditing</li> </ol>"},{"location":"Cloud/Azure/FR/04-sql-database/#organisation","title":"Organisation","text":"<ol> <li>Nommer clairement les tables et colonnes</li> <li>Documenter les sch\u00e9mas</li> <li>Utiliser des sch\u00e9mas pour organiser</li> <li>Versionner les scripts SQL (Git)</li> </ol>"},{"location":"Cloud/Azure/FR/04-sql-database/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/Azure/FR/04-sql-database/#exemple-1-pipeline-complet-blob-sql-database","title":"Exemple 1 : Pipeline complet Blob \u2192 SQL Database","text":"<p>Via Data Factory : 1. Source : Azure Blob Storage (CSV) 2. Activity : Copy Data 3. Sink : Azure SQL Database 4. Trigger : Schedule (quotidien)</p>"},{"location":"Cloud/Azure/FR/04-sql-database/#exemple-2-requetes-analytiques","title":"Exemple 2 : Requ\u00eates analytiques","text":"<pre><code>-- Top 10 utilisateurs par d\u00e9penses\nSELECT TOP 10\n    u.name,\n    SUM(o.amount) AS total_spent,\n    COUNT(o.id) AS order_count\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.created_at &gt;= DATEADD(month, -3, GETDATE())\nGROUP BY u.name\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"Cloud/Azure/FR/04-sql-database/#exemple-3-export-vers-powerbi","title":"Exemple 3 : Export vers PowerBI","text":"<ol> <li>Cr\u00e9er une vue pour PowerBI</li> <li>Connecter PowerBI \u00e0 la vue</li> <li>Cr\u00e9er des visualisations</li> <li>Publier le rapport</li> </ol>"},{"location":"Cloud/Azure/FR/04-sql-database/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>SQL Database = Base SQL cloud g\u00e9r\u00e9e par Microsoft</li> <li>Free Tier : 32 Go pendant 12 mois (Basic tier)</li> <li>Compatible SQL Server : Syntaxe standard</li> <li>Int\u00e9gration PowerBI : Connexion directe</li> <li>Scalable : De Basic \u00e0 Premium</li> </ol>"},{"location":"Cloud/Azure/FR/04-sql-database/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 5. Azure Synapse Analytics - Data Warehouse pour apprendre \u00e0 utiliser Synapse pour l'analyse de donn\u00e9es.</p>"},{"location":"Cloud/Azure/FR/05-synapse/","title":"5. Azure Synapse Analytics - Data Warehouse","text":""},{"location":"Cloud/Azure/FR/05-synapse/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Azure Synapse Analytics</li> <li>Cr\u00e9er un workspace Synapse</li> <li>Charger des donn\u00e9es</li> <li>Ex\u00e9cuter des requ\u00eates SQL avanc\u00e9es</li> <li>Int\u00e9grer avec PowerBI</li> </ul>"},{"location":"Cloud/Azure/FR/05-synapse/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Synapse</li> <li>Cr\u00e9er un workspace Synapse</li> <li>Charger des donn\u00e9es</li> <li>Requ\u00eates SQL avanc\u00e9es</li> <li>Int\u00e9gration avec PowerBI</li> <li>Bonnes pratiques</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#introduction-a-synapse","title":"Introduction \u00e0 Synapse","text":""},{"location":"Cloud/Azure/FR/05-synapse/#quest-ce-quazure-synapse-analytics","title":"Qu'est-ce qu'Azure Synapse Analytics ?","text":"<p>Azure Synapse Analytics = Plateforme d'analytics unifi\u00e9e</p> <ul> <li>Data Warehouse : Stockage et analyse de donn\u00e9es</li> <li>Big Data : Traitement de grandes quantit\u00e9s</li> <li>SQL : Requ\u00eates SQL standard</li> <li>Spark : Traitement distribu\u00e9</li> <li>Int\u00e9gration : Avec tous les services Azure</li> </ul>"},{"location":"Cloud/Azure/FR/05-synapse/#composants-synapse","title":"Composants Synapse","text":"<ol> <li>SQL Pool : Data warehouse SQL (anciennement SQL Data Warehouse)</li> <li>Spark Pool : Clusters Spark pour Big Data</li> <li>Synapse Studio : Interface web unifi\u00e9e</li> <li>Pipelines : ETL int\u00e9gr\u00e9s</li> <li>Notebooks : Python, SQL, Scala</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#free-tier-synapse","title":"Free Tier Synapse","text":"<p>Gratuit avec cr\u00e9dit Azure : - Utiliser les 200$ de cr\u00e9dit gratuit (30 jours) - Apr\u00e8s : facturation normale</p> <p>\u26a0\ufe0f Important : Synapse peut \u00eatre co\u00fbteux. Surveiller attentivement les co\u00fbts.</p>"},{"location":"Cloud/Azure/FR/05-synapse/#creer-un-workspace-synapse","title":"Cr\u00e9er un workspace Synapse","text":""},{"location":"Cloud/Azure/FR/05-synapse/#etape-1-acceder-a-synapse","title":"\u00c9tape 1 : Acc\u00e9der \u00e0 Synapse","text":"<ol> <li>Portail Azure \u2192 Rechercher \"Azure Synapse Analytics\"</li> <li>Cliquer sur \"Azure Synapse Analytics\"</li> <li>Cliquer sur \"Create\"</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#etape-2-configuration-de-base","title":"\u00c9tape 2 : Configuration de base","text":"<p>Informations de base : - Subscription : Choisir votre abonnement - Resource group : Cr\u00e9er ou utiliser existant - Workspace name : <code>my-synapse-workspace</code> - Region : Choisir la r\u00e9gion - Data Lake Storage Gen2 : Cr\u00e9er nouveau ou utiliser existant</p> <p>SQL Administrator : - SQL admin name : <code>sqladmin</code> - Password : Mot de passe fort</p>"},{"location":"Cloud/Azure/FR/05-synapse/#etape-3-configuration-sql-pool","title":"\u00c9tape 3 : Configuration SQL Pool","text":"<p>SQL Pool : - Create a SQL pool : \u2705 Oui (pour d\u00e9buter) - Performance level : DW100c (le moins cher) - Or : Cr\u00e9er plus tard (Serverless SQL)</p> <p>\u26a0\ufe0f Important : Serverless SQL = pay-per-query, plus \u00e9conomique pour d\u00e9buter.</p>"},{"location":"Cloud/Azure/FR/05-synapse/#etape-4-creer-le-workspace","title":"\u00c9tape 4 : Cr\u00e9er le workspace","text":"<ol> <li>Cliquer sur \"Review + create\"</li> <li>V\u00e9rifier la configuration</li> <li>Cliquer sur \"Create\"</li> <li>Attendre la cr\u00e9ation (5-10 minutes)</li> </ol> <p>\u26a0\ufe0f Important : Noter les credentials SQL.</p>"},{"location":"Cloud/Azure/FR/05-synapse/#charger-des-donnees","title":"Charger des donn\u00e9es","text":""},{"location":"Cloud/Azure/FR/05-synapse/#methode-1-copy-depuis-data-lake-storage","title":"M\u00e9thode 1 : COPY depuis Data Lake Storage","text":"<p>Le plus rapide pour grandes quantit\u00e9s :</p> <pre><code>-- Cr\u00e9er une table\nCREATE TABLE users (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at DATETIME2\n)\nWITH (\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n-- Charger depuis Data Lake Storage\nCOPY INTO users\nFROM 'https://mystorageaccount.dfs.core.windows.net/data-lake/raw/users.csv'\nWITH (\n    FILE_TYPE = 'CSV',\n    FIRSTROW = 2,\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n'\n);\n</code></pre>"},{"location":"Cloud/Azure/FR/05-synapse/#methode-2-via-synapse-pipelines","title":"M\u00e9thode 2 : Via Synapse Pipelines","text":"<p>Pipeline int\u00e9gr\u00e9 :</p> <ol> <li>Synapse Studio \u2192 \"Integrate\" \u2192 \"Pipelines\"</li> <li>Cr\u00e9er un nouveau pipeline</li> <li>Ajouter activit\u00e9 \"Copy Data\"</li> <li>Source : Azure Blob Storage ou Data Lake</li> <li>Sink : SQL Pool</li> <li>Ex\u00e9cuter le pipeline</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#methode-3-insert-petites-quantites","title":"M\u00e9thode 3 : INSERT (petites quantit\u00e9s)","text":"<pre><code>INSERT INTO users (id, name, email, created_at)\nVALUES (1, 'John Doe', 'john@example.com', '2024-01-01');\n</code></pre>"},{"location":"Cloud/Azure/FR/05-synapse/#methode-4-via-polybase-external-tables","title":"M\u00e9thode 4 : Via PolyBase (External Tables)","text":"<p>Cr\u00e9er une table externe :</p> <pre><code>-- Cr\u00e9er un credential\nCREATE DATABASE SCOPED CREDENTIAL BlobCredential\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'your-sas-token';\n\n-- Cr\u00e9er une source de donn\u00e9es externe\nCREATE EXTERNAL DATA SOURCE BlobStorage\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'wasbs://container@account.blob.core.windows.net',\n    CREDENTIAL = BlobCredential\n);\n\n-- Cr\u00e9er un format externe\nCREATE EXTERNAL FILE FORMAT CSVFormat\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (FIELD_TERMINATOR = ',')\n);\n\n-- Cr\u00e9er une table externe\nCREATE EXTERNAL TABLE users_external (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(100)\n)\nWITH (\n    LOCATION = 'raw/users.csv',\n    DATA_SOURCE = BlobStorage,\n    FILE_FORMAT = CSVFormat\n);\n\n-- Charger dans table interne\nINSERT INTO users\nSELECT * FROM users_external;\n</code></pre>"},{"location":"Cloud/Azure/FR/05-synapse/#requetes-sql-avancees","title":"Requ\u00eates SQL avanc\u00e9es","text":""},{"location":"Cloud/Azure/FR/05-synapse/#requetes-de-base","title":"Requ\u00eates de base","text":"<p>SELECT simple :</p> <pre><code>SELECT TOP 100 * FROM users;\n</code></pre> <p>Agr\u00e9gations :</p> <pre><code>SELECT \n    YEAR(created_at) AS year,\n    MONTH(created_at) AS month,\n    COUNT(*) AS user_count\nFROM users\nGROUP BY YEAR(created_at), MONTH(created_at)\nORDER BY year, month;\n</code></pre>"},{"location":"Cloud/Azure/FR/05-synapse/#window-functions","title":"Window Functions","text":"<p>ROW_NUMBER :</p> <pre><code>SELECT \n    id,\n    name,\n    created_at,\n    ROW_NUMBER() OVER (PARTITION BY YEAR(created_at) ORDER BY created_at) AS rank\nFROM users;\n</code></pre> <p>LAG/LEAD :</p> <pre><code>SELECT \n    date,\n    sales,\n    LAG(sales, 1) OVER (ORDER BY date) AS previous_sales,\n    LEAD(sales, 1) OVER (ORDER BY date) AS next_sales\nFROM daily_sales;\n</code></pre>"},{"location":"Cloud/Azure/FR/05-synapse/#distribution-et-performance","title":"Distribution et Performance","text":"<p>Distribution keys :</p> <pre><code>-- Distribution HASH (pour jointures)\nCREATE TABLE users (\n    id INT,\n    name VARCHAR(100)\n)\nWITH (\n    DISTRIBUTION = HASH(id),\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n-- Distribution ROUND_ROBIN (par d\u00e9faut)\nCREATE TABLE logs (\n    id INT,\n    message VARCHAR(MAX)\n)\nWITH (\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n</code></pre> <p>Clustered Columnstore Index : - Optimis\u00e9 pour analytics - Compression \u00e9lev\u00e9e - Requ\u00eates rapides sur grandes tables</p>"},{"location":"Cloud/Azure/FR/05-synapse/#integration-avec-powerbi","title":"Int\u00e9gration avec PowerBI","text":""},{"location":"Cloud/Azure/FR/05-synapse/#connexion-directe","title":"Connexion directe","text":"<p>\u00c9tape 1 : Dans PowerBI Desktop</p> <ol> <li>\"Get Data\" \u2192 \"Azure\" \u2192 \"Azure Synapse Analytics SQL\"</li> <li>Entrer les informations :</li> <li>Server : <code>my-synapse-workspace-ondemand.sql.azuresynapse.net</code> (Serverless)</li> <li>Database : Nom de la base</li> <li>Data connectivity mode : DirectQuery (recommand\u00e9)</li> </ol> <p>\u00c9tape 2 : Authentification</p> <ul> <li>Authentication method : Database</li> <li>Username : <code>sqladmin</code></li> <li>Password : Votre mot de passe</li> </ul> <p>\u00c9tape 3 : S\u00e9lectionner les tables</p> <ul> <li>Choisir les tables ou vues</li> <li>Cliquer sur \"Load\"</li> </ul>"},{"location":"Cloud/Azure/FR/05-synapse/#creer-des-vues-pour-powerbi","title":"Cr\u00e9er des vues pour PowerBI","text":"<p>Vue optimis\u00e9e :</p> <pre><code>CREATE VIEW vw_user_analytics AS\nSELECT \n    u.id,\n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name, u.email;\n</code></pre> <p>Utiliser la vue dans PowerBI : - Plus simple pour les utilisateurs - Logique m\u00e9tier centralis\u00e9e - Performance optimis\u00e9e</p>"},{"location":"Cloud/Azure/FR/05-synapse/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/Azure/FR/05-synapse/#performance","title":"Performance","text":"<ol> <li>Utiliser Columnstore Index pour analytics</li> <li>Choisir les bonnes distribution keys</li> <li>Partitionner les grandes tables</li> <li>Optimiser les requ\u00eates avec EXPLAIN</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#couts","title":"Co\u00fbts","text":"<ol> <li>Utiliser Serverless SQL pour d\u00e9buter (pay-per-query)</li> <li>Pauser le SQL Pool quand non utilis\u00e9</li> <li>Surveiller les co\u00fbts dans Azure Cost Management</li> <li>Utiliser les bonnes tailles de pool</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#organisation","title":"Organisation","text":"<ol> <li>Cr\u00e9er des sch\u00e9mas pour organiser</li> <li>Nommer clairement les tables et vues</li> <li>Documenter les sch\u00e9mas</li> <li>Utiliser des vues pour simplifier</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#securite","title":"S\u00e9curit\u00e9","text":"<ol> <li>Utiliser Azure AD pour authentification</li> <li>Limiter les acc\u00e8s avec firewall rules</li> <li>Chiffrer les donn\u00e9es (activ\u00e9 par d\u00e9faut)</li> <li>Auditer les acc\u00e8s</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/Azure/FR/05-synapse/#exemple-1-pipeline-complet-data-lake-synapse","title":"Exemple 1 : Pipeline complet Data Lake \u2192 Synapse","text":"<p>Pipeline Synapse : 1. Source : Data Lake Storage (Parquet) 2. Activity : Copy Data 3. Sink : SQL Pool 4. Trigger : Schedule (quotidien)</p>"},{"location":"Cloud/Azure/FR/05-synapse/#exemple-2-requetes-analytiques-complexes","title":"Exemple 2 : Requ\u00eates analytiques complexes","text":"<pre><code>-- Analyse des ventes avec window functions\nWITH monthly_sales AS (\n    SELECT \n        YEAR(sale_date) AS year,\n        MONTH(sale_date) AS month,\n        SUM(amount) AS total_sales\n    FROM sales\n    GROUP BY YEAR(sale_date), MONTH(sale_date)\n)\nSELECT \n    year,\n    month,\n    total_sales,\n    LAG(total_sales, 1) OVER (ORDER BY year, month) AS previous_month,\n    (total_sales - LAG(total_sales, 1) OVER (ORDER BY year, month)) / \n        LAG(total_sales, 1) OVER (ORDER BY year, month) * 100 AS growth_percent\nFROM monthly_sales\nORDER BY year, month;\n</code></pre>"},{"location":"Cloud/Azure/FR/05-synapse/#exemple-3-export-vers-powerbi","title":"Exemple 3 : Export vers PowerBI","text":"<ol> <li>Cr\u00e9er une vue analytique</li> <li>Connecter PowerBI \u00e0 la vue</li> <li>Cr\u00e9er des visualisations</li> <li>Publier le rapport</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Synapse = Plateforme analytics unifi\u00e9e</li> <li>SQL Pool pour data warehouse</li> <li>Serverless SQL pour pay-per-query</li> <li>Int\u00e9gration PowerBI native</li> <li>Scalable de quelques Go \u00e0 plusieurs Po</li> </ol>"},{"location":"Cloud/Azure/FR/05-synapse/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 6. Azure Databricks - Big Data Analytics pour apprendre \u00e0 utiliser Databricks pour le Big Data.</p>"},{"location":"Cloud/Azure/FR/06-databricks/","title":"6. Azure Databricks - Big Data Analytics","text":""},{"location":"Cloud/Azure/FR/06-databricks/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Azure Databricks</li> <li>Cr\u00e9er un workspace Databricks</li> <li>Utiliser des notebooks Python/SQL</li> <li>Traiter des donn\u00e9es avec Spark</li> <li>Int\u00e9grer avec autres services Azure</li> </ul>"},{"location":"Cloud/Azure/FR/06-databricks/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Databricks</li> <li>Cr\u00e9er un workspace Databricks</li> <li>Cr\u00e9er un cluster</li> <li>Notebooks Python/SQL</li> <li>Traitement de donn\u00e9es avec Spark</li> <li>Int\u00e9gration avec autres services</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#introduction-a-databricks","title":"Introduction \u00e0 Databricks","text":""},{"location":"Cloud/Azure/FR/06-databricks/#quest-ce-quazure-databricks","title":"Qu'est-ce qu'Azure Databricks ?","text":"<p>Azure Databricks = Plateforme Big Data bas\u00e9e sur Apache Spark</p> <ul> <li>Apache Spark : Moteur de traitement distribu\u00e9</li> <li>Notebooks : Python, SQL, Scala, R</li> <li>G\u00e9r\u00e9 : Microsoft g\u00e8re l'infrastructure</li> <li>Scalable : Clusters auto-scaling</li> </ul>"},{"location":"Cloud/Azure/FR/06-databricks/#cas-dusage-pour-data-analyst","title":"Cas d'usage pour Data Analyst","text":"<ul> <li>Big Data processing : Traiter de grandes quantit\u00e9s</li> <li>ETL : Transformations complexes</li> <li>Machine Learning : MLlib int\u00e9gr\u00e9</li> <li>Data Science : Notebooks interactifs</li> </ul>"},{"location":"Cloud/Azure/FR/06-databricks/#free-tier-databricks","title":"Free Tier Databricks","text":"<p>Gratuit avec cr\u00e9dit Azure : - Utiliser les 200$ de cr\u00e9dit gratuit (30 jours) - Apr\u00e8s : facturation normale</p> <p>\u26a0\ufe0f Important : Databricks peut \u00eatre co\u00fbteux. Surveiller attentivement les co\u00fbts.</p>"},{"location":"Cloud/Azure/FR/06-databricks/#creer-un-workspace-databricks","title":"Cr\u00e9er un workspace Databricks","text":""},{"location":"Cloud/Azure/FR/06-databricks/#etape-1-acceder-a-databricks","title":"\u00c9tape 1 : Acc\u00e9der \u00e0 Databricks","text":"<ol> <li>Portail Azure \u2192 Rechercher \"Azure Databricks\"</li> <li>Cliquer sur \"Azure Databricks\"</li> <li>Cliquer sur \"Create\"</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#etape-2-configuration-de-base","title":"\u00c9tape 2 : Configuration de base","text":"<p>Informations de base : - Subscription : Choisir votre abonnement - Resource group : Cr\u00e9er ou utiliser existant - Workspace name : <code>my-databricks-workspace</code> - Region : Choisir la r\u00e9gion - Pricing tier : Standard (ou Premium)</p> <p>Networking : - Virtual network : Cr\u00e9er nouveau ou utiliser existant - Public IP : \u2705 Enable (pour acc\u00e8s facile)</p>"},{"location":"Cloud/Azure/FR/06-databricks/#etape-3-creer-le-workspace","title":"\u00c9tape 3 : Cr\u00e9er le workspace","text":"<ol> <li>Cliquer sur \"Review + create\"</li> <li>V\u00e9rifier la configuration</li> <li>Cliquer sur \"Create\"</li> <li>Attendre la cr\u00e9ation (5-10 minutes)</li> </ol> <p>\u26a0\ufe0f Important : Noter l'URL du workspace.</p>"},{"location":"Cloud/Azure/FR/06-databricks/#etape-4-ouvrir-databricks","title":"\u00c9tape 4 : Ouvrir Databricks","text":"<ol> <li>Une fois cr\u00e9\u00e9, cliquer sur \"Launch Workspace\"</li> <li>Interface web Databricks</li> <li>Se connecter avec Azure AD</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#creer-un-cluster","title":"Cr\u00e9er un cluster","text":""},{"location":"Cloud/Azure/FR/06-databricks/#etape-1-acceder-aux-clusters","title":"\u00c9tape 1 : Acc\u00e9der aux clusters","text":"<ol> <li>Databricks Workspace \u2192 \"Compute\"</li> <li>Cliquer sur \"Create Cluster\"</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#etape-2-configuration-du-cluster","title":"\u00c9tape 2 : Configuration du cluster","text":"<p>Configuration de base : - Cluster name : <code>my-cluster</code> - Cluster mode : Standard (ou Single Node pour tests) - Databricks runtime version : Latest LTS (recommand\u00e9) - Python version : 3.11</p> <p>Node type : - Worker type : Standard_DS3_v2 (pour d\u00e9buter) - Driver type : Standard_DS3_v2 - Min workers : 0 (pour \u00e9conomiser) - Max workers : 2 (pour d\u00e9buter)</p> <p>\u26a0\ufe0f Important : Min workers = 0 permet l'auto-termination quand inactif.</p>"},{"location":"Cloud/Azure/FR/06-databricks/#etape-3-options-avancees","title":"\u00c9tape 3 : Options avanc\u00e9es","text":"<p>Auto-termination : - \u2705 Enable (arr\u00eate le cluster apr\u00e8s inactivit\u00e9) - Terminate after : 30 minutes</p> <p>Tags : - Ajouter des tags pour organisation</p>"},{"location":"Cloud/Azure/FR/06-databricks/#etape-4-creer-le-cluster","title":"\u00c9tape 4 : Cr\u00e9er le cluster","text":"<ol> <li>Cliquer sur \"Create Cluster\"</li> <li>Attendre le d\u00e9marrage (3-5 minutes)</li> <li>Cluster pr\u00eat quand status = \"Running\"</li> </ol> <p>\u26a0\ufe0f Important : Le cluster consomme des ressources m\u00eame inactif. L'arr\u00eater quand non utilis\u00e9.</p>"},{"location":"Cloud/Azure/FR/06-databricks/#notebooks-pythonsql","title":"Notebooks Python/SQL","text":""},{"location":"Cloud/Azure/FR/06-databricks/#creer-un-notebook","title":"Cr\u00e9er un notebook","text":"<p>\u00c9tape 1 : Cr\u00e9er un notebook</p> <ol> <li>Databricks Workspace \u2192 \"Workspace\"</li> <li>Clic droit \u2192 \"Create\" \u2192 \"Notebook\"</li> <li>Nom : <code>data-processing</code></li> <li>Language : Python (ou SQL)</li> <li>Cluster : Attacher au cluster cr\u00e9\u00e9</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#etape-2-utiliser-le-notebook","title":"\u00c9tape 2 : Utiliser le notebook","text":"<p>Cellules Python :</p> <pre><code># Cellule 1 : Importer des biblioth\u00e8ques\nimport pandas as pd\nfrom pyspark.sql import SparkSession\n\n# Cellule 2 : Cr\u00e9er une session Spark\nspark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()\n\n# Cellule 3 : Lire des donn\u00e9es\ndf = spark.read.csv(\"dbfs:/FileStore/data/users.csv\", header=True, inferSchema=True)\n\n# Cellule 4 : Afficher les donn\u00e9es\ndf.show()\n\n# Cellule 5 : Transformer\ndf_filtered = df.filter(df[\"status\"] == \"active\")\ndf_filtered.show()\n</code></pre> <p>Cellules SQL :</p> <pre><code>-- Cellule SQL : Cr\u00e9er une vue temporaire\nCREATE OR REPLACE TEMPORARY VIEW users AS\nSELECT * FROM csv.`dbfs:/FileStore/data/users.csv`\n\n-- Requ\u00eate SQL\nSELECT \n    YEAR(created_at) AS year,\n    COUNT(*) AS user_count\nFROM users\nGROUP BY YEAR(created_at)\nORDER BY year;\n</code></pre>"},{"location":"Cloud/Azure/FR/06-databricks/#executer-un-notebook","title":"Ex\u00e9cuter un notebook","text":"<ul> <li>Run cell : Ex\u00e9cuter une cellule</li> <li>Run all : Ex\u00e9cuter toutes les cellules</li> <li>Run all above : Ex\u00e9cuter toutes les cellules au-dessus</li> </ul>"},{"location":"Cloud/Azure/FR/06-databricks/#traitement-de-donnees-avec-spark","title":"Traitement de donn\u00e9es avec Spark","text":""},{"location":"Cloud/Azure/FR/06-databricks/#lire-des-donnees","title":"Lire des donn\u00e9es","text":"<p>Depuis Data Lake Storage :</p> <pre><code># Lire CSV\ndf = spark.read.csv(\n    \"abfss://container@account.dfs.core.windows.net/data/users.csv\",\n    header=True,\n    inferSchema=True\n)\n\n# Lire Parquet\ndf = spark.read.parquet(\n    \"abfss://container@account.dfs.core.windows.net/data/users.parquet\"\n)\n\n# Lire JSON\ndf = spark.read.json(\n    \"abfss://container@account.dfs.core.windows.net/data/users.json\"\n)\n</code></pre> <p>Depuis Azure Blob Storage :</p> <pre><code># Configurer l'acc\u00e8s\nspark.conf.set(\n    \"fs.azure.account.key.accountname.blob.core.windows.net\",\n    \"your-account-key\"\n)\n\n# Lire\ndf = spark.read.csv(\n    \"wasbs://container@accountname.blob.core.windows.net/data/users.csv\",\n    header=True\n)\n</code></pre>"},{"location":"Cloud/Azure/FR/06-databricks/#transformer-des-donnees","title":"Transformer des donn\u00e9es","text":"<p>Filtrer :</p> <pre><code>df_filtered = df.filter(df[\"age\"] &gt; 18)\n</code></pre> <p>S\u00e9lectionner des colonnes :</p> <pre><code>df_selected = df.select(\"id\", \"name\", \"email\")\n</code></pre> <p>Agr\u00e9gations :</p> <pre><code>df_aggregated = df.groupBy(\"category\").agg({\n    \"amount\": \"sum\",\n    \"id\": \"count\"\n})\n</code></pre> <p>Joindre :</p> <pre><code>df_joined = df1.join(df2, df1.id == df2.user_id, \"inner\")\n</code></pre>"},{"location":"Cloud/Azure/FR/06-databricks/#ecrire-des-donnees","title":"\u00c9crire des donn\u00e9es","text":"<p>Vers Data Lake Storage :</p> <pre><code># \u00c9crire en Parquet\ndf.write.mode(\"overwrite\").parquet(\n    \"abfss://container@account.dfs.core.windows.net/processed/users.parquet\"\n)\n\n# \u00c9crire en CSV\ndf.write.mode(\"overwrite\").csv(\n    \"abfss://container@account.dfs.core.windows.net/processed/users.csv\"\n)\n</code></pre>"},{"location":"Cloud/Azure/FR/06-databricks/#integration-avec-autres-services","title":"Int\u00e9gration avec autres services","text":""},{"location":"Cloud/Azure/FR/06-databricks/#databricks-data-lake-storage","title":"Databricks + Data Lake Storage","text":"<p>Acc\u00e8s direct :</p> <pre><code># Configurer l'acc\u00e8s\nspark.conf.set(\n    \"fs.azure.account.auth.type.account.dfs.core.windows.net\",\n    \"OAuth\"\n)\nspark.conf.set(\n    \"fs.azure.account.oauth.provider.type.account.dfs.core.windows.net\",\n    \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\"\n)\n\n# Lire\ndf = spark.read.parquet(\n    \"abfss://container@account.dfs.core.windows.net/data/users.parquet\"\n)\n</code></pre>"},{"location":"Cloud/Azure/FR/06-databricks/#databricks-azure-sql-database","title":"Databricks + Azure SQL Database","text":"<p>Lire depuis SQL Database :</p> <pre><code>df = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://server.database.windows.net:1433;database=db\") \\\n    .option(\"dbtable\", \"users\") \\\n    .option(\"user\", \"sqladmin\") \\\n    .option(\"password\", \"password\") \\\n    .load()\n</code></pre> <p>\u00c9crire vers SQL Database :</p> <pre><code>df.write \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://server.database.windows.net:1433;database=db\") \\\n    .option(\"dbtable\", \"users_processed\") \\\n    .option(\"user\", \"sqladmin\") \\\n    .option(\"password\", \"password\") \\\n    .mode(\"overwrite\") \\\n    .save()\n</code></pre>"},{"location":"Cloud/Azure/FR/06-databricks/#databricks-data-factory","title":"Databricks + Data Factory","text":"<p>Pipeline Data Factory : 1. Source : Azure Blob Storage 2. Activity : Databricks Notebook 3. Sink : Azure SQL Database</p> <p>Configuration : - Notebook path : <code>/Workspace/path/to/notebook</code> - Parameters : Passer des param\u00e8tres</p>"},{"location":"Cloud/Azure/FR/06-databricks/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"Cloud/Azure/FR/06-databricks/#performance","title":"Performance","text":"<ol> <li>Utiliser le cache pour r\u00e9utiliser des donn\u00e9es</li> <li>Partitionner les donn\u00e9es pour am\u00e9liorer les performances</li> <li>Optimiser les transformations pour r\u00e9duire le temps</li> <li>Utiliser le bon nombre de workers</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#couts","title":"Co\u00fbts","text":"<ol> <li>Arr\u00eater les clusters quand non utilis\u00e9s</li> <li>Utiliser auto-termination pour \u00e9conomiser</li> <li>Surveiller les co\u00fbts dans Azure Cost Management</li> <li>Utiliser des clusters plus petits pour d\u00e9buter</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#organisation","title":"Organisation","text":"<ol> <li>Organiser les notebooks dans des dossiers</li> <li>Nommer clairement les notebooks et clusters</li> <li>Documenter le code</li> <li>Versionner avec Git</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#securite","title":"S\u00e9curit\u00e9","text":"<ol> <li>Utiliser Azure AD pour authentification</li> <li>Limiter les acc\u00e8s avec RBAC</li> <li>Chiffrer les donn\u00e9es en transit et au repos</li> <li>Auditer les acc\u00e8s</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Cloud/Azure/FR/06-databricks/#exemple-1-pipeline-etl-complet","title":"Exemple 1 : Pipeline ETL complet","text":"<p>Notebook Databricks :</p> <pre><code># 1. Lire depuis Data Lake\ndf = spark.read.parquet(\n    \"abfss://container@account.dfs.core.windows.net/raw/users.parquet\"\n)\n\n# 2. Transformer\ndf_processed = df \\\n    .filter(df[\"status\"] == \"active\") \\\n    .select(\"id\", \"name\", \"email\", \"created_at\") \\\n    .withColumn(\"year\", year(col(\"created_at\")))\n\n# 3. \u00c9crire vers Data Lake\ndf_processed.write.mode(\"overwrite\").parquet(\n    \"abfss://container@account.dfs.core.windows.net/processed/users.parquet\"\n)\n</code></pre>"},{"location":"Cloud/Azure/FR/06-databricks/#exemple-2-analyse-avec-spark-sql","title":"Exemple 2 : Analyse avec Spark SQL","text":"<pre><code># Cr\u00e9er une vue temporaire\ndf.createOrReplaceTempView(\"users\")\n\n# Requ\u00eate SQL\nresult = spark.sql(\"\"\"\n    SELECT \n        YEAR(created_at) AS year,\n        COUNT(*) AS user_count,\n        COUNT(DISTINCT email) AS unique_emails\n    FROM users\n    GROUP BY YEAR(created_at)\n    ORDER BY year\n\"\"\")\n\nresult.show()\n</code></pre>"},{"location":"Cloud/Azure/FR/06-databricks/#exemple-3-integration-avec-data-factory","title":"Exemple 3 : Int\u00e9gration avec Data Factory","text":"<ol> <li>Cr\u00e9er un notebook Databricks</li> <li>Dans Data Factory, ajouter activit\u00e9 \"Databricks Notebook\"</li> <li>Configurer le notebook</li> <li>Ex\u00e9cuter le pipeline</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Databricks = Big Data avec Apache Spark</li> <li>Notebooks Python/SQL pour d\u00e9veloppement</li> <li>Clusters auto-scaling pour performance</li> <li>Int\u00e9gration native avec services Azure</li> <li>Payant : Surveiller les co\u00fbts</li> </ol>"},{"location":"Cloud/Azure/FR/06-databricks/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 7. Projets pratiques pour cr\u00e9er des projets complets avec Azure.</p>"},{"location":"Cloud/Azure/FR/07-projets/","title":"7. Projets pratiques Azure","text":""},{"location":"Cloud/Azure/FR/07-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Appliquer les connaissances acquises</li> <li>Cr\u00e9er des pipelines ETL complets</li> <li>Int\u00e9grer avec PowerBI</li> <li>Cr\u00e9er des projets pour votre portfolio</li> <li>Utiliser plusieurs services Azure</li> </ul>"},{"location":"Cloud/Azure/FR/07-projets/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Projet 1 : Pipeline ETL Blob \u2192 SQL Database</li> <li>Projet 2 : Data Lake avec Synapse</li> <li>Projet 3 : Analytics avec PowerBI</li> <li>Projet 4 : Pipeline automatis\u00e9 complet</li> <li>Bonnes pratiques pour portfolio</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#projet-1-pipeline-etl-blob-sql-database","title":"Projet 1 : Pipeline ETL Blob \u2192 SQL Database","text":""},{"location":"Cloud/Azure/FR/07-projets/#objectif","title":"Objectif","text":"<p>Cr\u00e9er un pipeline ETL qui charge des fichiers CSV depuis Blob Storage vers SQL Database.</p>"},{"location":"Cloud/Azure/FR/07-projets/#architecture","title":"Architecture","text":"<pre><code>Blob Storage (CSV) \u2192 Data Factory \u2192 SQL Database \u2192 PowerBI\n</code></pre>"},{"location":"Cloud/Azure/FR/07-projets/#etapes","title":"\u00c9tapes","text":""},{"location":"Cloud/Azure/FR/07-projets/#1-preparer-les-donnees","title":"1. Pr\u00e9parer les donn\u00e9es","text":"<p>Cr\u00e9er un container Blob Storage : - Nom : <code>raw-data</code> - Uploader un fichier CSV de test</p> <p>Exemple de donn\u00e9es CSV : <pre><code>id,name,email,created_at,status\n1,John Doe,john@example.com,2024-01-01,active\n2,Jane Smith,jane@example.com,2024-01-02,inactive\n</code></pre></p>"},{"location":"Cloud/Azure/FR/07-projets/#2-creer-une-base-sql-database","title":"2. Cr\u00e9er une base SQL Database","text":"<ol> <li>Portail Azure \u2192 Cr\u00e9er SQL Database</li> <li>Configuration :</li> <li>Name : <code>analytics-db</code></li> <li>Server : Cr\u00e9er nouveau serveur</li> <li>Service tier : Basic (gratuit 12 mois)</li> <li>Cr\u00e9er la base</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#3-creer-la-table-dans-sql-database","title":"3. Cr\u00e9er la table dans SQL Database","text":"<pre><code>CREATE TABLE users (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at DATETIME2,\n    status VARCHAR(20)\n);\n</code></pre>"},{"location":"Cloud/Azure/FR/07-projets/#4-creer-un-pipeline-data-factory","title":"4. Cr\u00e9er un pipeline Data Factory","text":"<ol> <li>Data Factory Studio \u2192 \"Author\" \u2192 \"Pipelines\"</li> <li>Cr\u00e9er un nouveau pipeline : <code>LoadCSVToSQL</code></li> <li>Ajouter activit\u00e9 \"Copy Data\"</li> <li>Configuration :</li> <li>Source : Azure Blob Storage (CSV)</li> <li>Sink : Azure SQL Database (table users)</li> <li>Publier le pipeline</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#5-executer-le-pipeline","title":"5. Ex\u00e9cuter le pipeline","text":"<ol> <li>Cliquer sur \"Trigger now\"</li> <li>V\u00e9rifier l'ex\u00e9cution dans \"Monitor\"</li> <li>V\u00e9rifier les donn\u00e9es dans SQL Database</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#resultat","title":"R\u00e9sultat","text":"<ul> <li>Fichiers CSV charg\u00e9s dans SQL Database</li> <li>Pipeline ETL fonctionnel</li> <li>Pr\u00eat pour analytics avec PowerBI</li> </ul>"},{"location":"Cloud/Azure/FR/07-projets/#projet-2-data-lake-avec-synapse","title":"Projet 2 : Data Lake avec Synapse","text":""},{"location":"Cloud/Azure/FR/07-projets/#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er un Data Lake complet avec ingestion, transformation et analytics.</p>"},{"location":"Cloud/Azure/FR/07-projets/#architecture_1","title":"Architecture","text":"<pre><code>Sources \u2192 Data Lake Storage (Raw) \u2192 Synapse (Transform) \u2192 Data Lake (Processed) \u2192 PowerBI\n                \u2193\n        Data Factory (Orchestration)\n</code></pre>"},{"location":"Cloud/Azure/FR/07-projets/#etapes_1","title":"\u00c9tapes","text":""},{"location":"Cloud/Azure/FR/07-projets/#1-structure-data-lake-storage","title":"1. Structure Data Lake Storage","text":"<pre><code>data-lake/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u251c\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"Cloud/Azure/FR/07-projets/#2-creer-un-workspace-synapse","title":"2. Cr\u00e9er un workspace Synapse","text":"<ol> <li>Portail Azure \u2192 Cr\u00e9er Azure Synapse Analytics</li> <li>Configuration :</li> <li>Workspace name : <code>my-synapse-workspace</code></li> <li>Data Lake Storage : Cr\u00e9er nouveau</li> <li>Cr\u00e9er le workspace</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#3-pipelines-data-factory-pour-transformation","title":"3. Pipelines Data Factory pour transformation","text":"<p>Pipeline pour users :</p> <ol> <li>Synapse Studio \u2192 \"Integrate\" \u2192 \"Pipelines\"</li> <li>Cr\u00e9er pipeline : <code>TransformUsers</code></li> <li>Activit\u00e9s :</li> <li>Source : Data Lake Storage (raw/users/)</li> <li>Data Flow : Transformer (filtrer, nettoyer)</li> <li>Sink : Data Lake Storage (processed/users/)</li> <li>Publier</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#4-tables-synapse-pour-analytics","title":"4. Tables Synapse pour analytics","text":"<pre><code>-- Cr\u00e9er une table externe\nCREATE EXTERNAL TABLE users_processed (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at DATETIME2\n)\nWITH (\n    LOCATION = 'processed/users/',\n    DATA_SOURCE = DataLakeStorage,\n    FILE_FORMAT = ParquetFormat\n);\n\n-- Requ\u00eate analytique\nSELECT \n    YEAR(created_at) AS year,\n    COUNT(*) AS user_count\nFROM users_processed\nGROUP BY YEAR(created_at);\n</code></pre>"},{"location":"Cloud/Azure/FR/07-projets/#5-automatisation-avec-triggers","title":"5. Automatisation avec Triggers","text":"<ol> <li>Pipeline \u2192 \"Add trigger\" \u2192 \"New/Edit\"</li> <li>Type : Schedule</li> <li>Recurrence : Daily</li> <li>Start time : 02:00</li> <li>Sauvegarder</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#resultat_1","title":"R\u00e9sultat","text":"<ul> <li>Data Lake fonctionnel</li> <li>Pipeline automatis\u00e9</li> <li>Analytics avec Synapse</li> <li>Projet complet pour portfolio</li> </ul>"},{"location":"Cloud/Azure/FR/07-projets/#projet-3-analytics-avec-powerbi","title":"Projet 3 : Analytics avec PowerBI","text":""},{"location":"Cloud/Azure/FR/07-projets/#objectif_2","title":"Objectif","text":"<p>Cr\u00e9er un syst\u00e8me d'analytics complet avec PowerBI connect\u00e9 \u00e0 Azure.</p>"},{"location":"Cloud/Azure/FR/07-projets/#etapes_2","title":"\u00c9tapes","text":""},{"location":"Cloud/Azure/FR/07-projets/#1-preparer-les-donnees_1","title":"1. Pr\u00e9parer les donn\u00e9es","text":"<p>Dans SQL Database ou Synapse : - Charger des donn\u00e9es - Cr\u00e9er des vues analytiques</p>"},{"location":"Cloud/Azure/FR/07-projets/#2-connecter-powerbi-a-azure-sql-database","title":"2. Connecter PowerBI \u00e0 Azure SQL Database","text":"<ol> <li>PowerBI Desktop \u2192 \"Get Data\"</li> <li>\"Azure\" \u2192 \"Azure SQL Database\"</li> <li>Configuration :</li> <li>Server : <code>my-sql-server.database.windows.net</code></li> <li>Database : <code>analytics-db</code></li> <li>Authentication : Database</li> <li>S\u00e9lectionner les tables ou vues</li> <li>Cliquer sur \"Load\"</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#3-creer-des-visualisations","title":"3. Cr\u00e9er des visualisations","text":"<p>Exemple : 1. Importer la table <code>users</code> 2. Cr\u00e9er un graphique : Nombre d'utilisateurs par mois 3. Ajouter des filtres 4. Cr\u00e9er un dashboard</p>"},{"location":"Cloud/Azure/FR/07-projets/#4-publier-sur-powerbi-service","title":"4. Publier sur PowerBI Service","text":"<ol> <li>PowerBI Desktop \u2192 \"Publish\"</li> <li>S\u00e9lectionner l'espace de travail</li> <li>Publier</li> <li>Acc\u00e9der au rapport sur powerbi.com</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#5-actualiser-les-donnees","title":"5. Actualiser les donn\u00e9es","text":"<ol> <li>PowerBI Service \u2192 Dataset \u2192 \"Schedule refresh\"</li> <li>Configuration :</li> <li>Frequency : Daily</li> <li>Time : 03:00</li> <li>Sauvegarder</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#resultat_2","title":"R\u00e9sultat","text":"<ul> <li>Analytics avec PowerBI</li> <li>Visualisations interactives</li> <li>Actualisation automatique</li> <li>Projet complet pour portfolio</li> </ul>"},{"location":"Cloud/Azure/FR/07-projets/#projet-4-pipeline-automatise-complet","title":"Projet 4 : Pipeline automatis\u00e9 complet","text":""},{"location":"Cloud/Azure/FR/07-projets/#objectif_3","title":"Objectif","text":"<p>Cr\u00e9er un pipeline ETL compl\u00e8tement automatis\u00e9 avec plusieurs services Azure.</p>"},{"location":"Cloud/Azure/FR/07-projets/#architecture-complete","title":"Architecture compl\u00e8te","text":"<pre><code>Fichier CSV upload\u00e9 \u2192 Blob Storage (raw/)\n    \u2193 (Event)\nAzure Function (Validation)\n    \u2193\nBlob Storage (validated/)\n    \u2193 (Trigger)\nData Factory Pipeline (Transform CSV \u2192 Parquet)\n    \u2193\nData Lake Storage (processed/)\n    \u2193\nSynapse (Analytics)\n    \u2193\nSQL Database (Results)\n    \u2193\nPowerBI (Visualization)\n</code></pre>"},{"location":"Cloud/Azure/FR/07-projets/#implementation","title":"Impl\u00e9mentation","text":""},{"location":"Cloud/Azure/FR/07-projets/#1-azure-function-de-validation","title":"1. Azure Function de validation","text":"<pre><code>import azure.functions as func\nimport logging\nimport csv\nfrom azure.storage.blob import BlobServiceClient\n\ndef main(blob: func.InputStream):\n    logging.info(f'Processing blob: {blob.name}')\n\n    # Lire le blob\n    content = blob.read().decode('utf-8')\n    reader = csv.DictReader(content.splitlines())\n\n    # Valider\n    valid_rows = []\n    for row in reader:\n        if row.get('email') and '@' in row['email']:\n            valid_rows.append(row)\n\n    # Uploader les donn\u00e9es valid\u00e9es\n    if valid_rows:\n        # Upload vers validated/\n        # ...\n\n    logging.info(f'Validated {len(valid_rows)} rows')\n</code></pre>"},{"location":"Cloud/Azure/FR/07-projets/#2-data-factory-pipeline-de-transformation","title":"2. Data Factory Pipeline de transformation","text":"<p>Pipeline : 1. Source : Blob Storage (validated/) 2. Data Flow : Transformer (nettoyer, enrichir) 3. Sink : Data Lake Storage (processed/parquet/)</p>"},{"location":"Cloud/Azure/FR/07-projets/#3-synapse-pour-analytics","title":"3. Synapse pour analytics","text":"<pre><code>-- Cr\u00e9er une vue analytique\nCREATE VIEW vw_user_analytics AS\nSELECT \n    u.id,\n    u.name,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n</code></pre>"},{"location":"Cloud/Azure/FR/07-projets/#4-powerbi-pour-visualisation","title":"4. PowerBI pour visualisation","text":"<ol> <li>Connecter PowerBI \u00e0 Synapse</li> <li>Utiliser la vue <code>vw_user_analytics</code></li> <li>Cr\u00e9er des visualisations</li> <li>Publier le rapport</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#resultat_3","title":"R\u00e9sultat","text":"<ul> <li>Pipeline compl\u00e8tement automatis\u00e9</li> <li>Validation automatique</li> <li>Transformation automatique</li> <li>Analytics disponibles imm\u00e9diatement</li> <li>Visualisations PowerBI</li> </ul>"},{"location":"Cloud/Azure/FR/07-projets/#bonnes-pratiques-pour-portfolio","title":"Bonnes pratiques pour portfolio","text":""},{"location":"Cloud/Azure/FR/07-projets/#documentation","title":"Documentation","text":"<p>Cr\u00e9er un README pour chaque projet :</p> <pre><code># Projet : Pipeline ETL Azure\n\n## Description\nPipeline ETL automatis\u00e9 pour transformer des donn\u00e9es CSV en Parquet.\n\n## Architecture\n- Blob Storage : Stockage\n- Data Factory : Transformation\n- SQL Database : Base de donn\u00e9es\n- PowerBI : Visualisation\n\n## R\u00e9sultats\n- R\u00e9duction des co\u00fbts de 50%\n- Temps de traitement r\u00e9duit de 70%\n</code></pre>"},{"location":"Cloud/Azure/FR/07-projets/#visualisations","title":"Visualisations","text":"<p>Cr\u00e9er des diagrammes : - Architecture du syst\u00e8me - Flux de donn\u00e9es - Sch\u00e9ma de donn\u00e9es</p> <p>Outils : - Draw.io - Lucidchart - Diagrammes ASCII dans README</p>"},{"location":"Cloud/Azure/FR/07-projets/#metriques","title":"M\u00e9triques","text":"<p>Inclure des m\u00e9triques : - Temps d'ex\u00e9cution avant/apr\u00e8s - Co\u00fbts avant/apr\u00e8s - Volume de donn\u00e9es trait\u00e9es - Performance des requ\u00eates</p>"},{"location":"Cloud/Azure/FR/07-projets/#code","title":"Code","text":"<p>Bonnes pratiques : - Code comment\u00e9 - Variables d'environnement pour configuration - Gestion d'erreurs - Logging</p>"},{"location":"Cloud/Azure/FR/07-projets/#github","title":"GitHub","text":"<p>Cr\u00e9er un repository : - README avec documentation - Scripts Data Factory (JSON) - Scripts SQL - Configuration - Diagrammes</p>"},{"location":"Cloud/Azure/FR/07-projets/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Projets pratiques : Essentiels pour portfolio</li> <li>Documentation : Expliquer l'architecture et les r\u00e9sultats</li> <li>M\u00e9triques : Montrer l'impact (performance, co\u00fbts)</li> <li>Code propre : Comment\u00e9 et organis\u00e9</li> <li>GitHub : Partager vos projets</li> </ol>"},{"location":"Cloud/Azure/FR/07-projets/#ressources","title":"\ud83d\udd17 Ressources","text":"<ul> <li>Azure Architecture Center</li> <li>Azure Solutions</li> <li>GitHub Azure Examples</li> </ul> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation Azure pour Data Analyst. Vous pouvez maintenant cr\u00e9er des projets complets sur Azure en utilisant les ressources gratuites disponibles.</p>"},{"location":"Cloud/Azure/PL/","title":"Szkolenie Azure dla Data Analyst - Przewodnik Bezp\u0142atny","text":""},{"location":"Cloud/Azure/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>Ten kurs prowadzi Ci\u0119 przez nauk\u0119 Microsoft Azure jako Data Analyst, u\u017cywaj\u0105c wy\u0142\u0105cznie bezp\u0142atnych zasob\u00f3w. Nauczysz si\u0119 u\u017cywa\u0107 niezb\u0119dnych us\u0142ug Azure do analizy danych bez wydawania ani grosza.</p>"},{"location":"Cloud/Azure/PL/#cele-edukacyjne","title":"\ud83c\udfaf Cele edukacyjne","text":"<ul> <li>Zrozumie\u0107 niezb\u0119dne us\u0142ugi Azure dla Data Analyst</li> <li>Tworzy\u0107 i zarz\u0105dza\u0107 bezp\u0142atnymi kontami Azure</li> <li>U\u017cywa\u0107 Azure Data Factory, SQL Database i innych us\u0142ug danych</li> <li>Budowa\u0107 potoki ETL na Azure</li> <li>Analizowa\u0107 dane z Azure</li> <li>Integrowa\u0107 z PowerBI</li> <li>Tworzy\u0107 praktyczne projekty do portfolio</li> </ul>"},{"location":"Cloud/Azure/PL/#wszystko-jest-bezpatne","title":"\ud83d\udcb0 Wszystko jest bezp\u0142atne!","text":"<p>Ten kurs wykorzystuje wy\u0142\u0105cznie: - \u2705 Bezp\u0142atne konto Azure : 200 USD kredytu (30 dni) + bezp\u0142atne us\u0142ugi - \u2705 Microsoft Learn : Bezp\u0142atne interaktywne kursy - \u2705 Dokumentacja Azure : Kompletne bezp\u0142atne przewodniki - \u2705 Labs Azure : Bezp\u0142atne praktyczne laboratoria</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"Cloud/Azure/PL/#struktura-kursu","title":"\ud83d\udcd6 Struktura kursu","text":""},{"location":"Cloud/Azure/PL/#1-rozpoczecie-z-azure","title":"1. Rozpocz\u0119cie z Azure","text":"<ul> <li>Utworzenie bezp\u0142atnego konta Azure</li> <li>Zrozumienie bezp\u0142atnych kredyt\u00f3w</li> <li>Nawigacja w portalu Azure</li> <li>Konfiguracja bezpiecze\u0144stwa (Azure AD)</li> </ul>"},{"location":"Cloud/Azure/PL/#2-azure-storage-przechowywanie-danych","title":"2. Azure Storage - Przechowywanie danych","text":"<ul> <li>Tworzenie kont magazynu</li> <li>Blob Storage, Data Lake Storage</li> <li>Przesy\u0142anie i zarz\u0105dzanie plikami</li> <li>Organizacja danych</li> </ul>"},{"location":"Cloud/Azure/PL/#3-azure-data-factory-etl-w-chmurze","title":"3. Azure Data Factory - ETL w chmurze","text":"<ul> <li>Tworzenie potok\u00f3w ETL</li> <li>Dzia\u0142ania transformacji</li> <li>Integracja ze \u017ar\u00f3d\u0142ami danych</li> <li>Orchestracja przep\u0142yw\u00f3w pracy</li> </ul>"},{"location":"Cloud/Azure/PL/#4-azure-sql-database-baza-danych","title":"4. Azure SQL Database - Baza danych","text":"<ul> <li>Tworzenie SQL Database (bezp\u0142atnie do 32 GB)</li> <li>Migracja danych</li> <li>Optymalizacja zapyta\u0144</li> <li>Integracja z PowerBI</li> </ul>"},{"location":"Cloud/Azure/PL/#5-azure-synapse-analytics-hurtownia-danych","title":"5. Azure Synapse Analytics - Hurtownia danych","text":"<ul> <li>Tworzenie obszaru roboczego Synapse</li> <li>\u0141adowanie danych</li> <li>Zaawansowane zapytania SQL</li> <li>Integracja z PowerBI</li> </ul>"},{"location":"Cloud/Azure/PL/#6-azure-databricks-analiza-big-data","title":"6. Azure Databricks - Analiza Big Data","text":"<ul> <li>Tworzenie klastra Databricks</li> <li>Przetwarzanie danych ze Spark</li> <li>Notatniki Python/SQL</li> <li>Integracja z innymi us\u0142ugami</li> </ul>"},{"location":"Cloud/Azure/PL/#7-projekty-praktyczne","title":"7. Projekty praktyczne","text":"<ul> <li>Kompletny potok ETL Azure</li> <li>Integracja PowerBI</li> <li>Projekt do portfolio</li> <li>Najlepsze praktyki</li> </ul>"},{"location":"Cloud/Azure/PL/#szybki-start","title":"\ud83d\ude80 Szybki start","text":""},{"location":"Cloud/Azure/PL/#krok-1-utworzenie-bezpatnego-konta-azure","title":"Krok 1: Utworzenie bezp\u0142atnego konta Azure","text":"<ol> <li>Przejd\u017a do: https://azure.microsoft.com/pl-pl/free/</li> <li>Kliknij \"Rozpocznij bezp\u0142atnie\"</li> <li>Zaloguj si\u0119 kontem Microsoft</li> <li>Zweryfikuj to\u017csamo\u015b\u0107 telefonicznie</li> <li>Wprowad\u017a informacje o karcie kredytowej (nie obci\u0105\u017cana)</li> </ol> <p>Wa\u017cne: Azure daje Ci 200 USD kredytu na 30 dni, nast\u0119pnie sta\u0142e bezp\u0142atne us\u0142ugi.</p>"},{"location":"Cloud/Azure/PL/#krok-2-eksploracja-bezpatnych-usug","title":"Krok 2: Eksploracja bezp\u0142atnych us\u0142ug","text":"<p>Bezp\u0142atne us\u0142ugi przydatne dla Data Analyst:</p> <ul> <li>Azure SQL Database : Bezp\u0142atnie do 32 GB (12 miesi\u0119cy)</li> <li>Azure Storage : 5 GB (12 miesi\u0119cy)</li> <li>Azure Data Factory : Bezp\u0142atnie do 5 potok\u00f3w</li> <li>Azure Functions : 1 milion wykona\u0144/miesi\u0105c (zawsze bezp\u0142atne)</li> <li>Azure Cosmos DB : 400 RU/s (zawsze bezp\u0142atne)</li> </ul>"},{"location":"Cloud/Azure/PL/#krok-3-przejscie-przez-kurs","title":"Krok 3: Przej\u015bcie przez kurs","text":"<ol> <li>Zacznij od modu\u0142u 1 (Rozpocz\u0119cie)</li> <li>Post\u0119puj zgodnie z kolejno\u015bci\u0105 modu\u0142\u00f3w</li> <li>U\u017cyj Microsoft Learn do laboratori\u00f3w</li> <li>\u0106wicz z projektami z modu\u0142u 7</li> <li>Monitoruj u\u017cycie w Azure Cost Management</li> </ol>"},{"location":"Cloud/Azure/PL/#niezbedne-usugi-azure-dla-data-analyst","title":"\ud83d\udcca Niezb\u0119dne us\u0142ugi Azure dla Data Analyst","text":"Us\u0142uga Zastosowanie Free Tier Storage Przechowywanie danych 5 GB (12 miesi\u0119cy) Data Factory ETL w chmurze 5 bezp\u0142atnych potok\u00f3w SQL Database Baza danych 32 GB (12 miesi\u0119cy) Synapse Hurtownia danych Bezp\u0142atny kredyt Databricks Big Data Bezp\u0142atny kredyt Functions Serverless 1M wykona\u0144/miesi\u0105c PowerBI Wizualizacja 1 bezp\u0142atny u\u017cytkownik"},{"location":"Cloud/Azure/PL/#zarzadzanie-kosztami","title":"\u26a0\ufe0f Zarz\u0105dzanie kosztami","text":""},{"location":"Cloud/Azure/PL/#wskazowki-aby-pozostac-bezpatnym","title":"Wskaz\u00f3wki, aby pozosta\u0107 bezp\u0142atnym","text":"<ol> <li>Monitorowanie rozlicze\u0144</li> <li>W\u0142\u0105cz alerty koszt\u00f3w</li> <li>Regularnie sprawdzaj Azure Cost Management</li> <li> <p>Zalecany limit: alert 5 USD</p> </li> <li> <p>U\u017cywanie bezp\u0142atnych us\u0142ug</p> </li> <li>Priorytetyzuj zawsze bezp\u0142atne us\u0142ugi</li> <li>U\u017cywaj bezp\u0142atnych kredyt\u00f3w m\u0105drze</li> <li> <p>Zatrzymuj nieu\u017cywane zasoby</p> </li> <li> <p>Usuwanie nieu\u017cywanych zasob\u00f3w</p> </li> <li>Zatrzymaj maszyny wirtualne</li> <li>Usu\u0144 puste konta magazynu</li> <li> <p>Regularne czyszczenie</p> </li> <li> <p>U\u017cywanie bezp\u0142atnych region\u00f3w</p> </li> <li>Niekt\u00f3re regiony oferuj\u0105 wi\u0119cej bezp\u0142atnych us\u0142ug</li> <li>Sprawd\u017a dost\u0119pno\u015b\u0107 wed\u0142ug regionu</li> </ol>"},{"location":"Cloud/Azure/PL/#bezpatne-zasoby","title":"\ud83d\udcda Bezp\u0142atne zasoby","text":""},{"location":"Cloud/Azure/PL/#oficjalne-szkolenie-microsoft","title":"Oficjalne szkolenie Microsoft","text":"<ul> <li>Microsoft Learn : https://learn.microsoft.com/pl-pl/</li> <li>Bezp\u0142atne interaktywne \u015bcie\u017cki</li> <li>Zintegrowane laboratoria Azure</li> <li>Przygotowanie do certyfikacji</li> <li> <p>Zalecane \u015bcie\u017cki:</p> <ul> <li>\"Azure Data Fundamentals\"</li> <li>\"Azure Data Factory\"</li> <li>\"Azure SQL Database\"</li> </ul> </li> <li> <p>Dokumentacja Azure : https://learn.microsoft.com/pl-pl/azure/</p> </li> <li>Kompletne przewodniki</li> <li>Samouczki krok po kroku</li> <li> <p>Przyk\u0142ady kodu</p> </li> <li> <p>Labs Azure : Bezp\u0142atne praktyczne laboratoria</p> </li> <li>Dost\u0119p przez Microsoft Learn</li> <li>Tymczasowe \u015brodowiska Azure</li> </ul>"},{"location":"Cloud/Azure/PL/#zewnetrzne-bezpatne-zasoby","title":"Zewn\u0119trzne bezp\u0142atne zasoby","text":"<ul> <li>YouTube : Oficjalny kana\u0142 Microsoft Azure</li> <li>GitHub : Przyk\u0142ady Microsoft Azure</li> <li>Blog Azure : Artyku\u0142y i samouczki</li> </ul>"},{"location":"Cloud/Azure/PL/#certyfikacje-opcjonalne","title":"\ud83c\udf93 Certyfikacje (opcjonalne)","text":""},{"location":"Cloud/Azure/PL/#az-900-azure-fundamentals","title":"AZ-900 : Azure Fundamentals","text":"<ul> <li>Koszt : ~100 USD</li> <li>Przygotowanie : Bezp\u0142atne (Microsoft Learn)</li> <li>Czas trwania : 2-3 tygodnie</li> <li>Poziom : Pocz\u0105tkuj\u0105cy</li> </ul>"},{"location":"Cloud/Azure/PL/#dp-900-azure-data-fundamentals","title":"DP-900 : Azure Data Fundamentals","text":"<ul> <li>Koszt : ~100 USD</li> <li>Przygotowanie : Bezp\u0142atne (Microsoft Learn)</li> <li>Czas trwania : 3-4 tygodnie</li> <li>Poziom : Data Analyst</li> </ul>"},{"location":"Cloud/Azure/PL/#dp-203-azure-data-engineer-associate","title":"DP-203 : Azure Data Engineer Associate","text":"<ul> <li>Koszt : ~165 USD</li> <li>Przygotowanie : Bezp\u0142atne (Microsoft Learn)</li> <li>Czas trwania : 2-3 miesi\u0105ce</li> <li>Poziom : Zaawansowany</li> </ul>"},{"location":"Cloud/Azure/PL/#integracja-z-powerbi","title":"\ud83d\udd17 Integracja z PowerBI","text":"<p>Azure doskonale integruje si\u0119 z PowerBI:</p> <ul> <li>Azure SQL Database \u2192 PowerBI (bezpo\u015brednie po\u0142\u0105czenie)</li> <li>Azure Data Factory \u2192 PowerBI (potok danych)</li> <li>Azure Synapse \u2192 PowerBI (hurtownia danych)</li> <li>Azure Storage \u2192 PowerBI (pliki)</li> </ul> <p>Ta integracja jest g\u0142\u00f3wn\u0105 zalet\u0105 dla Data Analyst u\u017cywaj\u0105cych PowerBI.</p>"},{"location":"Cloud/Azure/PL/#konwencje","title":"\ud83d\udcdd Konwencje","text":"<ul> <li>Wszystkie przyk\u0142ady u\u017cywaj\u0105 Free Tier</li> <li>Koszty wskazane, je\u015bli przekroczone</li> <li>Polecenia testowane na portalu Azure</li> <li>Czasy mog\u0105 si\u0119 r\u00f3\u017cni\u0107 w zale\u017cno\u015bci od regionu</li> </ul>"},{"location":"Cloud/Azure/PL/#wkad","title":"\ud83e\udd1d Wk\u0142ad","text":"<p>Ten kurs jest zaprojektowany tak, aby by\u0142 rozwijany. Nie wahaj si\u0119 proponowa\u0107 ulepsze\u0144 lub dodatkowych przypadk\u00f3w u\u017cycia.</p>"},{"location":"Cloud/Azure/PL/#dodatkowe-zasoby","title":"\ud83d\udcda Dodatkowe zasoby","text":"<ul> <li>Bezp\u0142atne konto Azure</li> <li>Microsoft Learn</li> <li>Dokumentacja Azure</li> <li>Kalkulator cen Azure</li> </ul>"},{"location":"Cloud/Azure/PL/01-getting-started/","title":"1. Rozpocz\u0119cie z Azure","text":""},{"location":"Cloud/Azure/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Utworzenie bezp\u0142atnego konta Azure</li> <li>Zrozumienie bezp\u0142atnych kredyt\u00f3w Azure</li> <li>Nawigacja w portalu Azure</li> <li>Konfiguracja podstawowego bezpiecze\u0144stwa (Azure AD)</li> <li>Monitorowanie koszt\u00f3w</li> </ul>"},{"location":"Cloud/Azure/PL/01-getting-started/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Utworzenie bezp\u0142atnego konta Azure</li> <li>Zrozumienie bezp\u0142atnych kredyt\u00f3w</li> <li>Nawigacja w portalu Azure</li> <li>Konfiguracja Azure AD (bezpiecze\u0144stwo)</li> <li>Monitorowanie koszt\u00f3w</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#utworzenie-bezpatnego-konta-azure","title":"Utworzenie bezp\u0142atnego konta Azure","text":""},{"location":"Cloud/Azure/PL/01-getting-started/#krok-1-rejestracja","title":"Krok 1: Rejestracja","text":"<ol> <li>Przej\u015b\u0107 na stron\u0119 Azure</li> <li>URL: https://azure.microsoft.com/pl-pl/free/</li> <li> <p>Klikn\u0105\u0107 \"Rozpocznij bezp\u0142atnie\"</p> </li> <li> <p>Zalogowa\u0107 si\u0119 kontem Microsoft</p> </li> <li>U\u017cy\u0107 istniej\u0105cego konta Microsoft</li> <li> <p>Lub utworzy\u0107 nowe konto Microsoft</p> </li> <li> <p>Weryfikacja to\u017csamo\u015bci</p> </li> <li>Kod otrzymany SMS lub email</li> <li> <p>Wprowadzi\u0107 kod weryfikacyjny</p> </li> <li> <p>Informacje osobiste</p> </li> <li>Nazwisko</li> <li>Imi\u0119</li> <li>Numer telefonu</li> <li> <p>Kraj</p> </li> <li> <p>Weryfikacja telefoniczna</p> </li> <li>Automatyczne po\u0142\u0105czenie lub SMS</li> <li> <p>Wprowadzi\u0107 kod weryfikacyjny</p> </li> <li> <p>Metoda p\u0142atno\u015bci</p> </li> <li>Wa\u017cne: Wymagana karta kredytowa, ale nie obci\u0105\u017cana</li> <li>Azure daje Ci 200 USD kredytu na 30 dni</li> <li>Po 30 dniach: sta\u0142e bezp\u0142atne us\u0142ugi</li> <li> <p>Mo\u017cesz usun\u0105\u0107 kart\u0119 p\u00f3\u017aniej (nie zalecane)</p> </li> <li> <p>Ko\u0144cowa weryfikacja to\u017csamo\u015bci</p> </li> <li>Weryfikacja SMS lub po\u0142\u0105czeniem</li> <li>Potwierdzenie konta</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#krok-2-potwierdzenie","title":"Krok 2: Potwierdzenie","text":"<ul> <li>Email potwierdzaj\u0105cy otrzymany</li> <li>Konto Azure aktywne natychmiast</li> <li>Dost\u0119p do portalu Azure</li> <li>200 USD kredytu dost\u0119pne na 30 dni</li> </ul> <p>\u26a0\ufe0f Wa\u017cne: Nie tworzy\u0107 wielu kont z t\u0105 sam\u0105 kart\u0105 kredytow\u0105 (ryzyko zawieszenia).</p>"},{"location":"Cloud/Azure/PL/01-getting-started/#zrozumienie-bezpatnych-kredytow","title":"Zrozumienie bezp\u0142atnych kredyt\u00f3w","text":""},{"location":"Cloud/Azure/PL/01-getting-started/#oferta-azure-bezpatna","title":"Oferta Azure bezp\u0142atna","text":"<p>Azure oferuje 3 typy bezp\u0142atnych us\u0142ug:</p>"},{"location":"Cloud/Azure/PL/01-getting-started/#1-kredyt-200-usd-30-dni","title":"1. Kredyt 200 USD (30 dni)","text":"<p>Co mo\u017cesz zrobi\u0107: - Testowa\u0107 dowoln\u0105 us\u0142ug\u0119 Azure - Tworzy\u0107 maszyny wirtualne - U\u017cywa\u0107 p\u0142atnych us\u0142ug - Eksperymentowa\u0107 swobodnie</p> <p>Warunki: - Wa\u017cne 30 dni po rejestracji - Je\u015bli kredyt wyczerpany przed 30 dniami: us\u0142ugi zatrzymane - Po 30 dniach: przej\u015bcie na sta\u0142e bezp\u0142atne us\u0142ugi</p>"},{"location":"Cloud/Azure/PL/01-getting-started/#2-bezpatne-usugi-przez-12-miesiecy","title":"2. Bezp\u0142atne us\u0142ugi przez 12 miesi\u0119cy","text":"<p>Us\u0142ugi przydatne dla Data Analyst:</p> <ul> <li>Azure SQL Database: Bezp\u0142atnie do 32 GB (12 miesi\u0119cy)</li> <li>Azure Storage: 5 GB (12 miesi\u0119cy)</li> <li>Azure App Service: 60 minut/dzie\u0144 (12 miesi\u0119cy)</li> <li>Azure Functions: 1 milion wykona\u0144/miesi\u0105c (zawsze bezp\u0142atne)</li> </ul> <p>Warunki: - Bezp\u0142atne przez 12 miesi\u0119cy po rejestracji - Limity miesi\u0119czne - Poza limitami: normalne rozliczanie</p>"},{"location":"Cloud/Azure/PL/01-getting-started/#3-zawsze-bezpatne-usugi","title":"3. Zawsze bezp\u0142atne us\u0142ugi","text":"<p>Us\u0142ugi przydatne dla Data Analyst:</p> <ul> <li>Azure Functions: 1 milion wykona\u0144/miesi\u0105c (zawsze bezp\u0142atne)</li> <li>Azure Cosmos DB: 400 RU/s (zawsze bezp\u0142atne)</li> <li>Azure Active Directory: 50 000 obiekt\u00f3w (zawsze bezp\u0142atne)</li> <li>Azure DevOps: 5 u\u017cytkownik\u00f3w (zawsze bezp\u0142atne)</li> </ul> <p>Warunki: - Bezp\u0142atne w niesko\u0144czono\u015b\u0107 - Limity miesi\u0119czne - Poza limitami: rozliczanie poza limitem</p>"},{"location":"Cloud/Azure/PL/01-getting-started/#sprawdzenie-kredytow","title":"Sprawdzenie kredyt\u00f3w","text":"<ol> <li>Przej\u015b\u0107 do portalu Azure</li> <li>\"Zarz\u0105dzanie kosztami + rozliczenia\"</li> <li>Zobaczy\u0107 pozosta\u0142e kredyty</li> <li>Zobaczy\u0107 u\u017cycie wed\u0142ug us\u0142ugi</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#nawigacja-w-portalu-azure","title":"Nawigacja w portalu Azure","text":""},{"location":"Cloud/Azure/PL/01-getting-started/#interfejs-gowny","title":"Interfejs g\u0142\u00f3wny","text":"<p>Kluczowe elementy:</p> <ol> <li>Pasek wyszukiwania (u g\u00f3ry)</li> <li>Szybkie wyszukiwanie us\u0142ug</li> <li> <p>Przyk\u0142ad: wpisa\u0107 \"SQL\" aby znale\u017a\u0107 SQL Database</p> </li> <li> <p>Menu Azure (ikona \u2630 u g\u00f3ry po lewej)</p> </li> <li>Wszystkie us\u0142ugi Azure</li> <li>Zorganizowane wed\u0142ug kategorii</li> <li> <p>Konfigurowalne ulubione</p> </li> <li> <p>Powiadomienia (u g\u00f3ry po prawej)</p> </li> <li>Alerty i powiadomienia</li> <li> <p>Status wdro\u017ce\u0144</p> </li> <li> <p>Ustawienia (u g\u00f3ry po prawej)</p> </li> <li>Ustawienia konta</li> <li>Motyw (jasny/ciemny)</li> <li> <p>J\u0119zyk</p> </li> <li> <p>Cloud Shell (ikona &gt;_ u g\u00f3ry)</p> </li> <li>Terminal w przegl\u0105darce</li> <li>PowerShell lub Bash</li> <li>Bardzo przydatne do polece\u0144</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#niezbedne-usugi-dla-data-analyst","title":"Niezb\u0119dne us\u0142ugi dla Data Analyst","text":"<p>W menu Azure, szuka\u0107:</p> <ul> <li>Konta magazynu: Przechowywanie danych</li> <li>Data Factory: ETL w chmurze</li> <li>Bazy danych SQL: Bazy danych SQL</li> <li>Synapse Analytics: Hurtownia danych</li> <li>Databricks: Analiza Big Data</li> <li>Funkcje: Przetwarzanie serverless</li> </ul>"},{"location":"Cloud/Azure/PL/01-getting-started/#pierwsze-poaczenie","title":"Pierwsze po\u0142\u0105czenie","text":"<ol> <li>Zalogowa\u0107 si\u0119: https://portal.azure.com/</li> <li>Eksplorowa\u0107 pulpit nawigacyjny</li> <li>Klikn\u0105\u0107 \"Wszystkie us\u0142ugi\" aby zobaczy\u0107 wszystkie us\u0142ugi</li> <li>U\u017cy\u0107 paska wyszukiwania aby znale\u017a\u0107 us\u0142ug\u0119</li> <li>Przypi\u0105\u0107 cz\u0119ste us\u0142ugi do pulpitu nawigacyjnego</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#konfiguracja-azure-ad-bezpieczenstwo","title":"Konfiguracja Azure AD (bezpiecze\u0144stwo)","text":""},{"location":"Cloud/Azure/PL/01-getting-started/#czym-jest-azure-ad","title":"Czym jest Azure AD?","text":"<p>Azure AD (Azure Active Directory) = Zarz\u0105dzanie to\u017csamo\u015bci\u0105 i dost\u0119pem</p> <ul> <li>Zarz\u0105dzanie u\u017cytkownikami</li> <li>Zarz\u0105dzanie uprawnieniami</li> <li>Zabezpieczanie dost\u0119pu do us\u0142ug</li> <li>Uwierzytelnianie wielosk\u0142adnikowe (MFA)</li> </ul>"},{"location":"Cloud/Azure/PL/01-getting-started/#najlepsze-praktyki-bezpieczenstwa","title":"Najlepsze praktyki bezpiecze\u0144stwa","text":""},{"location":"Cloud/Azure/PL/01-getting-started/#1-waczenie-uwierzytelniania-wieloskadnikowego-mfa","title":"1. W\u0142\u0105czenie uwierzytelniania wielosk\u0142adnikowego (MFA)","text":"<p>Dla konta administratora:</p> <ol> <li>Przej\u015b\u0107 do Azure AD</li> <li>\"U\u017cytkownicy\" \u2192 Wybra\u0107 swoje konto</li> <li>\"Uwierzytelnianie wielosk\u0142adnikowe\"</li> <li>Klikn\u0105\u0107 \"W\u0142\u0105cz\"</li> <li>Post\u0119powa\u0107 zgodnie z instrukcjami</li> </ol> <p>\u26a0\ufe0f Wa\u017cne: Zawsze w\u0142\u0105cza\u0107 MFA dla kont administrator\u00f3w.</p>"},{"location":"Cloud/Azure/PL/01-getting-started/#2-utworzenie-uzytkownikow-azure-ad-zalecane","title":"2. Utworzenie u\u017cytkownik\u00f3w Azure AD (zalecane)","text":"<p>Do pracy w zespole:</p> <ol> <li>Przej\u015b\u0107 do Azure AD</li> <li>\"U\u017cytkownicy\" \u2192 \"Nowy u\u017cytkownik\"</li> <li>Nazwa u\u017cytkownika: <code>data-analyst@twojadomena.onmicrosoft.com</code></li> <li>Has\u0142o tymczasowe</li> <li>Role: \"U\u017cytkownik\" (domy\u015blnie)</li> <li>Utworzy\u0107 u\u017cytkownika</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#3-role-azure-rbac","title":"3. Role Azure (RBAC)","text":"<p>Role przydatne dla Data Analyst:</p> <ul> <li>Wsp\u00f3\u0142autor: Mo\u017ce tworzy\u0107 i zarz\u0105dza\u0107 zasobami</li> <li>Czytelnik: Mo\u017ce tylko czyta\u0107</li> <li>Wsp\u00f3\u0142autor konta magazynu: Dost\u0119p do kont magazynu</li> <li>Wsp\u00f3\u0142autor SQL DB: Dost\u0119p do baz SQL</li> </ul> <p>Przypisa\u0107 rol\u0119:</p> <ol> <li>Przej\u015b\u0107 do zasobu (np. Konto magazynu)</li> <li>\"Kontrola dost\u0119pu (IAM)\"</li> <li>\"Dodaj\" \u2192 \"Dodaj przypisanie roli\"</li> <li>Wybra\u0107 rol\u0119</li> <li>Wybra\u0107 u\u017cytkownika</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#zalecane-zasady-bezpieczenstwa","title":"Zalecane zasady bezpiecze\u0144stwa","text":"<ol> <li>Silne has\u0142a</li> <li>Minimum 12 znak\u00f3w</li> <li> <p>Wymagana z\u0142o\u017cono\u015b\u0107</p> </li> <li> <p>Wyga\u015bni\u0119cie has\u0142a</p> </li> <li> <p>90 dni (zalecane)</p> </li> <li> <p>Blokada konta</p> </li> <li>Po 5 nieudanych pr\u00f3bach</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#monitorowanie-kosztow","title":"Monitorowanie koszt\u00f3w","text":""},{"location":"Cloud/Azure/PL/01-getting-started/#waczenie-alertow-kosztow","title":"W\u0142\u0105czenie alert\u00f3w koszt\u00f3w","text":"<p>Krok 1: Konfiguracja alert\u00f3w</p> <ol> <li>Przej\u015b\u0107 do \"Zarz\u0105dzanie kosztami + rozliczenia\"</li> <li>\"Alerty koszt\u00f3w\"</li> <li>\"Nowy alert koszt\u00f3w\"</li> <li>Pr\u00f3g: 5 USD (zalecane)</li> <li>Powiadomienie email</li> </ol> <p>Wynik: Email otrzymany, je\u015bli koszty przekrocz\u0105 5 USD.</p>"},{"location":"Cloud/Azure/PL/01-getting-started/#sprawdzenie-uzycia-kredytow","title":"Sprawdzenie u\u017cycia kredyt\u00f3w","text":"<ol> <li>\"Zarz\u0105dzanie kosztami + rozliczenia\"</li> <li>\"Kredyty Azure\"</li> <li>Zobaczy\u0107 pozosta\u0142e kredyty</li> <li>Zobaczy\u0107 u\u017cycie wed\u0142ug us\u0142ugi</li> <li>Zobaczy\u0107 dat\u0119 wyga\u015bni\u0119cia (30 dni)</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#azure-cost-management","title":"Azure Cost Management","text":"<ol> <li>\"Zarz\u0105dzanie kosztami + rozliczenia\" \u2192 \"Zarz\u0105dzanie kosztami\"</li> <li>Zobaczy\u0107 koszty wed\u0142ug us\u0142ugi</li> <li>Filtrowa\u0107 wed\u0142ug okresu</li> <li>Eksportowa\u0107 raporty</li> <li>Tworzy\u0107 bud\u017cety</li> </ol> <p>\u26a0\ufe0f Wa\u017cne: Sprawdza\u0107 regularnie (zalecane cotygodniowo).</p>"},{"location":"Cloud/Azure/PL/01-getting-started/#wskazowki-aby-pozostac-bezpatnym","title":"Wskaz\u00f3wki, aby pozosta\u0107 bezp\u0142atnym","text":"<ol> <li>Usuwanie nieu\u017cywanych zasob\u00f3w</li> <li>Zatrzyma\u0107 nieu\u017cywane maszyny wirtualne</li> <li>Usun\u0105\u0107 puste konta magazynu</li> <li> <p>Wyczy\u015bci\u0107 grupy zasob\u00f3w</p> </li> <li> <p>U\u017cywanie bezp\u0142atnych us\u0142ug</p> </li> <li>Priorytetyzowa\u0107 zawsze bezp\u0142atne us\u0142ugi</li> <li>U\u017cywa\u0107 kredyt\u00f3w m\u0105drze</li> <li> <p>Zatrzymywa\u0107 nieu\u017cywane us\u0142ugi</p> </li> <li> <p>Tworzenie bud\u017cet\u00f3w</p> </li> <li>\"Zarz\u0105dzanie kosztami\" \u2192 \"Bud\u017cety\"</li> <li>Utworzy\u0107 bud\u017cet 5 USD</li> <li> <p>Automatyczne alerty</p> </li> <li> <p>Zatrzymywanie nieu\u017cywanych us\u0142ug</p> </li> <li>Maszyny wirtualne: zatrzyma\u0107, gdy nieu\u017cywane</li> <li>Bazy danych: zatrzyma\u0107 lub wstrzyma\u0107</li> <li>Konta magazynu: usun\u0105\u0107, je\u015bli puste</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#grupy-zasobow","title":"Grupy zasob\u00f3w","text":"<p>Organizowanie zasob\u00f3w:</p> <ol> <li>Utworzy\u0107 grup\u0119 zasob\u00f3w: <code>rg-data-analyst-training</code></li> <li>Wszystkie zasoby szkoleniowe w tej grupie</li> <li>U\u0142atwia jednorazowe usuni\u0119cie</li> <li>U\u0142atwia zarz\u0105dzanie kosztami</li> </ol> <p>Utworzy\u0107 grup\u0119 zasob\u00f3w:</p> <ol> <li>\"Grupy zasob\u00f3w\" \u2192 \"Dodaj\"</li> <li>Nazwa: <code>rg-data-analyst-training</code></li> <li>Region: Wybra\u0107 najbli\u017cszy region</li> <li>Utworzy\u0107</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Bezp\u0142atne konto Azure: 200 USD kredytu (30 dni) + bezp\u0142atne us\u0142ugi</li> <li>Bezp\u0142atne kredyty: 3 typy (200 USD, 12 miesi\u0119cy, zawsze bezp\u0142atne)</li> <li>Bezpiecze\u0144stwo Azure AD: W\u0142\u0105czy\u0107 MFA, tworzy\u0107 u\u017cytkownik\u00f3w</li> <li>Monitorowanie: Alerty koszt\u00f3w niezb\u0119dne</li> <li>Pozosta\u0107 bezp\u0142atnym: Usuwa\u0107 nieu\u017cywane zasoby, u\u017cywa\u0107 grup zasob\u00f3w</li> </ol>"},{"location":"Cloud/Azure/PL/01-getting-started/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 2. Azure Storage - Przechowywanie danych, aby nauczy\u0107 si\u0119 przechowywa\u0107 dane na Azure.</p>"},{"location":"Cloud/Azure/PL/02-storage/","title":"2. Azure Storage - Przechowywanie danych","text":""},{"location":"Cloud/Azure/PL/02-storage/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Azure Storage i jego u\u017cycie</li> <li>Tworzy\u0107 konta przechowywania</li> <li>U\u017cywa\u0107 Blob Storage i Data Lake Storage</li> <li>Przesy\u0142a\u0107 i zarz\u0105dza\u0107 plikami</li> <li>Organizowa\u0107 dane</li> </ul>"},{"location":"Cloud/Azure/PL/02-storage/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Azure Storage</li> <li>Utworzy\u0107 konto przechowywania</li> <li>Blob Storage</li> <li>Data Lake Storage Gen2</li> <li>Przesy\u0142a\u0107 i zarz\u0105dza\u0107 plikami</li> <li>Integracja z innymi us\u0142ugami</li> </ol>"},{"location":"Cloud/Azure/PL/02-storage/#wprowadzenie-do-azure-storage","title":"Wprowadzenie do Azure Storage","text":""},{"location":"Cloud/Azure/PL/02-storage/#czym-jest-azure-storage","title":"Czym jest Azure Storage?","text":"<p>Azure Storage = Zarz\u0105dzana us\u0142uga przechowywania w chmurze</p> <ul> <li>Nieograniczone przechowywanie : Skalowalne wed\u0142ug potrzeb</li> <li>Wysoka dost\u0119pno\u015b\u0107 : 99.99% dost\u0119pno\u015bci</li> <li>Bezpieczne : Szyfrowanie domy\u015blnie</li> <li>Integracja : Ze wszystkimi us\u0142ugami Azure</li> </ul>"},{"location":"Cloud/Azure/PL/02-storage/#typy-przechowywania","title":"Typy przechowywania","text":"<ol> <li>Blob Storage : Pliki (CSV, JSON, Parquet, itp.)</li> <li>Data Lake Storage Gen2 : Data Lake z hierarchicznym systemem plik\u00f3w</li> <li>File Storage : Udzia\u0142y plik\u00f3w</li> <li>Queue Storage : Kolejki</li> <li>Table Storage : Przechowywanie NoSQL</li> </ol>"},{"location":"Cloud/Azure/PL/02-storage/#azure-storage-free-tier","title":"Azure Storage Free Tier","text":"<p>Darmowe 12 miesi\u0119cy : - 5 GB przechowywania Blob - 5 GB przechowywania File - 5 GB przechowywania Table - 5 GB przechowywania Queue</p> <p>Darmowe na zawsze : - 200 GB transferu danych wychodz\u0105cych/miesi\u0105c</p> <p>\u26a0\ufe0f Wa\u017cne : Poza tymi limitami, normalne rozliczanie.</p>"},{"location":"Cloud/Azure/PL/02-storage/#utworzyc-konto-przechowywania","title":"Utworzy\u0107 konto przechowywania","text":""},{"location":"Cloud/Azure/PL/02-storage/#krok-1-dostep-do-azure-storage","title":"Krok 1 : Dost\u0119p do Azure Storage","text":"<ol> <li>Portal Azure \u2192 Szuka\u0107 \"Storage accounts\"</li> <li>Klikn\u0105\u0107 \"Storage accounts\"</li> <li>Klikn\u0105\u0107 \"Create\"</li> </ol>"},{"location":"Cloud/Azure/PL/02-storage/#krok-2-podstawowa-konfiguracja","title":"Krok 2 : Podstawowa konfiguracja","text":"<p>Podstawowe informacje : - Subscription : Wybra\u0107 subskrypcj\u0119 - Resource group : Utworzy\u0107 lub u\u017cy\u0107 istniej\u0105cego - Storage account name : Nazwa unikalna globalnie (np. <code>mydataanalyststorage</code>) - Region : Wybra\u0107 najbli\u017cszy region (np. <code>France Central</code>)</p> <p>Opcje wydajno\u015bci : - Performance : Standard (zalecane do rozpocz\u0119cia) - Redundancy : LRS (Locally Redundant Storage) - najta\u0144sze</p>"},{"location":"Cloud/Azure/PL/02-storage/#krok-3-opcje-zaawansowane","title":"Krok 3 : Opcje zaawansowane","text":"<p>Bezpiecze\u0144stwo : - Secure transfer required : \u2705 W\u0142\u0105czy\u0107 (zalecane) - Allow Blob public access : \u274c Wy\u0142\u0105czy\u0107 (bezpiecze\u0144stwo)</p> <p>Data Lake Storage Gen2 : - Hierarchical namespace : \u2705 W\u0142\u0105czy\u0107 je\u015bli potrzeba Data Lake</p>"},{"location":"Cloud/Azure/PL/02-storage/#krok-4-utworzyc-konto","title":"Krok 4 : Utworzy\u0107 konto","text":"<ol> <li>Klikn\u0105\u0107 \"Review + create\"</li> <li>Sprawdzi\u0107 konfiguracj\u0119</li> <li>Klikn\u0105\u0107 \"Create\"</li> <li>Czeka\u0107 na utworzenie (1-2 minuty)</li> </ol> <p>\u26a0\ufe0f Wa\u017cne : Zanotowa\u0107 nazw\u0119 konta przechowywania.</p>"},{"location":"Cloud/Azure/PL/02-storage/#blob-storage","title":"Blob Storage","text":""},{"location":"Cloud/Azure/PL/02-storage/#czym-jest-blob-storage","title":"Czym jest Blob Storage?","text":"<p>Blob Storage = Przechowywanie obiekt\u00f3w dla plik\u00f3w</p> <ul> <li>Containers : Organizuj\u0105 pliki (jak foldery)</li> <li>Blobs : Pojedyncze pliki</li> <li>Typy : Block blobs, Page blobs, Append blobs</li> </ul>"},{"location":"Cloud/Azure/PL/02-storage/#utworzyc-container","title":"Utworzy\u0107 container","text":"<p>Przez portal Azure :</p> <ol> <li>Storage account \u2192 \"Containers\"</li> <li>Klikn\u0105\u0107 \"+ Container\"</li> <li>Nazwa : <code>raw-data</code> (lub inna)</li> <li>Poziom dost\u0119pu publicznego : Private (zalecane)</li> <li>Klikn\u0105\u0107 \"Create\"</li> </ol> <p>Przez Azure CLI :</p> <pre><code>az storage container create \\\n  --name raw-data \\\n  --account-name mydataanalyststorage \\\n  --auth-mode login\n</code></pre> <p>Przez Python :</p> <pre><code>from azure.storage.blob import BlobServiceClient\n\n# Po\u0142\u0105czenie\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=...\"\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\n# Utworzy\u0107 container\ncontainer_client = blob_service_client.create_container(\"raw-data\")\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#typy-blobow","title":"Typy blob\u00f3w","text":"<p>Block Blobs : - Pliki (CSV, JSON, Parquet, obrazy, itp.) - Do 4.75 TB na blob - Zalecane dla wi\u0119kszo\u015bci przypadk\u00f3w</p> <p>Page Blobs : - Dyski wirtualne - Do 8 TB</p> <p>Append Blobs : - Logi - Dane tylko do dodawania</p>"},{"location":"Cloud/Azure/PL/02-storage/#data-lake-storage-gen2","title":"Data Lake Storage Gen2","text":""},{"location":"Cloud/Azure/PL/02-storage/#czym-jest-data-lake-storage-gen2","title":"Czym jest Data Lake Storage Gen2?","text":"<p>Data Lake Storage Gen2 = Blob Storage + hierarchiczny system plik\u00f3w</p> <ul> <li>Zgodne z Blob Storage : U\u017cywa tych samych API</li> <li>System plik\u00f3w : Organizacja hierarchiczna</li> <li>Zoptymalizowane Big Data : Dla analytics i ML</li> <li>Integracja : Z Azure Synapse, Databricks, itp.</li> </ul>"},{"location":"Cloud/Azure/PL/02-storage/#waczyc-data-lake-storage-gen2","title":"W\u0142\u0105czy\u0107 Data Lake Storage Gen2","text":"<p>Podczas tworzenia konta : 1. W \"Advanced\" \u2192 W\u0142\u0105czy\u0107 \"Hierarchical namespace\" 2. Utworzy\u0107 konto</p> <p>\u26a0\ufe0f Wa\u017cne : Nie mo\u017cna w\u0142\u0105czy\u0107 po utworzeniu.</p>"},{"location":"Cloud/Azure/PL/02-storage/#struktura-data-lake","title":"Struktura Data Lake","text":"<pre><code>data-lake/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2502   \u251c\u2500\u2500 01/\n\u2502   \u2502   \u2514\u2500\u2500 02/\n\u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 2024/\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#tworzyc-pliki-i-foldery","title":"Tworzy\u0107 pliki i foldery","text":"<p>Przez portal Azure :</p> <ol> <li>Storage account \u2192 \"Data Lake\"</li> <li>Nawigowa\u0107 w strukturze</li> <li>Przesy\u0142a\u0107 pliki</li> <li>Tworzy\u0107 foldery</li> </ol> <p>Przez Python :</p> <pre><code>from azure.storage.filedatalake import DataLakeServiceClient\n\n# Po\u0142\u0105czenie\naccount_name = \"mydataanalyststorage\"\naccount_key = \"...\"\ndatalake_service_client = DataLakeServiceClient(\n    account_url=f\"https://{account_name}.dfs.core.windows.net\",\n    credential=account_key\n)\n\n# Utworzy\u0107 system plik\u00f3w\nfile_system_client = datalake_service_client.create_file_system(\"data-lake\")\n\n# Utworzy\u0107 katalog\ndirectory_client = file_system_client.create_directory(\"raw/2024\")\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#przesyac-i-zarzadzac-plikami","title":"Przesy\u0142a\u0107 i zarz\u0105dza\u0107 plikami","text":""},{"location":"Cloud/Azure/PL/02-storage/#przesac-plik","title":"Przes\u0142a\u0107 plik","text":"<p>Przez portal Azure :</p> <ol> <li>Container \u2192 \"Upload\"</li> <li>Wybra\u0107 plik</li> <li>Klikn\u0105\u0107 \"Upload\"</li> </ol> <p>Przez Azure CLI :</p> <pre><code>az storage blob upload \\\n  --account-name mydataanalyststorage \\\n  --container-name raw-data \\\n  --name data.csv \\\n  --file ./local-data.csv \\\n  --auth-mode login\n</code></pre> <p>Przez Python :</p> <pre><code>from azure.storage.blob import BlobServiceClient\n\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\ncontainer_client = blob_service_client.get_container_client(\"raw-data\")\n\n# Przes\u0142a\u0107 plik\nwith open(\"local-data.csv\", \"rb\") as data:\n    container_client.upload_blob(name=\"data.csv\", data=data)\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#pobrac-plik","title":"Pobra\u0107 plik","text":"<p>Przez Python :</p> <pre><code># Pobra\u0107 blob\nblob_client = container_client.get_blob_client(\"data.csv\")\nwith open(\"downloaded-data.csv\", \"wb\") as download_file:\n    download_file.write(blob_client.download_blob().readall())\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#listowac-pliki","title":"Listowa\u0107 pliki","text":"<p>Przez Python :</p> <pre><code># Listowa\u0107 wszystkie bloby w kontenerze\nblob_list = container_client.list_blobs()\nfor blob in blob_list:\n    print(f\"Name: {blob.name}, Size: {blob.size}\")\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#usunac-plik","title":"Usun\u0105\u0107 plik","text":"<p>Przez Python :</p> <pre><code># Usun\u0105\u0107 blob\nblob_client = container_client.get_blob_client(\"data.csv\")\nblob_client.delete_blob()\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#integracja-z-innymi-usugami","title":"Integracja z innymi us\u0142ugami","text":""},{"location":"Cloud/Azure/PL/02-storage/#azure-storage-data-factory","title":"Azure Storage + Data Factory","text":"<p>U\u017cycie : - \u0179r\u00f3d\u0142o danych dla pipeline'\u00f3w ETL - Miejsce docelowe dla przekszta\u0142conych danych</p> <p>Przyk\u0142ad : <pre><code>{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"connectionString\": \"...\",\n    \"container\": \"raw-data\"\n  }\n}\n</code></pre></p>"},{"location":"Cloud/Azure/PL/02-storage/#azure-storage-azure-sql-database","title":"Azure Storage + Azure SQL Database","text":"<p>U\u017cycie : - Importowa\u0107 dane z Blob Storage - Eksportowa\u0107 dane do Blob Storage</p> <p>Przyk\u0142ad SQL : <pre><code>-- Importowa\u0107 z Blob Storage\nBULK INSERT my_table\nFROM 'https://mystorageaccount.blob.core.windows.net/raw-data/data.csv'\nWITH (\n    FORMAT = 'CSV',\n    FIRSTROW = 2\n);\n</code></pre></p>"},{"location":"Cloud/Azure/PL/02-storage/#azure-storage-powerbi","title":"Azure Storage + PowerBI","text":"<p>U\u017cycie : - Po\u0142\u0105czy\u0107 PowerBI z Blob Storage - Analizowa\u0107 pliki bezpo\u015brednio</p> <p>Konfiguracja : 1. PowerBI \u2192 \"Get Data\" 2. \"Azure Blob Storage\" 3. Wprowadzi\u0107 URL kontenera 4. Wybra\u0107 pliki</p>"},{"location":"Cloud/Azure/PL/02-storage/#azure-storage-azure-functions","title":"Azure Storage + Azure Functions","text":"<p>U\u017cycie : - Wyzwala\u0107 Functions przy przes\u0142aniu - Automatycznie przetwarza\u0107 pliki</p> <p>Konfiguracja : 1. Function \u2192 \"Add trigger\" 2. \"Azure Blob Storage trigger\" 3. Skonfigurowa\u0107 kontener i \u015bcie\u017ck\u0119</p>"},{"location":"Cloud/Azure/PL/02-storage/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/Azure/PL/02-storage/#organizacja","title":"Organizacja","text":"<ol> <li>U\u017cywa\u0107 kontener\u00f3w aby organizowa\u0107 wed\u0142ug projektu</li> <li>Nazywa\u0107 jasno pliki i kontenery</li> <li>Organizowa\u0107 wed\u0142ug daty : <code>raw/2024/01/data.csv</code></li> <li>Rozdziela\u0107 wed\u0142ug typu : <code>raw/</code>, <code>processed/</code>, <code>analytics/</code></li> </ol>"},{"location":"Cloud/Azure/PL/02-storage/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>U\u017cywa\u0107 losowych nazw dla blob\u00f3w (unika\u0107 sekwencji)</li> <li>W\u0142\u0105czy\u0107 CDN je\u015bli potrzeba globalnej dystrybucji (p\u0142atne)</li> <li>U\u017cywa\u0107 blob\u00f3w blokowych dla wi\u0119kszo\u015bci przypadk\u00f3w</li> <li>Partycjonowa\u0107 dane aby poprawi\u0107 wydajno\u015b\u0107</li> </ol>"},{"location":"Cloud/Azure/PL/02-storage/#koszty","title":"Koszty","text":"<ol> <li>Monitorowa\u0107 u\u017cycie w Azure Cost Management</li> <li>Usuwa\u0107 niepotrzebne pliki</li> <li>U\u017cywa\u0107 odpowiednich klas przechowywania</li> <li>Konfigurowa\u0107 regu\u0142y cyklu \u017cycia aby automatyzowa\u0107</li> </ol>"},{"location":"Cloud/Azure/PL/02-storage/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":"<ol> <li>Nigdy nie udost\u0119pnia\u0107 publicznie kontener\u00f3w (opr\u00f3cz konkretnej potrzeby)</li> <li>U\u017cywa\u0107 SAS (Shared Access Signature) dla tymczasowego dost\u0119pu</li> <li>W\u0142\u0105czy\u0107 szyfrowanie domy\u015blnie</li> <li>U\u017cywa\u0107 Azure AD do uwierzytelniania</li> </ol>"},{"location":"Cloud/Azure/PL/02-storage/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/Azure/PL/02-storage/#przykad-1-przesac-plik-csv","title":"Przyk\u0142ad 1 : Przes\u0142a\u0107 plik CSV","text":"<pre><code>from azure.storage.blob import BlobServiceClient\nimport pandas as pd\n\n# Po\u0142\u0105czenie\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=...\"\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\ncontainer_client = blob_service_client.get_container_client(\"raw-data\")\n\n# Czyta\u0107 lokalny plik\ndf = pd.read_csv(\"local-data.csv\")\n\n# Przes\u0142a\u0107 do Azure Storage\nwith open(\"local-data.csv\", \"rb\") as data:\n    container_client.upload_blob(name=\"2024/01/data.csv\", data=data)\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#przykad-2-pobrac-i-przetworzyc","title":"Przyk\u0142ad 2 : Pobra\u0107 i przetworzy\u0107","text":"<pre><code># Pobra\u0107 z Azure Storage\nblob_client = container_client.get_blob_client(\"2024/01/data.csv\")\nwith open(\"downloaded-data.csv\", \"wb\") as download_file:\n    download_file.write(blob_client.download_blob().readall())\n\n# Przetworzy\u0107\ndf = pd.read_csv(\"downloaded-data.csv\")\n# ... przetwarzanie ...\n\n# Przes\u0142a\u0107 wynik\ndf.to_csv(\"processed-data.csv\", index=False)\nwith open(\"processed-data.csv\", \"rb\") as data:\n    container_client.upload_blob(name=\"processed/2024/01/data.csv\", data=data)\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#przykad-3-listowac-i-filtrowac","title":"Przyk\u0142ad 3 : Listowa\u0107 i filtrowa\u0107","text":"<pre><code># Listowa\u0107 wszystkie pliki w prefiksie\nblob_list = container_client.list_blobs(name_starts_with=\"2024/01/\")\nfor blob in blob_list:\n    print(f\"File: {blob.name}, Size: {blob.size} bytes, Modified: {blob.last_modified}\")\n</code></pre>"},{"location":"Cloud/Azure/PL/02-storage/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Azure Storage = Nieograniczone przechowywanie i wysoka dost\u0119pno\u015b\u0107</li> <li>Free Tier : 5 GB przez 12 miesi\u0119cy</li> <li>Blob Storage dla plik\u00f3w, Data Lake Gen2 dla Big Data</li> <li>Organizowa\u0107 z kontenerami i prefiksami</li> <li>Natywna integracja ze wszystkimi us\u0142ugami Azure data</li> </ol>"},{"location":"Cloud/Azure/PL/02-storage/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 3. Azure Data Factory - ETL w chmurze, aby nauczy\u0107 si\u0119 tworzy\u0107 pipeline'y ETL na Azure.</p>"},{"location":"Cloud/Azure/PL/03-data-factory/","title":"3. Azure Data Factory - ETL w chmurze","text":""},{"location":"Cloud/Azure/PL/03-data-factory/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Azure Data Factory i jego rol\u0119</li> <li>Tworzy\u0107 pipeline'y ETL</li> <li>U\u017cywa\u0107 dzia\u0142a\u0144 przekszta\u0142cania</li> <li>Integrowa\u0107 ze \u017ar\u00f3d\u0142ami danych</li> <li>Orkiestrowa\u0107 przep\u0142ywy pracy</li> </ul>"},{"location":"Cloud/Azure/PL/03-data-factory/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Data Factory</li> <li>Utworzy\u0107 Data Factory</li> <li>Utworzy\u0107 pipeline</li> <li>Dzia\u0142ania przekszta\u0142cania</li> <li>Integracja ze \u017ar\u00f3d\u0142ami danych</li> <li>Orkiestracja i harmonogramowanie</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#wprowadzenie-do-data-factory","title":"Wprowadzenie do Data Factory","text":""},{"location":"Cloud/Azure/PL/03-data-factory/#czym-jest-azure-data-factory","title":"Czym jest Azure Data Factory?","text":"<p>Azure Data Factory = Zarz\u0105dzana us\u0142uga ETL w chmurze</p> <ul> <li>ETL : Extract, Transform, Load</li> <li>Chmura : Brak infrastruktury do zarz\u0105dzania</li> <li>Zarz\u0105dzane : Microsoft zarz\u0105dza infrastruktur\u0105</li> <li>Skalowalne : Automatycznie dostosowuje si\u0119</li> </ul>"},{"location":"Cloud/Azure/PL/03-data-factory/#komponenty-data-factory","title":"Komponenty Data Factory","text":"<ol> <li>Pipelines : Przep\u0142ywy pracy ETL</li> <li>Activities : Kroki w pipeline</li> <li>Datasets : Reprezentacje danych</li> <li>Linked Services : Po\u0142\u0105czenia ze \u017ar\u00f3d\u0142ami</li> <li>Triggers : Automatyczne wyzwalanie</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#data-factory-free-tier","title":"Data Factory Free Tier","text":"<p>Darmowe na zawsze : - 5 darmowych pipeline'\u00f3w - Ograniczone dzia\u0142ania - Poza tym : rozliczanie wed\u0142ug u\u017cycia</p> <p>\u26a0\ufe0f Wa\u017cne : Monitorowa\u0107 koszty, zw\u0142aszcza dla dzia\u0142a\u0144 przekszta\u0142cania.</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#utworzyc-data-factory","title":"Utworzy\u0107 Data Factory","text":""},{"location":"Cloud/Azure/PL/03-data-factory/#krok-1-dostep-do-data-factory","title":"Krok 1 : Dost\u0119p do Data Factory","text":"<ol> <li>Portal Azure \u2192 Szuka\u0107 \"Data Factory\"</li> <li>Klikn\u0105\u0107 \"Data factories\"</li> <li>Klikn\u0105\u0107 \"Create\"</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#krok-2-podstawowa-konfiguracja","title":"Krok 2 : Podstawowa konfiguracja","text":"<p>Podstawowe informacje : - Subscription : Wybra\u0107 subskrypcj\u0119 - Resource group : Utworzy\u0107 lub u\u017cy\u0107 istniej\u0105cego - Name : <code>my-data-factory</code> (unikalne globalnie) - Version : V2 (zalecane) - Region : Wybra\u0107 najbli\u017cszy region</p> <p>Konfiguracja Git (opcjonalne) : - Configure Git later : Aby szybko rozpocz\u0105\u0107 - Lub skonfigurowa\u0107 Git/GitHub do wersjonowania</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#krok-3-utworzyc-data-factory","title":"Krok 3 : Utworzy\u0107 Data Factory","text":"<ol> <li>Klikn\u0105\u0107 \"Review + create\"</li> <li>Sprawdzi\u0107 konfiguracj\u0119</li> <li>Klikn\u0105\u0107 \"Create\"</li> <li>Czeka\u0107 na utworzenie (2-3 minuty)</li> </ol> <p>\u26a0\ufe0f Wa\u017cne : Zanotowa\u0107 nazw\u0119 Data Factory.</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#krok-4-otworzyc-data-factory-studio","title":"Krok 4 : Otworzy\u0107 Data Factory Studio","text":"<ol> <li>Po utworzeniu, klikn\u0105\u0107 \"Open Azure Data Factory Studio\"</li> <li>Interfejs web do tworzenia pipeline'\u00f3w</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#utworzyc-pipeline","title":"Utworzy\u0107 pipeline","text":""},{"location":"Cloud/Azure/PL/03-data-factory/#krok-1-utworzyc-linked-service","title":"Krok 1 : Utworzy\u0107 Linked Service","text":"<p>Linked Service = Po\u0142\u0105czenie ze \u017ar\u00f3d\u0142em danych</p> <p>Przyk\u0142ad : Azure Blob Storage</p> <ol> <li>Data Factory Studio \u2192 \"Manage\" \u2192 \"Linked services\"</li> <li>Klikn\u0105\u0107 \"+ New\"</li> <li>Szuka\u0107 \"Azure Blob Storage\"</li> <li>Konfiguracja :</li> <li>Name : <code>AzureBlobStorage1</code></li> <li>Storage account name : Wybra\u0107 konto</li> <li>Authentication method : Account key (lub inny)</li> <li>Klikn\u0105\u0107 \"Create\"</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#krok-2-utworzyc-dataset","title":"Krok 2 : Utworzy\u0107 Dataset","text":"<p>Dataset = Reprezentacja danych</p> <ol> <li>Data Factory Studio \u2192 \"Author\" \u2192 \"Datasets\"</li> <li>Klikn\u0105\u0107 \"+ New\"</li> <li>Wybra\u0107 \"Azure Blob Storage\"</li> <li>Konfiguracja :</li> <li>Name : <code>CSVData</code></li> <li>Linked service : <code>AzureBlobStorage1</code></li> <li>File path : <code>raw-data/</code></li> <li>File format : DelimitedText (CSV)</li> <li>Klikn\u0105\u0107 \"Create\"</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#krok-3-utworzyc-pipeline","title":"Krok 3 : Utworzy\u0107 pipeline","text":"<ol> <li>Data Factory Studio \u2192 \"Author\" \u2192 \"Pipelines\"</li> <li>Klikn\u0105\u0107 \"+ New pipeline\"</li> <li>Nazwa\u0107 pipeline : <code>CopyCSVToParquet</code></li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#krok-4-dodac-dziaanie","title":"Krok 4 : Doda\u0107 dzia\u0142anie","text":"<p>Przyk\u0142ad : Copy Data</p> <ol> <li>W pipeline, przeci\u0105gn\u0105\u0107 \"Copy Data\" z \"Move &amp; transform\"</li> <li>Skonfigurowa\u0107 :</li> <li>Source : Dataset <code>CSVData</code></li> <li>Sink (Destination) : Utworzy\u0107 nowy dataset Parquet</li> <li>Klikn\u0105\u0107 \"Publish\" aby zapisa\u0107</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#dziaania-przeksztacania","title":"Dzia\u0142ania przekszta\u0142cania","text":""},{"location":"Cloud/Azure/PL/03-data-factory/#copy-data","title":"Copy Data","text":"<p>Kopiowa\u0107 dane ze \u017ar\u00f3d\u0142a do miejsca docelowego</p> <p>Konfiguracja : - Source : Dataset \u017ar\u00f3d\u0142owy - Sink : Dataset docelowy - Mapping : Mapowanie kolumn</p> <p>Przyk\u0142ad : CSV \u2192 Parquet</p> <pre><code>{\n  \"name\": \"CopyCSVToParquet\",\n  \"type\": \"Copy\",\n  \"inputs\": [{\"referenceName\": \"CSVData\"}],\n  \"outputs\": [{\"referenceName\": \"ParquetData\"}],\n  \"typeProperties\": {\n    \"source\": {\"type\": \"DelimitedTextSource\"},\n    \"sink\": {\"type\": \"ParquetSink\"}\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/PL/03-data-factory/#data-flow","title":"Data Flow","text":"<p>Przekszta\u0142canie danych z interfejsem graficznym</p> <p>Kroki : 1. Utworzy\u0107 Data Flow 2. Doda\u0107 \u017ar\u00f3d\u0142o 3. Doda\u0107 przekszta\u0142cenia :    - Select : Wybiera\u0107 kolumny    - Filter : Filtrowa\u0107 wiersze    - Derived Column : Tworzy\u0107 kolumny obliczane    - Aggregate : Agregacje    - Join : \u0141\u0105czy\u0107 dane 4. Doda\u0107 sink</p> <p>Przyk\u0142ad przekszta\u0142ce\u0144 :</p> <pre><code>Source (CSV) \n  \u2192 Select (kolumny)\n  \u2192 Filter (status = 'active')\n  \u2192 Derived Column (nowa kolumna)\n  \u2192 Aggregate (SUM, COUNT)\n  \u2192 Sink (Parquet)\n</code></pre>"},{"location":"Cloud/Azure/PL/03-data-factory/#lookup","title":"Lookup","text":"<p>Wyszukiwa\u0107 warto\u015bci w innym \u017ar\u00f3dle</p> <p>U\u017cycie : - Walidowa\u0107 dane - Wzbogaca\u0107 dane - Sprawdza\u0107 referencje</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#stored-procedure","title":"Stored Procedure","text":"<p>Wykona\u0107 procedur\u0119 sk\u0142adowan\u0105 SQL</p> <p>U\u017cycie : - Przetwarzanie w SQL Database - Z\u0142o\u017cona logika biznesowa - Optymalizacja po stronie bazy</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#integracja-ze-zrodami-danych","title":"Integracja ze \u017ar\u00f3d\u0142ami danych","text":""},{"location":"Cloud/Azure/PL/03-data-factory/#azure-blob-storage","title":"Azure Blob Storage","text":"<p>\u0179r\u00f3d\u0142o danych :</p> <pre><code>{\n  \"type\": \"AzureBlobStorage\",\n  \"typeProperties\": {\n    \"connectionString\": \"...\",\n    \"container\": \"raw-data\"\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/PL/03-data-factory/#azure-sql-database","title":"Azure SQL Database","text":"<p>\u0179r\u00f3d\u0142o danych :</p> <pre><code>{\n  \"type\": \"AzureSqlDatabase\",\n  \"typeProperties\": {\n    \"connectionString\": \"...\",\n    \"tableName\": \"users\"\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/PL/03-data-factory/#azure-data-lake-storage-gen2","title":"Azure Data Lake Storage Gen2","text":"<p>\u0179r\u00f3d\u0142o danych :</p> <pre><code>{\n  \"type\": \"AzureBlobFS\",\n  \"typeProperties\": {\n    \"url\": \"https://account.dfs.core.windows.net\",\n    \"fileSystem\": \"data-lake\"\n  }\n}\n</code></pre>"},{"location":"Cloud/Azure/PL/03-data-factory/#pliki-lokalne-przez-self-hosted-ir","title":"Pliki lokalne (przez Self-hosted IR)","text":"<p>Integration Runtime : - Self-hosted IR do dost\u0119pu do plik\u00f3w lokalnych - Zainstalowa\u0107 na maszynie lokalnej - Po\u0142\u0105czy\u0107 z Data Factory</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#orkiestracja-i-harmonogramowanie","title":"Orkiestracja i harmonogramowanie","text":""},{"location":"Cloud/Azure/PL/03-data-factory/#wyzwalac-recznie","title":"Wyzwala\u0107 r\u0119cznie","text":"<ol> <li>Data Factory Studio \u2192 \"Monitor\"</li> <li>Wybra\u0107 pipeline</li> <li>Klikn\u0105\u0107 \"Trigger now\"</li> <li>Zobaczy\u0107 wykonanie w czasie rzeczywistym</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#planowac-pipeline-trigger","title":"Planowa\u0107 pipeline (Trigger)","text":"<p>Utworzy\u0107 trigger :</p> <ol> <li>Pipeline \u2192 \"Add trigger\" \u2192 \"New/Edit\"</li> <li>Typ : \"Schedule\"</li> <li>Konfiguracja :</li> <li>Name : <code>DailyTrigger</code></li> <li>Type : Schedule</li> <li>Recurrence : Daily</li> <li>Start time : 02:00</li> <li>Klikn\u0105\u0107 \"OK\"</li> </ol> <p>Typy trigger\u00f3w : - Schedule : Planowane (cron) - Event : Wyzwalane przez zdarzenie - Tumbling window : Okno przesuwne</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#wyzwalac-przez-zdarzenie","title":"Wyzwala\u0107 przez zdarzenie","text":"<p>Przyk\u0142ad : Nowy plik w Blob Storage</p> <ol> <li>Utworzy\u0107 trigger \"Storage event\"</li> <li>Skonfigurowa\u0107 :</li> <li>Storage account : Twoje konto</li> <li>Container : <code>raw-data</code></li> <li>Event type : Blob created</li> <li>Powi\u0105za\u0107 z pipeline</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/Azure/PL/03-data-factory/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>U\u017cywa\u0107 Data Flow dla z\u0142o\u017conych przekszta\u0142ce\u0144</li> <li>Optymalizowa\u0107 dzia\u0142ania aby zmniejszy\u0107 czas</li> <li>U\u017cywa\u0107 r\u00f3wnoleg\u0142o\u015bci gdy mo\u017cliwe</li> <li>Wybra\u0107 odpowiednie regiony aby zmniejszy\u0107 op\u00f3\u017anienie</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#koszty","title":"Koszty","text":"<ol> <li>Monitorowa\u0107 wykonania w Monitor</li> <li>U\u017cywa\u0107 5 darmowych pipeline'\u00f3w m\u0105drze</li> <li>Optymalizowa\u0107 Data Flows (kosztowne)</li> <li>Zatrzymywa\u0107 nieu\u017cywane pipeline'y</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#organizacja","title":"Organizacja","text":"<ol> <li>Nazywa\u0107 jasno pipeline'y i dzia\u0142ania</li> <li>Dokumentowa\u0107 przekszta\u0142cenia</li> <li>Wersjonowa\u0107 z Git</li> <li>Testowa\u0107 przed publikacj\u0105</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":"<ol> <li>U\u017cywa\u0107 Key Vault dla sekret\u00f3w</li> <li>Ogranicza\u0107 uprawnienia Linked Services</li> <li>Audytowa\u0107 wykonania</li> <li>Szyfrowa\u0107 dane w tranzycie</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/Azure/PL/03-data-factory/#przykad-1-prosty-pipeline-csv-parquet","title":"Przyk\u0142ad 1 : Prosty pipeline CSV \u2192 Parquet","text":"<p>Pipeline : 1. \u0179r\u00f3d\u0142o : Azure Blob Storage (CSV) 2. Dzia\u0142anie : Copy Data 3. Sink : Azure Blob Storage (Parquet)</p> <p>Konfiguracja : - \u0179r\u00f3d\u0142o : <code>raw-data/data.csv</code> - Sink : <code>processed-data/data.parquet</code> - Format : DelimitedText \u2192 Parquet</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#przykad-2-pipeline-z-przeksztaceniem","title":"Przyk\u0142ad 2 : Pipeline z przekszta\u0142ceniem","text":"<p>Pipeline : 1. \u0179r\u00f3d\u0142o : Azure SQL Database 2. Data Flow :    - Wybra\u0107 kolumny    - Filtrowa\u0107 wiersze    - Agregowa\u0107 3. Sink : Azure Blob Storage (Parquet)</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#przykad-3-pipeline-orkiestrowany","title":"Przyk\u0142ad 3 : Pipeline orkiestrowany","text":"<p>Pipeline : 1. Lookup : Sprawdzi\u0107 czy nowe dane 2. If Condition : Je\u015bli nowe dane 3. Copy Data : Skopiowa\u0107 do staging 4. Data Flow : Przekszta\u0142ci\u0107 5. Copy Data : Za\u0142adowa\u0107 do miejsca docelowego</p>"},{"location":"Cloud/Azure/PL/03-data-factory/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Data Factory = ETL w chmurze zarz\u0105dzane przez Microsoft</li> <li>Free Tier : 5 pipeline'\u00f3w darmowych</li> <li>Pipelines orkiestruj\u0105 dzia\u0142ania</li> <li>Data Flows dla z\u0142o\u017conych przekszta\u0142ce\u0144</li> <li>Triggers umo\u017cliwiaj\u0105 automatyzacj\u0119</li> </ol>"},{"location":"Cloud/Azure/PL/03-data-factory/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 4. Azure SQL Database - Baza danych, aby nauczy\u0107 si\u0119 u\u017cywa\u0107 SQL Database na Azure.</p>"},{"location":"Cloud/Azure/PL/04-sql-database/","title":"4. Azure SQL Database - Baza danych","text":""},{"location":"Cloud/Azure/PL/04-sql-database/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Azure SQL Database</li> <li>Utworzy\u0107 baz\u0119 SQL Database (darmowo do 32 GB)</li> <li>Migrowa\u0107 dane</li> <li>Optymalizowa\u0107 zapytania</li> <li>Integrowa\u0107 z PowerBI</li> </ul>"},{"location":"Cloud/Azure/PL/04-sql-database/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do SQL Database</li> <li>Utworzy\u0107 baz\u0119 SQL Database</li> <li>Po\u0142\u0105czy\u0107 si\u0119 z baz\u0105</li> <li>\u0141adowa\u0107 dane</li> <li>Zapytania SQL</li> <li>Integracja z PowerBI</li> </ol>"},{"location":"Cloud/Azure/PL/04-sql-database/#wprowadzenie-do-sql-database","title":"Wprowadzenie do SQL Database","text":""},{"location":"Cloud/Azure/PL/04-sql-database/#czym-jest-azure-sql-database","title":"Czym jest Azure SQL Database?","text":"<p>Azure SQL Database = Zarz\u0105dzana baza danych SQL w chmurze</p> <ul> <li>Zgodne z SQL Server : Standardowa sk\u0142adnia SQL</li> <li>Zarz\u0105dzane : Microsoft zarz\u0105dza infrastruktur\u0105</li> <li>Skalowalne : Od kilku GB do kilku TB</li> <li>Wysoka dost\u0119pno\u015b\u0107 : 99.99% dost\u0119pno\u015bci</li> </ul>"},{"location":"Cloud/Azure/PL/04-sql-database/#przypadki-uzycia-dla-data-analyst","title":"Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Data Warehouse : Centralizowa\u0107 dane</li> <li>Analytics : Z\u0142o\u017cone zapytania</li> <li>Business Intelligence : \u0179r\u00f3d\u0142o dla PowerBI</li> <li>Data Integration : Centralny punkt dla ETL</li> </ul>"},{"location":"Cloud/Azure/PL/04-sql-database/#sql-database-free-tier","title":"SQL Database Free Tier","text":"<p>Darmowe 12 miesi\u0119cy : - Basic tier : Do 32 GB - DTU : 5 DTU (Database Transaction Units) - Backup : Automatyczny (7 dni)</p> <p>\u26a0\ufe0f Wa\u017cne : Po 12 miesi\u0105cach, normalne rozliczanie. Monitorowa\u0107 koszty.</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#utworzyc-baze-sql-database","title":"Utworzy\u0107 baz\u0119 SQL Database","text":""},{"location":"Cloud/Azure/PL/04-sql-database/#krok-1-dostep-do-sql-database","title":"Krok 1 : Dost\u0119p do SQL Database","text":"<ol> <li>Portal Azure \u2192 Szuka\u0107 \"SQL databases\"</li> <li>Klikn\u0105\u0107 \"SQL databases\"</li> <li>Klikn\u0105\u0107 \"Create\"</li> </ol>"},{"location":"Cloud/Azure/PL/04-sql-database/#krok-2-podstawowa-konfiguracja","title":"Krok 2 : Podstawowa konfiguracja","text":"<p>Podstawowe informacje : - Subscription : Wybra\u0107 subskrypcj\u0119 - Resource group : Utworzy\u0107 lub u\u017cy\u0107 istniej\u0105cego - Database name : <code>analytics-db</code> - Server : Utworzy\u0107 nowy serwer lub u\u017cy\u0107 istniej\u0105cego</p> <p>Utworzy\u0107 serwer SQL : - Server name : <code>my-sql-server-xxxxx</code> (unikalne globalnie) - Location : Wybra\u0107 region - Authentication method : SQL authentication (lub Azure AD) - Server admin login : <code>sqladmin</code> (lub inny) - Password : Silne has\u0142o - Allow Azure services : \u2705 Tak (dla Data Factory)</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#krok-3-konfiguracja-bazy","title":"Krok 3 : Konfiguracja bazy","text":"<p>Compute + storage : - Service tier : Basic (dla Free Tier) - Compute tier : Serverless (lub Provisioned) - Storage : 2 GB (darmowe, rozszerzalne do 32 GB)</p> <p>\u26a0\ufe0f Wa\u017cne : Basic tier = 5 DTU, wystarczaj\u0105ce do rozpocz\u0119cia.</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#krok-4-konfiguracja-sieci","title":"Krok 4 : Konfiguracja sieci","text":"<p>Networking : - Public endpoint : \u2705 W\u0142\u0105czy\u0107 - Firewall rules :   - \u2705 Allow Azure services and resources   - Doda\u0107 Twoje IP dla dost\u0119pu lokalnego</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#krok-5-utworzyc-baze","title":"Krok 5 : Utworzy\u0107 baz\u0119","text":"<ol> <li>Klikn\u0105\u0107 \"Review + create\"</li> <li>Sprawdzi\u0107 konfiguracj\u0119</li> <li>Klikn\u0105\u0107 \"Create\"</li> <li>Czeka\u0107 na utworzenie (2-3 minuty)</li> </ol> <p>\u26a0\ufe0f Wa\u017cne : Zanotowa\u0107 nazw\u0119 serwera i credentials.</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#poaczyc-sie-z-baza","title":"Po\u0142\u0105czy\u0107 si\u0119 z baz\u0105","text":""},{"location":"Cloud/Azure/PL/04-sql-database/#przez-portal-azure-query-editor","title":"Przez portal Azure (Query Editor)","text":"<ol> <li>SQL Database \u2192 \"Query editor\"</li> <li>Wprowadzi\u0107 credentials</li> <li>Wykonywa\u0107 zapytania SQL</li> </ol>"},{"location":"Cloud/Azure/PL/04-sql-database/#przez-sql-server-management-studio-ssms","title":"Przez SQL Server Management Studio (SSMS)","text":"<p>Pobra\u0107 SSMS : - https://aka.ms/ssmsfullsetup</p> <p>Po\u0142\u0105czenie : - Server name : <code>my-sql-server-xxxxx.database.windows.net</code> - Authentication : SQL Server Authentication - Login : <code>sqladmin</code> - Password : Twoje has\u0142o</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#przez-azure-data-studio","title":"Przez Azure Data Studio","text":"<p>Pobra\u0107 Azure Data Studio : - https://aka.ms/azuredatastudio</p> <p>Zalety : - Darmowe i open-source - Nowoczesny interfejs - Wsparcie notebook\u00f3w - Integracja Git</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#przez-python-pyodbc","title":"Przez Python (pyodbc)","text":"<pre><code>import pyodbc\n\n# Po\u0142\u0105czenie\nserver = 'my-sql-server-xxxxx.database.windows.net'\ndatabase = 'analytics-db'\nusername = 'sqladmin'\npassword = 'your-password'\ndriver = '{ODBC Driver 17 for SQL Server}'\n\nconn = pyodbc.connect(\n    f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n)\n\n# Wykona\u0107 zapytanie\ncursor = conn.cursor()\ncursor.execute(\"SELECT * FROM users\")\nrows = cursor.fetchall()\nfor row in rows:\n    print(row)\n</code></pre>"},{"location":"Cloud/Azure/PL/04-sql-database/#adowac-dane","title":"\u0141adowa\u0107 dane","text":""},{"location":"Cloud/Azure/PL/04-sql-database/#metoda-1-insert-mae-ilosci","title":"Metoda 1 : INSERT (ma\u0142e ilo\u015bci)","text":"<pre><code>INSERT INTO users (id, name, email, created_at)\nVALUES (1, 'John Doe', 'john@example.com', '2024-01-01');\n</code></pre>"},{"location":"Cloud/Azure/PL/04-sql-database/#metoda-2-bulk-insert-z-blob-storage","title":"Metoda 2 : BULK INSERT z Blob Storage","text":"<p>Wymagania wst\u0119pne : - Utworzy\u0107 klucz SAS dla Blob Storage - Utworzy\u0107 credential w SQL Database</p> <p>Przyk\u0142ad :</p> <pre><code>-- Utworzy\u0107 credential\nCREATE DATABASE SCOPED CREDENTIAL BlobCredential\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'your-sas-token';\n\n-- Utworzy\u0107 zewn\u0119trzne \u017ar\u00f3d\u0142o danych\nCREATE EXTERNAL DATA SOURCE BlobStorage\nWITH (\n    TYPE = BLOB_STORAGE,\n    LOCATION = 'https://mystorageaccount.blob.core.windows.net',\n    CREDENTIAL = BlobCredential\n);\n\n-- Importowa\u0107 z Blob Storage\nBULK INSERT users\nFROM 'raw-data/users.csv'\nWITH (\n    DATA_SOURCE = 'BlobStorage',\n    FORMAT = 'CSV',\n    FIRSTROW = 2,\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n'\n);\n</code></pre>"},{"location":"Cloud/Azure/PL/04-sql-database/#metoda-3-przez-data-factory","title":"Metoda 3 : Przez Data Factory","text":"<p>Pipeline : 1. \u0179r\u00f3d\u0142o : Azure Blob Storage (CSV) 2. Dzia\u0142anie : Copy Data 3. Sink : Azure SQL Database</p> <p>Konfiguracja : - \u0179r\u00f3d\u0142o : <code>raw-data/users.csv</code> - Sink : Tabela <code>users</code> w SQL Database - Mapowanie : Kolumny automatyczne lub r\u0119czne</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#metoda-4-przez-python-pandas","title":"Metoda 4 : Przez Python (pandas)","text":"<pre><code>import pandas as pd\nimport pyodbc\n\n# Czyta\u0107 plik CSV\ndf = pd.read_csv('users.csv')\n\n# Po\u0142\u0105czenie\nconn = pyodbc.connect(connection_string)\n\n# Zapisa\u0107 w SQL Database\ndf.to_sql('users', conn, if_exists='append', index=False)\n</code></pre>"},{"location":"Cloud/Azure/PL/04-sql-database/#zapytania-sql","title":"Zapytania SQL","text":""},{"location":"Cloud/Azure/PL/04-sql-database/#podstawowe-zapytania","title":"Podstawowe zapytania","text":"<p>Prosty SELECT :</p> <pre><code>SELECT * FROM users LIMIT 10;\n</code></pre> <p>Filtrowa\u0107 :</p> <pre><code>SELECT id, name, email\nFROM users\nWHERE created_at &gt; '2024-01-01'\nORDER BY created_at DESC;\n</code></pre> <p>Agregacje :</p> <pre><code>SELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count,\n    COUNT(DISTINCT email) AS unique_emails\nFROM users\nGROUP BY DATE_TRUNC('month', created_at)\nORDER BY month;\n</code></pre>"},{"location":"Cloud/Azure/PL/04-sql-database/#zaawansowane-zapytania","title":"Zaawansowane zapytania","text":"<p>Funkcje okna :</p> <pre><code>SELECT \n    id,\n    name,\n    created_at,\n    ROW_NUMBER() OVER (PARTITION BY DATE_TRUNC('month', created_at) ORDER BY created_at) AS rank\nFROM users;\n</code></pre> <p>Z\u0142\u0105czenia :</p> <pre><code>SELECT \n    u.name,\n    o.amount,\n    o.created_at\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.created_at &gt; '2024-01-01';\n</code></pre> <p>CTE (Common Table Expressions) :</p> <pre><code>WITH monthly_users AS (\n    SELECT \n        DATE_TRUNC('month', created_at) AS month,\n        COUNT(*) AS user_count\n    FROM users\n    GROUP BY DATE_TRUNC('month', created_at)\n)\nSELECT \n    month,\n    user_count,\n    LAG(user_count, 1) OVER (ORDER BY month) AS previous_month\nFROM monthly_users;\n</code></pre>"},{"location":"Cloud/Azure/PL/04-sql-database/#integracja-z-powerbi","title":"Integracja z PowerBI","text":""},{"location":"Cloud/Azure/PL/04-sql-database/#bezposrednie-poaczenie","title":"Bezpo\u015brednie po\u0142\u0105czenie","text":"<p>Krok 1 : W PowerBI Desktop</p> <ol> <li>\"Get Data\" \u2192 \"Azure\" \u2192 \"Azure SQL Database\"</li> <li>Wprowadzi\u0107 informacje :</li> <li>Server : <code>my-sql-server-xxxxx.database.windows.net</code></li> <li>Database : <code>analytics-db</code></li> <li>Data connectivity mode : Import (lub DirectQuery)</li> </ol> <p>Krok 2 : Uwierzytelnianie</p> <ul> <li>Authentication method : Database</li> <li>Username : <code>sqladmin</code></li> <li>Password : Twoje has\u0142o</li> </ul> <p>Krok 3 : Wybra\u0107 tabele</p> <ul> <li>Wybra\u0107 tabele do importu</li> <li>Klikn\u0105\u0107 \"Load\"</li> </ul>"},{"location":"Cloud/Azure/PL/04-sql-database/#directquery-vs-import","title":"DirectQuery vs Import","text":"<p>Import : - \u2705 Szybkie dla wizualizacji - \u2705 Dzia\u0142a offline - \u274c Dane statyczne (wymaga od\u015bwie\u017cenia)</p> <p>DirectQuery : - \u2705 Dane w czasie rzeczywistym - \u2705 Brak limitu rozmiaru - \u274c Wolniejsze (zapytania przy ka\u017cdej interakcji)</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#tworzyc-wizualizacje","title":"Tworzy\u0107 wizualizacje","text":"<p>Przyk\u0142ad : 1. Importowa\u0107 tabel\u0119 <code>users</code> 2. Utworzy\u0107 wykres : Liczba u\u017cytkownik\u00f3w na miesi\u0105c 3. Doda\u0107 filtry 4. Opublikowa\u0107 na PowerBI Service</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/Azure/PL/04-sql-database/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>Tworzy\u0107 indeksy na cz\u0119sto u\u017cywanych kolumnach</li> <li>Optymalizowa\u0107 zapytania z EXPLAIN</li> <li>U\u017cywa\u0107 widok\u00f3w aby upro\u015bci\u0107</li> <li>Partycjonowa\u0107 du\u017ce tabele</li> </ol>"},{"location":"Cloud/Azure/PL/04-sql-database/#koszty","title":"Koszty","text":"<ol> <li>Monitorowa\u0107 u\u017cycie w Azure Cost Management</li> <li>U\u017cywa\u0107 Basic tier do rozpocz\u0119cia</li> <li>Zatrzyma\u0107 baz\u0119 je\u015bli nieu\u017cywana (Serverless)</li> <li>Czy\u015bci\u0107 niepotrzebne dane</li> </ol>"},{"location":"Cloud/Azure/PL/04-sql-database/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":"<ol> <li>U\u017cywa\u0107 Azure AD do uwierzytelniania</li> <li>Ogranicza\u0107 dost\u0119p z regu\u0142ami firewall</li> <li>Szyfrowa\u0107 dane (w\u0142\u0105czone domy\u015blnie)</li> <li>Audytowa\u0107 dost\u0119p z SQL Auditing</li> </ol>"},{"location":"Cloud/Azure/PL/04-sql-database/#organizacja","title":"Organizacja","text":"<ol> <li>Nazywa\u0107 jasno tabele i kolumny</li> <li>Dokumentowa\u0107 schematy</li> <li>U\u017cywa\u0107 schemat\u00f3w aby organizowa\u0107</li> <li>Wersjonowa\u0107 skrypty SQL (Git)</li> </ol>"},{"location":"Cloud/Azure/PL/04-sql-database/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/Azure/PL/04-sql-database/#przykad-1-kompletny-pipeline-blob-sql-database","title":"Przyk\u0142ad 1 : Kompletny pipeline Blob \u2192 SQL Database","text":"<p>Przez Data Factory : 1. \u0179r\u00f3d\u0142o : Azure Blob Storage (CSV) 2. Dzia\u0142anie : Copy Data 3. Sink : Azure SQL Database 4. Trigger : Schedule (codziennie)</p>"},{"location":"Cloud/Azure/PL/04-sql-database/#przykad-2-zapytania-analityczne","title":"Przyk\u0142ad 2 : Zapytania analityczne","text":"<pre><code>-- Top 10 u\u017cytkownik\u00f3w wed\u0142ug wydatk\u00f3w\nSELECT TOP 10\n    u.name,\n    SUM(o.amount) AS total_spent,\n    COUNT(o.id) AS order_count\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE o.created_at &gt;= DATEADD(month, -3, GETDATE())\nGROUP BY u.name\nORDER BY total_spent DESC;\n</code></pre>"},{"location":"Cloud/Azure/PL/04-sql-database/#przykad-3-eksport-do-powerbi","title":"Przyk\u0142ad 3 : Eksport do PowerBI","text":"<ol> <li>Utworzy\u0107 widok dla PowerBI</li> <li>Po\u0142\u0105czy\u0107 PowerBI z widokiem</li> <li>Tworzy\u0107 wizualizacje</li> <li>Opublikowa\u0107 raport</li> </ol>"},{"location":"Cloud/Azure/PL/04-sql-database/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>SQL Database = Baza SQL w chmurze zarz\u0105dzana przez Microsoft</li> <li>Free Tier : 32 GB przez 12 miesi\u0119cy (Basic tier)</li> <li>Zgodne z SQL Server : Standardowa sk\u0142adnia</li> <li>Integracja PowerBI : Bezpo\u015brednie po\u0142\u0105czenie</li> <li>Skalowalne : Od Basic do Premium</li> </ol>"},{"location":"Cloud/Azure/PL/04-sql-database/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 5. Azure Synapse Analytics - Hurtownia danych, aby nauczy\u0107 si\u0119 u\u017cywa\u0107 Synapse do analizy danych.</p>"},{"location":"Cloud/Azure/PL/05-synapse/","title":"5. Azure Synapse Analytics - Hurtownia danych","text":""},{"location":"Cloud/Azure/PL/05-synapse/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Azure Synapse Analytics</li> <li>Utworzy\u0107 obszar roboczy Synapse</li> <li>\u0141adowa\u0107 dane</li> <li>Wykonywa\u0107 zaawansowane zapytania SQL</li> <li>Integrowa\u0107 z PowerBI</li> </ul>"},{"location":"Cloud/Azure/PL/05-synapse/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Synapse</li> <li>Utworzy\u0107 obszar roboczy Synapse</li> <li>\u0141adowa\u0107 dane</li> <li>Zaawansowane zapytania SQL</li> <li>Integracja z PowerBI</li> <li>Dobre praktyki</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#wprowadzenie-do-synapse","title":"Wprowadzenie do Synapse","text":""},{"location":"Cloud/Azure/PL/05-synapse/#czym-jest-azure-synapse-analytics","title":"Czym jest Azure Synapse Analytics?","text":"<p>Azure Synapse Analytics = Ujednolicona platforma analytics</p> <ul> <li>Data Warehouse : Przechowywanie i analiza danych</li> <li>Big Data : Przetwarzanie du\u017cych ilo\u015bci</li> <li>SQL : Standardowe zapytania SQL</li> <li>Spark : Przetwarzanie rozproszone</li> <li>Integracja : Ze wszystkimi us\u0142ugami Azure</li> </ul>"},{"location":"Cloud/Azure/PL/05-synapse/#komponenty-synapse","title":"Komponenty Synapse","text":"<ol> <li>SQL Pool : Hurtownia danych SQL (dawniej SQL Data Warehouse)</li> <li>Spark Pool : Klastry Spark dla Big Data</li> <li>Synapse Studio : Ujednolicony interfejs web</li> <li>Pipelines : Zintegrowane ETL</li> <li>Notebooks : Python, SQL, Scala</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#synapse-free-tier","title":"Synapse Free Tier","text":"<p>Darmowe z kredytem Azure : - U\u017cywa\u0107 200$ darmowego kredytu (30 dni) - Po tym : normalne rozliczanie</p> <p>\u26a0\ufe0f Wa\u017cne : Synapse mo\u017ce by\u0107 kosztowne. Monitorowa\u0107 koszty uwa\u017cnie.</p>"},{"location":"Cloud/Azure/PL/05-synapse/#utworzyc-obszar-roboczy-synapse","title":"Utworzy\u0107 obszar roboczy Synapse","text":""},{"location":"Cloud/Azure/PL/05-synapse/#krok-1-dostep-do-synapse","title":"Krok 1 : Dost\u0119p do Synapse","text":"<ol> <li>Portal Azure \u2192 Szuka\u0107 \"Azure Synapse Analytics\"</li> <li>Klikn\u0105\u0107 \"Azure Synapse Analytics\"</li> <li>Klikn\u0105\u0107 \"Create\"</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#krok-2-podstawowa-konfiguracja","title":"Krok 2 : Podstawowa konfiguracja","text":"<p>Podstawowe informacje : - Subscription : Wybra\u0107 subskrypcj\u0119 - Resource group : Utworzy\u0107 lub u\u017cy\u0107 istniej\u0105cego - Workspace name : <code>my-synapse-workspace</code> - Region : Wybra\u0107 region - Data Lake Storage Gen2 : Utworzy\u0107 nowy lub u\u017cy\u0107 istniej\u0105cego</p> <p>Administrator SQL : - SQL admin name : <code>sqladmin</code> - Password : Silne has\u0142o</p>"},{"location":"Cloud/Azure/PL/05-synapse/#krok-3-konfiguracja-sql-pool","title":"Krok 3 : Konfiguracja SQL Pool","text":"<p>SQL Pool : - Create a SQL pool : \u2705 Tak (do rozpocz\u0119cia) - Performance level : DW100c (najta\u0144sze) - Lub : Utworzy\u0107 p\u00f3\u017aniej (Serverless SQL)</p> <p>\u26a0\ufe0f Wa\u017cne : Serverless SQL = pay-per-query, bardziej ekonomiczne do rozpocz\u0119cia.</p>"},{"location":"Cloud/Azure/PL/05-synapse/#krok-4-utworzyc-obszar-roboczy","title":"Krok 4 : Utworzy\u0107 obszar roboczy","text":"<ol> <li>Klikn\u0105\u0107 \"Review + create\"</li> <li>Sprawdzi\u0107 konfiguracj\u0119</li> <li>Klikn\u0105\u0107 \"Create\"</li> <li>Czeka\u0107 na utworzenie (5-10 minut)</li> </ol> <p>\u26a0\ufe0f Wa\u017cne : Zanotowa\u0107 credentials SQL.</p>"},{"location":"Cloud/Azure/PL/05-synapse/#adowac-dane","title":"\u0141adowa\u0107 dane","text":""},{"location":"Cloud/Azure/PL/05-synapse/#metoda-1-copy-z-data-lake-storage","title":"Metoda 1 : COPY z Data Lake Storage","text":"<p>Najszybsze dla du\u017cych ilo\u015bci :</p> <pre><code>-- Utworzy\u0107 tabel\u0119\nCREATE TABLE users (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at DATETIME2\n)\nWITH (\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n-- \u0141adowa\u0107 z Data Lake Storage\nCOPY INTO users\nFROM 'https://mystorageaccount.dfs.core.windows.net/data-lake/raw/users.csv'\nWITH (\n    FILE_TYPE = 'CSV',\n    FIRSTROW = 2,\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n'\n);\n</code></pre>"},{"location":"Cloud/Azure/PL/05-synapse/#metoda-2-przez-synapse-pipelines","title":"Metoda 2 : Przez Synapse Pipelines","text":"<p>Zintegrowany pipeline :</p> <ol> <li>Synapse Studio \u2192 \"Integrate\" \u2192 \"Pipelines\"</li> <li>Utworzy\u0107 nowy pipeline</li> <li>Doda\u0107 dzia\u0142anie \"Copy Data\"</li> <li>\u0179r\u00f3d\u0142o : Azure Blob Storage lub Data Lake</li> <li>Sink : SQL Pool</li> <li>Wykona\u0107 pipeline</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#metoda-3-insert-mae-ilosci","title":"Metoda 3 : INSERT (ma\u0142e ilo\u015bci)","text":"<pre><code>INSERT INTO users (id, name, email, created_at)\nVALUES (1, 'John Doe', 'john@example.com', '2024-01-01');\n</code></pre>"},{"location":"Cloud/Azure/PL/05-synapse/#metoda-4-przez-polybase-tabele-zewnetrzne","title":"Metoda 4 : Przez PolyBase (Tabele zewn\u0119trzne)","text":"<p>Utworzy\u0107 tabel\u0119 zewn\u0119trzn\u0105 :</p> <pre><code>-- Utworzy\u0107 credential\nCREATE DATABASE SCOPED CREDENTIAL BlobCredential\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'your-sas-token';\n\n-- Utworzy\u0107 zewn\u0119trzne \u017ar\u00f3d\u0142o danych\nCREATE EXTERNAL DATA SOURCE BlobStorage\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'wasbs://container@account.blob.core.windows.net',\n    CREDENTIAL = BlobCredential\n);\n\n-- Utworzy\u0107 zewn\u0119trzny format pliku\nCREATE EXTERNAL FILE FORMAT CSVFormat\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (FIELD_TERMINATOR = ',')\n);\n\n-- Utworzy\u0107 tabel\u0119 zewn\u0119trzn\u0105\nCREATE EXTERNAL TABLE users_external (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(100)\n)\nWITH (\n    LOCATION = 'raw/users.csv',\n    DATA_SOURCE = BlobStorage,\n    FILE_FORMAT = CSVFormat\n);\n\n-- \u0141adowa\u0107 do tabeli wewn\u0119trznej\nINSERT INTO users\nSELECT * FROM users_external;\n</code></pre>"},{"location":"Cloud/Azure/PL/05-synapse/#zaawansowane-zapytania-sql","title":"Zaawansowane zapytania SQL","text":""},{"location":"Cloud/Azure/PL/05-synapse/#podstawowe-zapytania","title":"Podstawowe zapytania","text":"<p>Prosty SELECT :</p> <pre><code>SELECT TOP 100 * FROM users;\n</code></pre> <p>Agregacje :</p> <pre><code>SELECT \n    YEAR(created_at) AS year,\n    MONTH(created_at) AS month,\n    COUNT(*) AS user_count\nFROM users\nGROUP BY YEAR(created_at), MONTH(created_at)\nORDER BY year, month;\n</code></pre>"},{"location":"Cloud/Azure/PL/05-synapse/#funkcje-okna","title":"Funkcje okna","text":"<p>ROW_NUMBER :</p> <pre><code>SELECT \n    id,\n    name,\n    created_at,\n    ROW_NUMBER() OVER (PARTITION BY YEAR(created_at) ORDER BY created_at) AS rank\nFROM users;\n</code></pre> <p>LAG/LEAD :</p> <pre><code>SELECT \n    date,\n    sales,\n    LAG(sales, 1) OVER (ORDER BY date) AS previous_sales,\n    LEAD(sales, 1) OVER (ORDER BY date) AS next_sales\nFROM daily_sales;\n</code></pre>"},{"location":"Cloud/Azure/PL/05-synapse/#dystrybucja-i-wydajnosc","title":"Dystrybucja i wydajno\u015b\u0107","text":"<p>Klucze dystrybucji :</p> <pre><code>-- Dystrybucja HASH (dla z\u0142\u0105cze\u0144)\nCREATE TABLE users (\n    id INT,\n    name VARCHAR(100)\n)\nWITH (\n    DISTRIBUTION = HASH(id),\n    CLUSTERED COLUMNSTORE INDEX\n);\n\n-- Dystrybucja ROUND_ROBIN (domy\u015blnie)\nCREATE TABLE logs (\n    id INT,\n    message VARCHAR(MAX)\n)\nWITH (\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n);\n</code></pre> <p>Clustered Columnstore Index : - Zoptymalizowane dla analytics - Wysoka kompresja - Szybkie zapytania na du\u017cych tabelach</p>"},{"location":"Cloud/Azure/PL/05-synapse/#integracja-z-powerbi","title":"Integracja z PowerBI","text":""},{"location":"Cloud/Azure/PL/05-synapse/#bezposrednie-poaczenie","title":"Bezpo\u015brednie po\u0142\u0105czenie","text":"<p>Krok 1 : W PowerBI Desktop</p> <ol> <li>\"Get Data\" \u2192 \"Azure\" \u2192 \"Azure Synapse Analytics SQL\"</li> <li>Wprowadzi\u0107 informacje :</li> <li>Server : <code>my-synapse-workspace-ondemand.sql.azuresynapse.net</code> (Serverless)</li> <li>Database : Nazwa bazy</li> <li>Data connectivity mode : DirectQuery (zalecane)</li> </ol> <p>Krok 2 : Uwierzytelnianie</p> <ul> <li>Authentication method : Database</li> <li>Username : <code>sqladmin</code></li> <li>Password : Twoje has\u0142o</li> </ul> <p>Krok 3 : Wybra\u0107 tabele</p> <ul> <li>Wybra\u0107 tabele lub widoki</li> <li>Klikn\u0105\u0107 \"Load\"</li> </ul>"},{"location":"Cloud/Azure/PL/05-synapse/#tworzyc-widoki-dla-powerbi","title":"Tworzy\u0107 widoki dla PowerBI","text":"<p>Zoptymalizowany widok :</p> <pre><code>CREATE VIEW vw_user_analytics AS\nSELECT \n    u.id,\n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name, u.email;\n</code></pre> <p>U\u017cywa\u0107 widoku w PowerBI : - Prostsze dla u\u017cytkownik\u00f3w - Centralizowana logika biznesowa - Zoptymalizowana wydajno\u015b\u0107</p>"},{"location":"Cloud/Azure/PL/05-synapse/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/Azure/PL/05-synapse/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>U\u017cywa\u0107 Columnstore Index dla analytics</li> <li>Wybra\u0107 odpowiednie klucze dystrybucji</li> <li>Partycjonowa\u0107 du\u017ce tabele</li> <li>Optymalizowa\u0107 zapytania z EXPLAIN</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#koszty","title":"Koszty","text":"<ol> <li>U\u017cywa\u0107 Serverless SQL do rozpocz\u0119cia (pay-per-query)</li> <li>Wstrzyma\u0107 SQL Pool gdy nieu\u017cywany</li> <li>Monitorowa\u0107 koszty w Azure Cost Management</li> <li>U\u017cywa\u0107 odpowiednich rozmiar\u00f3w pool</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#organizacja","title":"Organizacja","text":"<ol> <li>Tworzy\u0107 schematy aby organizowa\u0107</li> <li>Nazywa\u0107 jasno tabele i widoki</li> <li>Dokumentowa\u0107 schematy</li> <li>U\u017cywa\u0107 widok\u00f3w aby upro\u015bci\u0107</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":"<ol> <li>U\u017cywa\u0107 Azure AD do uwierzytelniania</li> <li>Ogranicza\u0107 dost\u0119p z regu\u0142ami firewall</li> <li>Szyfrowa\u0107 dane (w\u0142\u0105czone domy\u015blnie)</li> <li>Audytowa\u0107 dost\u0119p</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/Azure/PL/05-synapse/#przykad-1-kompletny-pipeline-data-lake-synapse","title":"Przyk\u0142ad 1 : Kompletny pipeline Data Lake \u2192 Synapse","text":"<p>Pipeline Synapse : 1. \u0179r\u00f3d\u0142o : Data Lake Storage (Parquet) 2. Dzia\u0142anie : Copy Data 3. Sink : SQL Pool 4. Trigger : Schedule (codziennie)</p>"},{"location":"Cloud/Azure/PL/05-synapse/#przykad-2-zozone-zapytania-analityczne","title":"Przyk\u0142ad 2 : Z\u0142o\u017cone zapytania analityczne","text":"<pre><code>-- Analiza sprzeda\u017cy z funkcjami okna\nWITH monthly_sales AS (\n    SELECT \n        YEAR(sale_date) AS year,\n        MONTH(sale_date) AS month,\n        SUM(amount) AS total_sales\n    FROM sales\n    GROUP BY YEAR(sale_date), MONTH(sale_date)\n)\nSELECT \n    year,\n    month,\n    total_sales,\n    LAG(total_sales, 1) OVER (ORDER BY year, month) AS previous_month,\n    (total_sales - LAG(total_sales, 1) OVER (ORDER BY year, month)) / \n        LAG(total_sales, 1) OVER (ORDER BY year, month) * 100 AS growth_percent\nFROM monthly_sales\nORDER BY year, month;\n</code></pre>"},{"location":"Cloud/Azure/PL/05-synapse/#przykad-3-eksport-do-powerbi","title":"Przyk\u0142ad 3 : Eksport do PowerBI","text":"<ol> <li>Utworzy\u0107 widok analityczny</li> <li>Po\u0142\u0105czy\u0107 PowerBI z widokiem</li> <li>Tworzy\u0107 wizualizacje</li> <li>Opublikowa\u0107 raport</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Synapse = Ujednolicona platforma analytics</li> <li>SQL Pool dla hurtowni danych</li> <li>Serverless SQL dla pay-per-query</li> <li>Natywna integracja PowerBI</li> <li>Skalowalne od kilku GB do kilku PB</li> </ol>"},{"location":"Cloud/Azure/PL/05-synapse/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 6. Azure Databricks - Analiza Big Data, aby nauczy\u0107 si\u0119 u\u017cywa\u0107 Databricks dla Big Data.</p>"},{"location":"Cloud/Azure/PL/06-databricks/","title":"6. Azure Databricks - Analiza Big Data","text":""},{"location":"Cloud/Azure/PL/06-databricks/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Azure Databricks</li> <li>Utworzy\u0107 obszar roboczy Databricks</li> <li>U\u017cywa\u0107 notebook\u00f3w Python/SQL</li> <li>Przetwarza\u0107 dane ze Spark</li> <li>Integrowa\u0107 z innymi us\u0142ugami Azure</li> </ul>"},{"location":"Cloud/Azure/PL/06-databricks/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Databricks</li> <li>Utworzy\u0107 obszar roboczy Databricks</li> <li>Utworzy\u0107 klaster</li> <li>Notebooki Python/SQL</li> <li>Przetwarzanie danych ze Spark</li> <li>Integracja z innymi us\u0142ugami</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#wprowadzenie-do-databricks","title":"Wprowadzenie do Databricks","text":""},{"location":"Cloud/Azure/PL/06-databricks/#czym-jest-azure-databricks","title":"Czym jest Azure Databricks?","text":"<p>Azure Databricks = Platforma Big Data oparta na Apache Spark</p> <ul> <li>Apache Spark : Silnik przetwarzania rozproszonego</li> <li>Notebooki : Python, SQL, Scala, R</li> <li>Zarz\u0105dzane : Microsoft zarz\u0105dza infrastruktur\u0105</li> <li>Skalowalne : Klastry auto-scaling</li> </ul>"},{"location":"Cloud/Azure/PL/06-databricks/#przypadki-uzycia-dla-data-analyst","title":"Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Przetwarzanie Big Data : Przetwarza\u0107 du\u017ce ilo\u015bci</li> <li>ETL : Z\u0142o\u017cone przekszta\u0142cenia</li> <li>Machine Learning : Zintegrowany MLlib</li> <li>Data Science : Interaktywne notebooki</li> </ul>"},{"location":"Cloud/Azure/PL/06-databricks/#databricks-free-tier","title":"Databricks Free Tier","text":"<p>Darmowe z kredytem Azure : - U\u017cywa\u0107 200$ darmowego kredytu (30 dni) - Po tym : normalne rozliczanie</p> <p>\u26a0\ufe0f Wa\u017cne : Databricks mo\u017ce by\u0107 kosztowne. Monitorowa\u0107 koszty uwa\u017cnie.</p>"},{"location":"Cloud/Azure/PL/06-databricks/#utworzyc-obszar-roboczy-databricks","title":"Utworzy\u0107 obszar roboczy Databricks","text":""},{"location":"Cloud/Azure/PL/06-databricks/#krok-1-dostep-do-databricks","title":"Krok 1 : Dost\u0119p do Databricks","text":"<ol> <li>Portal Azure \u2192 Szuka\u0107 \"Azure Databricks\"</li> <li>Klikn\u0105\u0107 \"Azure Databricks\"</li> <li>Klikn\u0105\u0107 \"Create\"</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#krok-2-podstawowa-konfiguracja","title":"Krok 2 : Podstawowa konfiguracja","text":"<p>Podstawowe informacje : - Subscription : Wybra\u0107 subskrypcj\u0119 - Resource group : Utworzy\u0107 lub u\u017cy\u0107 istniej\u0105cego - Workspace name : <code>my-databricks-workspace</code> - Region : Wybra\u0107 region - Pricing tier : Standard (lub Premium)</p> <p>Networking : - Virtual network : Utworzy\u0107 nowy lub u\u017cy\u0107 istniej\u0105cego - Public IP : \u2705 W\u0142\u0105czy\u0107 (dla \u0142atwego dost\u0119pu)</p>"},{"location":"Cloud/Azure/PL/06-databricks/#krok-3-utworzyc-obszar-roboczy","title":"Krok 3 : Utworzy\u0107 obszar roboczy","text":"<ol> <li>Klikn\u0105\u0107 \"Review + create\"</li> <li>Sprawdzi\u0107 konfiguracj\u0119</li> <li>Klikn\u0105\u0107 \"Create\"</li> <li>Czeka\u0107 na utworzenie (5-10 minut)</li> </ol> <p>\u26a0\ufe0f Wa\u017cne : Zanotowa\u0107 URL obszaru roboczego.</p>"},{"location":"Cloud/Azure/PL/06-databricks/#krok-4-otworzyc-databricks","title":"Krok 4 : Otworzy\u0107 Databricks","text":"<ol> <li>Po utworzeniu, klikn\u0105\u0107 \"Launch Workspace\"</li> <li>Interfejs web Databricks</li> <li>Zalogowa\u0107 si\u0119 z Azure AD</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#utworzyc-klaster","title":"Utworzy\u0107 klaster","text":""},{"location":"Cloud/Azure/PL/06-databricks/#krok-1-dostep-do-klastrow","title":"Krok 1 : Dost\u0119p do klastr\u00f3w","text":"<ol> <li>Databricks Workspace \u2192 \"Compute\"</li> <li>Klikn\u0105\u0107 \"Create Cluster\"</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#krok-2-konfiguracja-klastra","title":"Krok 2 : Konfiguracja klastra","text":"<p>Podstawowa konfiguracja : - Cluster name : <code>my-cluster</code> - Cluster mode : Standard (lub Single Node dla test\u00f3w) - Databricks runtime version : Latest LTS (zalecane) - Python version : 3.11</p> <p>Typ w\u0119z\u0142a : - Worker type : Standard_DS3_v2 (do rozpocz\u0119cia) - Driver type : Standard_DS3_v2 - Min workers : 0 (aby oszcz\u0119dzi\u0107) - Max workers : 2 (do rozpocz\u0119cia)</p> <p>\u26a0\ufe0f Wa\u017cne : Min workers = 0 umo\u017cliwia auto-termination gdy nieaktywny.</p>"},{"location":"Cloud/Azure/PL/06-databricks/#krok-3-opcje-zaawansowane","title":"Krok 3 : Opcje zaawansowane","text":"<p>Auto-termination : - \u2705 W\u0142\u0105czy\u0107 (zatrzymuje klaster po nieaktywno\u015bci) - Terminate after : 30 minut</p> <p>Tagi : - Doda\u0107 tagi do organizacji</p>"},{"location":"Cloud/Azure/PL/06-databricks/#krok-4-utworzyc-klaster","title":"Krok 4 : Utworzy\u0107 klaster","text":"<ol> <li>Klikn\u0105\u0107 \"Create Cluster\"</li> <li>Czeka\u0107 na uruchomienie (3-5 minut)</li> <li>Klaster gotowy gdy status = \"Running\"</li> </ol> <p>\u26a0\ufe0f Wa\u017cne : Klaster zu\u017cywa zasoby nawet nieaktywny. Zatrzyma\u0107 gdy nieu\u017cywany.</p>"},{"location":"Cloud/Azure/PL/06-databricks/#notebooki-pythonsql","title":"Notebooki Python/SQL","text":""},{"location":"Cloud/Azure/PL/06-databricks/#utworzyc-notebook","title":"Utworzy\u0107 notebook","text":"<p>Krok 1 : Utworzy\u0107 notebook</p> <ol> <li>Databricks Workspace \u2192 \"Workspace\"</li> <li>Klik prawy \u2192 \"Create\" \u2192 \"Notebook\"</li> <li>Nazwa : <code>data-processing</code></li> <li>J\u0119zyk : Python (lub SQL)</li> <li>Klaster : Do\u0142\u0105czy\u0107 do utworzonego klastra</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#krok-2-uzywac-notebook","title":"Krok 2 : U\u017cywa\u0107 notebook","text":"<p>Kom\u00f3rki Python :</p> <pre><code># Kom\u00f3rka 1 : Importowa\u0107 biblioteki\nimport pandas as pd\nfrom pyspark.sql import SparkSession\n\n# Kom\u00f3rka 2 : Utworzy\u0107 sesj\u0119 Spark\nspark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()\n\n# Kom\u00f3rka 3 : Czyta\u0107 dane\ndf = spark.read.csv(\"dbfs:/FileStore/data/users.csv\", header=True, inferSchema=True)\n\n# Kom\u00f3rka 4 : Wy\u015bwietli\u0107 dane\ndf.show()\n\n# Kom\u00f3rka 5 : Przekszta\u0142ci\u0107\ndf_filtered = df.filter(df[\"status\"] == \"active\")\ndf_filtered.show()\n</code></pre> <p>Kom\u00f3rki SQL :</p> <pre><code>-- Kom\u00f3rka SQL : Utworzy\u0107 widok tymczasowy\nCREATE OR REPLACE TEMPORARY VIEW users AS\nSELECT * FROM csv.`dbfs:/FileStore/data/users.csv`\n\n-- Zapytanie SQL\nSELECT \n    YEAR(created_at) AS year,\n    COUNT(*) AS user_count\nFROM users\nGROUP BY YEAR(created_at)\nORDER BY year;\n</code></pre>"},{"location":"Cloud/Azure/PL/06-databricks/#wykonac-notebook","title":"Wykona\u0107 notebook","text":"<ul> <li>Run cell : Wykona\u0107 kom\u00f3rk\u0119</li> <li>Run all : Wykona\u0107 wszystkie kom\u00f3rki</li> <li>Run all above : Wykona\u0107 wszystkie kom\u00f3rki powy\u017cej</li> </ul>"},{"location":"Cloud/Azure/PL/06-databricks/#przetwarzanie-danych-ze-spark","title":"Przetwarzanie danych ze Spark","text":""},{"location":"Cloud/Azure/PL/06-databricks/#czytac-dane","title":"Czyta\u0107 dane","text":"<p>Z Data Lake Storage :</p> <pre><code># Czyta\u0107 CSV\ndf = spark.read.csv(\n    \"abfss://container@account.dfs.core.windows.net/data/users.csv\",\n    header=True,\n    inferSchema=True\n)\n\n# Czyta\u0107 Parquet\ndf = spark.read.parquet(\n    \"abfss://container@account.dfs.core.windows.net/data/users.parquet\"\n)\n\n# Czyta\u0107 JSON\ndf = spark.read.json(\n    \"abfss://container@account.dfs.core.windows.net/data/users.json\"\n)\n</code></pre> <p>Z Azure Blob Storage :</p> <pre><code># Skonfigurowa\u0107 dost\u0119p\nspark.conf.set(\n    \"fs.azure.account.key.accountname.blob.core.windows.net\",\n    \"your-account-key\"\n)\n\n# Czyta\u0107\ndf = spark.read.csv(\n    \"wasbs://container@accountname.blob.core.windows.net/data/users.csv\",\n    header=True\n)\n</code></pre>"},{"location":"Cloud/Azure/PL/06-databricks/#przeksztacac-dane","title":"Przekszta\u0142ca\u0107 dane","text":"<p>Filtrowa\u0107 :</p> <pre><code>df_filtered = df.filter(df[\"age\"] &gt; 18)\n</code></pre> <p>Wybiera\u0107 kolumny :</p> <pre><code>df_selected = df.select(\"id\", \"name\", \"email\")\n</code></pre> <p>Agregacje :</p> <pre><code>df_aggregated = df.groupBy(\"category\").agg({\n    \"amount\": \"sum\",\n    \"id\": \"count\"\n})\n</code></pre> <p>\u0141\u0105czy\u0107 :</p> <pre><code>df_joined = df1.join(df2, df1.id == df2.user_id, \"inner\")\n</code></pre>"},{"location":"Cloud/Azure/PL/06-databricks/#zapisywac-dane","title":"Zapisywa\u0107 dane","text":"<p>Do Data Lake Storage :</p> <pre><code># Zapisa\u0107 w Parquet\ndf.write.mode(\"overwrite\").parquet(\n    \"abfss://container@account.dfs.core.windows.net/processed/users.parquet\"\n)\n\n# Zapisa\u0107 w CSV\ndf.write.mode(\"overwrite\").csv(\n    \"abfss://container@account.dfs.core.windows.net/processed/users.csv\"\n)\n</code></pre>"},{"location":"Cloud/Azure/PL/06-databricks/#integracja-z-innymi-usugami","title":"Integracja z innymi us\u0142ugami","text":""},{"location":"Cloud/Azure/PL/06-databricks/#databricks-data-lake-storage","title":"Databricks + Data Lake Storage","text":"<p>Bezpo\u015bredni dost\u0119p :</p> <pre><code># Skonfigurowa\u0107 dost\u0119p\nspark.conf.set(\n    \"fs.azure.account.auth.type.account.dfs.core.windows.net\",\n    \"OAuth\"\n)\nspark.conf.set(\n    \"fs.azure.account.oauth.provider.type.account.dfs.core.windows.net\",\n    \"org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider\"\n)\n\n# Czyta\u0107\ndf = spark.read.parquet(\n    \"abfss://container@account.dfs.core.windows.net/data/users.parquet\"\n)\n</code></pre>"},{"location":"Cloud/Azure/PL/06-databricks/#databricks-azure-sql-database","title":"Databricks + Azure SQL Database","text":"<p>Czyta\u0107 z SQL Database :</p> <pre><code>df = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://server.database.windows.net:1433;database=db\") \\\n    .option(\"dbtable\", \"users\") \\\n    .option(\"user\", \"sqladmin\") \\\n    .option(\"password\", \"password\") \\\n    .load()\n</code></pre> <p>Zapisa\u0107 do SQL Database :</p> <pre><code>df.write \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://server.database.windows.net:1433;database=db\") \\\n    .option(\"dbtable\", \"users_processed\") \\\n    .option(\"user\", \"sqladmin\") \\\n    .option(\"password\", \"password\") \\\n    .mode(\"overwrite\") \\\n    .save()\n</code></pre>"},{"location":"Cloud/Azure/PL/06-databricks/#databricks-data-factory","title":"Databricks + Data Factory","text":"<p>Pipeline Data Factory : 1. \u0179r\u00f3d\u0142o : Azure Blob Storage 2. Dzia\u0142anie : Databricks Notebook 3. Sink : Azure SQL Database</p> <p>Konfiguracja : - Notebook path : <code>/Workspace/path/to/notebook</code> - Parameters : Przekaza\u0107 parametry</p>"},{"location":"Cloud/Azure/PL/06-databricks/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"Cloud/Azure/PL/06-databricks/#wydajnosc","title":"Wydajno\u015b\u0107","text":"<ol> <li>U\u017cywa\u0107 cache aby ponownie u\u017cywa\u0107 danych</li> <li>Partycjonowa\u0107 dane aby poprawi\u0107 wydajno\u015b\u0107</li> <li>Optymalizowa\u0107 przekszta\u0142cenia aby zmniejszy\u0107 czas</li> <li>U\u017cywa\u0107 odpowiedniej liczby workers</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#koszty","title":"Koszty","text":"<ol> <li>Zatrzymywa\u0107 klastry gdy nieu\u017cywane</li> <li>U\u017cywa\u0107 auto-termination aby oszcz\u0119dzi\u0107</li> <li>Monitorowa\u0107 koszty w Azure Cost Management</li> <li>U\u017cywa\u0107 mniejszych klastr\u00f3w do rozpocz\u0119cia</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#organizacja","title":"Organizacja","text":"<ol> <li>Organizowa\u0107 notebooki w folderach</li> <li>Nazywa\u0107 jasno notebooki i klastry</li> <li>Dokumentowa\u0107 kod</li> <li>Wersjonowa\u0107 z Git</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":"<ol> <li>U\u017cywa\u0107 Azure AD do uwierzytelniania</li> <li>Ogranicza\u0107 dost\u0119p z RBAC</li> <li>Szyfrowa\u0107 dane w tranzycie i w spoczynku</li> <li>Audytowa\u0107 dost\u0119p</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Cloud/Azure/PL/06-databricks/#przykad-1-kompletny-pipeline-etl","title":"Przyk\u0142ad 1 : Kompletny pipeline ETL","text":"<p>Notebook Databricks :</p> <pre><code># 1. Czyta\u0107 z Data Lake\ndf = spark.read.parquet(\n    \"abfss://container@account.dfs.core.windows.net/raw/users.parquet\"\n)\n\n# 2. Przekszta\u0142ci\u0107\ndf_processed = df \\\n    .filter(df[\"status\"] == \"active\") \\\n    .select(\"id\", \"name\", \"email\", \"created_at\") \\\n    .withColumn(\"year\", year(col(\"created_at\")))\n\n# 3. Zapisa\u0107 do Data Lake\ndf_processed.write.mode(\"overwrite\").parquet(\n    \"abfss://container@account.dfs.core.windows.net/processed/users.parquet\"\n)\n</code></pre>"},{"location":"Cloud/Azure/PL/06-databricks/#przykad-2-analiza-ze-spark-sql","title":"Przyk\u0142ad 2 : Analiza ze Spark SQL","text":"<pre><code># Utworzy\u0107 widok tymczasowy\ndf.createOrReplaceTempView(\"users\")\n\n# Zapytanie SQL\nresult = spark.sql(\"\"\"\n    SELECT \n        YEAR(created_at) AS year,\n        COUNT(*) AS user_count,\n        COUNT(DISTINCT email) AS unique_emails\n    FROM users\n    GROUP BY YEAR(created_at)\n    ORDER BY year\n\"\"\")\n\nresult.show()\n</code></pre>"},{"location":"Cloud/Azure/PL/06-databricks/#przykad-3-integracja-z-data-factory","title":"Przyk\u0142ad 3 : Integracja z Data Factory","text":"<ol> <li>Utworzy\u0107 notebook Databricks</li> <li>W Data Factory, doda\u0107 dzia\u0142anie \"Databricks Notebook\"</li> <li>Skonfigurowa\u0107 notebook</li> <li>Wykona\u0107 pipeline</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Databricks = Big Data z Apache Spark</li> <li>Notebooki Python/SQL do rozwoju</li> <li>Klastry auto-scaling dla wydajno\u015bci</li> <li>Natywna integracja z us\u0142ugami Azure</li> <li>P\u0142atne : Monitorowa\u0107 koszty</li> </ol>"},{"location":"Cloud/Azure/PL/06-databricks/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 7. Projekty praktyczne, aby tworzy\u0107 kompletne projekty z Azure.</p>"},{"location":"Cloud/Azure/PL/07-projets/","title":"7. Projekty praktyczne Azure","text":""},{"location":"Cloud/Azure/PL/07-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Stosowa\u0107 zdobyt\u0105 wiedz\u0119</li> <li>Tworzy\u0107 kompletne pipeline'y ETL</li> <li>Integrowa\u0107 z PowerBI</li> <li>Tworzy\u0107 projekty dla portfolio</li> <li>U\u017cywa\u0107 wielu us\u0142ug Azure</li> </ul>"},{"location":"Cloud/Azure/PL/07-projets/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Projekt 1 : Pipeline ETL Blob \u2192 SQL Database</li> <li>Projekt 2 : Data Lake z Synapse</li> <li>Projekt 3 : Analytics z PowerBI</li> <li>Projekt 4 : Kompletny zautomatyzowany pipeline</li> <li>Dobre praktyki dla portfolio</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#projekt-1-pipeline-etl-blob-sql-database","title":"Projekt 1 : Pipeline ETL Blob \u2192 SQL Database","text":""},{"location":"Cloud/Azure/PL/07-projets/#cel","title":"Cel","text":"<p>Utworzy\u0107 pipeline ETL kt\u00f3ry \u0142aduje pliki CSV z Blob Storage do SQL Database.</p>"},{"location":"Cloud/Azure/PL/07-projets/#architektura","title":"Architektura","text":"<pre><code>Blob Storage (CSV) \u2192 Data Factory \u2192 SQL Database \u2192 PowerBI\n</code></pre>"},{"location":"Cloud/Azure/PL/07-projets/#kroki","title":"Kroki","text":""},{"location":"Cloud/Azure/PL/07-projets/#1-przygotowac-dane","title":"1. Przygotowa\u0107 dane","text":"<p>Utworzy\u0107 kontener Blob Storage : - Nazwa : <code>raw-data</code> - Przes\u0142a\u0107 plik CSV testowy</p> <p>Przyk\u0142ad danych CSV : <pre><code>id,name,email,created_at,status\n1,John Doe,john@example.com,2024-01-01,active\n2,Jane Smith,jane@example.com,2024-01-02,inactive\n</code></pre></p>"},{"location":"Cloud/Azure/PL/07-projets/#2-utworzyc-baze-sql-database","title":"2. Utworzy\u0107 baz\u0119 SQL Database","text":"<ol> <li>Portal Azure \u2192 Utworzy\u0107 SQL Database</li> <li>Konfiguracja :</li> <li>Name : <code>analytics-db</code></li> <li>Server : Utworzy\u0107 nowy serwer</li> <li>Service tier : Basic (darmowe 12 miesi\u0119cy)</li> <li>Utworzy\u0107 baz\u0119</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#3-utworzyc-tabele-w-sql-database","title":"3. Utworzy\u0107 tabel\u0119 w SQL Database","text":"<pre><code>CREATE TABLE users (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at DATETIME2,\n    status VARCHAR(20)\n);\n</code></pre>"},{"location":"Cloud/Azure/PL/07-projets/#4-utworzyc-pipeline-data-factory","title":"4. Utworzy\u0107 pipeline Data Factory","text":"<ol> <li>Data Factory Studio \u2192 \"Author\" \u2192 \"Pipelines\"</li> <li>Utworzy\u0107 nowy pipeline : <code>LoadCSVToSQL</code></li> <li>Doda\u0107 dzia\u0142anie \"Copy Data\"</li> <li>Konfiguracja :</li> <li>Source : Azure Blob Storage (CSV)</li> <li>Sink : Azure SQL Database (tabela users)</li> <li>Opublikowa\u0107 pipeline</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#5-wykonac-pipeline","title":"5. Wykona\u0107 pipeline","text":"<ol> <li>Klikn\u0105\u0107 \"Trigger now\"</li> <li>Sprawdzi\u0107 wykonanie w \"Monitor\"</li> <li>Sprawdzi\u0107 dane w SQL Database</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#wynik","title":"Wynik","text":"<ul> <li>Pliki CSV za\u0142adowane w SQL Database</li> <li>Funkcjonalny pipeline ETL</li> <li>Gotowe do analytics z PowerBI</li> </ul>"},{"location":"Cloud/Azure/PL/07-projets/#projekt-2-data-lake-z-synapse","title":"Projekt 2 : Data Lake z Synapse","text":""},{"location":"Cloud/Azure/PL/07-projets/#cel_1","title":"Cel","text":"<p>Utworzy\u0107 kompletny Data Lake z ingerencj\u0105, przekszta\u0142caniem i analytics.</p>"},{"location":"Cloud/Azure/PL/07-projets/#architektura_1","title":"Architektura","text":"<pre><code>\u0179r\u00f3d\u0142a \u2192 Data Lake Storage (Raw) \u2192 Synapse (Transform) \u2192 Data Lake (Processed) \u2192 PowerBI\n                \u2193\n        Data Factory (Orkiestracja)\n</code></pre>"},{"location":"Cloud/Azure/PL/07-projets/#kroki_1","title":"Kroki","text":""},{"location":"Cloud/Azure/PL/07-projets/#1-struktura-data-lake-storage","title":"1. Struktura Data Lake Storage","text":"<pre><code>data-lake/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u251c\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 orders/\n\u2502   \u2514\u2500\u2500 products/\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 results/\n</code></pre>"},{"location":"Cloud/Azure/PL/07-projets/#2-utworzyc-obszar-roboczy-synapse","title":"2. Utworzy\u0107 obszar roboczy Synapse","text":"<ol> <li>Portal Azure \u2192 Utworzy\u0107 Azure Synapse Analytics</li> <li>Konfiguracja :</li> <li>Workspace name : <code>my-synapse-workspace</code></li> <li>Data Lake Storage : Utworzy\u0107 nowy</li> <li>Utworzy\u0107 obszar roboczy</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#3-pipeliney-data-factory-do-przeksztacania","title":"3. Pipeline'y Data Factory do przekszta\u0142cania","text":"<p>Pipeline dla users :</p> <ol> <li>Synapse Studio \u2192 \"Integrate\" \u2192 \"Pipelines\"</li> <li>Utworzy\u0107 pipeline : <code>TransformUsers</code></li> <li>Dzia\u0142ania :</li> <li>Source : Data Lake Storage (raw/users/)</li> <li>Data Flow : Przekszta\u0142ci\u0107 (filtrowa\u0107, czy\u015bci\u0107)</li> <li>Sink : Data Lake Storage (processed/users/)</li> <li>Opublikowa\u0107</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#4-tabele-synapse-do-analytics","title":"4. Tabele Synapse do analytics","text":"<pre><code>-- Utworzy\u0107 tabel\u0119 zewn\u0119trzn\u0105\nCREATE EXTERNAL TABLE users_processed (\n    id INT,\n    name VARCHAR(100),\n    email VARCHAR(100),\n    created_at DATETIME2\n)\nWITH (\n    LOCATION = 'processed/users/',\n    DATA_SOURCE = DataLakeStorage,\n    FILE_FORMAT = ParquetFormat\n);\n\n-- Zapytanie analityczne\nSELECT \n    YEAR(created_at) AS year,\n    COUNT(*) AS user_count\nFROM users_processed\nGROUP BY YEAR(created_at);\n</code></pre>"},{"location":"Cloud/Azure/PL/07-projets/#5-automatyzacja-z-triggers","title":"5. Automatyzacja z Triggers","text":"<ol> <li>Pipeline \u2192 \"Add trigger\" \u2192 \"New/Edit\"</li> <li>Typ : Schedule</li> <li>Recurrence : Daily</li> <li>Start time : 02:00</li> <li>Zapisa\u0107</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#wynik_1","title":"Wynik","text":"<ul> <li>Funkcjonalny Data Lake</li> <li>Zautomatyzowany pipeline</li> <li>Analytics z Synapse</li> <li>Kompletny projekt dla portfolio</li> </ul>"},{"location":"Cloud/Azure/PL/07-projets/#projekt-3-analytics-z-powerbi","title":"Projekt 3 : Analytics z PowerBI","text":""},{"location":"Cloud/Azure/PL/07-projets/#cel_2","title":"Cel","text":"<p>Utworzy\u0107 kompletny system analytics z PowerBI po\u0142\u0105czonym z Azure.</p>"},{"location":"Cloud/Azure/PL/07-projets/#kroki_2","title":"Kroki","text":""},{"location":"Cloud/Azure/PL/07-projets/#1-przygotowac-dane_1","title":"1. Przygotowa\u0107 dane","text":"<p>W SQL Database lub Synapse : - \u0141adowa\u0107 dane - Tworzy\u0107 widoki analityczne</p>"},{"location":"Cloud/Azure/PL/07-projets/#2-poaczyc-powerbi-z-azure-sql-database","title":"2. Po\u0142\u0105czy\u0107 PowerBI z Azure SQL Database","text":"<ol> <li>PowerBI Desktop \u2192 \"Get Data\"</li> <li>\"Azure\" \u2192 \"Azure SQL Database\"</li> <li>Konfiguracja :</li> <li>Server : <code>my-sql-server.database.windows.net</code></li> <li>Database : <code>analytics-db</code></li> <li>Authentication : Database</li> <li>Wybra\u0107 tabele lub widoki</li> <li>Klikn\u0105\u0107 \"Load\"</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#3-tworzyc-wizualizacje","title":"3. Tworzy\u0107 wizualizacje","text":"<p>Przyk\u0142ad : 1. Importowa\u0107 tabel\u0119 <code>users</code> 2. Utworzy\u0107 wykres : Liczba u\u017cytkownik\u00f3w na miesi\u0105c 3. Doda\u0107 filtry 4. Utworzy\u0107 dashboard</p>"},{"location":"Cloud/Azure/PL/07-projets/#4-opublikowac-na-powerbi-service","title":"4. Opublikowa\u0107 na PowerBI Service","text":"<ol> <li>PowerBI Desktop \u2192 \"Publish\"</li> <li>Wybra\u0107 obszar roboczy</li> <li>Opublikowa\u0107</li> <li>Dost\u0119p do raportu na powerbi.com</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#5-odswiezac-dane","title":"5. Od\u015bwie\u017ca\u0107 dane","text":"<ol> <li>PowerBI Service \u2192 Dataset \u2192 \"Schedule refresh\"</li> <li>Konfiguracja :</li> <li>Frequency : Daily</li> <li>Time : 03:00</li> <li>Zapisa\u0107</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#wynik_2","title":"Wynik","text":"<ul> <li>Analytics z PowerBI</li> <li>Interaktywne wizualizacje</li> <li>Automatyczne od\u015bwie\u017canie</li> <li>Kompletny projekt dla portfolio</li> </ul>"},{"location":"Cloud/Azure/PL/07-projets/#projekt-4-kompletny-zautomatyzowany-pipeline","title":"Projekt 4 : Kompletny zautomatyzowany pipeline","text":""},{"location":"Cloud/Azure/PL/07-projets/#cel_3","title":"Cel","text":"<p>Utworzy\u0107 kompletnie zautomatyzowany pipeline ETL z wieloma us\u0142ugami Azure.</p>"},{"location":"Cloud/Azure/PL/07-projets/#kompletna-architektura","title":"Kompletna architektura","text":"<pre><code>Plik CSV przes\u0142any \u2192 Blob Storage (raw/)\n    \u2193 (Event)\nAzure Function (Walidacja)\n    \u2193\nBlob Storage (validated/)\n    \u2193 (Trigger)\nData Factory Pipeline (Przekszta\u0142\u0107 CSV \u2192 Parquet)\n    \u2193\nData Lake Storage (processed/)\n    \u2193\nSynapse (Analytics)\n    \u2193\nSQL Database (Results)\n    \u2193\nPowerBI (Wizualizacja)\n</code></pre>"},{"location":"Cloud/Azure/PL/07-projets/#implementacja","title":"Implementacja","text":""},{"location":"Cloud/Azure/PL/07-projets/#1-azure-function-walidacji","title":"1. Azure Function walidacji","text":"<pre><code>import azure.functions as func\nimport logging\nimport csv\nfrom azure.storage.blob import BlobServiceClient\n\ndef main(blob: func.InputStream):\n    logging.info(f'Processing blob: {blob.name}')\n\n    # Czyta\u0107 blob\n    content = blob.read().decode('utf-8')\n    reader = csv.DictReader(content.splitlines())\n\n    # Walidowa\u0107\n    valid_rows = []\n    for row in reader:\n        if row.get('email') and '@' in row['email']:\n            valid_rows.append(row)\n\n    # Przes\u0142a\u0107 zwalidowane dane\n    if valid_rows:\n        # Przes\u0142a\u0107 do validated/\n        # ...\n\n    logging.info(f'Validated {len(valid_rows)} rows')\n</code></pre>"},{"location":"Cloud/Azure/PL/07-projets/#2-data-factory-pipeline-przeksztacania","title":"2. Data Factory Pipeline przekszta\u0142cania","text":"<p>Pipeline : 1. Source : Blob Storage (validated/) 2. Data Flow : Przekszta\u0142ci\u0107 (czy\u015bci\u0107, wzbogaca\u0107) 3. Sink : Data Lake Storage (processed/parquet/)</p>"},{"location":"Cloud/Azure/PL/07-projets/#3-synapse-do-analytics","title":"3. Synapse do analytics","text":"<pre><code>-- Utworzy\u0107 widok analityczny\nCREATE VIEW vw_user_analytics AS\nSELECT \n    u.id,\n    u.name,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n</code></pre>"},{"location":"Cloud/Azure/PL/07-projets/#4-powerbi-do-wizualizacji","title":"4. PowerBI do wizualizacji","text":"<ol> <li>Po\u0142\u0105czy\u0107 PowerBI z Synapse</li> <li>U\u017cywa\u0107 widoku <code>vw_user_analytics</code></li> <li>Tworzy\u0107 wizualizacje</li> <li>Opublikowa\u0107 raport</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#wynik_3","title":"Wynik","text":"<ul> <li>Kompletnie zautomatyzowany pipeline</li> <li>Automatyczna walidacja</li> <li>Automatyczne przekszta\u0142canie</li> <li>Analytics dost\u0119pne natychmiast</li> <li>Wizualizacje PowerBI</li> </ul>"},{"location":"Cloud/Azure/PL/07-projets/#dobre-praktyki-dla-portfolio","title":"Dobre praktyki dla portfolio","text":""},{"location":"Cloud/Azure/PL/07-projets/#dokumentacja","title":"Dokumentacja","text":"<p>Utworzy\u0107 README dla ka\u017cdego projektu :</p> <pre><code># Projekt : Pipeline ETL Azure\n\n## Opis\nZautomatyzowany pipeline ETL do przekszta\u0142cania danych CSV w Parquet.\n\n## Architektura\n- Blob Storage : Przechowywanie\n- Data Factory : Przekszta\u0142canie\n- SQL Database : Baza danych\n- PowerBI : Wizualizacja\n\n## Wyniki\n- Redukcja koszt\u00f3w o 50%\n- Czas przetwarzania zmniejszony o 70%\n</code></pre>"},{"location":"Cloud/Azure/PL/07-projets/#wizualizacje","title":"Wizualizacje","text":"<p>Tworzy\u0107 diagramy : - Architektura systemu - Przep\u0142yw danych - Schemat danych</p> <p>Narz\u0119dzia : - Draw.io - Lucidchart - Diagramy ASCII w README</p>"},{"location":"Cloud/Azure/PL/07-projets/#metryki","title":"Metryki","text":"<p>Uwzgl\u0119dnia\u0107 metryki : - Czas wykonania przed/po - Koszty przed/po - Wolumen przetworzonych danych - Wydajno\u015b\u0107 zapyta\u0144</p>"},{"location":"Cloud/Azure/PL/07-projets/#kod","title":"Kod","text":"<p>Dobre praktyki : - Kod skomentowany - Zmienne \u015brodowiskowe do konfiguracji - Obs\u0142uga b\u0142\u0119d\u00f3w - Logowanie</p>"},{"location":"Cloud/Azure/PL/07-projets/#github","title":"GitHub","text":"<p>Utworzy\u0107 repozytorium : - README z dokumentacj\u0105 - Skrypty Data Factory (JSON) - Skrypty SQL - Konfiguracja - Diagramy</p>"},{"location":"Cloud/Azure/PL/07-projets/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Projekty praktyczne : Niezb\u0119dne dla portfolio</li> <li>Dokumentacja : Wyja\u015bnia\u0107 architektur\u0119 i wyniki</li> <li>Metryki : Pokazywa\u0107 wp\u0142yw (wydajno\u015b\u0107, koszty)</li> <li>Czysty kod : Skomentowany i zorganizowany</li> <li>GitHub : Dzieli\u0107 si\u0119 projektami</li> </ol>"},{"location":"Cloud/Azure/PL/07-projets/#zasoby","title":"\ud83d\udd17 Zasoby","text":"<ul> <li>Azure Architecture Center</li> <li>Azure Solutions</li> <li>GitHub Azure Examples</li> </ul> <p>Gratulacje ! Uko\u0144czy\u0142e\u015b formacj\u0119 Azure dla Data Analyst. Mo\u017cesz teraz tworzy\u0107 kompletne projekty na Azure u\u017cywaj\u0105c dost\u0119pnych darmowych zasob\u00f3w.</p>"},{"location":"Docker/EN/","title":"Docker Training for Data Analyst","text":""},{"location":"Docker/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through learning Docker as a Data Analyst. Docker is a containerization platform that allows you to create, deploy, and run applications in isolated containers.</p>"},{"location":"Docker/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand Docker and containerization</li> <li>Install Docker</li> <li>Create and manage containers</li> <li>Build Docker images</li> <li>Use Docker Compose</li> <li>Integrate Docker into your data workflows</li> <li>Create practical projects for your portfolio</li> </ul>"},{"location":"Docker/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 Docker Desktop : Free for personal/educational use - \u2705 Docker Hub : Free public registry - \u2705 Official Documentation : Complete free guides - \u2705 Online Tutorials : Free resources</p> <p>Total Budget: $0</p>"},{"location":"Docker/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"Docker/EN/#1-docker-getting-started","title":"1. Docker Getting Started","text":"<ul> <li>Install Docker</li> <li>Basic concepts</li> <li>First containers</li> <li>Essential commands</li> </ul>"},{"location":"Docker/EN/#2-containers","title":"2. Containers","text":"<ul> <li>Create containers</li> <li>Manage lifecycle</li> <li>Execute commands</li> <li>Logs and debugging</li> </ul>"},{"location":"Docker/EN/#3-docker-images","title":"3. Docker Images","text":"<ul> <li>Understand images</li> <li>Download images</li> <li>Create custom images</li> <li>Manage images</li> </ul>"},{"location":"Docker/EN/#4-dockerfile","title":"4. Dockerfile","text":"<ul> <li>Write a Dockerfile</li> <li>Best practices</li> <li>Optimize images</li> <li>Multi-stage builds</li> </ul>"},{"location":"Docker/EN/#5-docker-compose","title":"5. Docker Compose","text":"<ul> <li>Orchestrate multiple containers</li> <li>docker-compose.yml file</li> <li>Services and networks</li> <li>Environment variables</li> </ul>"},{"location":"Docker/EN/#6-volumes-and-networks","title":"6. Volumes and Networks","text":"<ul> <li>Manage volumes</li> <li>Create networks</li> <li>Share data</li> <li>Data persistence</li> </ul>"},{"location":"Docker/EN/#7-best-practices","title":"7. Best Practices","text":"<ul> <li>Security</li> <li>Performance</li> <li>Organization</li> <li>Maintenance</li> </ul>"},{"location":"Docker/EN/#8-practical-projects","title":"8. Practical Projects","text":"<ul> <li>Containerize a Python application</li> <li>Data pipeline with Docker</li> <li>Complete stack with Docker Compose</li> <li>Portfolio projects</li> </ul>"},{"location":"Docker/EN/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"Docker/EN/#prerequisites","title":"Prerequisites","text":"<ul> <li>Operating System : Windows, Linux, or macOS</li> <li>4 GB RAM : Minimum recommended</li> <li>Disk Space : 20 GB free</li> </ul>"},{"location":"Docker/EN/#quick-installation","title":"Quick Installation","text":"<p>Windows/Mac: 1. Download Docker Desktop: https://www.docker.com/products/docker-desktop 2. Install and launch Docker Desktop 3. Verify installation: <code>docker --version</code></p> <p>Linux: <pre><code># Install Docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n\n# Start Docker\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# Verify\ndocker --version\n</code></pre></p>"},{"location":"Docker/EN/#first-container","title":"First Container","text":"<pre><code># Run Hello World container\ndocker run hello-world\n\n# Run interactive container\ndocker run -it ubuntu bash\n</code></pre>"},{"location":"Docker/EN/#use-cases-for-data-analyst","title":"\ud83d\udcca Use Cases for Data Analyst","text":"<ul> <li>Reproducible environments : Same environment everywhere</li> <li>Isolation : Separate dependencies</li> <li>Deployment : Deploy applications easily</li> <li>CI/CD : Integrate into pipelines</li> <li>Data Science : Isolated Python/R environments</li> </ul>"},{"location":"Docker/EN/#free-resources","title":"\ud83d\udcda Free Resources","text":""},{"location":"Docker/EN/#official-documentation","title":"Official Documentation","text":"<ul> <li>Docker Documentation : https://docs.docker.com/</li> <li>Docker Hub : https://hub.docker.com/</li> <li>Docker Playground : https://labs.play-with-docker.com/</li> </ul>"},{"location":"Docker/EN/#certifications-optional","title":"\ud83c\udf93 Certifications (Optional)","text":""},{"location":"Docker/EN/#docker-certified-associate-dca","title":"Docker Certified Associate (DCA)","text":"<ul> <li>Cost : ~$195</li> <li>Preparation : Free documentation</li> <li>Duration : 2-4 weeks</li> <li>Level : Intermediate</li> </ul>"},{"location":"Docker/EN/01-getting-started/","title":"1. Docker Getting Started","text":""},{"location":"Docker/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Docker and containerization</li> <li>Install Docker</li> <li>Understand basic concepts</li> <li>Run your first container</li> </ul>"},{"location":"Docker/EN/01-getting-started/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Docker</li> <li>Installation</li> <li>Basic Concepts</li> <li>First Containers</li> <li>Essential Commands</li> </ol>"},{"location":"Docker/EN/01-getting-started/#introduction-to-docker","title":"Introduction to Docker","text":""},{"location":"Docker/EN/01-getting-started/#what-is-docker","title":"What is Docker?","text":"<p>Docker = Containerization platform</p> <ul> <li>Containers : Isolated and lightweight environments</li> <li>Portable : Works everywhere (Windows, Linux, macOS)</li> <li>Efficient : Uses fewer resources than VMs</li> <li>Fast : Starts in seconds</li> </ul>"},{"location":"Docker/EN/01-getting-started/#why-docker-for-data-analyst","title":"Why Docker for Data Analyst?","text":"<ul> <li>Reproducibility : Same environment everywhere</li> <li>Isolation : Separate Python/R dependencies</li> <li>Simplicity : Easy to share and deploy</li> <li>Performance : Faster than VMs</li> </ul>"},{"location":"Docker/EN/01-getting-started/#installation","title":"Installation","text":""},{"location":"Docker/EN/01-getting-started/#windows","title":"Windows","text":"<ol> <li>Go to: https://www.docker.com/products/docker-desktop</li> <li>Download Docker Desktop for Windows</li> <li>Install the <code>.exe</code> file</li> <li>Restart if needed</li> </ol>"},{"location":"Docker/EN/01-getting-started/#linux","title":"Linux","text":"<pre><code># Update packages\nsudo apt update\n\n# Install dependencies\nsudo apt install apt-transport-https ca-certificates curl gnupg lsb-release\n\n# Add Docker GPG key\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Add Docker repository\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io\n\n# Start Docker\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# Verify\ndocker --version\n</code></pre>"},{"location":"Docker/EN/01-getting-started/#macos","title":"macOS","text":"<ol> <li>Go to: https://www.docker.com/products/docker-desktop</li> <li>Download Docker Desktop for Mac</li> <li>Install the <code>.dmg</code> file</li> <li>Open Docker from Applications</li> </ol>"},{"location":"Docker/EN/01-getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"Docker/EN/01-getting-started/#images","title":"Images","text":"<p>Image = Read-only template for creating containers</p> <ul> <li>Template : Contains OS, applications, dependencies</li> <li>Immutable : Doesn't change once created</li> <li>Lightweight : Shares common layers</li> </ul>"},{"location":"Docker/EN/01-getting-started/#containers","title":"Containers","text":"<p>Container = Executable instance of an image</p> <ul> <li>Isolated : Separate environment</li> <li>Ephemeral : Can be created/destroyed easily</li> <li>Portable : Works anywhere Docker is installed</li> </ul>"},{"location":"Docker/EN/01-getting-started/#first-containers","title":"First Containers","text":""},{"location":"Docker/EN/01-getting-started/#hello-world","title":"Hello World","text":"<pre><code># Run Hello World container\ndocker run hello-world\n</code></pre>"},{"location":"Docker/EN/01-getting-started/#interactive-container","title":"Interactive Container","text":"<pre><code># Run interactive Ubuntu container\ndocker run -it ubuntu bash\n\n# Inside container\nls\npwd\nexit\n</code></pre>"},{"location":"Docker/EN/01-getting-started/#background-container","title":"Background Container","text":"<pre><code># Run container in background\ndocker run -d --name my-container nginx\n\n# See running containers\ndocker ps\n\n# See logs\ndocker logs my-container\n\n# Stop container\ndocker stop my-container\n</code></pre>"},{"location":"Docker/EN/01-getting-started/#essential-commands","title":"Essential Commands","text":""},{"location":"Docker/EN/01-getting-started/#container-management","title":"Container Management","text":"<pre><code># List running containers\ndocker ps\n\n# List all containers\ndocker ps -a\n\n# Start container\ndocker start my-container\n\n# Stop container\ndocker stop my-container\n\n# Remove container\ndocker rm my-container\n</code></pre>"},{"location":"Docker/EN/01-getting-started/#image-management","title":"Image Management","text":"<pre><code># List images\ndocker images\n\n# Download image\ndocker pull ubuntu\n\n# Remove image\ndocker rmi ubuntu\n</code></pre>"},{"location":"Docker/EN/01-getting-started/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Docker = Containerization to isolate applications</li> <li>Images are templates, Containers are instances</li> <li>Docker Hub to find images</li> <li>Basic commands : run, ps, stop, rm</li> <li>Portable : Works everywhere</li> </ol>"},{"location":"Docker/EN/01-getting-started/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 2. Containers to deepen container management.</p>"},{"location":"Docker/EN/02-containers/","title":"2. Docker Containers","text":""},{"location":"Docker/EN/02-containers/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create and manage containers</li> <li>Understand lifecycle</li> <li>Execute commands</li> <li>Manage logs</li> <li>Configure containers</li> </ul>"},{"location":"Docker/EN/02-containers/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Container Lifecycle</li> <li>Create Containers</li> <li>Execute Commands</li> <li>Logs and Debugging</li> <li>Configuration</li> </ol>"},{"location":"Docker/EN/02-containers/#container-lifecycle","title":"Container Lifecycle","text":""},{"location":"Docker/EN/02-containers/#container-states","title":"Container States","text":"<ol> <li>Created : Container created but not started</li> <li>Running : Container running</li> <li>Paused : Container paused</li> <li>Stopped : Container stopped</li> <li>Removed : Container removed</li> </ol>"},{"location":"Docker/EN/02-containers/#lifecycle-commands","title":"Lifecycle Commands","text":"<pre><code># Create a container\ndocker create --name my-container ubuntu\n\n# Start a container\ndocker start my-container\n\n# Stop a container\ndocker stop my-container\n\n# Restart a container\ndocker restart my-container\n\n# Pause\ndocker pause my-container\n\n# Resume\ndocker unpause my-container\n\n# Remove a container\ndocker rm my-container\n</code></pre>"},{"location":"Docker/EN/02-containers/#create-containers","title":"Create Containers","text":""},{"location":"Docker/EN/02-containers/#create-with-docker-run","title":"Create with docker run","text":"<pre><code># Create and start a container\ndocker run ubuntu echo \"Hello\"\n\n# Create without starting\ndocker create --name my-container ubuntu\n\n# Create with custom name\ndocker run --name my-app ubuntu\n</code></pre>"},{"location":"Docker/EN/02-containers/#important-options","title":"Important Options","text":"<pre><code># Interactive mode\ndocker run -it ubuntu bash\n\n# Detached mode (background)\ndocker run -d nginx\n\n# Expose a port\ndocker run -p 8080:80 nginx\n\n# Mount a volume\ndocker run -v /host/path:/container/path ubuntu\n\n# Environment variables\ndocker run -e MY_VAR=value ubuntu\n\n# Container name\ndocker run --name my-container ubuntu\n</code></pre>"},{"location":"Docker/EN/02-containers/#execute-commands","title":"Execute Commands","text":""},{"location":"Docker/EN/02-containers/#execute-in-running-container","title":"Execute in Running Container","text":"<pre><code># Execute a command\ndocker exec my-container ls\n\n# Interactive mode\ndocker exec -it my-container bash\n\n# Execute Python\ndocker exec -it my-container python\n</code></pre>"},{"location":"Docker/EN/02-containers/#execute-on-startup","title":"Execute on Startup","text":"<pre><code># Default command\ndocker run ubuntu echo \"Hello\"\n\n# Override command\ndocker run ubuntu ls -la\n\n# Execute a script\ndocker run -v $(pwd):/app ubuntu bash /app/script.sh\n</code></pre>"},{"location":"Docker/EN/02-containers/#logs-and-debugging","title":"Logs and Debugging","text":""},{"location":"Docker/EN/02-containers/#view-logs","title":"View Logs","text":"<pre><code># Container logs\ndocker logs my-container\n\n# Follow logs (tail -f)\ndocker logs -f my-container\n\n# Last lines\ndocker logs --tail 100 my-container\n\n# With timestamp\ndocker logs -t my-container\n</code></pre>"},{"location":"Docker/EN/02-containers/#inspect-a-container","title":"Inspect a Container","text":"<pre><code># Complete information\ndocker inspect my-container\n\n# Specific information\ndocker inspect --format='{{.State.Status}}' my-container\n\n# Network configuration\ndocker inspect --format='{{.NetworkSettings.IPAddress}}' my-container\n</code></pre>"},{"location":"Docker/EN/02-containers/#statistics","title":"Statistics","text":"<pre><code># Real-time statistics\ndocker stats\n\n# Container statistics\ndocker stats my-container\n\n# Statistics without streaming\ndocker stats --no-stream\n</code></pre>"},{"location":"Docker/EN/02-containers/#configuration","title":"Configuration","text":""},{"location":"Docker/EN/02-containers/#environment-variables","title":"Environment Variables","text":"<pre><code># One variable\ndocker run -e MY_VAR=value ubuntu\n\n# Multiple variables\ndocker run -e VAR1=value1 -e VAR2=value2 ubuntu\n\n# .env file\ndocker run --env-file .env ubuntu\n</code></pre>"},{"location":"Docker/EN/02-containers/#ports","title":"Ports","text":"<pre><code># Expose a port\ndocker run -p 8080:80 nginx\n\n# Expose multiple ports\ndocker run -p 8080:80 -p 3306:3306 my-app\n\n# Expose all ports\ndocker run -P nginx\n</code></pre>"},{"location":"Docker/EN/02-containers/#volumes","title":"Volumes","text":"<pre><code># Named volume\ndocker run -v my-volume:/data ubuntu\n\n# Bind mount\ndocker run -v /host/path:/container/path ubuntu\n\n# Anonymous volume\ndocker run -v /data ubuntu\n</code></pre>"},{"location":"Docker/EN/02-containers/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Lifecycle : Created \u2192 Running \u2192 Stopped \u2192 Removed</li> <li>docker run : Creates and starts</li> <li>docker exec : Executes in running container</li> <li>docker logs : View logs</li> <li>Configuration : Variables, ports, volumes</li> </ol>"},{"location":"Docker/EN/02-containers/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 3. Docker Images to learn image management.</p>"},{"location":"Docker/EN/03-images/","title":"3. Docker Images","text":""},{"location":"Docker/EN/03-images/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Docker images</li> <li>Download images</li> <li>Create custom images</li> <li>Manage images</li> <li>Optimize images</li> </ul>"},{"location":"Docker/EN/03-images/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Images</li> <li>Download Images</li> <li>Create Images</li> <li>Manage Images</li> <li>Optimization</li> </ol>"},{"location":"Docker/EN/03-images/#introduction-to-images","title":"Introduction to Images","text":""},{"location":"Docker/EN/03-images/#what-is-an-image","title":"What is an Image?","text":"<p>Image = Read-only template</p> <ul> <li>Template : For creating containers</li> <li>Layering : Composed of layers</li> <li>Immutable : Doesn't change</li> <li>Shared : Multiple containers can use the same image</li> </ul>"},{"location":"Docker/EN/03-images/#image-structure","title":"Image Structure","text":"<pre><code>Image\n\u251c\u2500\u2500 Layer 1 : Base OS (Ubuntu)\n\u251c\u2500\u2500 Layer 2 : System tools\n\u251c\u2500\u2500 Layer 3 : Python\n\u2514\u2500\u2500 Layer 4 : Your application\n</code></pre>"},{"location":"Docker/EN/03-images/#download-images","title":"Download Images","text":""},{"location":"Docker/EN/03-images/#docker-hub","title":"Docker Hub","text":"<p>Docker Hub = Public image registry</p> <ul> <li>Official images : python, postgres, nginx, etc.</li> <li>Community images : Created by community</li> <li>Free : For public use</li> </ul>"},{"location":"Docker/EN/03-images/#download-an-image","title":"Download an Image","text":"<pre><code># Download an image\ndocker pull python:3.11\n\n# Download latest version\ndocker pull python:latest\n\n# Download specific version\ndocker pull python:3.11-slim\n\n# Search images\ndocker search python\n</code></pre>"},{"location":"Docker/EN/03-images/#popular-images-for-data-analyst","title":"Popular Images for Data Analyst","text":"<pre><code># Python\ndocker pull python:3.11\n\n# Jupyter Notebook\ndocker pull jupyter/scipy-notebook\n\n# PostgreSQL\ndocker pull postgres:15\n\n# MySQL\ndocker pull mysql:8.0\n\n# Redis\ndocker pull redis:7\n</code></pre>"},{"location":"Docker/EN/03-images/#create-images","title":"Create Images","text":""},{"location":"Docker/EN/03-images/#with-dockerfile","title":"With Dockerfile","text":"<p>Create a Dockerfile:</p> <pre><code># Dockerfile\nFROM python:3.11\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]\n</code></pre> <p>Build the image:</p> <pre><code># Build an image\ndocker build -t my-app:latest .\n\n# With specific tag\ndocker build -t my-app:v1.0 .\n\n# From specific Dockerfile\ndocker build -f Dockerfile.prod -t my-app:prod .\n</code></pre>"},{"location":"Docker/EN/03-images/#commit-from-container","title":"Commit from Container","text":"<pre><code># Create a container\ndocker run -it ubuntu bash\n\n# Make modifications in container\napt update\napt install python3\n\n# Create image from container\ndocker commit container-id my-image:tag\n</code></pre>"},{"location":"Docker/EN/03-images/#manage-images","title":"Manage Images","text":""},{"location":"Docker/EN/03-images/#list-images","title":"List Images","text":"<pre><code># List all images\ndocker images\n\n# Filter by name\ndocker images python\n\n# Show only IDs\ndocker images -q\n</code></pre>"},{"location":"Docker/EN/03-images/#remove-images","title":"Remove Images","text":"<pre><code># Remove an image\ndocker rmi my-image:tag\n\n# Remove by ID\ndocker rmi image-id\n\n# Remove unused images\ndocker image prune\n\n# Remove all images\ndocker rmi $(docker images -q)\n</code></pre>"},{"location":"Docker/EN/03-images/#tag-images","title":"Tag Images","text":"<pre><code># Create a tag\ndocker tag my-image:latest my-image:v1.0\n\n# Tag for Docker Hub\ndocker tag my-image:latest username/my-image:latest\n</code></pre>"},{"location":"Docker/EN/03-images/#inspect-an-image","title":"Inspect an Image","text":"<pre><code># Complete information\ndocker inspect my-image\n\n# Layer history\ndocker history my-image\n\n# Image size\ndocker images my-image\n</code></pre>"},{"location":"Docker/EN/03-images/#optimization","title":"Optimization","text":""},{"location":"Docker/EN/03-images/#lightweight-images","title":"Lightweight Images","text":"<p>Use slim images:</p> <pre><code># Instead of\nFROM python:3.11\n\n# Use\nFROM python:3.11-slim\n</code></pre> <p>Multi-stage builds:</p> <pre><code># Stage 1 : Build\nFROM python:3.11 as builder\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user -r requirements.txt\n\n# Stage 2 : Runtime\nFROM python:3.11-slim\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY . .\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/EN/03-images/#reduce-size","title":"Reduce Size","text":"<p>Best practices: 1. Use <code>.dockerignore</code> 2. Combine RUN commands 3. Use lightweight base images 4. Clean caches</p>"},{"location":"Docker/EN/03-images/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Images are templates for containers</li> <li>Docker Hub to find images</li> <li>Dockerfile to create images</li> <li>Layering enables sharing</li> <li>Optimization reduces size</li> </ol>"},{"location":"Docker/EN/03-images/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 4. Dockerfile to learn writing Dockerfiles.</p>"},{"location":"Docker/EN/04-dockerfile/","title":"4. Dockerfile","text":""},{"location":"Docker/EN/04-dockerfile/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Write a Dockerfile</li> <li>Understand instructions</li> <li>Optimize Dockerfiles</li> <li>Use multi-stage builds</li> <li>Best practices</li> </ul>"},{"location":"Docker/EN/04-dockerfile/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Dockerfile</li> <li>Basic Instructions</li> <li>Complete Dockerfile</li> <li>Optimization</li> <li>Multi-stage Builds</li> </ol>"},{"location":"Docker/EN/04-dockerfile/#introduction-to-dockerfile","title":"Introduction to Dockerfile","text":""},{"location":"Docker/EN/04-dockerfile/#what-is-a-dockerfile","title":"What is a Dockerfile?","text":"<p>Dockerfile = Instructions to build an image</p> <ul> <li>Text : Simple text file</li> <li>Instructions : Each line is an instruction</li> <li>Automation : Automates image creation</li> <li>Versioned : Can be versioned with Git</li> </ul>"},{"location":"Docker/EN/04-dockerfile/#basic-structure","title":"Basic Structure","text":"<pre><code># Comment\nFROM base-image\nRUN command\nCOPY source destination\nCMD [\"executable\", \"param\"]\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#basic-instructions","title":"Basic Instructions","text":""},{"location":"Docker/EN/04-dockerfile/#from","title":"FROM","text":"<p>Defines base image:</p> <pre><code>FROM python:3.11\nFROM ubuntu:22.04\nFROM alpine:latest\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#workdir","title":"WORKDIR","text":"<p>Defines working directory:</p> <pre><code>WORKDIR /app\nWORKDIR /usr/src/app\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#copy-add","title":"COPY / ADD","text":"<p>Copy files:</p> <pre><code># COPY (recommended)\nCOPY requirements.txt .\nCOPY . .\n\n# ADD (with automatic extraction)\nADD archive.tar.gz /app\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#run","title":"RUN","text":"<p>Execute commands:</p> <pre><code>RUN apt update\nRUN pip install -r requirements.txt\n\n# Combine to reduce layers\nRUN apt update &amp;&amp; \\\n    apt install -y python3 &amp;&amp; \\\n    apt clean\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#cmd-entrypoint","title":"CMD / ENTRYPOINT","text":"<p>Default command:</p> <pre><code># CMD (can be overridden)\nCMD [\"python\", \"app.py\"]\n\n# ENTRYPOINT (cannot be overridden)\nENTRYPOINT [\"python\"]\nCMD [\"app.py\"]\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#env","title":"ENV","text":"<p>Environment variables:</p> <pre><code>ENV PYTHONUNBUFFERED=1\nENV APP_ENV=production\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#expose","title":"EXPOSE","text":"<p>Expose ports:</p> <pre><code>EXPOSE 8080\nEXPOSE 3306\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#complete-dockerfile","title":"Complete Dockerfile","text":""},{"location":"Docker/EN/04-dockerfile/#example-1-python-application","title":"Example 1 : Python Application","text":"<pre><code># Base image\nFROM python:3.11-slim\n\n# Working directory\nWORKDIR /app\n\n# Environment variables\nENV PYTHONUNBUFFERED=1\n\n# Copy requirements\nCOPY requirements.txt .\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy code\nCOPY . .\n\n# Expose port\nEXPOSE 8000\n\n# Default command\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#example-2-application-with-data","title":"Example 2 : Application with Data","text":"<pre><code>FROM python:3.11\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt update &amp;&amp; \\\n    apt install -y postgresql-client &amp;&amp; \\\n    apt clean\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copy application\nCOPY . .\n\n# Create volume for data\nVOLUME [\"/app/data\"]\n\n# Expose port\nEXPOSE 8080\n\n# Command\nCMD [\"python\", \"main.py\"]\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#optimization","title":"Optimization","text":""},{"location":"Docker/EN/04-dockerfile/#dockerignore","title":".dockerignore","text":"<p>Create a <code>.dockerignore</code> file:</p> <pre><code>__pycache__\n*.pyc\n.git\n.env\nnode_modules\n*.log\n.DS_Store\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#reduce-layers","title":"Reduce Layers","text":"<p>Bad: <pre><code>RUN apt update\nRUN apt install -y python3\nRUN apt install -y pip\nRUN apt clean\n</code></pre></p> <p>Good: <pre><code>RUN apt update &amp;&amp; \\\n    apt install -y python3 pip &amp;&amp; \\\n    apt clean\n</code></pre></p>"},{"location":"Docker/EN/04-dockerfile/#instruction-order","title":"Instruction Order","text":"<p>Put instructions that change little first:</p> <pre><code># First dependencies (change little)\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Then code (changes often)\nCOPY . .\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#multi-stage-builds","title":"Multi-stage Builds","text":""},{"location":"Docker/EN/04-dockerfile/#why-multi-stage","title":"Why Multi-stage?","text":"<ul> <li>Reduce size : Final image smaller</li> <li>Security : Exclude build tools</li> <li>Performance : Optimize builds</li> </ul>"},{"location":"Docker/EN/04-dockerfile/#example","title":"Example","text":"<pre><code># Stage 1 : Build\nFROM python:3.11 as builder\nWORKDIR /build\nCOPY requirements.txt .\nRUN pip install --user -r requirements.txt\n\n# Stage 2 : Runtime\nFROM python:3.11-slim\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY . .\nENV PATH=/root/.local/bin:$PATH\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/EN/04-dockerfile/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Dockerfile automates image creation</li> <li>Instructions : FROM, RUN, COPY, CMD</li> <li>Optimization : Reduce layers</li> <li>Multi-stage : Lighter images</li> <li>.dockerignore : Exclude files</li> </ol>"},{"location":"Docker/EN/04-dockerfile/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 5. Docker Compose to orchestrate multiple containers.</p>"},{"location":"Docker/EN/05-docker-compose/","title":"5. Docker Compose","text":""},{"location":"Docker/EN/05-docker-compose/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Docker Compose</li> <li>Orchestrate multiple containers</li> <li>Create docker-compose.yml files</li> <li>Manage services and networks</li> <li>Use environment variables</li> </ul>"},{"location":"Docker/EN/05-docker-compose/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Docker Compose</li> <li>docker-compose.yml File</li> <li>Services</li> <li>Networks and Volumes</li> <li>Commands</li> </ol>"},{"location":"Docker/EN/05-docker-compose/#introduction-to-docker-compose","title":"Introduction to Docker Compose","text":""},{"location":"Docker/EN/05-docker-compose/#what-is-docker-compose","title":"What is Docker Compose?","text":"<p>Docker Compose = Tool to orchestrate multiple containers</p> <ul> <li>Multi-containers : Manages multiple containers</li> <li>Configuration : Simple YAML file</li> <li>Orchestration : Starts/stops all services</li> <li>Networks : Automatically creates networks</li> </ul>"},{"location":"Docker/EN/05-docker-compose/#why-docker-compose","title":"Why Docker Compose?","text":"<ul> <li>Simplicity : One file for everything</li> <li>Reproducibility : Same environment everywhere</li> <li>Development : Complete local stack</li> <li>Production : Simplified deployment</li> </ul>"},{"location":"Docker/EN/05-docker-compose/#docker-composeyml-file","title":"docker-compose.yml File","text":""},{"location":"Docker/EN/05-docker-compose/#basic-structure","title":"Basic Structure","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    image: nginx\n    ports:\n      - \"8080:80\"\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_PASSWORD: password\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#complete-example","title":"Complete Example","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8080:8000\"\n    environment:\n      - DATABASE_URL=postgresql://db:5432/mydb\n    depends_on:\n      - db\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: mydb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#services","title":"Services","text":""},{"location":"Docker/EN/05-docker-compose/#define-a-service","title":"Define a Service","text":"<pre><code>services:\n  app:\n    image: python:3.11\n    command: python app.py\n    working_dir: /app\n    volumes:\n      - .:/app\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#build-vs-image","title":"Build vs Image","text":"<pre><code># Use existing image\nservices:\n  web:\n    image: nginx:latest\n\n# Build from Dockerfile\nservices:\n  app:\n    build: .\n    # or\n    build:\n      context: .\n      dockerfile: Dockerfile.prod\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#dependencies","title":"Dependencies","text":"<pre><code>services:\n  app:\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres\n\n  redis:\n    image: redis\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#networks-and-volumes","title":"Networks and Volumes","text":""},{"location":"Docker/EN/05-docker-compose/#networks","title":"Networks","text":"<pre><code>services:\n  app:\n    networks:\n      - frontend\n      - backend\n\n  db:\n    networks:\n      - backend\n\nnetworks:\n  frontend:\n  backend:\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#volumes","title":"Volumes","text":"<pre><code>services:\n  db:\n    volumes:\n      - db-data:/var/lib/postgresql/data\n      - ./backup:/backup\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#commands","title":"Commands","text":""},{"location":"Docker/EN/05-docker-compose/#start-services","title":"Start Services","text":"<pre><code># Start all services\ndocker-compose up\n\n# In background\ndocker-compose up -d\n\n# Rebuild images\ndocker-compose up --build\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#stop-services","title":"Stop Services","text":"<pre><code># Stop services\ndocker-compose stop\n\n# Stop and remove\ndocker-compose down\n\n# Remove with volumes\ndocker-compose down -v\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#service-management","title":"Service Management","text":"<pre><code># See running services\ndocker-compose ps\n\n# View logs\ndocker-compose logs\n\n# Logs of a service\ndocker-compose logs web\n\n# Execute a command\ndocker-compose exec web bash\n\n# Restart a service\ndocker-compose restart web\n</code></pre>"},{"location":"Docker/EN/05-docker-compose/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Docker Compose orchestrates multiple containers</li> <li>docker-compose.yml defines configuration</li> <li>Services are containers</li> <li>Networks and Volumes for communication and data</li> <li>Commands : up, down, logs, exec</li> </ol>"},{"location":"Docker/EN/05-docker-compose/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 6. Volumes and Networks to deepen.</p>"},{"location":"Docker/EN/06-volumes-networks/","title":"6. Docker Volumes and Networks","text":""},{"location":"Docker/EN/06-volumes-networks/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Docker volumes</li> <li>Manage data persistence</li> <li>Create and manage networks</li> <li>Share data between containers</li> <li>Configure network communication</li> </ul>"},{"location":"Docker/EN/06-volumes-networks/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Volumes</li> <li>Bind Mounts</li> <li>Networks</li> <li>Container Communication</li> <li>Practical Examples</li> </ol>"},{"location":"Docker/EN/06-volumes-networks/#volumes","title":"Volumes","text":""},{"location":"Docker/EN/06-volumes-networks/#what-is-a-volume","title":"What is a Volume?","text":"<p>Volume = Persistent storage for data</p> <ul> <li>Persistent : Survives container deletion</li> <li>Managed by Docker : Stored in <code>/var/lib/docker/volumes</code></li> <li>Shareable : Multiple containers can use it</li> <li>Performant : Faster than bind mounts</li> </ul>"},{"location":"Docker/EN/06-volumes-networks/#create-a-volume","title":"Create a Volume","text":"<pre><code># Create a volume\ndocker volume create my-volume\n\n# List volumes\ndocker volume ls\n\n# Inspect a volume\ndocker volume inspect my-volume\n\n# Remove a volume\ndocker volume rm my-volume\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#use-a-volume","title":"Use a Volume","text":"<pre><code># Named volume\ndocker run -v my-volume:/data ubuntu\n\n# Anonymous volume\ndocker run -v /data ubuntu\n\n# In docker-compose.yml\nvolumes:\n  - my-volume:/data\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#bind-mounts","title":"Bind Mounts","text":""},{"location":"Docker/EN/06-volumes-networks/#what-is-a-bind-mount","title":"What is a Bind Mount?","text":"<p>Bind Mount = Direct link to host directory</p> <ul> <li>Direct : Direct access to host files</li> <li>Development : Ideal for development</li> <li>Performance : Depends on host filesystem</li> </ul>"},{"location":"Docker/EN/06-volumes-networks/#use-a-bind-mount","title":"Use a Bind Mount","text":"<pre><code># Bind mount\ndocker run -v /host/path:/container/path ubuntu\n\n# With docker-compose.yml\nvolumes:\n  - ./data:/app/data\n  - /absolute/path:/container/path\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#differences-volume-vs-bind-mount","title":"Differences : Volume vs Bind Mount","text":"<p>Volume: - Managed by Docker - Better performance - Portable - Recommended for production</p> <p>Bind Mount: - Direct link - Direct access - Depends on host system - Recommended for development</p>"},{"location":"Docker/EN/06-volumes-networks/#networks","title":"Networks","text":""},{"location":"Docker/EN/06-volumes-networks/#network-types","title":"Network Types","text":"<ol> <li>Bridge : Default network (isolation)</li> <li>Host : Uses host network</li> <li>None : No network</li> <li>Overlay : For Docker Swarm</li> </ol>"},{"location":"Docker/EN/06-volumes-networks/#create-a-network","title":"Create a Network","text":"<pre><code># Create a network\ndocker network create my-network\n\n# List networks\ndocker network ls\n\n# Inspect a network\ndocker network inspect my-network\n\n# Remove a network\ndocker network rm my-network\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#connect-a-container","title":"Connect a Container","text":"<pre><code># Connect on startup\ndocker run --network my-network ubuntu\n\n# Connect existing container\ndocker network connect my-network container-id\n\n# Disconnect\ndocker network disconnect my-network container-id\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#container-communication","title":"Container Communication","text":""},{"location":"Docker/EN/06-volumes-networks/#same-network","title":"Same Network","text":"<pre><code># Create a network\ndocker network create app-network\n\n# Container 1\ndocker run --name app --network app-network my-app\n\n# Container 2 (can communicate with app)\ndocker run --name db --network app-network postgres\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#with-docker-compose","title":"With Docker Compose","text":"<pre><code>services:\n  app:\n    networks:\n      - app-network\n\n  db:\n    networks:\n      - app-network\n\nnetworks:\n  app-network:\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#dns-resolution","title":"DNS Resolution","text":"<p>Containers can find each other by name:</p> <pre><code># In app container\nimport psycopg2\nconn = psycopg2.connect(\n    host=\"db\",  # Service name\n    database=\"mydb\"\n)\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#practical-examples","title":"Practical Examples","text":""},{"location":"Docker/EN/06-volumes-networks/#example-1-database-with-volume","title":"Example 1 : Database with Volume","text":"<pre><code>version: '3.8'\n\nservices:\n  db:\n    image: postgres:15\n    volumes:\n      - db-data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_DB: mydb\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#example-2-application-with-bind-mount","title":"Example 2 : Application with Bind Mount","text":"<pre><code>version: '3.8'\n\nservices:\n  app:\n    build: .\n    volumes:\n      - ./src:/app/src  # Development\n    networks:\n      - app-network\n\nnetworks:\n  app-network:\n</code></pre>"},{"location":"Docker/EN/06-volumes-networks/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Volumes for Docker-managed persistence</li> <li>Bind Mounts for direct access</li> <li>Networks for communication</li> <li>DNS : Resolution by service name</li> <li>Docker Compose simplifies management</li> </ol>"},{"location":"Docker/EN/06-volumes-networks/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 7. Best Practices for best practices.</p>"},{"location":"Docker/EN/07-best-practices/","title":"7. Docker Best Practices","text":""},{"location":"Docker/EN/07-best-practices/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Secure containers</li> <li>Optimize performance</li> <li>Organize projects</li> <li>Maintain images</li> <li>Manage resources</li> </ul>"},{"location":"Docker/EN/07-best-practices/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Security</li> <li>Performance</li> <li>Organization</li> <li>Maintenance</li> <li>Resources</li> </ol>"},{"location":"Docker/EN/07-best-practices/#security","title":"Security","text":""},{"location":"Docker/EN/07-best-practices/#use-official-images","title":"Use Official Images","text":"<pre><code># Good\nFROM python:3.11-slim\n\n# Avoid\nFROM random-user/python:latest\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#dont-use-root","title":"Don't Use Root","text":"<pre><code># Create non-root user\nRUN useradd -m appuser\nUSER appuser\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#limit-privileges","title":"Limit Privileges","text":"<pre><code># Don't use --privileged\ndocker run --privileged my-container  # Avoid\n\n# Use specific capabilities if needed\ndocker run --cap-add NET_ADMIN my-container\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#secrets","title":"Secrets","text":"<pre><code># docker-compose.yml\nservices:\n  app:\n    secrets:\n      - db_password\n\nsecrets:\n  db_password:\n    file: ./secrets/password.txt\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#performance","title":"Performance","text":""},{"location":"Docker/EN/07-best-practices/#use-dockerignore","title":"Use .dockerignore","text":"<pre><code>__pycache__\n*.pyc\n.git\n.env\nnode_modules\n*.log\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#optimize-layers","title":"Optimize Layers","text":"<pre><code># Bad\nRUN apt update\nRUN apt install -y python3\nRUN apt clean\n\n# Good\nRUN apt update &amp;&amp; \\\n    apt install -y python3 &amp;&amp; \\\n    apt clean\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#build-cache","title":"Build Cache","text":"<p>Instruction order:</p> <pre><code># First dependencies (change little)\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Then code (changes often)\nCOPY . .\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#organization","title":"Organization","text":""},{"location":"Docker/EN/07-best-practices/#project-structure","title":"Project Structure","text":"<pre><code>my-project/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 app.py\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 data.csv\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#image-tags","title":"Image Tags","text":"<pre><code># Semantic tags\ndocker build -t my-app:1.0.0 .\ndocker build -t my-app:latest .\n\n# Environment tags\ndocker build -t my-app:dev .\ndocker build -t my-app:prod .\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#maintenance","title":"Maintenance","text":""},{"location":"Docker/EN/07-best-practices/#clean-resources","title":"Clean Resources","text":"<pre><code># Remove stopped containers\ndocker container prune\n\n# Remove unused images\ndocker image prune\n\n# Remove unused volumes\ndocker volume prune\n\n# Clean everything\ndocker system prune -a\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#update-images","title":"Update Images","text":"<pre><code># Update an image\ndocker pull python:3.11\n\n# Rebuild\ndocker-compose build --no-cache\ndocker-compose up\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#resources","title":"Resources","text":""},{"location":"Docker/EN/07-best-practices/#limit-resources","title":"Limit Resources","text":"<pre><code># docker-compose.yml\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 512M\n        reservations:\n          cpus: '0.5'\n          memory: 256M\n</code></pre>"},{"location":"Docker/EN/07-best-practices/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Security : Official images, non-root</li> <li>Performance : Optimize layers, cache</li> <li>Organization : Clear structure, tags</li> <li>Maintenance : Clean regularly</li> <li>Resources : Limit usage</li> </ol>"},{"location":"Docker/EN/07-best-practices/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 8. Practical Projects to create complete projects.</p>"},{"location":"Docker/EN/08-projets/","title":"8. Docker Practical Projects","text":""},{"location":"Docker/EN/08-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Containerize a Python application</li> <li>Create a data pipeline with Docker</li> <li>Complete stack with Docker Compose</li> <li>Portfolio projects</li> </ul>"},{"location":"Docker/EN/08-projets/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Project 1 : Python Application</li> <li>Project 2 : Data Pipeline</li> <li>Project 3 : Complete Stack</li> <li>Project 4 : Web Application</li> </ol>"},{"location":"Docker/EN/08-projets/#project-1-python-application","title":"Project 1 : Python Application","text":""},{"location":"Docker/EN/08-projets/#objective","title":"Objective","text":"<p>Containerize a simple Python application.</p>"},{"location":"Docker/EN/08-projets/#structure","title":"Structure","text":"<pre><code>python-app/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 app.py\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 data.csv\n</code></pre>"},{"location":"Docker/EN/08-projets/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/EN/08-projets/#apppy","title":"app.py","text":"<pre><code>import pandas as pd\n\ndef main():\n    df = pd.read_csv('data/data.csv')\n    print(f\"Loaded {len(df)} rows\")\n    print(df.head())\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"Docker/EN/08-projets/#build-and-run","title":"Build and Run","text":"<pre><code># Build\ndocker build -t python-app .\n\n# Run\ndocker run -v $(pwd)/data:/app/data python-app\n</code></pre>"},{"location":"Docker/EN/08-projets/#project-2-data-pipeline","title":"Project 2 : Data Pipeline","text":""},{"location":"Docker/EN/08-projets/#objective_1","title":"Objective","text":"<p>Create an ETL pipeline with Docker.</p>"},{"location":"Docker/EN/08-projets/#structure_1","title":"Structure","text":"<pre><code>etl-pipeline/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 extract/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 extract.py\n\u251c\u2500\u2500 transform/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 transform.py\n\u2514\u2500\u2500 load/\n    \u251c\u2500\u2500 Dockerfile\n    \u2514\u2500\u2500 load.py\n</code></pre>"},{"location":"Docker/EN/08-projets/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  extract:\n    build: ./extract\n    volumes:\n      - ./data:/data\n\n  transform:\n    build: ./transform\n    depends_on:\n      - extract\n    volumes:\n      - ./data:/data\n\n  load:\n    build: ./load\n    depends_on:\n      - transform\n    volumes:\n      - ./data:/data\n</code></pre>"},{"location":"Docker/EN/08-projets/#project-3-complete-stack","title":"Project 3 : Complete Stack","text":""},{"location":"Docker/EN/08-projets/#objective_2","title":"Objective","text":"<p>Complete stack with database and application.</p>"},{"location":"Docker/EN/08-projets/#docker-composeyml_1","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8080:8000\"\n    environment:\n      - DATABASE_URL=postgresql://user:password@db:5432/mydb\n    depends_on:\n      - db\n    volumes:\n      - ./src:/app\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: mydb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db-data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n\n  redis:\n    image: redis:7\n    ports:\n      - \"6379:6379\"\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/EN/08-projets/#project-4-web-application","title":"Project 4 : Web Application","text":""},{"location":"Docker/EN/08-projets/#objective_3","title":"Objective","text":"<p>Flask web application with database.</p>"},{"location":"Docker/EN/08-projets/#dockerfile_1","title":"Dockerfile","text":"<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\"]\n</code></pre>"},{"location":"Docker/EN/08-projets/#docker-composeyml_2","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"5000:5000\"\n    environment:\n      - FLASK_ENV=development\n      - DATABASE_URL=postgresql://user:password@db:5432/appdb\n    depends_on:\n      - db\n    volumes:\n      - .:/app\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: appdb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/EN/08-projets/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Containerization : Isolate applications</li> <li>Docker Compose : Orchestrate multiple services</li> <li>Volumes : Persist data</li> <li>Networks : Communication between services</li> <li>Portfolio : Demonstrable projects</li> </ol>"},{"location":"Docker/EN/08-projets/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>Docker Examples</li> <li>Docker Documentation</li> </ul> <p>Congratulations! You have completed the Docker training. You can now containerize your applications.</p>"},{"location":"Docker/FR/","title":"Formation Docker pour Data Analyst","text":""},{"location":"Docker/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'apprentissage de Docker en tant que Data Analyst. Docker est une plateforme de conteneurisation qui permet de cr\u00e9er, d\u00e9ployer et ex\u00e9cuter des applications dans des conteneurs isol\u00e9s.</p>"},{"location":"Docker/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre Docker et la conteneurisation</li> <li>Installer Docker</li> <li>Cr\u00e9er et g\u00e9rer des conteneurs</li> <li>Construire des images Docker</li> <li>Utiliser Docker Compose</li> <li>Int\u00e9grer Docker dans vos workflows de donn\u00e9es</li> <li>Cr\u00e9er des projets pratiques pour votre portfolio</li> </ul>"},{"location":"Docker/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 Docker Desktop : Gratuit pour usage personnel/\u00e9ducation - \u2705 Docker Hub : Registre public gratuit - \u2705 Documentation officielle : Guides complets gratuits - \u2705 Tutoriels en ligne : Ressources gratuites</p> <p>Budget total : 0\u20ac</p>"},{"location":"Docker/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"Docker/FR/#1-prise-en-main-docker","title":"1. Prise en main Docker","text":"<ul> <li>Installer Docker</li> <li>Concepts de base</li> <li>Premiers conteneurs</li> <li>Commandes essentielles</li> </ul>"},{"location":"Docker/FR/#2-conteneurs","title":"2. Conteneurs","text":"<ul> <li>Cr\u00e9er des conteneurs</li> <li>G\u00e9rer le cycle de vie</li> <li>Ex\u00e9cuter des commandes</li> <li>Logs et d\u00e9bogage</li> </ul>"},{"location":"Docker/FR/#3-images-docker","title":"3. Images Docker","text":"<ul> <li>Comprendre les images</li> <li>T\u00e9l\u00e9charger des images</li> <li>Cr\u00e9er des images personnalis\u00e9es</li> <li>G\u00e9rer les images</li> </ul>"},{"location":"Docker/FR/#4-dockerfile","title":"4. Dockerfile","text":"<ul> <li>\u00c9crire un Dockerfile</li> <li>Bonnes pratiques</li> <li>Optimisation des images</li> <li>Multi-stage builds</li> </ul>"},{"location":"Docker/FR/#5-docker-compose","title":"5. Docker Compose","text":"<ul> <li>Orchestrer plusieurs conteneurs</li> <li>Fichier docker-compose.yml</li> <li>Services et r\u00e9seaux</li> <li>Variables d'environnement</li> </ul>"},{"location":"Docker/FR/#6-volumes-et-reseaux","title":"6. Volumes et R\u00e9seaux","text":"<ul> <li>G\u00e9rer les volumes</li> <li>Cr\u00e9er des r\u00e9seaux</li> <li>Partager des donn\u00e9es</li> <li>Persistance des donn\u00e9es</li> </ul>"},{"location":"Docker/FR/#7-bonnes-pratiques","title":"7. Bonnes pratiques","text":"<ul> <li>S\u00e9curit\u00e9</li> <li>Performance</li> <li>Organisation</li> <li>Maintenance</li> </ul>"},{"location":"Docker/FR/#8-projets-pratiques","title":"8. Projets pratiques","text":"<ul> <li>Conteneuriser une application Python</li> <li>Pipeline de donn\u00e9es avec Docker</li> <li>Stack compl\u00e8te avec Docker Compose</li> <li>Projets pour portfolio</li> </ul>"},{"location":"Docker/FR/#demarrage-rapide","title":"\ud83d\ude80 D\u00e9marrage rapide","text":""},{"location":"Docker/FR/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Syst\u00e8me d'exploitation : Windows, Linux, ou macOS</li> <li>4 Go RAM : Minimum recommand\u00e9</li> <li>Espace disque : 20 Go libres</li> </ul>"},{"location":"Docker/FR/#installation-rapide","title":"Installation rapide","text":"<p>Windows/Mac : 1. T\u00e9l\u00e9charger Docker Desktop : https://www.docker.com/products/docker-desktop 2. Installer et lancer Docker Desktop 3. V\u00e9rifier l'installation : <code>docker --version</code></p> <p>Linux : <pre><code># Installer Docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n\n# D\u00e9marrer Docker\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# V\u00e9rifier\ndocker --version\n</code></pre></p>"},{"location":"Docker/FR/#premier-conteneur","title":"Premier conteneur","text":"<pre><code># Ex\u00e9cuter un conteneur Hello World\ndocker run hello-world\n\n# Ex\u00e9cuter un conteneur interactif\ndocker run -it ubuntu bash\n</code></pre>"},{"location":"Docker/FR/#cas-dusage-pour-data-analyst","title":"\ud83d\udcca Cas d'usage pour Data Analyst","text":"<ul> <li>Environnements reproductibles : M\u00eame environnement partout</li> <li>Isolation : S\u00e9parer les d\u00e9pendances</li> <li>D\u00e9ploiement : D\u00e9ployer facilement des applications</li> <li>CI/CD : Int\u00e9grer dans les pipelines</li> <li>Data Science : Environnements Python/R isol\u00e9s</li> </ul>"},{"location":"Docker/FR/#ressources-gratuites","title":"\ud83d\udcda Ressources gratuites","text":""},{"location":"Docker/FR/#documentation-officielle","title":"Documentation officielle","text":"<ul> <li>Docker Documentation : https://docs.docker.com/</li> <li>Guides complets</li> <li>Docker Hub : https://hub.docker.com/</li> <li>Images publiques</li> <li>Docker Playground : https://labs.play-with-docker.com/</li> <li>Environnement de test en ligne</li> </ul>"},{"location":"Docker/FR/#ressources-externes","title":"Ressources externes","text":"<ul> <li>YouTube : Tutoriels Docker</li> <li>GitHub : Exemples Docker</li> <li>Stack Overflow : Questions et r\u00e9ponses</li> </ul>"},{"location":"Docker/FR/#certifications-optionnel","title":"\ud83c\udf93 Certifications (optionnel)","text":""},{"location":"Docker/FR/#docker-certified-associate-dca","title":"Docker Certified Associate (DCA)","text":"<ul> <li>Co\u00fbt : ~$195</li> <li>Pr\u00e9paration : Documentation gratuite</li> <li>Dur\u00e9e : 2-4 semaines</li> <li>Niveau : Interm\u00e9diaire</li> </ul>"},{"location":"Docker/FR/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>Tous les exemples sont test\u00e9s sur Docker Desktop</li> <li>Les commandes fonctionnent sur Windows, Linux, et macOS</li> <li>Les chemins peuvent varier selon le syst\u00e8me</li> </ul>"},{"location":"Docker/FR/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>Cette formation est con\u00e7ue pour \u00eatre \u00e9volutive. N'h\u00e9sitez pas \u00e0 proposer des am\u00e9liorations.</p>"},{"location":"Docker/FR/#ressources-complementaires","title":"\ud83d\udcda Ressources compl\u00e9mentaires","text":"<ul> <li>Documentation Docker</li> <li>Docker Hub</li> <li>Docker Playground</li> <li>Docker GitHub</li> </ul>"},{"location":"Docker/FR/01-getting-started/","title":"1. Prise en main Docker","text":""},{"location":"Docker/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Docker et la conteneurisation</li> <li>Installer Docker</li> <li>Comprendre les concepts de base</li> <li>Ex\u00e9cuter votre premier conteneur</li> </ul>"},{"location":"Docker/FR/01-getting-started/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Docker</li> <li>Installation</li> <li>Concepts de base</li> <li>Premiers conteneurs</li> <li>Commandes essentielles</li> </ol>"},{"location":"Docker/FR/01-getting-started/#introduction-a-docker","title":"Introduction \u00e0 Docker","text":""},{"location":"Docker/FR/01-getting-started/#quest-ce-que-docker","title":"Qu'est-ce que Docker ?","text":"<p>Docker = Plateforme de conteneurisation</p> <ul> <li>Conteneurs : Environnements isol\u00e9s et l\u00e9gers</li> <li>Portable : Fonctionne partout (Windows, Linux, macOS)</li> <li>Efficace : Utilise moins de ressources que les VMs</li> <li>Rapide : D\u00e9marrage en secondes</li> </ul>"},{"location":"Docker/FR/01-getting-started/#pourquoi-docker-pour-data-analyst","title":"Pourquoi Docker pour Data Analyst ?","text":"<ul> <li>Reproductibilit\u00e9 : M\u00eame environnement partout</li> <li>Isolation : S\u00e9parer les d\u00e9pendances Python/R</li> <li>Simplicit\u00e9 : Facile \u00e0 partager et d\u00e9ployer</li> <li>Performance : Plus rapide que les VMs</li> </ul>"},{"location":"Docker/FR/01-getting-started/#docker-vs-virtual-machines","title":"Docker vs Virtual Machines","text":"<p>Docker (Conteneurs) : - Plus l\u00e9ger - D\u00e9marrage rapide - Partage le noyau OS - Moins de ressources</p> <p>Virtual Machines : - Plus lourd - D\u00e9marrage plus lent - OS complet - Plus de ressources</p>"},{"location":"Docker/FR/01-getting-started/#installation","title":"Installation","text":""},{"location":"Docker/FR/01-getting-started/#windows","title":"Windows","text":"<p>\u00c9tape 1 : T\u00e9l\u00e9charger Docker Desktop</p> <ol> <li>Aller sur : https://www.docker.com/products/docker-desktop</li> <li>T\u00e9l\u00e9charger Docker Desktop pour Windows</li> <li>Installer le fichier <code>.exe</code></li> <li>Red\u00e9marrer l'ordinateur si demand\u00e9</li> </ol> <p>\u00c9tape 2 : Lancer Docker Desktop</p> <ol> <li>Ouvrir Docker Desktop</li> <li>Attendre que Docker d\u00e9marre (ic\u00f4ne dans la barre des t\u00e2ches)</li> <li>V\u00e9rifier : <code>docker --version</code></li> </ol> <p>Pr\u00e9requis Windows : - Windows 10 64-bit : Pro, Enterprise, ou Education - WSL 2 activ\u00e9 - Virtualisation activ\u00e9e dans le BIOS</p>"},{"location":"Docker/FR/01-getting-started/#linux","title":"Linux","text":"<p>Ubuntu/Debian :</p> <pre><code># Mettre \u00e0 jour les paquets\nsudo apt update\n\n# Installer les d\u00e9pendances\nsudo apt install apt-transport-https ca-certificates curl gnupg lsb-release\n\n# Ajouter la cl\u00e9 GPG Docker\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Ajouter le repository Docker\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Installer Docker\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io\n\n# D\u00e9marrer Docker\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# V\u00e9rifier\ndocker --version\n</code></pre> <p>CentOS/RHEL :</p> <pre><code># Installer les d\u00e9pendances\nsudo yum install -y yum-utils\n\n# Ajouter le repository Docker\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n\n# Installer Docker\nsudo yum install docker-ce docker-ce-cli containerd.io\n\n# D\u00e9marrer Docker\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# V\u00e9rifier\ndocker --version\n</code></pre>"},{"location":"Docker/FR/01-getting-started/#macos","title":"macOS","text":"<p>\u00c9tape 1 : T\u00e9l\u00e9charger Docker Desktop</p> <ol> <li>Aller sur : https://www.docker.com/products/docker-desktop</li> <li>T\u00e9l\u00e9charger Docker Desktop pour Mac</li> <li>Installer le fichier <code>.dmg</code></li> <li>Glisser Docker dans Applications</li> </ol> <p>\u00c9tape 2 : Lancer Docker Desktop</p> <ol> <li>Ouvrir Docker depuis Applications</li> <li>Attendre que Docker d\u00e9marre</li> <li>V\u00e9rifier : <code>docker --version</code></li> </ol>"},{"location":"Docker/FR/01-getting-started/#concepts-de-base","title":"Concepts de base","text":""},{"location":"Docker/FR/01-getting-started/#images","title":"Images","text":"<p>Image = Mod\u00e8le en lecture seule pour cr\u00e9er des conteneurs</p> <ul> <li>Template : Contient l'OS, les applications, les d\u00e9pendances</li> <li>Immuable : Ne change pas une fois cr\u00e9\u00e9e</li> <li>L\u00e9g\u00e8re : Partage les couches communes</li> </ul>"},{"location":"Docker/FR/01-getting-started/#conteneurs","title":"Conteneurs","text":"<p>Conteneur = Instance ex\u00e9cutable d'une image</p> <ul> <li>Isol\u00e9 : Environnement s\u00e9par\u00e9</li> <li>\u00c9ph\u00e9m\u00e8re : Peut \u00eatre cr\u00e9\u00e9/d\u00e9truit facilement</li> <li>Portable : Fonctionne partout o\u00f9 Docker est install\u00e9</li> </ul>"},{"location":"Docker/FR/01-getting-started/#dockerfile","title":"Dockerfile","text":"<p>Dockerfile = Instructions pour construire une image</p> <ul> <li>D\u00e9finit : L'environnement et les applications</li> <li>Automatise : La cr\u00e9ation d'images</li> <li>Versionne : Peut \u00eatre versionn\u00e9 avec Git</li> </ul>"},{"location":"Docker/FR/01-getting-started/#docker-hub","title":"Docker Hub","text":"<p>Docker Hub = Registre public d'images Docker</p> <ul> <li>Images publiques : Python, PostgreSQL, Redis, etc.</li> <li>Gratuit : Pour usage public</li> <li>Partage : Partagez vos images</li> </ul>"},{"location":"Docker/FR/01-getting-started/#premiers-conteneurs","title":"Premiers conteneurs","text":""},{"location":"Docker/FR/01-getting-started/#hello-world","title":"Hello World","text":"<pre><code># Ex\u00e9cuter le conteneur Hello World\ndocker run hello-world\n</code></pre> <p>Ce qui se passe : 1. Docker t\u00e9l\u00e9charge l'image <code>hello-world</code> (si pas pr\u00e9sente) 2. Cr\u00e9e un conteneur 3. Ex\u00e9cute le conteneur 4. Affiche le message 5. Arr\u00eate le conteneur</p>"},{"location":"Docker/FR/01-getting-started/#conteneur-interactif","title":"Conteneur interactif","text":"<pre><code># Ex\u00e9cuter un conteneur Ubuntu interactif\ndocker run -it ubuntu bash\n\n# Dans le conteneur\nls\npwd\nexit\n</code></pre> <p>Options : - <code>-i</code> : Mode interactif (stdin) - <code>-t</code> : Allouer un terminal - <code>ubuntu</code> : Image \u00e0 utiliser - <code>bash</code> : Commande \u00e0 ex\u00e9cuter</p>"},{"location":"Docker/FR/01-getting-started/#conteneur-en-arriere-plan","title":"Conteneur en arri\u00e8re-plan","text":"<pre><code># Ex\u00e9cuter un conteneur en arri\u00e8re-plan\ndocker run -d --name my-container nginx\n\n# Voir les conteneurs en cours\ndocker ps\n\n# Voir les logs\ndocker logs my-container\n\n# Arr\u00eater le conteneur\ndocker stop my-container\n</code></pre>"},{"location":"Docker/FR/01-getting-started/#commandes-essentielles","title":"Commandes essentielles","text":""},{"location":"Docker/FR/01-getting-started/#gestion-des-conteneurs","title":"Gestion des conteneurs","text":"<pre><code># Lister les conteneurs en cours\ndocker ps\n\n# Lister tous les conteneurs\ndocker ps -a\n\n# Cr\u00e9er un conteneur\ndocker create --name my-container ubuntu\n\n# D\u00e9marrer un conteneur\ndocker start my-container\n\n# Arr\u00eater un conteneur\ndocker stop my-container\n\n# Red\u00e9marrer un conteneur\ndocker restart my-container\n\n# Supprimer un conteneur\ndocker rm my-container\n\n# Supprimer un conteneur en cours (force)\ndocker rm -f my-container\n</code></pre>"},{"location":"Docker/FR/01-getting-started/#gestion-des-images","title":"Gestion des images","text":"<pre><code># Lister les images\ndocker images\n\n# T\u00e9l\u00e9charger une image\ndocker pull ubuntu\n\n# Supprimer une image\ndocker rmi ubuntu\n\n# Chercher des images sur Docker Hub\ndocker search python\n</code></pre>"},{"location":"Docker/FR/01-getting-started/#execution-de-commandes","title":"Ex\u00e9cution de commandes","text":"<pre><code># Ex\u00e9cuter une commande dans un conteneur\ndocker exec my-container ls\n\n# Ex\u00e9cuter une commande interactive\ndocker exec -it my-container bash\n\n# Voir les logs\ndocker logs my-container\n\n# Suivre les logs en temps r\u00e9el\ndocker logs -f my-container\n</code></pre>"},{"location":"Docker/FR/01-getting-started/#informations","title":"Informations","text":"<pre><code># Informations syst\u00e8me Docker\ndocker info\n\n# Version Docker\ndocker --version\n\n# Statistiques des conteneurs\ndocker stats\n\n# Inspecter un conteneur\ndocker inspect my-container\n</code></pre>"},{"location":"Docker/FR/01-getting-started/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Docker/FR/01-getting-started/#exemple-1-conteneur-python","title":"Exemple 1 : Conteneur Python","text":"<pre><code># Ex\u00e9cuter Python dans un conteneur\ndocker run -it python:3.11 python\n\n# Dans Python\nprint(\"Hello from Docker!\")\nexit()\n</code></pre>"},{"location":"Docker/FR/01-getting-started/#exemple-2-conteneur-avec-volume","title":"Exemple 2 : Conteneur avec volume","text":"<pre><code># Cr\u00e9er un fichier local\necho \"print('Hello from file')\" &gt; script.py\n\n# Ex\u00e9cuter Python avec volume\ndocker run -v $(pwd):/app -w /app python:3.11 python script.py\n</code></pre>"},{"location":"Docker/FR/01-getting-started/#exemple-3-conteneur-avec-port","title":"Exemple 3 : Conteneur avec port","text":"<pre><code># Ex\u00e9cuter un serveur web\ndocker run -d -p 8080:80 --name web-server nginx\n\n# Acc\u00e9der au serveur\n# Ouvrir : http://localhost:8080\n\n# Arr\u00eater\ndocker stop web-server\ndocker rm web-server\n</code></pre>"},{"location":"Docker/FR/01-getting-started/#depannage","title":"D\u00e9pannage","text":""},{"location":"Docker/FR/01-getting-started/#probleme-docker-ne-demarre-pas","title":"Probl\u00e8me : Docker ne d\u00e9marre pas","text":"<p>Solutions : 1. V\u00e9rifier que la virtualisation est activ\u00e9e (BIOS) 2. V\u00e9rifier WSL 2 (Windows) 3. Red\u00e9marrer Docker Desktop 4. V\u00e9rifier les logs : Docker Desktop \u2192 Troubleshoot</p>"},{"location":"Docker/FR/01-getting-started/#probleme-permission-denied-linux","title":"Probl\u00e8me : Permission denied (Linux)","text":"<p>Solutions : <pre><code># Ajouter l'utilisateur au groupe docker\nsudo usermod -aG docker $USER\n\n# Se d\u00e9connecter et reconnecter\n# Ou\nnewgrp docker\n</code></pre></p>"},{"location":"Docker/FR/01-getting-started/#probleme-conteneur-ne-demarre-pas","title":"Probl\u00e8me : Conteneur ne d\u00e9marre pas","text":"<p>Solutions : 1. V\u00e9rifier les logs : <code>docker logs container-name</code> 2. V\u00e9rifier les ressources : <code>docker stats</code> 3. V\u00e9rifier la configuration : <code>docker inspect container-name</code></p>"},{"location":"Docker/FR/01-getting-started/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Docker = Conteneurisation pour isoler les applications</li> <li>Images sont les mod\u00e8les, Conteneurs sont les instances</li> <li>Docker Hub pour trouver des images</li> <li>Commandes de base : run, ps, stop, rm</li> <li>Portable : Fonctionne partout</li> </ol>"},{"location":"Docker/FR/01-getting-started/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 2. Conteneurs pour approfondir la gestion des conteneurs.</p>"},{"location":"Docker/FR/02-containers/","title":"2. Conteneurs Docker","text":""},{"location":"Docker/FR/02-containers/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er et g\u00e9rer des conteneurs</li> <li>Comprendre le cycle de vie</li> <li>Ex\u00e9cuter des commandes</li> <li>G\u00e9rer les logs</li> <li>Configurer les conteneurs</li> </ul>"},{"location":"Docker/FR/02-containers/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Cycle de vie d'un conteneur</li> <li>Cr\u00e9er des conteneurs</li> <li>Ex\u00e9cuter des commandes</li> <li>Logs et d\u00e9bogage</li> <li>Configuration</li> </ol>"},{"location":"Docker/FR/02-containers/#cycle-de-vie-dun-conteneur","title":"Cycle de vie d'un conteneur","text":""},{"location":"Docker/FR/02-containers/#etats-dun-conteneur","title":"\u00c9tats d'un conteneur","text":"<ol> <li>Created : Conteneur cr\u00e9\u00e9 mais pas d\u00e9marr\u00e9</li> <li>Running : Conteneur en cours d'ex\u00e9cution</li> <li>Paused : Conteneur en pause</li> <li>Stopped : Conteneur arr\u00eat\u00e9</li> <li>Removed : Conteneur supprim\u00e9</li> </ol>"},{"location":"Docker/FR/02-containers/#commandes-de-cycle-de-vie","title":"Commandes de cycle de vie","text":"<pre><code># Cr\u00e9er un conteneur\ndocker create --name my-container ubuntu\n\n# D\u00e9marrer un conteneur\ndocker start my-container\n\n# Arr\u00eater un conteneur\ndocker stop my-container\n\n# Red\u00e9marrer un conteneur\ndocker restart my-container\n\n# Mettre en pause\ndocker pause my-container\n\n# Reprendre\ndocker unpause my-container\n\n# Supprimer un conteneur\ndocker rm my-container\n</code></pre>"},{"location":"Docker/FR/02-containers/#creer-des-conteneurs","title":"Cr\u00e9er des conteneurs","text":""},{"location":"Docker/FR/02-containers/#creer-avec-docker-run","title":"Cr\u00e9er avec docker run","text":"<pre><code># Cr\u00e9er et d\u00e9marrer un conteneur\ndocker run ubuntu echo \"Hello\"\n\n# Cr\u00e9er sans d\u00e9marrer\ndocker create --name my-container ubuntu\n\n# Cr\u00e9er avec nom personnalis\u00e9\ndocker run --name my-app ubuntu\n</code></pre>"},{"location":"Docker/FR/02-containers/#options-importantes","title":"Options importantes","text":"<pre><code># Mode interactif\ndocker run -it ubuntu bash\n\n# Mode d\u00e9tach\u00e9 (arri\u00e8re-plan)\ndocker run -d nginx\n\n# Exposer un port\ndocker run -p 8080:80 nginx\n\n# Monter un volume\ndocker run -v /host/path:/container/path ubuntu\n\n# Variables d'environnement\ndocker run -e MY_VAR=value ubuntu\n\n# Nom du conteneur\ndocker run --name my-container ubuntu\n</code></pre>"},{"location":"Docker/FR/02-containers/#executer-des-commandes","title":"Ex\u00e9cuter des commandes","text":""},{"location":"Docker/FR/02-containers/#executer-dans-un-conteneur-en-cours","title":"Ex\u00e9cuter dans un conteneur en cours","text":"<pre><code># Ex\u00e9cuter une commande\ndocker exec my-container ls\n\n# Mode interactif\ndocker exec -it my-container bash\n\n# Ex\u00e9cuter Python\ndocker exec -it my-container python\n</code></pre>"},{"location":"Docker/FR/02-containers/#executer-au-demarrage","title":"Ex\u00e9cuter au d\u00e9marrage","text":"<pre><code># Commande par d\u00e9faut\ndocker run ubuntu echo \"Hello\"\n\n# Override la commande\ndocker run ubuntu ls -la\n\n# Ex\u00e9cuter un script\ndocker run -v $(pwd):/app ubuntu bash /app/script.sh\n</code></pre>"},{"location":"Docker/FR/02-containers/#logs-et-debogage","title":"Logs et d\u00e9bogage","text":""},{"location":"Docker/FR/02-containers/#voir-les-logs","title":"Voir les logs","text":"<pre><code># Logs d'un conteneur\ndocker logs my-container\n\n# Suivre les logs (tail -f)\ndocker logs -f my-container\n\n# Derni\u00e8res lignes\ndocker logs --tail 100 my-container\n\n# Avec timestamp\ndocker logs -t my-container\n</code></pre>"},{"location":"Docker/FR/02-containers/#inspecter-un-conteneur","title":"Inspecter un conteneur","text":"<pre><code># Informations compl\u00e8tes\ndocker inspect my-container\n\n# Informations sp\u00e9cifiques\ndocker inspect --format='{{.State.Status}}' my-container\n\n# Configuration r\u00e9seau\ndocker inspect --format='{{.NetworkSettings.IPAddress}}' my-container\n</code></pre>"},{"location":"Docker/FR/02-containers/#statistiques","title":"Statistiques","text":"<pre><code># Statistiques en temps r\u00e9el\ndocker stats\n\n# Statistiques d'un conteneur\ndocker stats my-container\n\n# Statistiques sans streaming\ndocker stats --no-stream\n</code></pre>"},{"location":"Docker/FR/02-containers/#configuration","title":"Configuration","text":""},{"location":"Docker/FR/02-containers/#variables-denvironnement","title":"Variables d'environnement","text":"<pre><code># Une variable\ndocker run -e MY_VAR=value ubuntu\n\n# Plusieurs variables\ndocker run -e VAR1=value1 -e VAR2=value2 ubuntu\n\n# Fichier .env\ndocker run --env-file .env ubuntu\n</code></pre>"},{"location":"Docker/FR/02-containers/#ports","title":"Ports","text":"<pre><code># Exposer un port\ndocker run -p 8080:80 nginx\n\n# Exposer plusieurs ports\ndocker run -p 8080:80 -p 3306:3306 my-app\n\n# Exposer tous les ports\ndocker run -P nginx\n</code></pre>"},{"location":"Docker/FR/02-containers/#volumes","title":"Volumes","text":"<pre><code># Volume nomm\u00e9\ndocker run -v my-volume:/data ubuntu\n\n# Bind mount\ndocker run -v /host/path:/container/path ubuntu\n\n# Volume anonyme\ndocker run -v /data ubuntu\n</code></pre>"},{"location":"Docker/FR/02-containers/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Cycle de vie : Created \u2192 Running \u2192 Stopped \u2192 Removed</li> <li>docker run : Cr\u00e9e et d\u00e9marre</li> <li>docker exec : Ex\u00e9cute dans un conteneur en cours</li> <li>docker logs : Voir les logs</li> <li>Configuration : Variables, ports, volumes</li> </ol>"},{"location":"Docker/FR/02-containers/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 3. Images Docker pour apprendre \u00e0 g\u00e9rer les images.</p>"},{"location":"Docker/FR/03-images/","title":"3. Images Docker","text":""},{"location":"Docker/FR/03-images/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les images Docker</li> <li>T\u00e9l\u00e9charger des images</li> <li>Cr\u00e9er des images personnalis\u00e9es</li> <li>G\u00e9rer les images</li> <li>Optimiser les images</li> </ul>"},{"location":"Docker/FR/03-images/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction aux images</li> <li>T\u00e9l\u00e9charger des images</li> <li>Cr\u00e9er des images</li> <li>G\u00e9rer les images</li> <li>Optimisation</li> </ol>"},{"location":"Docker/FR/03-images/#introduction-aux-images","title":"Introduction aux images","text":""},{"location":"Docker/FR/03-images/#quest-ce-quune-image","title":"Qu'est-ce qu'une image ?","text":"<p>Image = Mod\u00e8le en lecture seule</p> <ul> <li>Template : Pour cr\u00e9er des conteneurs</li> <li>Layering : Compos\u00e9e de couches</li> <li>Immuable : Ne change pas</li> <li>Partag\u00e9e : Plusieurs conteneurs peuvent utiliser la m\u00eame image</li> </ul>"},{"location":"Docker/FR/03-images/#structure-dune-image","title":"Structure d'une image","text":"<pre><code>Image\n\u251c\u2500\u2500 Couche 1 : OS de base (Ubuntu)\n\u251c\u2500\u2500 Couche 2 : Outils syst\u00e8me\n\u251c\u2500\u2500 Couche 3 : Python\n\u2514\u2500\u2500 Couche 4 : Votre application\n</code></pre>"},{"location":"Docker/FR/03-images/#telecharger-des-images","title":"T\u00e9l\u00e9charger des images","text":""},{"location":"Docker/FR/03-images/#docker-hub","title":"Docker Hub","text":"<p>Docker Hub = Registre public d'images</p> <ul> <li>Images officielles : python, postgres, nginx, etc.</li> <li>Images communautaires : Cr\u00e9\u00e9es par la communaut\u00e9</li> <li>Gratuit : Pour usage public</li> </ul>"},{"location":"Docker/FR/03-images/#telecharger-une-image","title":"T\u00e9l\u00e9charger une image","text":"<pre><code># T\u00e9l\u00e9charger une image\ndocker pull python:3.11\n\n# T\u00e9l\u00e9charger la derni\u00e8re version\ndocker pull python:latest\n\n# T\u00e9l\u00e9charger une version sp\u00e9cifique\ndocker pull python:3.11-slim\n\n# Chercher des images\ndocker search python\n</code></pre>"},{"location":"Docker/FR/03-images/#images-populaires-pour-data-analyst","title":"Images populaires pour Data Analyst","text":"<pre><code># Python\ndocker pull python:3.11\n\n# Jupyter Notebook\ndocker pull jupyter/scipy-notebook\n\n# PostgreSQL\ndocker pull postgres:15\n\n# MySQL\ndocker pull mysql:8.0\n\n# Redis\ndocker pull redis:7\n</code></pre>"},{"location":"Docker/FR/03-images/#creer-des-images","title":"Cr\u00e9er des images","text":""},{"location":"Docker/FR/03-images/#avec-dockerfile","title":"Avec Dockerfile","text":"<p>Cr\u00e9er un Dockerfile :</p> <pre><code># Dockerfile\nFROM python:3.11\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]\n</code></pre> <p>Construire l'image :</p> <pre><code># Construire une image\ndocker build -t my-app:latest .\n\n# Avec tag sp\u00e9cifique\ndocker build -t my-app:v1.0 .\n\n# Depuis un Dockerfile sp\u00e9cifique\ndocker build -f Dockerfile.prod -t my-app:prod .\n</code></pre>"},{"location":"Docker/FR/03-images/#commit-depuis-un-conteneur","title":"Commit depuis un conteneur","text":"<pre><code># Cr\u00e9er un conteneur\ndocker run -it ubuntu bash\n\n# Faire des modifications dans le conteneur\napt update\napt install python3\n\n# Cr\u00e9er une image depuis le conteneur\ndocker commit container-id my-image:tag\n</code></pre>"},{"location":"Docker/FR/03-images/#gerer-les-images","title":"G\u00e9rer les images","text":""},{"location":"Docker/FR/03-images/#lister-les-images","title":"Lister les images","text":"<pre><code># Lister toutes les images\ndocker images\n\n# Filtrer par nom\ndocker images python\n\n# Afficher seulement les IDs\ndocker images -q\n</code></pre>"},{"location":"Docker/FR/03-images/#supprimer-des-images","title":"Supprimer des images","text":"<pre><code># Supprimer une image\ndocker rmi my-image:tag\n\n# Supprimer par ID\ndocker rmi image-id\n\n# Supprimer toutes les images non utilis\u00e9es\ndocker image prune\n\n# Supprimer toutes les images\ndocker rmi $(docker images -q)\n</code></pre>"},{"location":"Docker/FR/03-images/#taguer-des-images","title":"Taguer des images","text":"<pre><code># Cr\u00e9er un tag\ndocker tag my-image:latest my-image:v1.0\n\n# Taguer pour Docker Hub\ndocker tag my-image:latest username/my-image:latest\n</code></pre>"},{"location":"Docker/FR/03-images/#inspecter-une-image","title":"Inspecter une image","text":"<pre><code># Informations compl\u00e8tes\ndocker inspect my-image\n\n# Historique des couches\ndocker history my-image\n\n# Taille de l'image\ndocker images my-image\n</code></pre>"},{"location":"Docker/FR/03-images/#optimisation","title":"Optimisation","text":""},{"location":"Docker/FR/03-images/#images-legeres","title":"Images l\u00e9g\u00e8res","text":"<p>Utiliser des images slim :</p> <pre><code># Au lieu de\nFROM python:3.11\n\n# Utiliser\nFROM python:3.11-slim\n</code></pre> <p>Multi-stage builds :</p> <pre><code># Stage 1 : Build\nFROM python:3.11 as builder\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user -r requirements.txt\n\n# Stage 2 : Runtime\nFROM python:3.11-slim\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY . .\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/FR/03-images/#reduire-la-taille","title":"R\u00e9duire la taille","text":"<p>Bonnes pratiques : 1. Utiliser <code>.dockerignore</code> 2. Combiner les RUN 3. Utiliser des images de base l\u00e9g\u00e8res 4. Nettoyer les caches</p>"},{"location":"Docker/FR/03-images/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Images sont les mod\u00e8les pour conteneurs</li> <li>Docker Hub pour trouver des images</li> <li>Dockerfile pour cr\u00e9er des images</li> <li>Layering permet le partage</li> <li>Optimisation r\u00e9duit la taille</li> </ol>"},{"location":"Docker/FR/03-images/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 4. Dockerfile pour apprendre \u00e0 \u00e9crire des Dockerfiles.</p>"},{"location":"Docker/FR/04-dockerfile/","title":"4. Dockerfile","text":""},{"location":"Docker/FR/04-dockerfile/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>\u00c9crire un Dockerfile</li> <li>Comprendre les instructions</li> <li>Optimiser les Dockerfiles</li> <li>Utiliser les multi-stage builds</li> <li>Bonnes pratiques</li> </ul>"},{"location":"Docker/FR/04-dockerfile/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction au Dockerfile</li> <li>Instructions de base</li> <li>Dockerfile complet</li> <li>Optimisation</li> <li>Multi-stage builds</li> </ol>"},{"location":"Docker/FR/04-dockerfile/#introduction-au-dockerfile","title":"Introduction au Dockerfile","text":""},{"location":"Docker/FR/04-dockerfile/#quest-ce-quun-dockerfile","title":"Qu'est-ce qu'un Dockerfile ?","text":"<p>Dockerfile = Instructions pour construire une image</p> <ul> <li>Texte : Fichier texte simple</li> <li>Instructions : Chaque ligne est une instruction</li> <li>Automatisation : Automatise la cr\u00e9ation d'images</li> <li>Versionn\u00e9 : Peut \u00eatre versionn\u00e9 avec Git</li> </ul>"},{"location":"Docker/FR/04-dockerfile/#structure-de-base","title":"Structure de base","text":"<pre><code># Commentaire\nFROM base-image\nRUN command\nCOPY source destination\nCMD [\"executable\", \"param\"]\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#instructions-de-base","title":"Instructions de base","text":""},{"location":"Docker/FR/04-dockerfile/#from","title":"FROM","text":"<p>D\u00e9finit l'image de base :</p> <pre><code>FROM python:3.11\nFROM ubuntu:22.04\nFROM alpine:latest\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#workdir","title":"WORKDIR","text":"<p>D\u00e9finit le r\u00e9pertoire de travail :</p> <pre><code>WORKDIR /app\nWORKDIR /usr/src/app\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#copy-add","title":"COPY / ADD","text":"<p>Copier des fichiers :</p> <pre><code># COPY (recommand\u00e9)\nCOPY requirements.txt .\nCOPY . .\n\n# ADD (avec extraction automatique)\nADD archive.tar.gz /app\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#run","title":"RUN","text":"<p>Ex\u00e9cuter des commandes :</p> <pre><code>RUN apt update\nRUN pip install -r requirements.txt\n\n# Combiner pour r\u00e9duire les couches\nRUN apt update &amp;&amp; \\\n    apt install -y python3 &amp;&amp; \\\n    apt clean\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#cmd-entrypoint","title":"CMD / ENTRYPOINT","text":"<p>Commande par d\u00e9faut :</p> <pre><code># CMD (peut \u00eatre override)\nCMD [\"python\", \"app.py\"]\n\n# ENTRYPOINT (ne peut pas \u00eatre override)\nENTRYPOINT [\"python\"]\nCMD [\"app.py\"]\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#env","title":"ENV","text":"<p>Variables d'environnement :</p> <pre><code>ENV PYTHONUNBUFFERED=1\nENV APP_ENV=production\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#expose","title":"EXPOSE","text":"<p>Exposer des ports :</p> <pre><code>EXPOSE 8080\nEXPOSE 3306\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#dockerfile-complet","title":"Dockerfile complet","text":""},{"location":"Docker/FR/04-dockerfile/#exemple-1-application-python","title":"Exemple 1 : Application Python","text":"<pre><code># Image de base\nFROM python:3.11-slim\n\n# R\u00e9pertoire de travail\nWORKDIR /app\n\n# Variables d'environnement\nENV PYTHONUNBUFFERED=1\n\n# Copier requirements\nCOPY requirements.txt .\n\n# Installer les d\u00e9pendances\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copier le code\nCOPY . .\n\n# Exposer le port\nEXPOSE 8000\n\n# Commande par d\u00e9faut\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#exemple-2-application-avec-donnees","title":"Exemple 2 : Application avec donn\u00e9es","text":"<pre><code>FROM python:3.11\n\nWORKDIR /app\n\n# Installer les d\u00e9pendances syst\u00e8me\nRUN apt update &amp;&amp; \\\n    apt install -y postgresql-client &amp;&amp; \\\n    apt clean\n\n# Installer les d\u00e9pendances Python\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copier l'application\nCOPY . .\n\n# Cr\u00e9er un volume pour les donn\u00e9es\nVOLUME [\"/app/data\"]\n\n# Exposer le port\nEXPOSE 8080\n\n# Commande\nCMD [\"python\", \"main.py\"]\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#optimisation","title":"Optimisation","text":""},{"location":"Docker/FR/04-dockerfile/#dockerignore","title":".dockerignore","text":"<p>Cr\u00e9er un fichier <code>.dockerignore</code> :</p> <pre><code>__pycache__\n*.pyc\n.git\n.env\nnode_modules\n*.log\n.DS_Store\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#reduire-les-couches","title":"R\u00e9duire les couches","text":"<p>Mauvais : <pre><code>RUN apt update\nRUN apt install -y python3\nRUN apt install -y pip\nRUN apt clean\n</code></pre></p> <p>Bon : <pre><code>RUN apt update &amp;&amp; \\\n    apt install -y python3 pip &amp;&amp; \\\n    apt clean\n</code></pre></p>"},{"location":"Docker/FR/04-dockerfile/#ordre-des-instructions","title":"Ordre des instructions","text":"<p>Mettre les instructions qui changent peu en premier :</p> <pre><code># D'abord les d\u00e9pendances (changent peu)\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Ensuite le code (change souvent)\nCOPY . .\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#multi-stage-builds","title":"Multi-stage builds","text":""},{"location":"Docker/FR/04-dockerfile/#pourquoi-multi-stage","title":"Pourquoi multi-stage ?","text":"<ul> <li>R\u00e9duire la taille : Image finale plus petite</li> <li>S\u00e9curit\u00e9 : Exclure les outils de build</li> <li>Performance : Optimiser les builds</li> </ul>"},{"location":"Docker/FR/04-dockerfile/#exemple","title":"Exemple","text":"<pre><code># Stage 1 : Build\nFROM python:3.11 as builder\nWORKDIR /build\nCOPY requirements.txt .\nRUN pip install --user -r requirements.txt\n\n# Stage 2 : Runtime\nFROM python:3.11-slim\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY . .\nENV PATH=/root/.local/bin:$PATH\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/FR/04-dockerfile/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Dockerfile automatise la cr\u00e9ation d'images</li> <li>Instructions : FROM, RUN, COPY, CMD</li> <li>Optimisation : R\u00e9duire les couches</li> <li>Multi-stage : Images plus l\u00e9g\u00e8res</li> <li>.dockerignore : Exclure des fichiers</li> </ol>"},{"location":"Docker/FR/04-dockerfile/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 5. Docker Compose pour orchestrer plusieurs conteneurs.</p>"},{"location":"Docker/FR/05-docker-compose/","title":"5. Docker Compose","text":""},{"location":"Docker/FR/05-docker-compose/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Docker Compose</li> <li>Orchestrer plusieurs conteneurs</li> <li>Cr\u00e9er des fichiers docker-compose.yml</li> <li>G\u00e9rer les services et r\u00e9seaux</li> <li>Utiliser les variables d'environnement</li> </ul>"},{"location":"Docker/FR/05-docker-compose/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Docker Compose</li> <li>Fichier docker-compose.yml</li> <li>Services</li> <li>R\u00e9seaux et Volumes</li> <li>Commandes</li> </ol>"},{"location":"Docker/FR/05-docker-compose/#introduction-a-docker-compose","title":"Introduction \u00e0 Docker Compose","text":""},{"location":"Docker/FR/05-docker-compose/#quest-ce-que-docker-compose","title":"Qu'est-ce que Docker Compose ?","text":"<p>Docker Compose = Outil pour orchestrer plusieurs conteneurs</p> <ul> <li>Multi-conteneurs : G\u00e8re plusieurs conteneurs</li> <li>Configuration : Fichier YAML simple</li> <li>Orchestration : D\u00e9marre/arr\u00eate tous les services</li> <li>R\u00e9seaux : Cr\u00e9e automatiquement des r\u00e9seaux</li> </ul>"},{"location":"Docker/FR/05-docker-compose/#pourquoi-docker-compose","title":"Pourquoi Docker Compose ?","text":"<ul> <li>Simplicit\u00e9 : Un fichier pour tout</li> <li>Reproductibilit\u00e9 : M\u00eame environnement partout</li> <li>D\u00e9veloppement : Stack compl\u00e8te locale</li> <li>Production : D\u00e9ploiement simplifi\u00e9</li> </ul>"},{"location":"Docker/FR/05-docker-compose/#fichier-docker-composeyml","title":"Fichier docker-compose.yml","text":""},{"location":"Docker/FR/05-docker-compose/#structure-de-base","title":"Structure de base","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    image: nginx\n    ports:\n      - \"8080:80\"\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_PASSWORD: password\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#exemple-complet","title":"Exemple complet","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8080:8000\"\n    environment:\n      - DATABASE_URL=postgresql://db:5432/mydb\n    depends_on:\n      - db\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: mydb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#services","title":"Services","text":""},{"location":"Docker/FR/05-docker-compose/#definir-un-service","title":"D\u00e9finir un service","text":"<pre><code>services:\n  app:\n    image: python:3.11\n    command: python app.py\n    working_dir: /app\n    volumes:\n      - .:/app\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#build-vs-image","title":"Build vs Image","text":"<pre><code># Utiliser une image existante\nservices:\n  web:\n    image: nginx:latest\n\n# Construire depuis Dockerfile\nservices:\n  app:\n    build: .\n    # ou\n    build:\n      context: .\n      dockerfile: Dockerfile.prod\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#dependances","title":"D\u00e9pendances","text":"<pre><code>services:\n  app:\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres\n\n  redis:\n    image: redis\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#reseaux-et-volumes","title":"R\u00e9seaux et Volumes","text":""},{"location":"Docker/FR/05-docker-compose/#reseaux","title":"R\u00e9seaux","text":"<pre><code>services:\n  app:\n    networks:\n      - frontend\n      - backend\n\n  db:\n    networks:\n      - backend\n\nnetworks:\n  frontend:\n  backend:\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#volumes","title":"Volumes","text":"<pre><code>services:\n  db:\n    volumes:\n      - db-data:/var/lib/postgresql/data\n      - ./backup:/backup\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#commandes","title":"Commandes","text":""},{"location":"Docker/FR/05-docker-compose/#demarrer-les-services","title":"D\u00e9marrer les services","text":"<pre><code># D\u00e9marrer tous les services\ndocker-compose up\n\n# En arri\u00e8re-plan\ndocker-compose up -d\n\n# Reconstruire les images\ndocker-compose up --build\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#arreter-les-services","title":"Arr\u00eater les services","text":"<pre><code># Arr\u00eater les services\ndocker-compose stop\n\n# Arr\u00eater et supprimer\ndocker-compose down\n\n# Supprimer avec volumes\ndocker-compose down -v\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#gestion-des-services","title":"Gestion des services","text":"<pre><code># Voir les services en cours\ndocker-compose ps\n\n# Voir les logs\ndocker-compose logs\n\n# Logs d'un service\ndocker-compose logs web\n\n# Ex\u00e9cuter une commande\ndocker-compose exec web bash\n\n# Red\u00e9marrer un service\ndocker-compose restart web\n</code></pre>"},{"location":"Docker/FR/05-docker-compose/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Docker Compose orchestre plusieurs conteneurs</li> <li>docker-compose.yml d\u00e9finit la configuration</li> <li>Services sont les conteneurs</li> <li>R\u00e9seaux et Volumes pour la communication et donn\u00e9es</li> <li>Commandes : up, down, logs, exec</li> </ol>"},{"location":"Docker/FR/05-docker-compose/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 6. Volumes et R\u00e9seaux pour approfondir.</p>"},{"location":"Docker/FR/06-volumes-networks/","title":"6. Volumes et R\u00e9seaux Docker","text":""},{"location":"Docker/FR/06-volumes-networks/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les volumes Docker</li> <li>G\u00e9rer la persistance des donn\u00e9es</li> <li>Cr\u00e9er et g\u00e9rer des r\u00e9seaux</li> <li>Partager des donn\u00e9es entre conteneurs</li> <li>Configurer la communication r\u00e9seau</li> </ul>"},{"location":"Docker/FR/06-volumes-networks/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Volumes</li> <li>Bind Mounts</li> <li>R\u00e9seaux</li> <li>Communication entre conteneurs</li> <li>Exemples pratiques</li> </ol>"},{"location":"Docker/FR/06-volumes-networks/#volumes","title":"Volumes","text":""},{"location":"Docker/FR/06-volumes-networks/#quest-ce-quun-volume","title":"Qu'est-ce qu'un volume ?","text":"<p>Volume = Stockage persistant pour les donn\u00e9es</p> <ul> <li>Persistant : Survit \u00e0 la suppression du conteneur</li> <li>G\u00e9r\u00e9 par Docker : Stock\u00e9 dans <code>/var/lib/docker/volumes</code></li> <li>Partageable : Plusieurs conteneurs peuvent l'utiliser</li> <li>Performant : Plus rapide que bind mounts</li> </ul>"},{"location":"Docker/FR/06-volumes-networks/#creer-un-volume","title":"Cr\u00e9er un volume","text":"<pre><code># Cr\u00e9er un volume\ndocker volume create my-volume\n\n# Lister les volumes\ndocker volume ls\n\n# Inspecter un volume\ndocker volume inspect my-volume\n\n# Supprimer un volume\ndocker volume rm my-volume\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#utiliser-un-volume","title":"Utiliser un volume","text":"<pre><code># Volume nomm\u00e9\ndocker run -v my-volume:/data ubuntu\n\n# Volume anonyme\ndocker run -v /data ubuntu\n\n# Dans docker-compose.yml\nvolumes:\n  - my-volume:/data\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#bind-mounts","title":"Bind Mounts","text":""},{"location":"Docker/FR/06-volumes-networks/#quest-ce-quun-bind-mount","title":"Qu'est-ce qu'un bind mount ?","text":"<p>Bind Mount = Lien direct vers un r\u00e9pertoire h\u00f4te</p> <ul> <li>Direct : Acc\u00e8s direct aux fichiers h\u00f4te</li> <li>D\u00e9veloppement : Id\u00e9al pour le d\u00e9veloppement</li> <li>Performance : D\u00e9pend du syst\u00e8me de fichiers h\u00f4te</li> </ul>"},{"location":"Docker/FR/06-volumes-networks/#utiliser-un-bind-mount","title":"Utiliser un bind mount","text":"<pre><code># Bind mount\ndocker run -v /host/path:/container/path ubuntu\n\n# Avec docker-compose.yml\nvolumes:\n  - ./data:/app/data\n  - /absolute/path:/container/path\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#differences-volume-vs-bind-mount","title":"Diff\u00e9rences : Volume vs Bind Mount","text":"<p>Volume : - G\u00e9r\u00e9 par Docker - Meilleure performance - Portable - Recommand\u00e9 pour production</p> <p>Bind Mount : - Lien direct - Acc\u00e8s direct - D\u00e9pend du syst\u00e8me h\u00f4te - Recommand\u00e9 pour d\u00e9veloppement</p>"},{"location":"Docker/FR/06-volumes-networks/#reseaux","title":"R\u00e9seaux","text":""},{"location":"Docker/FR/06-volumes-networks/#types-de-reseaux","title":"Types de r\u00e9seaux","text":"<ol> <li>Bridge : R\u00e9seau par d\u00e9faut (isolation)</li> <li>Host : Utilise le r\u00e9seau h\u00f4te</li> <li>None : Pas de r\u00e9seau</li> <li>Overlay : Pour Docker Swarm</li> </ol>"},{"location":"Docker/FR/06-volumes-networks/#creer-un-reseau","title":"Cr\u00e9er un r\u00e9seau","text":"<pre><code># Cr\u00e9er un r\u00e9seau\ndocker network create my-network\n\n# Lister les r\u00e9seaux\ndocker network ls\n\n# Inspecter un r\u00e9seau\ndocker network inspect my-network\n\n# Supprimer un r\u00e9seau\ndocker network rm my-network\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#connecter-un-conteneur","title":"Connecter un conteneur","text":"<pre><code># Connecter au d\u00e9marrage\ndocker run --network my-network ubuntu\n\n# Connecter un conteneur existant\ndocker network connect my-network container-id\n\n# D\u00e9connecter\ndocker network disconnect my-network container-id\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#communication-entre-conteneurs","title":"Communication entre conteneurs","text":""},{"location":"Docker/FR/06-volumes-networks/#meme-reseau","title":"M\u00eame r\u00e9seau","text":"<pre><code># Cr\u00e9er un r\u00e9seau\ndocker network create app-network\n\n# Conteneur 1\ndocker run --name app --network app-network my-app\n\n# Conteneur 2 (peut communiquer avec app)\ndocker run --name db --network app-network postgres\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#avec-docker-compose","title":"Avec Docker Compose","text":"<pre><code>services:\n  app:\n    networks:\n      - app-network\n\n  db:\n    networks:\n      - app-network\n\nnetworks:\n  app-network:\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#resolution-dns","title":"R\u00e9solution DNS","text":"<p>Les conteneurs peuvent se trouver par nom :</p> <pre><code># Dans le conteneur app\nimport psycopg2\nconn = psycopg2.connect(\n    host=\"db\",  # Nom du service\n    database=\"mydb\"\n)\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Docker/FR/06-volumes-networks/#exemple-1-base-de-donnees-avec-volume","title":"Exemple 1 : Base de donn\u00e9es avec volume","text":"<pre><code>version: '3.8'\n\nservices:\n  db:\n    image: postgres:15\n    volumes:\n      - db-data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_DB: mydb\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#exemple-2-application-avec-bind-mount","title":"Exemple 2 : Application avec bind mount","text":"<pre><code>version: '3.8'\n\nservices:\n  app:\n    build: .\n    volumes:\n      - ./src:/app/src  # D\u00e9veloppement\n    networks:\n      - app-network\n\nnetworks:\n  app-network:\n</code></pre>"},{"location":"Docker/FR/06-volumes-networks/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Volumes pour persistance g\u00e9r\u00e9e par Docker</li> <li>Bind Mounts pour acc\u00e8s direct</li> <li>R\u00e9seaux pour communication</li> <li>DNS : R\u00e9solution par nom de service</li> <li>Docker Compose simplifie la gestion</li> </ol>"},{"location":"Docker/FR/06-volumes-networks/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 7. Bonnes pratiques pour les meilleures pratiques.</p>"},{"location":"Docker/FR/07-best-practices/","title":"7. Bonnes pratiques Docker","text":""},{"location":"Docker/FR/07-best-practices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>S\u00e9curiser les conteneurs</li> <li>Optimiser les performances</li> <li>Organiser les projets</li> <li>Maintenir les images</li> <li>G\u00e9rer les ressources</li> </ul>"},{"location":"Docker/FR/07-best-practices/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>S\u00e9curit\u00e9</li> <li>Performance</li> <li>Organisation</li> <li>Maintenance</li> <li>Ressources</li> </ol>"},{"location":"Docker/FR/07-best-practices/#securite","title":"S\u00e9curit\u00e9","text":""},{"location":"Docker/FR/07-best-practices/#utiliser-des-images-officielles","title":"Utiliser des images officielles","text":"<pre><code># Bon\nFROM python:3.11-slim\n\n# \u00c9viter\nFROM random-user/python:latest\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#ne-pas-utiliser-root","title":"Ne pas utiliser root","text":"<pre><code># Cr\u00e9er un utilisateur non-root\nRUN useradd -m appuser\nUSER appuser\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#limiter-les-privileges","title":"Limiter les privil\u00e8ges","text":"<pre><code># Ne pas utiliser --privileged\ndocker run --privileged my-container  # \u00c9viter\n\n# Utiliser des capabilities sp\u00e9cifiques si n\u00e9cessaire\ndocker run --cap-add NET_ADMIN my-container\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#secrets","title":"Secrets","text":"<pre><code># docker-compose.yml\nservices:\n  app:\n    secrets:\n      - db_password\n\nsecrets:\n  db_password:\n    file: ./secrets/password.txt\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#performance","title":"Performance","text":""},{"location":"Docker/FR/07-best-practices/#utiliser-dockerignore","title":"Utiliser .dockerignore","text":"<pre><code>__pycache__\n*.pyc\n.git\n.env\nnode_modules\n*.log\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#optimiser-les-couches","title":"Optimiser les couches","text":"<pre><code># Mauvais\nRUN apt update\nRUN apt install -y python3\nRUN apt clean\n\n# Bon\nRUN apt update &amp;&amp; \\\n    apt install -y python3 &amp;&amp; \\\n    apt clean\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#cache-des-builds","title":"Cache des builds","text":"<p>Ordre des instructions :</p> <pre><code># D'abord les d\u00e9pendances (changent peu)\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Ensuite le code (change souvent)\nCOPY . .\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#organisation","title":"Organisation","text":""},{"location":"Docker/FR/07-best-practices/#structure-de-projet","title":"Structure de projet","text":"<pre><code>my-project/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 app.py\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 data.csv\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#tags-dimages","title":"Tags d'images","text":"<pre><code># Tags s\u00e9mantiques\ndocker build -t my-app:1.0.0 .\ndocker build -t my-app:latest .\n\n# Tags pour environnement\ndocker build -t my-app:dev .\ndocker build -t my-app:prod .\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#maintenance","title":"Maintenance","text":""},{"location":"Docker/FR/07-best-practices/#nettoyer-les-ressources","title":"Nettoyer les ressources","text":"<pre><code># Supprimer les conteneurs arr\u00eat\u00e9s\ndocker container prune\n\n# Supprimer les images non utilis\u00e9es\ndocker image prune\n\n# Supprimer les volumes non utilis\u00e9s\ndocker volume prune\n\n# Nettoyer tout\ndocker system prune -a\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#mettre-a-jour-les-images","title":"Mettre \u00e0 jour les images","text":"<pre><code># Mettre \u00e0 jour une image\ndocker pull python:3.11\n\n# Reconstruire\ndocker-compose build --no-cache\ndocker-compose up\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#ressources","title":"Ressources","text":""},{"location":"Docker/FR/07-best-practices/#limiter-les-ressources","title":"Limiter les ressources","text":"<pre><code># docker-compose.yml\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 512M\n        reservations:\n          cpus: '0.5'\n          memory: 256M\n</code></pre>"},{"location":"Docker/FR/07-best-practices/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>S\u00e9curit\u00e9 : Images officielles, non-root</li> <li>Performance : Optimiser les couches, cache</li> <li>Organisation : Structure claire, tags</li> <li>Maintenance : Nettoyer r\u00e9guli\u00e8rement</li> <li>Ressources : Limiter l'utilisation</li> </ol>"},{"location":"Docker/FR/07-best-practices/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 8. Projets pratiques pour cr\u00e9er des projets complets.</p>"},{"location":"Docker/FR/08-projets/","title":"8. Projets pratiques Docker","text":""},{"location":"Docker/FR/08-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Conteneuriser une application Python</li> <li>Cr\u00e9er un pipeline de donn\u00e9es avec Docker</li> <li>Stack compl\u00e8te avec Docker Compose</li> <li>Projets pour portfolio</li> </ul>"},{"location":"Docker/FR/08-projets/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Projet 1 : Application Python</li> <li>Projet 2 : Pipeline de donn\u00e9es</li> <li>Projet 3 : Stack compl\u00e8te</li> <li>Projet 4 : Application web</li> </ol>"},{"location":"Docker/FR/08-projets/#projet-1-application-python","title":"Projet 1 : Application Python","text":""},{"location":"Docker/FR/08-projets/#objectif","title":"Objectif","text":"<p>Conteneuriser une application Python simple.</p>"},{"location":"Docker/FR/08-projets/#structure","title":"Structure","text":"<pre><code>python-app/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 app.py\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 data.csv\n</code></pre>"},{"location":"Docker/FR/08-projets/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/FR/08-projets/#apppy","title":"app.py","text":"<pre><code>import pandas as pd\n\ndef main():\n    df = pd.read_csv('data/data.csv')\n    print(f\"Loaded {len(df)} rows\")\n    print(df.head())\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"Docker/FR/08-projets/#construire-et-executer","title":"Construire et ex\u00e9cuter","text":"<pre><code># Construire\ndocker build -t python-app .\n\n# Ex\u00e9cuter\ndocker run -v $(pwd)/data:/app/data python-app\n</code></pre>"},{"location":"Docker/FR/08-projets/#projet-2-pipeline-de-donnees","title":"Projet 2 : Pipeline de donn\u00e9es","text":""},{"location":"Docker/FR/08-projets/#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er un pipeline ETL avec Docker.</p>"},{"location":"Docker/FR/08-projets/#structure_1","title":"Structure","text":"<pre><code>etl-pipeline/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 extract/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 extract.py\n\u251c\u2500\u2500 transform/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 transform.py\n\u2514\u2500\u2500 load/\n    \u251c\u2500\u2500 Dockerfile\n    \u2514\u2500\u2500 load.py\n</code></pre>"},{"location":"Docker/FR/08-projets/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  extract:\n    build: ./extract\n    volumes:\n      - ./data:/data\n\n  transform:\n    build: ./transform\n    depends_on:\n      - extract\n    volumes:\n      - ./data:/data\n\n  load:\n    build: ./load\n    depends_on:\n      - transform\n    volumes:\n      - ./data:/data\n</code></pre>"},{"location":"Docker/FR/08-projets/#projet-3-stack-complete","title":"Projet 3 : Stack compl\u00e8te","text":""},{"location":"Docker/FR/08-projets/#objectif_2","title":"Objectif","text":"<p>Stack compl\u00e8te avec base de donn\u00e9es et application.</p>"},{"location":"Docker/FR/08-projets/#docker-composeyml_1","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8080:8000\"\n    environment:\n      - DATABASE_URL=postgresql://user:password@db:5432/mydb\n    depends_on:\n      - db\n    volumes:\n      - ./src:/app\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: mydb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db-data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n\n  redis:\n    image: redis:7\n    ports:\n      - \"6379:6379\"\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/FR/08-projets/#projet-4-application-web","title":"Projet 4 : Application web","text":""},{"location":"Docker/FR/08-projets/#objectif_3","title":"Objectif","text":"<p>Application web Flask avec base de donn\u00e9es.</p>"},{"location":"Docker/FR/08-projets/#dockerfile_1","title":"Dockerfile","text":"<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\"]\n</code></pre>"},{"location":"Docker/FR/08-projets/#docker-composeyml_2","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"5000:5000\"\n    environment:\n      - FLASK_ENV=development\n      - DATABASE_URL=postgresql://user:password@db:5432/appdb\n    depends_on:\n      - db\n    volumes:\n      - .:/app\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: appdb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/FR/08-projets/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Conteneurisation : Isoler les applications</li> <li>Docker Compose : Orchestrer plusieurs services</li> <li>Volumes : Persister les donn\u00e9es</li> <li>R\u00e9seaux : Communication entre services</li> <li>Portfolio : Projets d\u00e9montrables</li> </ol>"},{"location":"Docker/FR/08-projets/#ressources","title":"\ud83d\udd17 Ressources","text":"<ul> <li>Docker Examples</li> <li>Docker Documentation</li> </ul> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation Docker. Vous pouvez maintenant conteneuriser vos applications.</p>"},{"location":"Docker/PL/","title":"Szkolenie Docker dla Data Analyst","text":""},{"location":"Docker/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>To szkolenie poprowadzi Ci\u0119 przez nauk\u0119 Docker jako Data Analyst. Docker to platforma konteneryzacji, kt\u00f3ra pozwala tworzy\u0107, wdra\u017ca\u0107 i uruchamia\u0107 aplikacje w izolowanych kontenerach.</p>"},{"location":"Docker/PL/#cele-szkoleniowe","title":"\ud83c\udfaf Cele szkoleniowe","text":"<ul> <li>Zrozumie\u0107 Docker i konteneryzacj\u0119</li> <li>Zainstalowa\u0107 Docker</li> <li>Tworzy\u0107 i zarz\u0105dza\u0107 kontenerami</li> <li>Budowa\u0107 obrazy Docker</li> <li>U\u017cywa\u0107 Docker Compose</li> <li>Integrowa\u0107 Docker w przep\u0142ywy danych</li> <li>Tworzy\u0107 praktyczne projekty do portfolio</li> </ul>"},{"location":"Docker/PL/#wszystko-jest-darmowe","title":"\ud83d\udcb0 Wszystko jest darmowe!","text":"<p>To szkolenie u\u017cywa tylko: - \u2705 Docker Desktop : Darmowy do u\u017cytku osobistego/edukacyjnego - \u2705 Docker Hub : Darmowy rejestr publiczny - \u2705 Oficjalna dokumentacja : Kompletne darmowe przewodniki - \u2705 Tutoriale online : Darmowe zasoby</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"Docker/PL/#struktura-szkolenia","title":"\ud83d\udcd6 Struktura szkolenia","text":""},{"location":"Docker/PL/#1-rozpoczecie-z-docker","title":"1. Rozpocz\u0119cie z Docker","text":"<ul> <li>Zainstalowa\u0107 Docker</li> <li>Podstawowe koncepcje</li> <li>Pierwsze kontenery</li> <li>Podstawowe polecenia</li> </ul>"},{"location":"Docker/PL/#2-kontenery","title":"2. Kontenery","text":"<ul> <li>Tworzy\u0107 kontenery</li> <li>Zarz\u0105dza\u0107 cyklem \u017cycia</li> <li>Wykonywa\u0107 polecenia</li> <li>Logi i debugowanie</li> </ul>"},{"location":"Docker/PL/#3-obrazy-docker","title":"3. Obrazy Docker","text":"<ul> <li>Zrozumie\u0107 obrazy</li> <li>Pobiera\u0107 obrazy</li> <li>Tworzy\u0107 niestandardowe obrazy</li> <li>Zarz\u0105dza\u0107 obrazami</li> </ul>"},{"location":"Docker/PL/#4-dockerfile","title":"4. Dockerfile","text":"<ul> <li>Pisa\u0107 Dockerfile</li> <li>Dobre praktyki</li> <li>Optymalizowa\u0107 obrazy</li> <li>Multi-stage builds</li> </ul>"},{"location":"Docker/PL/#5-docker-compose","title":"5. Docker Compose","text":"<ul> <li>Orkiestrowa\u0107 wiele kontener\u00f3w</li> <li>Plik docker-compose.yml</li> <li>Us\u0142ugi i sieci</li> <li>Zmienne \u015brodowiskowe</li> </ul>"},{"location":"Docker/PL/#6-wolumeny-i-sieci","title":"6. Wolumeny i Sieci","text":"<ul> <li>Zarz\u0105dza\u0107 wolumenami</li> <li>Tworzy\u0107 sieci</li> <li>Dzieli\u0107 dane</li> <li>Trwa\u0142o\u015b\u0107 danych</li> </ul>"},{"location":"Docker/PL/#7-dobre-praktyki","title":"7. Dobre praktyki","text":"<ul> <li>Bezpiecze\u0144stwo</li> <li>Wydajno\u015b\u0107</li> <li>Organizacja</li> <li>Konserwacja</li> </ul>"},{"location":"Docker/PL/#8-projekty-praktyczne","title":"8. Projekty praktyczne","text":"<ul> <li>Konteneryzowa\u0107 aplikacj\u0119 Python</li> <li>Pipeline danych z Docker</li> <li>Kompletny stack z Docker Compose</li> <li>Projekty do portfolio</li> </ul>"},{"location":"Docker/PL/#szybki-start","title":"\ud83d\ude80 Szybki start","text":""},{"location":"Docker/PL/#wymagania-wstepne","title":"Wymagania wst\u0119pne","text":"<ul> <li>System operacyjny : Windows, Linux lub macOS</li> <li>4 GB RAM : Minimum zalecane</li> <li>Miejsce na dysku : 20 GB wolne</li> </ul>"},{"location":"Docker/PL/#szybka-instalacja","title":"Szybka instalacja","text":"<p>Windows/Mac: 1. Pobra\u0107 Docker Desktop: https://www.docker.com/products/docker-desktop 2. Zainstalowa\u0107 i uruchomi\u0107 Docker Desktop 3. Sprawdzi\u0107 instalacj\u0119: <code>docker --version</code></p> <p>Linux: <pre><code># Zainstalowa\u0107 Docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n\n# Uruchomi\u0107 Docker\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# Sprawdzi\u0107\ndocker --version\n</code></pre></p>"},{"location":"Docker/PL/#pierwszy-kontener","title":"Pierwszy kontener","text":"<pre><code># Uruchomi\u0107 kontener Hello World\ndocker run hello-world\n\n# Uruchomi\u0107 kontener interaktywny\ndocker run -it ubuntu bash\n</code></pre>"},{"location":"Docker/PL/#przypadki-uzycia-dla-data-analyst","title":"\ud83d\udcca Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Reprodukowalne \u015brodowiska : To samo \u015brodowisko wsz\u0119dzie</li> <li>Izolacja : Oddzieli\u0107 zale\u017cno\u015bci</li> <li>Wdra\u017canie : \u0141atwo wdra\u017ca\u0107 aplikacje</li> <li>CI/CD : Integrowa\u0107 w pipeline'y</li> <li>Data Science : Izolowane \u015brodowiska Python/R</li> </ul>"},{"location":"Docker/PL/#darmowe-zasoby","title":"\ud83d\udcda Darmowe zasoby","text":""},{"location":"Docker/PL/#oficjalna-dokumentacja","title":"Oficjalna dokumentacja","text":"<ul> <li>Dokumentacja Docker : https://docs.docker.com/</li> <li>Docker Hub : https://hub.docker.com/</li> <li>Docker Playground : https://labs.play-with-docker.com/</li> </ul>"},{"location":"Docker/PL/#certyfikacje-opcjonalne","title":"\ud83c\udf93 Certyfikacje (opcjonalne)","text":""},{"location":"Docker/PL/#docker-certified-associate-dca","title":"Docker Certified Associate (DCA)","text":"<ul> <li>Koszt : ~$195</li> <li>Przygotowanie : Darmowa dokumentacja</li> <li>Czas trwania : 2-4 tygodnie</li> <li>Poziom : \u015arednio zaawansowany</li> </ul>"},{"location":"Docker/PL/01-getting-started/","title":"1. Rozpocz\u0119cie z Docker","text":""},{"location":"Docker/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Docker i konteneryzacj\u0119</li> <li>Zainstalowa\u0107 Docker</li> <li>Zrozumie\u0107 podstawowe koncepcje</li> <li>Uruchomi\u0107 pierwszy kontener</li> </ul>"},{"location":"Docker/PL/01-getting-started/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Docker</li> <li>Instalacja</li> <li>Podstawowe koncepcje</li> <li>Pierwsze kontenery</li> <li>Podstawowe polecenia</li> </ol>"},{"location":"Docker/PL/01-getting-started/#wprowadzenie-do-docker","title":"Wprowadzenie do Docker","text":""},{"location":"Docker/PL/01-getting-started/#czym-jest-docker","title":"Czym jest Docker?","text":"<p>Docker = Platforma konteneryzacji</p> <ul> <li>Kontenery : Izolowane i lekkie \u015brodowiska</li> <li>Przeno\u015bny : Dzia\u0142a wsz\u0119dzie (Windows, Linux, macOS)</li> <li>Wydajny : U\u017cywa mniej zasob\u00f3w ni\u017c VMs</li> <li>Szybki : Uruchamia si\u0119 w sekundach</li> </ul>"},{"location":"Docker/PL/01-getting-started/#dlaczego-docker-dla-data-analyst","title":"Dlaczego Docker dla Data Analyst?","text":"<ul> <li>Reprodukowalno\u015b\u0107 : To samo \u015brodowisko wsz\u0119dzie</li> <li>Izolacja : Oddzieli\u0107 zale\u017cno\u015bci Python/R</li> <li>Prostota : \u0141atwo dzieli\u0107 i wdra\u017ca\u0107</li> <li>Wydajno\u015b\u0107 : Szybszy ni\u017c VMs</li> </ul>"},{"location":"Docker/PL/01-getting-started/#instalacja","title":"Instalacja","text":""},{"location":"Docker/PL/01-getting-started/#windows","title":"Windows","text":"<ol> <li>Przej\u015b\u0107 do: https://www.docker.com/products/docker-desktop</li> <li>Pobra\u0107 Docker Desktop dla Windows</li> <li>Zainstalowa\u0107 plik <code>.exe</code></li> <li>Uruchomi\u0107 ponownie je\u015bli potrzebne</li> </ol>"},{"location":"Docker/PL/01-getting-started/#linux","title":"Linux","text":"<pre><code># Zaktualizowa\u0107 pakiety\nsudo apt update\n\n# Zainstalowa\u0107 zale\u017cno\u015bci\nsudo apt install apt-transport-https ca-certificates curl gnupg lsb-release\n\n# Doda\u0107 klucz GPG Docker\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Doda\u0107 repozytorium Docker\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Zainstalowa\u0107 Docker\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io\n\n# Uruchomi\u0107 Docker\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# Sprawdzi\u0107\ndocker --version\n</code></pre>"},{"location":"Docker/PL/01-getting-started/#macos","title":"macOS","text":"<ol> <li>Przej\u015b\u0107 do: https://www.docker.com/products/docker-desktop</li> <li>Pobra\u0107 Docker Desktop dla Mac</li> <li>Zainstalowa\u0107 plik <code>.dmg</code></li> <li>Otworzy\u0107 Docker z Aplikacji</li> </ol>"},{"location":"Docker/PL/01-getting-started/#podstawowe-koncepcje","title":"Podstawowe koncepcje","text":""},{"location":"Docker/PL/01-getting-started/#obrazy","title":"Obrazy","text":"<p>Obraz = Szablon tylko do odczytu do tworzenia kontener\u00f3w</p> <ul> <li>Szablon : Zawiera OS, aplikacje, zale\u017cno\u015bci</li> <li>Niezmienny : Nie zmienia si\u0119 po utworzeniu</li> <li>Lekki : Dzieli wsp\u00f3lne warstwy</li> </ul>"},{"location":"Docker/PL/01-getting-started/#kontenery","title":"Kontenery","text":"<p>Kontener = Wykonywalna instancja obrazu</p> <ul> <li>Izolowany : Oddzielne \u015brodowisko</li> <li>Efemeryczny : Mo\u017ce by\u0107 \u0142atwo tworzony/usuwany</li> <li>Przeno\u015bny : Dzia\u0142a wsz\u0119dzie gdzie Docker jest zainstalowany</li> </ul>"},{"location":"Docker/PL/01-getting-started/#pierwsze-kontenery","title":"Pierwsze kontenery","text":""},{"location":"Docker/PL/01-getting-started/#hello-world","title":"Hello World","text":"<pre><code># Uruchomi\u0107 kontener Hello World\ndocker run hello-world\n</code></pre>"},{"location":"Docker/PL/01-getting-started/#kontener-interaktywny","title":"Kontener interaktywny","text":"<pre><code># Uruchomi\u0107 interaktywny kontener Ubuntu\ndocker run -it ubuntu bash\n\n# W kontenerze\nls\npwd\nexit\n</code></pre>"},{"location":"Docker/PL/01-getting-started/#kontener-w-tle","title":"Kontener w tle","text":"<pre><code># Uruchomi\u0107 kontener w tle\ndocker run -d --name my-container nginx\n\n# Zobaczy\u0107 uruchomione kontenery\ndocker ps\n\n# Zobaczy\u0107 logi\ndocker logs my-container\n\n# Zatrzyma\u0107 kontener\ndocker stop my-container\n</code></pre>"},{"location":"Docker/PL/01-getting-started/#podstawowe-polecenia","title":"Podstawowe polecenia","text":""},{"location":"Docker/PL/01-getting-started/#zarzadzanie-kontenerami","title":"Zarz\u0105dzanie kontenerami","text":"<pre><code># Listowa\u0107 uruchomione kontenery\ndocker ps\n\n# Listowa\u0107 wszystkie kontenery\ndocker ps -a\n\n# Uruchomi\u0107 kontener\ndocker start my-container\n\n# Zatrzyma\u0107 kontener\ndocker stop my-container\n\n# Usun\u0105\u0107 kontener\ndocker rm my-container\n</code></pre>"},{"location":"Docker/PL/01-getting-started/#zarzadzanie-obrazami","title":"Zarz\u0105dzanie obrazami","text":"<pre><code># Listowa\u0107 obrazy\ndocker images\n\n# Pobra\u0107 obraz\ndocker pull ubuntu\n\n# Usun\u0105\u0107 obraz\ndocker rmi ubuntu\n</code></pre>"},{"location":"Docker/PL/01-getting-started/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Docker = Konteneryzacja do izolacji aplikacji</li> <li>Obrazy to szablony, Kontenery to instancje</li> <li>Docker Hub do znajdowania obraz\u00f3w</li> <li>Podstawowe polecenia : run, ps, stop, rm</li> <li>Przeno\u015bny : Dzia\u0142a wsz\u0119dzie</li> </ol>"},{"location":"Docker/PL/01-getting-started/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 2. Kontenery, aby pog\u0142\u0119bi\u0107 zarz\u0105dzanie kontenerami.</p>"},{"location":"Docker/PL/02-containers/","title":"2. Kontenery Docker","text":""},{"location":"Docker/PL/02-containers/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 i zarz\u0105dza\u0107 kontenerami</li> <li>Zrozumie\u0107 cykl \u017cycia</li> <li>Wykonywa\u0107 polecenia</li> <li>Zarz\u0105dza\u0107 logami</li> <li>Konfigurowa\u0107 kontenery</li> </ul>"},{"location":"Docker/PL/02-containers/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Cykl \u017cycia kontenera</li> <li>Tworzy\u0107 kontenery</li> <li>Wykonywa\u0107 polecenia</li> <li>Logi i debugowanie</li> <li>Konfiguracja</li> </ol>"},{"location":"Docker/PL/02-containers/#cykl-zycia-kontenera","title":"Cykl \u017cycia kontenera","text":""},{"location":"Docker/PL/02-containers/#stany-kontenera","title":"Stany kontenera","text":"<ol> <li>Created : Kontener utworzony ale nie uruchomiony</li> <li>Running : Kontener w trakcie wykonania</li> <li>Paused : Kontener wstrzymany</li> <li>Stopped : Kontener zatrzymany</li> <li>Removed : Kontener usuni\u0119ty</li> </ol>"},{"location":"Docker/PL/02-containers/#polecenia-cyklu-zycia","title":"Polecenia cyklu \u017cycia","text":"<pre><code># Utworzy\u0107 kontener\ndocker create --name my-container ubuntu\n\n# Uruchomi\u0107 kontener\ndocker start my-container\n\n# Zatrzyma\u0107 kontener\ndocker stop my-container\n\n# Uruchomi\u0107 ponownie kontener\ndocker restart my-container\n\n# Wstrzyma\u0107\ndocker pause my-container\n\n# Wznowi\u0107\ndocker unpause my-container\n\n# Usun\u0105\u0107 kontener\ndocker rm my-container\n</code></pre>"},{"location":"Docker/PL/02-containers/#tworzyc-kontenery","title":"Tworzy\u0107 kontenery","text":""},{"location":"Docker/PL/02-containers/#tworzyc-z-docker-run","title":"Tworzy\u0107 z docker run","text":"<pre><code># Utworzy\u0107 i uruchomi\u0107 kontener\ndocker run ubuntu echo \"Hello\"\n\n# Utworzy\u0107 bez uruchamiania\ndocker create --name my-container ubuntu\n\n# Utworzy\u0107 z niestandardow\u0105 nazw\u0105\ndocker run --name my-app ubuntu\n</code></pre>"},{"location":"Docker/PL/02-containers/#wazne-opcje","title":"Wa\u017cne opcje","text":"<pre><code># Tryb interaktywny\ndocker run -it ubuntu bash\n\n# Tryb od\u0142\u0105czony (w tle)\ndocker run -d nginx\n\n# Udost\u0119pni\u0107 port\ndocker run -p 8080:80 nginx\n\n# Zamontowa\u0107 wolumen\ndocker run -v /host/path:/container/path ubuntu\n\n# Zmienne \u015brodowiskowe\ndocker run -e MY_VAR=value ubuntu\n\n# Nazwa kontenera\ndocker run --name my-container ubuntu\n</code></pre>"},{"location":"Docker/PL/02-containers/#wykonywac-polecenia","title":"Wykonywa\u0107 polecenia","text":""},{"location":"Docker/PL/02-containers/#wykonywac-w-uruchomionym-kontenerze","title":"Wykonywa\u0107 w uruchomionym kontenerze","text":"<pre><code># Wykona\u0107 polecenie\ndocker exec my-container ls\n\n# Tryb interaktywny\ndocker exec -it my-container bash\n\n# Wykona\u0107 Python\ndocker exec -it my-container python\n</code></pre>"},{"location":"Docker/PL/02-containers/#wykonywac-przy-starcie","title":"Wykonywa\u0107 przy starcie","text":"<pre><code># Polecenie domy\u015blne\ndocker run ubuntu echo \"Hello\"\n\n# Nadpisa\u0107 polecenie\ndocker run ubuntu ls -la\n\n# Wykona\u0107 skrypt\ndocker run -v $(pwd):/app ubuntu bash /app/script.sh\n</code></pre>"},{"location":"Docker/PL/02-containers/#logi-i-debugowanie","title":"Logi i debugowanie","text":""},{"location":"Docker/PL/02-containers/#zobaczyc-logi","title":"Zobaczy\u0107 logi","text":"<pre><code># Logi kontenera\ndocker logs my-container\n\n# \u015aledzi\u0107 logi (tail -f)\ndocker logs -f my-container\n\n# Ostatnie linie\ndocker logs --tail 100 my-container\n\n# Z timestampem\ndocker logs -t my-container\n</code></pre>"},{"location":"Docker/PL/02-containers/#sprawdzic-kontener","title":"Sprawdzi\u0107 kontener","text":"<pre><code># Pe\u0142ne informacje\ndocker inspect my-container\n\n# Konkretne informacje\ndocker inspect --format='{{.State.Status}}' my-container\n\n# Konfiguracja sieci\ndocker inspect --format='{{.NetworkSettings.IPAddress}}' my-container\n</code></pre>"},{"location":"Docker/PL/02-containers/#statystyki","title":"Statystyki","text":"<pre><code># Statystyki w czasie rzeczywistym\ndocker stats\n\n# Statystyki kontenera\ndocker stats my-container\n\n# Statystyki bez streamingu\ndocker stats --no-stream\n</code></pre>"},{"location":"Docker/PL/02-containers/#konfiguracja","title":"Konfiguracja","text":""},{"location":"Docker/PL/02-containers/#zmienne-srodowiskowe","title":"Zmienne \u015brodowiskowe","text":"<pre><code># Jedna zmienna\ndocker run -e MY_VAR=value ubuntu\n\n# Wiele zmiennych\ndocker run -e VAR1=value1 -e VAR2=value2 ubuntu\n\n# Plik .env\ndocker run --env-file .env ubuntu\n</code></pre>"},{"location":"Docker/PL/02-containers/#porty","title":"Porty","text":"<pre><code># Udost\u0119pni\u0107 port\ndocker run -p 8080:80 nginx\n\n# Udost\u0119pni\u0107 wiele port\u00f3w\ndocker run -p 8080:80 -p 3306:3306 my-app\n\n# Udost\u0119pni\u0107 wszystkie porty\ndocker run -P nginx\n</code></pre>"},{"location":"Docker/PL/02-containers/#wolumeny","title":"Wolumeny","text":"<pre><code># Wolumen nazwany\ndocker run -v my-volume:/data ubuntu\n\n# Bind mount\ndocker run -v /host/path:/container/path ubuntu\n\n# Wolumen anonimowy\ndocker run -v /data ubuntu\n</code></pre>"},{"location":"Docker/PL/02-containers/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Cykl \u017cycia : Created \u2192 Running \u2192 Stopped \u2192 Removed</li> <li>docker run : Tworzy i uruchamia</li> <li>docker exec : Wykonuje w uruchomionym kontenerze</li> <li>docker logs : Zobaczy\u0107 logi</li> <li>Konfiguracja : Zmienne, porty, wolumeny</li> </ol>"},{"location":"Docker/PL/02-containers/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 3. Obrazy Docker, aby nauczy\u0107 si\u0119 zarz\u0105dzania obrazami.</p>"},{"location":"Docker/PL/03-images/","title":"3. Obrazy Docker","text":""},{"location":"Docker/PL/03-images/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 obrazy Docker</li> <li>Pobiera\u0107 obrazy</li> <li>Tworzy\u0107 niestandardowe obrazy</li> <li>Zarz\u0105dza\u0107 obrazami</li> <li>Optymalizowa\u0107 obrazy</li> </ul>"},{"location":"Docker/PL/03-images/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do obraz\u00f3w</li> <li>Pobiera\u0107 obrazy</li> <li>Tworzy\u0107 obrazy</li> <li>Zarz\u0105dza\u0107 obrazami</li> <li>Optymalizacja</li> </ol>"},{"location":"Docker/PL/03-images/#wprowadzenie-do-obrazow","title":"Wprowadzenie do obraz\u00f3w","text":""},{"location":"Docker/PL/03-images/#czym-jest-obraz","title":"Czym jest obraz?","text":"<p>Obraz = Szablon tylko do odczytu</p> <ul> <li>Szablon : Do tworzenia kontener\u00f3w</li> <li>Warstwy : Sk\u0142ada si\u0119 z warstw</li> <li>Niezmienny : Nie zmienia si\u0119</li> <li>Dzielony : Wiele kontener\u00f3w mo\u017ce u\u017cywa\u0107 tego samego obrazu</li> </ul>"},{"location":"Docker/PL/03-images/#struktura-obrazu","title":"Struktura obrazu","text":"<pre><code>Obraz\n\u251c\u2500\u2500 Warstwa 1 : OS bazowy (Ubuntu)\n\u251c\u2500\u2500 Warstwa 2 : Narz\u0119dzia systemowe\n\u251c\u2500\u2500 Warstwa 3 : Python\n\u2514\u2500\u2500 Warstwa 4 : Twoja aplikacja\n</code></pre>"},{"location":"Docker/PL/03-images/#pobierac-obrazy","title":"Pobiera\u0107 obrazy","text":""},{"location":"Docker/PL/03-images/#docker-hub","title":"Docker Hub","text":"<p>Docker Hub = Publiczny rejestr obraz\u00f3w</p> <ul> <li>Obrazy oficjalne : python, postgres, nginx, itp.</li> <li>Obrazy spo\u0142eczno\u015bciowe : Utworzone przez spo\u0142eczno\u015b\u0107</li> <li>Darmowy : Do u\u017cytku publicznego</li> </ul>"},{"location":"Docker/PL/03-images/#pobierac-obraz","title":"Pobiera\u0107 obraz","text":"<pre><code># Pobiera\u0107 obraz\ndocker pull python:3.11\n\n# Pobiera\u0107 najnowsz\u0105 wersj\u0119\ndocker pull python:latest\n\n# Pobiera\u0107 konkretn\u0105 wersj\u0119\ndocker pull python:3.11-slim\n\n# Szuka\u0107 obraz\u00f3w\ndocker search python\n</code></pre>"},{"location":"Docker/PL/03-images/#popularne-obrazy-dla-data-analyst","title":"Popularne obrazy dla Data Analyst","text":"<pre><code># Python\ndocker pull python:3.11\n\n# Jupyter Notebook\ndocker pull jupyter/scipy-notebook\n\n# PostgreSQL\ndocker pull postgres:15\n\n# MySQL\ndocker pull mysql:8.0\n\n# Redis\ndocker pull redis:7\n</code></pre>"},{"location":"Docker/PL/03-images/#tworzyc-obrazy","title":"Tworzy\u0107 obrazy","text":""},{"location":"Docker/PL/03-images/#z-dockerfile","title":"Z Dockerfile","text":"<p>Utworzy\u0107 Dockerfile:</p> <pre><code># Dockerfile\nFROM python:3.11\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]\n</code></pre> <p>Zbudowa\u0107 obraz:</p> <pre><code># Zbudowa\u0107 obraz\ndocker build -t my-app:latest .\n\n# Z konkretnym tagiem\ndocker build -t my-app:v1.0 .\n\n# Z konkretnego Dockerfile\ndocker build -f Dockerfile.prod -t my-app:prod .\n</code></pre>"},{"location":"Docker/PL/03-images/#commit-z-kontenera","title":"Commit z kontenera","text":"<pre><code># Utworzy\u0107 kontener\ndocker run -it ubuntu bash\n\n# Wprowadzi\u0107 modyfikacje w kontenerze\napt update\napt install python3\n\n# Utworzy\u0107 obraz z kontenera\ndocker commit container-id my-image:tag\n</code></pre>"},{"location":"Docker/PL/03-images/#zarzadzac-obrazami","title":"Zarz\u0105dza\u0107 obrazami","text":""},{"location":"Docker/PL/03-images/#listowac-obrazy","title":"Listowa\u0107 obrazy","text":"<pre><code># Listowa\u0107 wszystkie obrazy\ndocker images\n\n# Filtrowa\u0107 po nazwie\ndocker images python\n\n# Pokaza\u0107 tylko ID\ndocker images -q\n</code></pre>"},{"location":"Docker/PL/03-images/#usunac-obrazy","title":"Usun\u0105\u0107 obrazy","text":"<pre><code># Usun\u0105\u0107 obraz\ndocker rmi my-image:tag\n\n# Usun\u0105\u0107 po ID\ndocker rmi image-id\n\n# Usun\u0105\u0107 nieu\u017cywane obrazy\ndocker image prune\n\n# Usun\u0105\u0107 wszystkie obrazy\ndocker rmi $(docker images -q)\n</code></pre>"},{"location":"Docker/PL/03-images/#tagowac-obrazy","title":"Tagowa\u0107 obrazy","text":"<pre><code># Utworzy\u0107 tag\ndocker tag my-image:latest my-image:v1.0\n\n# Tagowa\u0107 dla Docker Hub\ndocker tag my-image:latest username/my-image:latest\n</code></pre>"},{"location":"Docker/PL/03-images/#sprawdzic-obraz","title":"Sprawdzi\u0107 obraz","text":"<pre><code># Pe\u0142ne informacje\ndocker inspect my-image\n\n# Historia warstw\ndocker history my-image\n\n# Rozmiar obrazu\ndocker images my-image\n</code></pre>"},{"location":"Docker/PL/03-images/#optymalizacja","title":"Optymalizacja","text":""},{"location":"Docker/PL/03-images/#obrazy-lekkie","title":"Obrazy lekkie","text":"<p>U\u017cywa\u0107 obraz\u00f3w slim:</p> <pre><code># Zamiast\nFROM python:3.11\n\n# U\u017cywa\u0107\nFROM python:3.11-slim\n</code></pre> <p>Multi-stage builds:</p> <pre><code># Stage 1 : Build\nFROM python:3.11 as builder\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user -r requirements.txt\n\n# Stage 2 : Runtime\nFROM python:3.11-slim\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY . .\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/PL/03-images/#zmniejszyc-rozmiar","title":"Zmniejszy\u0107 rozmiar","text":"<p>Dobre praktyki: 1. U\u017cywa\u0107 <code>.dockerignore</code> 2. \u0141\u0105czy\u0107 polecenia RUN 3. U\u017cywa\u0107 lekkich obraz\u00f3w bazowych 4. Czy\u015bci\u0107 cache</p>"},{"location":"Docker/PL/03-images/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Obrazy to szablony dla kontener\u00f3w</li> <li>Docker Hub do znajdowania obraz\u00f3w</li> <li>Dockerfile do tworzenia obraz\u00f3w</li> <li>Warstwy umo\u017cliwiaj\u0105 dzielenie</li> <li>Optymalizacja zmniejsza rozmiar</li> </ol>"},{"location":"Docker/PL/03-images/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 4. Dockerfile, aby nauczy\u0107 si\u0119 pisa\u0107 Dockerfile.</p>"},{"location":"Docker/PL/04-dockerfile/","title":"4. Dockerfile","text":""},{"location":"Docker/PL/04-dockerfile/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Pisa\u0107 Dockerfile</li> <li>Zrozumie\u0107 instrukcje</li> <li>Optymalizowa\u0107 Dockerfile</li> <li>U\u017cywa\u0107 multi-stage builds</li> <li>Dobre praktyki</li> </ul>"},{"location":"Docker/PL/04-dockerfile/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Dockerfile</li> <li>Podstawowe instrukcje</li> <li>Kompletny Dockerfile</li> <li>Optymalizacja</li> <li>Multi-stage builds</li> </ol>"},{"location":"Docker/PL/04-dockerfile/#wprowadzenie-do-dockerfile","title":"Wprowadzenie do Dockerfile","text":""},{"location":"Docker/PL/04-dockerfile/#czym-jest-dockerfile","title":"Czym jest Dockerfile?","text":"<p>Dockerfile = Instrukcje do budowania obrazu</p> <ul> <li>Tekst : Prosty plik tekstowy</li> <li>Instrukcje : Ka\u017cda linia to instrukcja</li> <li>Automatyzacja : Automatyzuje tworzenie obraz\u00f3w</li> <li>Wersjonowany : Mo\u017ce by\u0107 wersjonowany z Git</li> </ul>"},{"location":"Docker/PL/04-dockerfile/#podstawowa-struktura","title":"Podstawowa struktura","text":"<pre><code># Komentarz\nFROM base-image\nRUN command\nCOPY source destination\nCMD [\"executable\", \"param\"]\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#podstawowe-instrukcje","title":"Podstawowe instrukcje","text":""},{"location":"Docker/PL/04-dockerfile/#from","title":"FROM","text":"<p>Definiuje obraz bazowy:</p> <pre><code>FROM python:3.11\nFROM ubuntu:22.04\nFROM alpine:latest\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#workdir","title":"WORKDIR","text":"<p>Definiuje katalog roboczy:</p> <pre><code>WORKDIR /app\nWORKDIR /usr/src/app\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#copy-add","title":"COPY / ADD","text":"<p>Kopiowa\u0107 pliki:</p> <pre><code># COPY (zalecane)\nCOPY requirements.txt .\nCOPY . .\n\n# ADD (z automatyczn\u0105 ekstrakcj\u0105)\nADD archive.tar.gz /app\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#run","title":"RUN","text":"<p>Wykonywa\u0107 polecenia:</p> <pre><code>RUN apt update\nRUN pip install -r requirements.txt\n\n# \u0141\u0105czy\u0107 aby zmniejszy\u0107 warstwy\nRUN apt update &amp;&amp; \\\n    apt install -y python3 &amp;&amp; \\\n    apt clean\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#cmd-entrypoint","title":"CMD / ENTRYPOINT","text":"<p>Polecenie domy\u015blne:</p> <pre><code># CMD (mo\u017ce by\u0107 nadpisane)\nCMD [\"python\", \"app.py\"]\n\n# ENTRYPOINT (nie mo\u017ce by\u0107 nadpisane)\nENTRYPOINT [\"python\"]\nCMD [\"app.py\"]\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#env","title":"ENV","text":"<p>Zmienne \u015brodowiskowe:</p> <pre><code>ENV PYTHONUNBUFFERED=1\nENV APP_ENV=production\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#expose","title":"EXPOSE","text":"<p>Udost\u0119pni\u0107 porty:</p> <pre><code>EXPOSE 8080\nEXPOSE 3306\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#kompletny-dockerfile","title":"Kompletny Dockerfile","text":""},{"location":"Docker/PL/04-dockerfile/#przykad-1-aplikacja-python","title":"Przyk\u0142ad 1 : Aplikacja Python","text":"<pre><code># Obraz bazowy\nFROM python:3.11-slim\n\n# Katalog roboczy\nWORKDIR /app\n\n# Zmienne \u015brodowiskowe\nENV PYTHONUNBUFFERED=1\n\n# Kopiowa\u0107 requirements\nCOPY requirements.txt .\n\n# Zainstalowa\u0107 zale\u017cno\u015bci\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Kopiowa\u0107 kod\nCOPY . .\n\n# Udost\u0119pni\u0107 port\nEXPOSE 8000\n\n# Polecenie domy\u015blne\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#przykad-2-aplikacja-z-danymi","title":"Przyk\u0142ad 2 : Aplikacja z danymi","text":"<pre><code>FROM python:3.11\n\nWORKDIR /app\n\n# Zainstalowa\u0107 zale\u017cno\u015bci systemowe\nRUN apt update &amp;&amp; \\\n    apt install -y postgresql-client &amp;&amp; \\\n    apt clean\n\n# Zainstalowa\u0107 zale\u017cno\u015bci Python\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Kopiowa\u0107 aplikacj\u0119\nCOPY . .\n\n# Utworzy\u0107 wolumen dla danych\nVOLUME [\"/app/data\"]\n\n# Udost\u0119pni\u0107 port\nEXPOSE 8080\n\n# Polecenie\nCMD [\"python\", \"main.py\"]\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#optymalizacja","title":"Optymalizacja","text":""},{"location":"Docker/PL/04-dockerfile/#dockerignore","title":".dockerignore","text":"<p>Utworzy\u0107 plik <code>.dockerignore</code>:</p> <pre><code>__pycache__\n*.pyc\n.git\n.env\nnode_modules\n*.log\n.DS_Store\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#zmniejszyc-warstwy","title":"Zmniejszy\u0107 warstwy","text":"<p>Z\u0142e: <pre><code>RUN apt update\nRUN apt install -y python3\nRUN apt install -y pip\nRUN apt clean\n</code></pre></p> <p>Dobre: <pre><code>RUN apt update &amp;&amp; \\\n    apt install -y python3 pip &amp;&amp; \\\n    apt clean\n</code></pre></p>"},{"location":"Docker/PL/04-dockerfile/#kolejnosc-instrukcji","title":"Kolejno\u015b\u0107 instrukcji","text":"<p>Umie\u015bci\u0107 instrukcje kt\u00f3re zmieniaj\u0105 si\u0119 rzadko najpierw:</p> <pre><code># Najpierw zale\u017cno\u015bci (zmieniaj\u0105 si\u0119 rzadko)\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Potem kod (zmienia si\u0119 cz\u0119sto)\nCOPY . .\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#multi-stage-builds","title":"Multi-stage builds","text":""},{"location":"Docker/PL/04-dockerfile/#dlaczego-multi-stage","title":"Dlaczego multi-stage?","text":"<ul> <li>Zmniejszy\u0107 rozmiar : Obraz ko\u0144cowy mniejszy</li> <li>Bezpiecze\u0144stwo : Wykluczy\u0107 narz\u0119dzia build</li> <li>Wydajno\u015b\u0107 : Optymalizowa\u0107 buildy</li> </ul>"},{"location":"Docker/PL/04-dockerfile/#przykad","title":"Przyk\u0142ad","text":"<pre><code># Stage 1 : Build\nFROM python:3.11 as builder\nWORKDIR /build\nCOPY requirements.txt .\nRUN pip install --user -r requirements.txt\n\n# Stage 2 : Runtime\nFROM python:3.11-slim\nWORKDIR /app\nCOPY --from=builder /root/.local /root/.local\nCOPY . .\nENV PATH=/root/.local/bin:$PATH\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/PL/04-dockerfile/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Dockerfile automatyzuje tworzenie obraz\u00f3w</li> <li>Instrukcje : FROM, RUN, COPY, CMD</li> <li>Optymalizacja : Zmniejszy\u0107 warstwy</li> <li>Multi-stage : L\u017cejsze obrazy</li> <li>.dockerignore : Wykluczy\u0107 pliki</li> </ol>"},{"location":"Docker/PL/04-dockerfile/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 5. Docker Compose, aby orkiestrowa\u0107 wiele kontener\u00f3w.</p>"},{"location":"Docker/PL/05-docker-compose/","title":"5. Docker Compose","text":""},{"location":"Docker/PL/05-docker-compose/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Docker Compose</li> <li>Orkiestrowa\u0107 wiele kontener\u00f3w</li> <li>Tworzy\u0107 pliki docker-compose.yml</li> <li>Zarz\u0105dza\u0107 us\u0142ugami i sieciami</li> <li>U\u017cywa\u0107 zmiennych \u015brodowiskowych</li> </ul>"},{"location":"Docker/PL/05-docker-compose/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Docker Compose</li> <li>Plik docker-compose.yml</li> <li>Us\u0142ugi</li> <li>Sieci i Wolumeny</li> <li>Polecenia</li> </ol>"},{"location":"Docker/PL/05-docker-compose/#wprowadzenie-do-docker-compose","title":"Wprowadzenie do Docker Compose","text":""},{"location":"Docker/PL/05-docker-compose/#czym-jest-docker-compose","title":"Czym jest Docker Compose?","text":"<p>Docker Compose = Narz\u0119dzie do orkiestracji wielu kontener\u00f3w</p> <ul> <li>Multi-kontenery : Zarz\u0105dza wieloma kontenerami</li> <li>Konfiguracja : Prosty plik YAML</li> <li>Orkiestracja : Uruchamia/zatrzymuje wszystkie us\u0142ugi</li> <li>Sieci : Automatycznie tworzy sieci</li> </ul>"},{"location":"Docker/PL/05-docker-compose/#dlaczego-docker-compose","title":"Dlaczego Docker Compose?","text":"<ul> <li>Prostota : Jeden plik dla wszystkiego</li> <li>Reprodukowalno\u015b\u0107 : To samo \u015brodowisko wsz\u0119dzie</li> <li>Rozw\u00f3j : Kompletny stack lokalny</li> <li>Produkcja : Uproszczone wdra\u017canie</li> </ul>"},{"location":"Docker/PL/05-docker-compose/#plik-docker-composeyml","title":"Plik docker-compose.yml","text":""},{"location":"Docker/PL/05-docker-compose/#podstawowa-struktura","title":"Podstawowa struktura","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    image: nginx\n    ports:\n      - \"8080:80\"\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_PASSWORD: password\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#kompletny-przykad","title":"Kompletny przyk\u0142ad","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8080:8000\"\n    environment:\n      - DATABASE_URL=postgresql://db:5432/mydb\n    depends_on:\n      - db\n    volumes:\n      - ./src:/app\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: mydb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#usugi","title":"Us\u0142ugi","text":""},{"location":"Docker/PL/05-docker-compose/#zdefiniowac-usuge","title":"Zdefiniowa\u0107 us\u0142ug\u0119","text":"<pre><code>services:\n  app:\n    image: python:3.11\n    command: python app.py\n    working_dir: /app\n    volumes:\n      - .:/app\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#build-vs-image","title":"Build vs Image","text":"<pre><code># U\u017cywa\u0107 istniej\u0105cego obrazu\nservices:\n  web:\n    image: nginx:latest\n\n# Budowa\u0107 z Dockerfile\nservices:\n  app:\n    build: .\n    # lub\n    build:\n      context: .\n      dockerfile: Dockerfile.prod\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#zaleznosci","title":"Zale\u017cno\u015bci","text":"<pre><code>services:\n  app:\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres\n\n  redis:\n    image: redis\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#sieci-i-wolumeny","title":"Sieci i Wolumeny","text":""},{"location":"Docker/PL/05-docker-compose/#sieci","title":"Sieci","text":"<pre><code>services:\n  app:\n    networks:\n      - frontend\n      - backend\n\n  db:\n    networks:\n      - backend\n\nnetworks:\n  frontend:\n  backend:\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#wolumeny","title":"Wolumeny","text":"<pre><code>services:\n  db:\n    volumes:\n      - db-data:/var/lib/postgresql/data\n      - ./backup:/backup\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#polecenia","title":"Polecenia","text":""},{"location":"Docker/PL/05-docker-compose/#uruchomic-usugi","title":"Uruchomi\u0107 us\u0142ugi","text":"<pre><code># Uruchomi\u0107 wszystkie us\u0142ugi\ndocker-compose up\n\n# W tle\ndocker-compose up -d\n\n# Przebudowa\u0107 obrazy\ndocker-compose up --build\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#zatrzymac-usugi","title":"Zatrzyma\u0107 us\u0142ugi","text":"<pre><code># Zatrzyma\u0107 us\u0142ugi\ndocker-compose stop\n\n# Zatrzyma\u0107 i usun\u0105\u0107\ndocker-compose down\n\n# Usun\u0105\u0107 z wolumenami\ndocker-compose down -v\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#zarzadzanie-usugami","title":"Zarz\u0105dzanie us\u0142ugami","text":"<pre><code># Zobaczy\u0107 uruchomione us\u0142ugi\ndocker-compose ps\n\n# Zobaczy\u0107 logi\ndocker-compose logs\n\n# Logi us\u0142ugi\ndocker-compose logs web\n\n# Wykona\u0107 polecenie\ndocker-compose exec web bash\n\n# Uruchomi\u0107 ponownie us\u0142ug\u0119\ndocker-compose restart web\n</code></pre>"},{"location":"Docker/PL/05-docker-compose/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Docker Compose orkiestruje wiele kontener\u00f3w</li> <li>docker-compose.yml definiuje konfiguracj\u0119</li> <li>Us\u0142ugi to kontenery</li> <li>Sieci i Wolumeny dla komunikacji i danych</li> <li>Polecenia : up, down, logs, exec</li> </ol>"},{"location":"Docker/PL/05-docker-compose/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 6. Wolumeny i Sieci, aby pog\u0142\u0119bi\u0107.</p>"},{"location":"Docker/PL/06-volumes-networks/","title":"6. Wolumeny i Sieci Docker","text":""},{"location":"Docker/PL/06-volumes-networks/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 wolumeny Docker</li> <li>Zarz\u0105dza\u0107 trwa\u0142o\u015bci\u0105 danych</li> <li>Tworzy\u0107 i zarz\u0105dza\u0107 sieciami</li> <li>Dzieli\u0107 dane mi\u0119dzy kontenerami</li> <li>Konfigurowa\u0107 komunikacj\u0119 sieciow\u0105</li> </ul>"},{"location":"Docker/PL/06-volumes-networks/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wolumeny</li> <li>Bind Mounts</li> <li>Sieci</li> <li>Komunikacja mi\u0119dzy kontenerami</li> <li>Przyk\u0142ady praktyczne</li> </ol>"},{"location":"Docker/PL/06-volumes-networks/#wolumeny","title":"Wolumeny","text":""},{"location":"Docker/PL/06-volumes-networks/#czym-jest-wolumen","title":"Czym jest wolumen?","text":"<p>Wolumen = Trwa\u0142y magazyn dla danych</p> <ul> <li>Trwa\u0142y : Przetrwa usuni\u0119cie kontenera</li> <li>Zarz\u0105dzany przez Docker : Przechowywany w <code>/var/lib/docker/volumes</code></li> <li>Dzielony : Wiele kontener\u00f3w mo\u017ce go u\u017cywa\u0107</li> <li>Wydajny : Szybszy ni\u017c bind mounts</li> </ul>"},{"location":"Docker/PL/06-volumes-networks/#utworzyc-wolumen","title":"Utworzy\u0107 wolumen","text":"<pre><code># Utworzy\u0107 wolumen\ndocker volume create my-volume\n\n# Listowa\u0107 wolumeny\ndocker volume ls\n\n# Sprawdzi\u0107 wolumen\ndocker volume inspect my-volume\n\n# Usun\u0105\u0107 wolumen\ndocker volume rm my-volume\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#uzywac-wolumenu","title":"U\u017cywa\u0107 wolumenu","text":"<pre><code># Wolumen nazwany\ndocker run -v my-volume:/data ubuntu\n\n# Wolumen anonimowy\ndocker run -v /data ubuntu\n\n# W docker-compose.yml\nvolumes:\n  - my-volume:/data\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#bind-mounts","title":"Bind Mounts","text":""},{"location":"Docker/PL/06-volumes-networks/#czym-jest-bind-mount","title":"Czym jest bind mount?","text":"<p>Bind Mount = Bezpo\u015brednie po\u0142\u0105czenie z katalogiem hosta</p> <ul> <li>Bezpo\u015bredni : Bezpo\u015bredni dost\u0119p do plik\u00f3w hosta</li> <li>Rozw\u00f3j : Idealny do rozwoju</li> <li>Wydajno\u015b\u0107 : Zale\u017cy od systemu plik\u00f3w hosta</li> </ul>"},{"location":"Docker/PL/06-volumes-networks/#uzywac-bind-mount","title":"U\u017cywa\u0107 bind mount","text":"<pre><code># Bind mount\ndocker run -v /host/path:/container/path ubuntu\n\n# Z docker-compose.yml\nvolumes:\n  - ./data:/app/data\n  - /absolute/path:/container/path\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#roznice-wolumen-vs-bind-mount","title":"R\u00f3\u017cnice : Wolumen vs Bind Mount","text":"<p>Wolumen: - Zarz\u0105dzany przez Docker - Lepsza wydajno\u015b\u0107 - Przeno\u015bny - Zalecany do produkcji</p> <p>Bind Mount: - Bezpo\u015brednie po\u0142\u0105czenie - Bezpo\u015bredni dost\u0119p - Zale\u017cy od systemu hosta - Zalecany do rozwoju</p>"},{"location":"Docker/PL/06-volumes-networks/#sieci","title":"Sieci","text":""},{"location":"Docker/PL/06-volumes-networks/#typy-sieci","title":"Typy sieci","text":"<ol> <li>Bridge : Sie\u0107 domy\u015blna (izolacja)</li> <li>Host : U\u017cywa sieci hosta</li> <li>None : Brak sieci</li> <li>Overlay : Dla Docker Swarm</li> </ol>"},{"location":"Docker/PL/06-volumes-networks/#utworzyc-siec","title":"Utworzy\u0107 sie\u0107","text":"<pre><code># Utworzy\u0107 sie\u0107\ndocker network create my-network\n\n# Listowa\u0107 sieci\ndocker network ls\n\n# Sprawdzi\u0107 sie\u0107\ndocker network inspect my-network\n\n# Usun\u0105\u0107 sie\u0107\ndocker network rm my-network\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#poaczyc-kontener","title":"Po\u0142\u0105czy\u0107 kontener","text":"<pre><code># Po\u0142\u0105czy\u0107 przy starcie\ndocker run --network my-network ubuntu\n\n# Po\u0142\u0105czy\u0107 istniej\u0105cy kontener\ndocker network connect my-network container-id\n\n# Roz\u0142\u0105czy\u0107\ndocker network disconnect my-network container-id\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#komunikacja-miedzy-kontenerami","title":"Komunikacja mi\u0119dzy kontenerami","text":""},{"location":"Docker/PL/06-volumes-networks/#ta-sama-siec","title":"Ta sama sie\u0107","text":"<pre><code># Utworzy\u0107 sie\u0107\ndocker network create app-network\n\n# Kontener 1\ndocker run --name app --network app-network my-app\n\n# Kontener 2 (mo\u017ce komunikowa\u0107 si\u0119 z app)\ndocker run --name db --network app-network postgres\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#z-docker-compose","title":"Z Docker Compose","text":"<pre><code>services:\n  app:\n    networks:\n      - app-network\n\n  db:\n    networks:\n      - app-network\n\nnetworks:\n  app-network:\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#rozwiazanie-dns","title":"Rozwi\u0105zanie DNS","text":"<p>Kontenery mog\u0105 znajdowa\u0107 si\u0119 po nazwie:</p> <pre><code># W kontenerze app\nimport psycopg2\nconn = psycopg2.connect(\n    host=\"db\",  # Nazwa us\u0142ugi\n    database=\"mydb\"\n)\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#przykady-praktyczne","title":"Przyk\u0142ady praktyczne","text":""},{"location":"Docker/PL/06-volumes-networks/#przykad-1-baza-danych-z-wolumenem","title":"Przyk\u0142ad 1 : Baza danych z wolumenem","text":"<pre><code>version: '3.8'\n\nservices:\n  db:\n    image: postgres:15\n    volumes:\n      - db-data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_DB: mydb\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#przykad-2-aplikacja-z-bind-mount","title":"Przyk\u0142ad 2 : Aplikacja z bind mount","text":"<pre><code>version: '3.8'\n\nservices:\n  app:\n    build: .\n    volumes:\n      - ./src:/app/src  # Rozw\u00f3j\n    networks:\n      - app-network\n\nnetworks:\n  app-network:\n</code></pre>"},{"location":"Docker/PL/06-volumes-networks/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Wolumeny dla trwa\u0142o\u015bci zarz\u0105dzanej przez Docker</li> <li>Bind Mounts dla bezpo\u015bredniego dost\u0119pu</li> <li>Sieci dla komunikacji</li> <li>DNS : Rozwi\u0105zanie po nazwie us\u0142ugi</li> <li>Docker Compose upraszcza zarz\u0105dzanie</li> </ol>"},{"location":"Docker/PL/06-volumes-networks/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 7. Dobre praktyki, aby pozna\u0107 najlepsze praktyki.</p>"},{"location":"Docker/PL/07-best-practices/","title":"7. Dobre praktyki Docker","text":""},{"location":"Docker/PL/07-best-practices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zabezpiecza\u0107 kontenery</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> <li>Organizowa\u0107 projekty</li> <li>Konserwowa\u0107 obrazy</li> <li>Zarz\u0105dza\u0107 zasobami</li> </ul>"},{"location":"Docker/PL/07-best-practices/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Bezpiecze\u0144stwo</li> <li>Wydajno\u015b\u0107</li> <li>Organizacja</li> <li>Konserwacja</li> <li>Zasoby</li> </ol>"},{"location":"Docker/PL/07-best-practices/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":""},{"location":"Docker/PL/07-best-practices/#uzywac-obrazow-oficjalnych","title":"U\u017cywa\u0107 obraz\u00f3w oficjalnych","text":"<pre><code># Dobre\nFROM python:3.11-slim\n\n# Unika\u0107\nFROM random-user/python:latest\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#nie-uzywac-root","title":"Nie u\u017cywa\u0107 root","text":"<pre><code># Utworzy\u0107 u\u017cytkownika nie-root\nRUN useradd -m appuser\nUSER appuser\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#ograniczac-uprawnienia","title":"Ogranicza\u0107 uprawnienia","text":"<pre><code># Nie u\u017cywa\u0107 --privileged\ndocker run --privileged my-container  # Unika\u0107\n\n# U\u017cywa\u0107 konkretnych capabilities je\u015bli potrzebne\ndocker run --cap-add NET_ADMIN my-container\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#sekrety","title":"Sekrety","text":"<pre><code># docker-compose.yml\nservices:\n  app:\n    secrets:\n      - db_password\n\nsecrets:\n  db_password:\n    file: ./secrets/password.txt\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#wydajnosc","title":"Wydajno\u015b\u0107","text":""},{"location":"Docker/PL/07-best-practices/#uzywac-dockerignore","title":"U\u017cywa\u0107 .dockerignore","text":"<pre><code>__pycache__\n*.pyc\n.git\n.env\nnode_modules\n*.log\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#optymalizowac-warstwy","title":"Optymalizowa\u0107 warstwy","text":"<pre><code># Z\u0142e\nRUN apt update\nRUN apt install -y python3\nRUN apt clean\n\n# Dobre\nRUN apt update &amp;&amp; \\\n    apt install -y python3 &amp;&amp; \\\n    apt clean\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#cache-buildow","title":"Cache build\u00f3w","text":"<p>Kolejno\u015b\u0107 instrukcji:</p> <pre><code># Najpierw zale\u017cno\u015bci (zmieniaj\u0105 si\u0119 rzadko)\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Potem kod (zmienia si\u0119 cz\u0119sto)\nCOPY . .\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#organizacja","title":"Organizacja","text":""},{"location":"Docker/PL/07-best-practices/#struktura-projektu","title":"Struktura projektu","text":"<pre><code>my-project/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 app.py\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 data.csv\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#tagi-obrazow","title":"Tagi obraz\u00f3w","text":"<pre><code># Tagi semantyczne\ndocker build -t my-app:1.0.0 .\ndocker build -t my-app:latest .\n\n# Tagi dla \u015brodowiska\ndocker build -t my-app:dev .\ndocker build -t my-app:prod .\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#konserwacja","title":"Konserwacja","text":""},{"location":"Docker/PL/07-best-practices/#czyscic-zasoby","title":"Czy\u015bci\u0107 zasoby","text":"<pre><code># Usun\u0105\u0107 zatrzymane kontenery\ndocker container prune\n\n# Usun\u0105\u0107 nieu\u017cywane obrazy\ndocker image prune\n\n# Usun\u0105\u0107 nieu\u017cywane wolumeny\ndocker volume prune\n\n# Wyczy\u015bci\u0107 wszystko\ndocker system prune -a\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#aktualizowac-obrazy","title":"Aktualizowa\u0107 obrazy","text":"<pre><code># Aktualizowa\u0107 obraz\ndocker pull python:3.11\n\n# Przebudowa\u0107\ndocker-compose build --no-cache\ndocker-compose up\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#zasoby","title":"Zasoby","text":""},{"location":"Docker/PL/07-best-practices/#ograniczac-zasoby","title":"Ogranicza\u0107 zasoby","text":"<pre><code># docker-compose.yml\nservices:\n  app:\n    deploy:\n      resources:\n        limits:\n          cpus: '1'\n          memory: 512M\n        reservations:\n          cpus: '0.5'\n          memory: 256M\n</code></pre>"},{"location":"Docker/PL/07-best-practices/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Bezpiecze\u0144stwo : Obrazy oficjalne, nie-root</li> <li>Wydajno\u015b\u0107 : Optymalizowa\u0107 warstwy, cache</li> <li>Organizacja : Czytelna struktura, tagi</li> <li>Konserwacja : Czy\u015bci\u0107 regularnie</li> <li>Zasoby : Ogranicza\u0107 u\u017cycie</li> </ol>"},{"location":"Docker/PL/07-best-practices/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 8. Projekty praktyczne, aby tworzy\u0107 kompletne projekty.</p>"},{"location":"Docker/PL/08-projets/","title":"8. Projekty praktyczne Docker","text":""},{"location":"Docker/PL/08-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Konteneryzowa\u0107 aplikacj\u0119 Python</li> <li>Tworzy\u0107 pipeline danych z Docker</li> <li>Kompletny stack z Docker Compose</li> <li>Projekty do portfolio</li> </ul>"},{"location":"Docker/PL/08-projets/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Projekt 1 : Aplikacja Python</li> <li>Projekt 2 : Pipeline danych</li> <li>Projekt 3 : Kompletny stack</li> <li>Projekt 4 : Aplikacja web</li> </ol>"},{"location":"Docker/PL/08-projets/#projekt-1-aplikacja-python","title":"Projekt 1 : Aplikacja Python","text":""},{"location":"Docker/PL/08-projets/#cel","title":"Cel","text":"<p>Konteneryzowa\u0107 prost\u0105 aplikacj\u0119 Python.</p>"},{"location":"Docker/PL/08-projets/#struktura","title":"Struktura","text":"<pre><code>python-app/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 app.py\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 data.csv\n</code></pre>"},{"location":"Docker/PL/08-projets/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"Docker/PL/08-projets/#apppy","title":"app.py","text":"<pre><code>import pandas as pd\n\ndef main():\n    df = pd.read_csv('data/data.csv')\n    print(f\"Loaded {len(df)} rows\")\n    print(df.head())\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"Docker/PL/08-projets/#zbudowac-i-uruchomic","title":"Zbudowa\u0107 i uruchomi\u0107","text":"<pre><code># Zbudowa\u0107\ndocker build -t python-app .\n\n# Uruchomi\u0107\ndocker run -v $(pwd)/data:/app/data python-app\n</code></pre>"},{"location":"Docker/PL/08-projets/#projekt-2-pipeline-danych","title":"Projekt 2 : Pipeline danych","text":""},{"location":"Docker/PL/08-projets/#cel_1","title":"Cel","text":"<p>Utworzy\u0107 pipeline ETL z Docker.</p>"},{"location":"Docker/PL/08-projets/#struktura_1","title":"Struktura","text":"<pre><code>etl-pipeline/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 extract/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 extract.py\n\u251c\u2500\u2500 transform/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 transform.py\n\u2514\u2500\u2500 load/\n    \u251c\u2500\u2500 Dockerfile\n    \u2514\u2500\u2500 load.py\n</code></pre>"},{"location":"Docker/PL/08-projets/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  extract:\n    build: ./extract\n    volumes:\n      - ./data:/data\n\n  transform:\n    build: ./transform\n    depends_on:\n      - extract\n    volumes:\n      - ./data:/data\n\n  load:\n    build: ./load\n    depends_on:\n      - transform\n    volumes:\n      - ./data:/data\n</code></pre>"},{"location":"Docker/PL/08-projets/#projekt-3-kompletny-stack","title":"Projekt 3 : Kompletny stack","text":""},{"location":"Docker/PL/08-projets/#cel_2","title":"Cel","text":"<p>Kompletny stack z baz\u0105 danych i aplikacj\u0105.</p>"},{"location":"Docker/PL/08-projets/#docker-composeyml_1","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8080:8000\"\n    environment:\n      - DATABASE_URL=postgresql://user:password@db:5432/mydb\n    depends_on:\n      - db\n    volumes:\n      - ./src:/app\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: mydb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db-data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n\n  redis:\n    image: redis:7\n    ports:\n      - \"6379:6379\"\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/PL/08-projets/#projekt-4-aplikacja-web","title":"Projekt 4 : Aplikacja web","text":""},{"location":"Docker/PL/08-projets/#cel_3","title":"Cel","text":"<p>Aplikacja web Flask z baz\u0105 danych.</p>"},{"location":"Docker/PL/08-projets/#dockerfile_1","title":"Dockerfile","text":"<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\"]\n</code></pre>"},{"location":"Docker/PL/08-projets/#docker-composeyml_2","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"5000:5000\"\n    environment:\n      - FLASK_ENV=development\n      - DATABASE_URL=postgresql://user:password@db:5432/appdb\n    depends_on:\n      - db\n    volumes:\n      - .:/app\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: appdb\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"Docker/PL/08-projets/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Konteneryzacja : Izolowa\u0107 aplikacje</li> <li>Docker Compose : Orkiestrowa\u0107 wiele us\u0142ug</li> <li>Wolumeny : Trwa\u0107 dane</li> <li>Sieci : Komunikacja mi\u0119dzy us\u0142ugami</li> <li>Portfolio : Projekty demonstrowalne</li> </ol>"},{"location":"Docker/PL/08-projets/#zasoby","title":"\ud83d\udd17 Zasoby","text":"<ul> <li>Przyk\u0142ady Docker</li> <li>Dokumentacja Docker</li> </ul> <p>Gratulacje ! Uko\u0144czy\u0142e\u015b szkolenie Docker. Mo\u017cesz teraz konteneryzowa\u0107 swoje aplikacje.</p>"},{"location":"Git/EN/","title":"Git Training for Data Analyst","text":""},{"location":"Git/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through learning Git as a Data Analyst. Git is a distributed version control system essential for managing code, scripts, and project documentation.</p>"},{"location":"Git/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand Git and version control</li> <li>Install Git</li> <li>Master basic commands</li> <li>Manage branches</li> <li>Work with remote repositories (GitHub, GitLab)</li> <li>Collaborate on projects</li> <li>Use Git in your data workflows</li> <li>Create practical projects for your portfolio</li> </ul>"},{"location":"Git/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 Git : Open-source and free - \u2705 GitHub : Free account (unlimited) - \u2705 GitLab : Free account - \u2705 Official Documentation : Complete free guides - \u2705 Online Tutorials : Free resources</p> <p>Total Budget: $0</p>"},{"location":"Git/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"Git/EN/#1-git-getting-started","title":"1. Git Getting Started","text":"<ul> <li>Install Git</li> <li>Initial configuration</li> <li>First repositories</li> <li>Basic concepts</li> </ul>"},{"location":"Git/EN/#2-basic-commands","title":"2. Basic Commands","text":"<ul> <li>Create a repository</li> <li>Add and commit</li> <li>View history</li> <li>Undo changes</li> </ul>"},{"location":"Git/EN/#3-branching","title":"3. Branching","text":"<ul> <li>Create and manage branches</li> <li>Merge branches</li> <li>Resolve conflicts</li> <li>Branch workflow</li> </ul>"},{"location":"Git/EN/#4-remote-repositories","title":"4. Remote Repositories","text":"<ul> <li>GitHub and GitLab</li> <li>Clone a repository</li> <li>Push and Pull</li> <li>Synchronization</li> </ul>"},{"location":"Git/EN/#5-collaboration","title":"5. Collaboration","text":"<ul> <li>Fork and Pull Requests</li> <li>Issues and Projects</li> <li>Code Review</li> <li>Team workflow</li> </ul>"},{"location":"Git/EN/#6-advanced-features","title":"6. Advanced Features","text":"<ul> <li>Stash</li> <li>Rebase</li> <li>Tags</li> <li>Hooks</li> </ul>"},{"location":"Git/EN/#7-best-practices","title":"7. Best Practices","text":"<ul> <li>Commit messages</li> <li>Project structure</li> <li>.gitignore</li> <li>Documentation</li> </ul>"},{"location":"Git/EN/#8-practical-projects","title":"8. Practical Projects","text":"<ul> <li>GitHub Portfolio</li> <li>Collaborative project</li> <li>Python scripts management</li> <li>Project documentation</li> </ul>"},{"location":"Git/EN/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"Git/EN/#prerequisites","title":"Prerequisites","text":"<ul> <li>Operating System : Windows, Linux, or macOS</li> <li>Internet Connection : For GitHub/GitLab</li> <li>Text Editor : VS Code, Sublime, etc.</li> </ul>"},{"location":"Git/EN/#quick-installation","title":"Quick Installation","text":"<p>Windows: 1. Download Git: https://git-scm.com/download/win 2. Install with default options 3. Verify: <code>git --version</code></p> <p>Linux: <pre><code># Ubuntu/Debian\nsudo apt install git\n\n# CentOS/RHEL\nsudo yum install git\n\n# Verify\ngit --version\n</code></pre></p> <p>macOS: <pre><code># With Homebrew\nbrew install git\n\n# Verify\ngit --version\n</code></pre></p>"},{"location":"Git/EN/#initial-configuration","title":"Initial Configuration","text":"<pre><code># Configure your name\ngit config --global user.name \"Your Name\"\n\n# Configure your email\ngit config --global user.email \"your.email@example.com\"\n\n# Verify configuration\ngit config --list\n</code></pre>"},{"location":"Git/EN/#first-repository","title":"First Repository","text":"<pre><code># Create a new repository\nmkdir my-project\ncd my-project\ngit init\n\n# Create a file\necho \"# My Project\" &gt; README.md\n\n# Add and commit\ngit add README.md\ngit commit -m \"First commit\"\n</code></pre>"},{"location":"Git/EN/#use-cases-for-data-analyst","title":"\ud83d\udcca Use Cases for Data Analyst","text":"<ul> <li>Versioning : Manage versions of your Python/R scripts</li> <li>Collaboration : Work in teams on projects</li> <li>Portfolio : Showcase your projects on GitHub</li> <li>Documentation : Version your documentation</li> <li>Backup : Backup your code online</li> </ul>"},{"location":"Git/EN/#free-resources","title":"\ud83d\udcda Free Resources","text":""},{"location":"Git/EN/#official-documentation","title":"Official Documentation","text":"<ul> <li>Git Documentation : https://git-scm.com/doc</li> <li>GitHub Guides : https://guides.github.com/</li> <li>GitLab Documentation : https://docs.gitlab.com/</li> </ul>"},{"location":"Git/EN/01-getting-started/","title":"1. Git Getting Started","text":""},{"location":"Git/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Git and version control</li> <li>Install Git</li> <li>Configure Git</li> <li>Create your first repository</li> <li>Understand basic concepts</li> </ul>"},{"location":"Git/EN/01-getting-started/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Git</li> <li>Installation</li> <li>Configuration</li> <li>First Repository</li> <li>Basic Concepts</li> </ol>"},{"location":"Git/EN/01-getting-started/#introduction-to-git","title":"Introduction to Git","text":""},{"location":"Git/EN/01-getting-started/#what-is-git","title":"What is Git?","text":"<p>Git = Distributed version control system</p> <ul> <li>Versioning : Tracks file changes</li> <li>Distributed : Each developer has a complete copy</li> <li>Collaboration : Facilitates team work</li> <li>History : Preserves complete history</li> </ul>"},{"location":"Git/EN/01-getting-started/#why-git-for-data-analyst","title":"Why Git for Data Analyst?","text":"<ul> <li>Scripts : Version your Python/R scripts</li> <li>Projects : Manage your portfolio projects</li> <li>Collaboration : Work in teams</li> <li>Backup : Backup online (GitHub)</li> <li>Documentation : Version your documentation</li> </ul>"},{"location":"Git/EN/01-getting-started/#installation","title":"Installation","text":""},{"location":"Git/EN/01-getting-started/#windows","title":"Windows","text":"<ol> <li>Go to: https://git-scm.com/download/win</li> <li>Download the installer</li> <li>Run the installer</li> <li>Accept default options</li> </ol>"},{"location":"Git/EN/01-getting-started/#linux","title":"Linux","text":"<p>Ubuntu/Debian: <pre><code>sudo apt update\nsudo apt install git\ngit --version\n</code></pre></p>"},{"location":"Git/EN/01-getting-started/#macos","title":"macOS","text":"<p>With Homebrew: <pre><code>brew install git\ngit --version\n</code></pre></p>"},{"location":"Git/EN/01-getting-started/#configuration","title":"Configuration","text":""},{"location":"Git/EN/01-getting-started/#global-configuration","title":"Global Configuration","text":"<pre><code># Configure your name\ngit config --global user.name \"Your Name\"\n\n# Configure your email\ngit config --global user.email \"your.email@example.com\"\n\n# Configure default editor\ngit config --global core.editor \"code --wait\"  # VS Code\n</code></pre>"},{"location":"Git/EN/01-getting-started/#verify-configuration","title":"Verify Configuration","text":"<pre><code># See all configuration\ngit config --list\n\n# See specific configuration\ngit config user.name\n</code></pre>"},{"location":"Git/EN/01-getting-started/#first-repository","title":"First Repository","text":""},{"location":"Git/EN/01-getting-started/#create-a-new-repository","title":"Create a New Repository","text":"<pre><code># Create a directory\nmkdir my-project\ncd my-project\n\n# Initialize Git\ngit init\n\n# Verify\nls -la  # See .git folder\n</code></pre>"},{"location":"Git/EN/01-getting-started/#first-commit","title":"First Commit","text":"<pre><code># Create a file\necho \"# My Project\" &gt; README.md\n\n# See status\ngit status\n\n# Add the file\ngit add README.md\n\n# Commit\ngit commit -m \"First commit: add README\"\n</code></pre>"},{"location":"Git/EN/01-getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"Git/EN/01-getting-started/#repository","title":"Repository","text":"<p>Repository = Folder with Git history</p> <ul> <li>Local : On your machine</li> <li>Remote : On GitHub/GitLab</li> <li>.git : Hidden folder containing history</li> </ul>"},{"location":"Git/EN/01-getting-started/#commit","title":"Commit","text":"<p>Commit = Point in history</p> <ul> <li>Snapshot : Capture of file state</li> <li>Message : Description of changes</li> <li>Author : Name and email</li> <li>Hash : Unique identifier (SHA-1)</li> </ul>"},{"location":"Git/EN/01-getting-started/#branch","title":"Branch","text":"<p>Branch = Development line</p> <ul> <li>main/master : Main branch</li> <li>Other branches : For new features</li> <li>Isolation : Isolated work</li> </ul>"},{"location":"Git/EN/01-getting-started/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Git = Distributed version control</li> <li>Repository = Folder with history</li> <li>Commit = Point in history</li> <li>Branch = Development line</li> <li>Staging = Preparation area</li> </ol>"},{"location":"Git/EN/01-getting-started/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 2. Basic Commands to master essential commands.</p>"},{"location":"Git/EN/02-basic-commands/","title":"2. Git Basic Commands","text":""},{"location":"Git/EN/02-basic-commands/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create and manage a repository</li> <li>Add and commit files</li> <li>View history</li> <li>Undo changes</li> <li>Ignore files</li> </ul>"},{"location":"Git/EN/02-basic-commands/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Create a Repository</li> <li>Add and Commit</li> <li>View History</li> <li>Undo Changes</li> <li>.gitignore</li> </ol>"},{"location":"Git/EN/02-basic-commands/#create-a-repository","title":"Create a Repository","text":""},{"location":"Git/EN/02-basic-commands/#initialize-a-local-repository","title":"Initialize a Local Repository","text":"<pre><code># Create a new repository\nmkdir my-project\ncd my-project\ngit init\n\n# Verify\nls -la  # See .git\n</code></pre>"},{"location":"Git/EN/02-basic-commands/#clone-an-existing-repository","title":"Clone an Existing Repository","text":"<pre><code># Clone from GitHub\ngit clone https://github.com/username/repo.git\n\n# Clone into specific folder\ngit clone https://github.com/username/repo.git my-folder\n\n# Clone with SSH\ngit clone git@github.com:username/repo.git\n</code></pre>"},{"location":"Git/EN/02-basic-commands/#add-and-commit","title":"Add and Commit","text":""},{"location":"Git/EN/02-basic-commands/#basic-workflow","title":"Basic Workflow","text":"<pre><code># 1. See status\ngit status\n\n# 2. Add files\ngit add file.py\ngit add .  # All files\n\n# 3. Commit\ngit commit -m \"Commit message\"\n</code></pre>"},{"location":"Git/EN/02-basic-commands/#commit-messages","title":"Commit Messages","text":"<p>Good format: <pre><code>Type: Short description (max 50 characters)\n\nDetailed description if needed\n</code></pre></p> <p>Examples: <pre><code>feat: Add data analysis function\nfix: Fix calculation bug\ndocs: Update README\n</code></pre></p>"},{"location":"Git/EN/02-basic-commands/#view-history","title":"View History","text":""},{"location":"Git/EN/02-basic-commands/#git-log","title":"git log","text":"<pre><code># Complete history\ngit log\n\n# Compact history\ngit log --oneline\n\n# History with graph\ngit log --graph --oneline --all\n\n# Limit number\ngit log -5  # Last 5 commits\n</code></pre>"},{"location":"Git/EN/02-basic-commands/#undo-changes","title":"Undo Changes","text":""},{"location":"Git/EN/02-basic-commands/#undo-in-working-directory","title":"Undo in Working Directory","text":"<pre><code># Undo file modifications\ngit restore file.py\n\n# Undo all modifications\ngit restore .\n</code></pre>"},{"location":"Git/EN/02-basic-commands/#undo-in-staging","title":"Undo in Staging","text":"<pre><code># Remove from staging\ngit restore --staged file.py\n</code></pre>"},{"location":"Git/EN/02-basic-commands/#modify-last-commit","title":"Modify Last Commit","text":"<pre><code># Modify message\ngit commit --amend -m \"New message\"\n\n# Add forgotten files\ngit add forgotten_file.py\ngit commit --amend --no-edit\n</code></pre>"},{"location":"Git/EN/02-basic-commands/#gitignore","title":".gitignore","text":""},{"location":"Git/EN/02-basic-commands/#create-gitignore","title":"Create .gitignore","text":"<pre><code># Python\n__pycache__/\n*.py[cod]\nvenv/\nenv/\n\n# Jupyter\n.ipynb_checkpoints\n*.ipynb\n\n# Data\n*.csv\n*.xlsx\ndata/\n\n# IDE\n.vscode/\n.idea/\n\n# Secrets\n.env\n*.key\n</code></pre>"},{"location":"Git/EN/02-basic-commands/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>git add : Add to staging</li> <li>git commit : Create commit</li> <li>git log : View history</li> <li>git status : View status</li> <li>.gitignore : Exclude files</li> </ol>"},{"location":"Git/EN/02-basic-commands/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 3. Branching to learn branch management.</p>"},{"location":"Git/EN/03-branching/","title":"3. Git Branching","text":""},{"location":"Git/EN/03-branching/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand branches</li> <li>Create and manage branches</li> <li>Merge branches</li> <li>Resolve conflicts</li> <li>Branch workflow</li> </ul>"},{"location":"Git/EN/03-branching/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Branches</li> <li>Create Branches</li> <li>Merge Branches</li> <li>Resolve Conflicts</li> <li>Workflow</li> </ol>"},{"location":"Git/EN/03-branching/#introduction-to-branches","title":"Introduction to Branches","text":""},{"location":"Git/EN/03-branching/#what-is-a-branch","title":"What is a Branch?","text":"<p>Branch = Independent development line</p> <ul> <li>Isolation : Isolated work</li> <li>Parallel : Multiple branches at once</li> <li>Merge : Combine changes</li> <li>main/master : Main branch</li> </ul>"},{"location":"Git/EN/03-branching/#create-branches","title":"Create Branches","text":""},{"location":"Git/EN/03-branching/#create-a-new-branch","title":"Create a New Branch","text":"<pre><code># Create a branch\ngit branch feature-analysis\n\n# Create and switch\ngit checkout -b feature-analysis\n\n# New syntax (Git 2.23+)\ngit switch -c feature-analysis\n</code></pre>"},{"location":"Git/EN/03-branching/#switch-between-branches","title":"Switch Between Branches","text":"<pre><code># Switch to a branch\ngit checkout feature-analysis\n\n# New syntax\ngit switch feature-analysis\n\n# Return to main\ngit checkout main\n</code></pre>"},{"location":"Git/EN/03-branching/#merge-branches","title":"Merge Branches","text":""},{"location":"Git/EN/03-branching/#merge","title":"Merge","text":"<pre><code># Switch to main\ngit checkout main\n\n# Merge the branch\ngit merge feature-analysis\n</code></pre>"},{"location":"Git/EN/03-branching/#resolve-conflicts","title":"Resolve Conflicts","text":""},{"location":"Git/EN/03-branching/#when-conflicts-occur","title":"When Conflicts Occur","text":"<ul> <li>Same line modified : In two different branches</li> <li>File deleted : In one branch, modified in other</li> </ul>"},{"location":"Git/EN/03-branching/#resolve-a-conflict","title":"Resolve a Conflict","text":"<p>Step 1 : Identify conflict</p> <pre><code>git status\n</code></pre> <p>Step 2 : Open file</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nprint(\"Main version\")\n=======\nprint(\"Feature version\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-analysis\n</code></pre> <p>Step 3 : Resolve manually</p> <pre><code>print(\"Combined version\")\n</code></pre> <p>Step 4 : Mark as resolved</p> <pre><code>git add file.py\ngit commit\n</code></pre>"},{"location":"Git/EN/03-branching/#workflow","title":"Workflow","text":""},{"location":"Git/EN/03-branching/#simple-workflow","title":"Simple Workflow","text":"<pre><code># 1. Create branch for feature\ngit checkout -b feature-new-function\n\n# 2. Work on branch\n# ... modifications ...\n\n# 3. Commit\ngit add .\ngit commit -m \"Add new feature\"\n\n# 4. Merge into main\ngit checkout main\ngit merge feature-new-function\n\n# 5. Delete branch\ngit branch -d feature-new-function\n</code></pre>"},{"location":"Git/EN/03-branching/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Branches : Isolated development lines</li> <li>git branch : Create/manage branches</li> <li>git merge : Merge branches</li> <li>Conflicts : Resolve manually</li> <li>Workflow : One branch per feature</li> </ol>"},{"location":"Git/EN/03-branching/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 4. Remote Repositories to work with GitHub/GitLab.</p>"},{"location":"Git/EN/04-remote-repositories/","title":"4. Git Remote Repositories","text":""},{"location":"Git/EN/04-remote-repositories/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand remote repositories</li> <li>Work with GitHub/GitLab</li> <li>Clone repositories</li> <li>Push and Pull</li> <li>Synchronization</li> </ul>"},{"location":"Git/EN/04-remote-repositories/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Remote Repositories</li> <li>GitHub and GitLab</li> <li>Clone a Repository</li> <li>Push and Pull</li> <li>Synchronization</li> </ol>"},{"location":"Git/EN/04-remote-repositories/#introduction-to-remote-repositories","title":"Introduction to Remote Repositories","text":""},{"location":"Git/EN/04-remote-repositories/#what-is-a-remote-repository","title":"What is a Remote Repository?","text":"<p>Remote Repository = Copy of repository on a server</p> <ul> <li>GitHub : Popular service</li> <li>GitLab : Open-source alternative</li> <li>Backup : Online backup</li> </ul>"},{"location":"Git/EN/04-remote-repositories/#github-and-gitlab","title":"GitHub and GitLab","text":""},{"location":"Git/EN/04-remote-repositories/#create-github-account","title":"Create GitHub Account","text":"<ol> <li>Go to: https://github.com</li> <li>Click \"Sign up\"</li> <li>Fill form</li> <li>Verify email</li> </ol>"},{"location":"Git/EN/04-remote-repositories/#create-github-repository","title":"Create GitHub Repository","text":"<ol> <li>Click \"New repository\"</li> <li>Name the repository</li> <li>Choose public/private</li> <li>Click \"Create repository\"</li> </ol>"},{"location":"Git/EN/04-remote-repositories/#clone-a-repository","title":"Clone a Repository","text":""},{"location":"Git/EN/04-remote-repositories/#clone-from-github","title":"Clone from GitHub","text":"<pre><code># Clone with HTTPS\ngit clone https://github.com/username/repo.git\n\n# Clone with SSH\ngit clone git@github.com:username/repo.git\n</code></pre>"},{"location":"Git/EN/04-remote-repositories/#push-and-pull","title":"Push and Pull","text":""},{"location":"Git/EN/04-remote-repositories/#add-a-remote","title":"Add a Remote","text":"<pre><code># Add a remote\ngit remote add origin https://github.com/username/repo.git\n\n# See remotes\ngit remote -v\n</code></pre>"},{"location":"Git/EN/04-remote-repositories/#push-send","title":"Push (Send)","text":"<pre><code># First push\ngit push -u origin main\n\n# Subsequent pushes\ngit push\n</code></pre>"},{"location":"Git/EN/04-remote-repositories/#pull-retrieve","title":"Pull (Retrieve)","text":"<pre><code># Retrieve and merge\ngit pull\n\n# Retrieve only\ngit fetch\n\n# Merge after fetch\ngit merge origin/main\n</code></pre>"},{"location":"Git/EN/04-remote-repositories/#synchronization","title":"Synchronization","text":""},{"location":"Git/EN/04-remote-repositories/#basic-workflow","title":"Basic Workflow","text":"<pre><code># 1. Retrieve latest changes\ngit pull\n\n# 2. Work locally\n# ... modifications ...\n\n# 3. Add and commit\ngit add .\ngit commit -m \"Modifications\"\n\n# 4. Send\ngit push\n</code></pre>"},{"location":"Git/EN/04-remote-repositories/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Remote : Repository on server</li> <li>git clone : Copy a repository</li> <li>git push : Send changes</li> <li>git pull : Retrieve changes</li> <li>Synchronization : Pull before Push</li> </ol>"},{"location":"Git/EN/04-remote-repositories/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 5. Collaboration to learn collaboration.</p>"},{"location":"Git/EN/05-collaboration/","title":"5. Git Collaboration","text":""},{"location":"Git/EN/05-collaboration/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Fork and Pull Requests</li> <li>Issues and Projects</li> <li>Code Review</li> <li>Team workflow</li> <li>Best practices</li> </ul>"},{"location":"Git/EN/05-collaboration/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Fork</li> <li>Pull Requests</li> <li>Issues</li> <li>Code Review</li> <li>Team Workflow</li> </ol>"},{"location":"Git/EN/05-collaboration/#fork","title":"Fork","text":""},{"location":"Git/EN/05-collaboration/#what-is-a-fork","title":"What is a Fork?","text":"<p>Fork = Copy of a repository in your account</p> <ul> <li>Complete copy : All files and history</li> <li>Independent : Changes without affecting original</li> <li>Contribution : Propose changes via PR</li> </ul>"},{"location":"Git/EN/05-collaboration/#fork-a-repository","title":"Fork a Repository","text":"<p>On GitHub:</p> <ol> <li>Go to repository</li> <li>Click \"Fork\"</li> <li>Choose your account</li> <li>Repository is copied</li> </ol> <p>Clone your fork:</p> <pre><code># Clone your fork\ngit clone https://github.com/your-username/repo.git\n\n# Add original as upstream\ngit remote add upstream https://github.com/original-owner/repo.git\n</code></pre>"},{"location":"Git/EN/05-collaboration/#pull-requests","title":"Pull Requests","text":""},{"location":"Git/EN/05-collaboration/#create-a-pull-request","title":"Create a Pull Request","text":"<p>Step 1 : Create a branch</p> <pre><code>git checkout -b feature-my-contribution\n</code></pre> <p>Step 2 : Make modifications</p> <pre><code># ... modifications ...\ngit add .\ngit commit -m \"Add new feature\"\n</code></pre> <p>Step 3 : Push branch</p> <pre><code>git push -u origin feature-my-contribution\n</code></pre> <p>Step 4 : Create PR on GitHub</p> <ol> <li>Go to your fork</li> <li>Click \"Compare &amp; pull request\"</li> <li>Fill form</li> <li>Click \"Create pull request\"</li> </ol>"},{"location":"Git/EN/05-collaboration/#issues","title":"Issues","text":""},{"location":"Git/EN/05-collaboration/#create-an-issue","title":"Create an Issue","text":"<p>On GitHub:</p> <ol> <li>Go to repository</li> <li>Click \"Issues\"</li> <li>Click \"New Issue\"</li> <li>Fill form</li> </ol>"},{"location":"Git/EN/05-collaboration/#issue-types","title":"Issue Types","text":"<p>Bug Report: - Bug description - Steps to reproduce - Expected behavior</p> <p>Feature Request: - Feature description - Use cases</p>"},{"location":"Git/EN/05-collaboration/#code-review","title":"Code Review","text":""},{"location":"Git/EN/05-collaboration/#review-process","title":"Review Process","text":"<ol> <li>Create PR : With clear description</li> <li>Wait for review : Maintainers check</li> <li>Fix : If requested</li> <li>Approve : Once validated</li> <li>Merge : By maintainer</li> </ol>"},{"location":"Git/EN/05-collaboration/#team-workflow","title":"Team Workflow","text":""},{"location":"Git/EN/05-collaboration/#standard-workflow","title":"Standard Workflow","text":"<pre><code># 1. Retrieve latest changes\ngit pull origin main\n\n# 2. Create a branch\ngit checkout -b feature-new-function\n\n# 3. Work\n# ... modifications ...\n\n# 4. Commit regularly\ngit add .\ngit commit -m \"Clear message\"\n\n# 5. Push branch\ngit push -u origin feature-new-function\n\n# 6. Create PR\n# On GitHub/GitLab\n\n# 7. After merge, clean up\ngit checkout main\ngit pull origin main\ngit branch -d feature-new-function\n</code></pre>"},{"location":"Git/EN/05-collaboration/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Fork : Copy of a repository</li> <li>Pull Request : Propose changes</li> <li>Issues : Track problems</li> <li>Code Review : Code verification</li> <li>Workflow : Structured process</li> </ol>"},{"location":"Git/EN/05-collaboration/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 6. Advanced Features to deepen.</p>"},{"location":"Git/EN/06-advanced/","title":"6. Git Advanced Features","text":""},{"location":"Git/EN/06-advanced/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Use Stash</li> <li>Understand Rebase</li> <li>Manage Tags</li> <li>Use Hooks</li> <li>Advanced commands</li> </ul>"},{"location":"Git/EN/06-advanced/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Stash</li> <li>Rebase</li> <li>Tags</li> <li>Hooks</li> <li>Advanced Commands</li> </ol>"},{"location":"Git/EN/06-advanced/#stash","title":"Stash","text":""},{"location":"Git/EN/06-advanced/#what-is-stash","title":"What is Stash?","text":"<p>Stash = Temporarily save changes</p> <ul> <li>Temporary : Uncommitted changes</li> <li>Quick : Switch branches quickly</li> <li>Recoverable : Retrieve later</li> </ul>"},{"location":"Git/EN/06-advanced/#use-stash","title":"Use Stash","text":"<pre><code># Save changes\ngit stash\n\n# With message\ngit stash save \"Descriptive message\"\n\n# Apply last stash\ngit stash apply\n\n# Apply and remove\ngit stash pop\n\n# List stashes\ngit stash list\n</code></pre>"},{"location":"Git/EN/06-advanced/#rebase","title":"Rebase","text":""},{"location":"Git/EN/06-advanced/#what-is-rebase","title":"What is Rebase?","text":"<p>Rebase = Reapply commits on another base</p> <ul> <li>Linear history : Cleaner</li> <li>Rewriting : Modifies history</li> <li>Caution : Don't rebase on shared branches</li> </ul>"},{"location":"Git/EN/06-advanced/#simple-rebase","title":"Simple Rebase","text":"<pre><code># Rebase on main\ngit checkout feature-branch\ngit rebase main\n\n# Interactive rebase\ngit rebase -i HEAD~3\n</code></pre>"},{"location":"Git/EN/06-advanced/#tags","title":"Tags","text":""},{"location":"Git/EN/06-advanced/#what-is-a-tag","title":"What is a Tag?","text":"<p>Tag = Pointer to specific commit</p> <ul> <li>Version : Mark versions</li> <li>Release : Release points</li> <li>Reference : Stable reference</li> </ul>"},{"location":"Git/EN/06-advanced/#create-a-tag","title":"Create a Tag","text":"<pre><code># Lightweight tag\ngit tag v1.0.0\n\n# Annotated tag (recommended)\ngit tag -a v1.0.0 -m \"Version 1.0.0\"\n\n# Push tag\ngit push origin v1.0.0\n</code></pre>"},{"location":"Git/EN/06-advanced/#hooks","title":"Hooks","text":""},{"location":"Git/EN/06-advanced/#what-is-a-hook","title":"What is a Hook?","text":"<p>Hook = Script executed at certain events</p> <ul> <li>Automation : Execute actions</li> <li>Validation : Check before commit</li> <li>Notification : Notify after push</li> </ul>"},{"location":"Git/EN/06-advanced/#example-hook","title":"Example Hook","text":"<p><code>.git/hooks/pre-commit</code>:</p> <pre><code>#!/bin/bash\n# Run tests before commit\npython -m pytest tests/\n\nif [ $? -ne 0 ]; then\n    echo \"Tests failed, commit cancelled\"\n    exit 1\nfi\n</code></pre>"},{"location":"Git/EN/06-advanced/#advanced-commands","title":"Advanced Commands","text":""},{"location":"Git/EN/06-advanced/#cherry-pick","title":"Cherry-pick","text":"<pre><code># Apply specific commit\ngit cherry-pick &lt;hash&gt;\n</code></pre>"},{"location":"Git/EN/06-advanced/#reflog","title":"Reflog","text":"<pre><code># See action history\ngit reflog\n\n# Recover lost commit\ngit checkout &lt;hash&gt;\n</code></pre>"},{"location":"Git/EN/06-advanced/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Stash : Temporarily save</li> <li>Rebase : Rewrite history</li> <li>Tags : Mark versions</li> <li>Hooks : Automate actions</li> <li>Advanced commands : Powerful tools</li> </ol>"},{"location":"Git/EN/06-advanced/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 7. Best Practices for best practices.</p>"},{"location":"Git/EN/07-best-practices/","title":"7. Git Best Practices","text":""},{"location":"Git/EN/07-best-practices/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Effective commit messages</li> <li>Project structure</li> <li>Complete .gitignore</li> <li>Documentation</li> <li>Optimal workflow</li> </ul>"},{"location":"Git/EN/07-best-practices/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Commit Messages</li> <li>Project Structure</li> <li>.gitignore</li> <li>Documentation</li> <li>Workflow</li> </ol>"},{"location":"Git/EN/07-best-practices/#commit-messages","title":"Commit Messages","text":""},{"location":"Git/EN/07-best-practices/#recommended-format","title":"Recommended Format","text":"<pre><code>Type: Short description (max 50 characters)\n\nDetailed description if needed (72 characters per line)\n\n- Point 1\n- Point 2\n</code></pre>"},{"location":"Git/EN/07-best-practices/#commit-types","title":"Commit Types","text":"<ul> <li>feat : New feature</li> <li>fix : Bug fix</li> <li>docs : Documentation</li> <li>style : Formatting</li> <li>refactor : Refactoring</li> <li>test : Tests</li> <li>chore : Maintenance tasks</li> </ul>"},{"location":"Git/EN/07-best-practices/#project-structure","title":"Project Structure","text":""},{"location":"Git/EN/07-best-practices/#recommended-structure","title":"Recommended Structure","text":"<pre><code>my-project/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_main.py\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 guide.md\n</code></pre>"},{"location":"Git/EN/07-best-practices/#readmemd","title":"README.md","text":"<p>Essential content:</p> <pre><code># Project Name\n\nShort project description.\n\n## Installation\n\n\\`\\`\\`bash\npip install -r requirements.txt\n\\`\\`\\`\n\n## Usage\n\n\\`\\`\\`python\nfrom src.main import function\nfunction()\n\\`\\`\\`\n</code></pre>"},{"location":"Git/EN/07-best-practices/#gitignore","title":".gitignore","text":""},{"location":"Git/EN/07-best-practices/#complete-gitignore-for-python","title":"Complete .gitignore for Python","text":"<pre><code># Python\n__pycache__/\n*.py[cod]\nvenv/\nenv/\n\n# Jupyter\n.ipynb_checkpoints\n*.ipynb\n\n# Data\n*.csv\n*.xlsx\ndata/\n\n# IDE\n.vscode/\n.idea/\n\n# Secrets\n.env\n*.key\n</code></pre>"},{"location":"Git/EN/07-best-practices/#documentation","title":"Documentation","text":""},{"location":"Git/EN/07-best-practices/#code-documentation","title":"Code Documentation","text":"<p>Python Docstrings:</p> <pre><code>def analyze_data(file):\n    \"\"\"\n    Analyze a CSV data file.\n\n    Args:\n        file (str): Path to CSV file\n\n    Returns:\n        dict: Dictionary with statistics\n    \"\"\"\n    # Code...\n</code></pre>"},{"location":"Git/EN/07-best-practices/#workflow","title":"Workflow","text":""},{"location":"Git/EN/07-best-practices/#recommended-workflow","title":"Recommended Workflow","text":"<ol> <li>Create a branch : For each feature</li> <li>Commit regularly : Small frequent commits</li> <li>Test : Before pushing</li> <li>Pull Request : For review</li> <li>Merge : After approval</li> </ol>"},{"location":"Git/EN/07-best-practices/#golden-rules","title":"Golden Rules","text":"<ul> <li>One commit = One logical change</li> <li>Clear and descriptive messages</li> <li>Test before pushing</li> <li>Never force push on main</li> <li>Synchronize regularly</li> </ul>"},{"location":"Git/EN/07-best-practices/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Messages : Clear and structured</li> <li>Structure : Organized and logical</li> <li>.gitignore : Complete and adapted</li> <li>Documentation : README and docstrings</li> <li>Workflow : Regular and consistent</li> </ol>"},{"location":"Git/EN/07-best-practices/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 8. Practical Projects to create complete projects.</p>"},{"location":"Git/EN/08-projets/","title":"8. Git Practical Projects","text":""},{"location":"Git/EN/08-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create a GitHub portfolio</li> <li>Manage a collaborative project</li> <li>Version Python scripts</li> <li>Document a project</li> <li>Portfolio projects</li> </ul>"},{"location":"Git/EN/08-projets/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Project 1 : GitHub Portfolio</li> <li>Project 2 : Collaborative Project</li> <li>Project 3 : Versioned Python Scripts</li> <li>Project 4 : Project Documentation</li> </ol>"},{"location":"Git/EN/08-projets/#project-1-github-portfolio","title":"Project 1 : GitHub Portfolio","text":""},{"location":"Git/EN/08-projets/#objective","title":"Objective","text":"<p>Create a professional portfolio on GitHub.</p>"},{"location":"Git/EN/08-projets/#structure","title":"Structure","text":"<pre><code>portfolio/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 projects/\n\u2502   \u251c\u2500\u2500 project1/\n\u2502   \u251c\u2500\u2500 project2/\n\u2502   \u2514\u2500\u2500 project3/\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 utilities.py\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 resume.md\n</code></pre>"},{"location":"Git/EN/08-projets/#create-repository","title":"Create Repository","text":"<pre><code># Create local repository\nmkdir portfolio\ncd portfolio\ngit init\n\n# Create structure\nmkdir projects scripts docs\n\n# Create README\necho \"# My Portfolio\" &gt; README.md\n\n# First commit\ngit add .\ngit commit -m \"Initial commit: portfolio\"\n\n# Create on GitHub and push\ngit remote add origin https://github.com/username/portfolio.git\ngit push -u origin main\n</code></pre>"},{"location":"Git/EN/08-projets/#project-2-collaborative-project","title":"Project 2 : Collaborative Project","text":""},{"location":"Git/EN/08-projets/#workflow","title":"Workflow","text":"<pre><code># 1. Clone repository\ngit clone https://github.com/team/project.git\ncd project\n\n# 2. Create branch\ngit checkout -b feature-my-contribution\n\n# 3. Work\n# ... modifications ...\n\n# 4. Commit\ngit add .\ngit commit -m \"feat: Add new feature\"\n\n# 5. Synchronize with main\ngit fetch origin\ngit rebase origin/main\n\n# 6. Push\ngit push -u origin feature-my-contribution\n\n# 7. Create Pull Request on GitHub\n</code></pre>"},{"location":"Git/EN/08-projets/#project-3-versioned-python-scripts","title":"Project 3 : Versioned Python Scripts","text":""},{"location":"Git/EN/08-projets/#structure_1","title":"Structure","text":"<pre><code>data-scripts/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 data_loader.py\n\u2502   \u251c\u2500\u2500 analyzer.py\n\u2502   \u2514\u2500\u2500 visualizer.py\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 .gitkeep\n</code></pre>"},{"location":"Git/EN/08-projets/#workflow_1","title":"Workflow","text":"<pre><code># Initialize\ngit init\ngit add .\ngit commit -m \"Initial commit: analysis scripts\"\n\n# Create branch for new feature\ngit checkout -b feature-new-analysis\n\n# Develop\n# ... code ...\n\n# Commit\ngit add src/analyzer.py\ngit commit -m \"feat: Add advanced statistical analysis\"\n\n# Merge\ngit checkout main\ngit merge feature-new-analysis\n\n# Tag a version\ngit tag -a v1.0.0 -m \"Version 1.0.0\"\ngit push origin main --tags\n</code></pre>"},{"location":"Git/EN/08-projets/#project-4-project-documentation","title":"Project 4 : Project Documentation","text":""},{"location":"Git/EN/08-projets/#structure_2","title":"Structure","text":"<pre><code>project-docs/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u251c\u2500\u2500 usage.md\n\u2502   \u2514\u2500\u2500 api.md\n\u2514\u2500\u2500 CHANGELOG.md\n</code></pre>"},{"location":"Git/EN/08-projets/#documentation-workflow","title":"Documentation Workflow","text":"<pre><code># Create branch for documentation\ngit checkout -b docs/add-usage-guide\n\n# Add documentation\n# ... write docs/usage.md ...\n\n# Commit\ngit add docs/usage.md\ngit commit -m \"docs: Add usage guide\"\n\n# Push and create PR\ngit push -u origin docs/add-usage-guide\n</code></pre>"},{"location":"Git/EN/08-projets/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Portfolio : Showcase your projects</li> <li>Collaboration : Structured workflow</li> <li>Versioning : Manage versions</li> <li>Documentation : Essential</li> <li>GitHub : Professional platform</li> </ol>"},{"location":"Git/EN/08-projets/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>GitHub Guides</li> <li>Git Documentation</li> </ul> <p>Congratulations! You have completed the Git training. You can now manage your projects efficiently with Git and GitHub.</p>"},{"location":"Git/FR/","title":"Formation Git pour Data Analyst","text":""},{"location":"Git/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'apprentissage de Git en tant que Data Analyst. Git est un syst\u00e8me de contr\u00f4le de version distribu\u00e9 essentiel pour g\u00e9rer le code, les scripts et la documentation de vos projets.</p>"},{"location":"Git/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre Git et le contr\u00f4le de version</li> <li>Installer Git</li> <li>Ma\u00eetriser les commandes de base</li> <li>G\u00e9rer les branches</li> <li>Travailler avec des d\u00e9p\u00f4ts distants (GitHub, GitLab)</li> <li>Collaborer sur des projets</li> <li>Utiliser Git dans vos workflows de donn\u00e9es</li> <li>Cr\u00e9er des projets pratiques pour votre portfolio</li> </ul>"},{"location":"Git/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 Git : Open-source et gratuit - \u2705 GitHub : Compte gratuit (illimit\u00e9) - \u2705 GitLab : Compte gratuit - \u2705 Documentation officielle : Guides complets gratuits - \u2705 Tutoriels en ligne : Ressources gratuites</p> <p>Budget total : 0\u20ac</p>"},{"location":"Git/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"Git/FR/#1-prise-en-main-git","title":"1. Prise en main Git","text":"<ul> <li>Installer Git</li> <li>Configuration initiale</li> <li>Premiers d\u00e9p\u00f4ts</li> <li>Concepts de base</li> </ul>"},{"location":"Git/FR/#2-commandes-de-base","title":"2. Commandes de base","text":"<ul> <li>Cr\u00e9er un d\u00e9p\u00f4t</li> <li>Ajouter et commiter</li> <li>Voir l'historique</li> <li>Annuler des modifications</li> </ul>"},{"location":"Git/FR/#3-branches","title":"3. Branches","text":"<ul> <li>Cr\u00e9er et g\u00e9rer des branches</li> <li>Fusionner des branches</li> <li>R\u00e9soudre les conflits</li> <li>Workflow avec branches</li> </ul>"},{"location":"Git/FR/#4-depots-distants","title":"4. D\u00e9p\u00f4ts distants","text":"<ul> <li>GitHub et GitLab</li> <li>Cloner un d\u00e9p\u00f4t</li> <li>Push et Pull</li> <li>Synchronisation</li> </ul>"},{"location":"Git/FR/#5-collaboration","title":"5. Collaboration","text":"<ul> <li>Fork et Pull Requests</li> <li>Issues et Projects</li> <li>Code Review</li> <li>Workflow en \u00e9quipe</li> </ul>"},{"location":"Git/FR/#6-fonctionnalites-avancees","title":"6. Fonctionnalit\u00e9s avanc\u00e9es","text":"<ul> <li>Stash</li> <li>Rebase</li> <li>Tags</li> <li>Hooks</li> </ul>"},{"location":"Git/FR/#7-bonnes-pratiques","title":"7. Bonnes pratiques","text":"<ul> <li>Messages de commit</li> <li>Structure de projet</li> <li>.gitignore</li> <li>Documentation</li> </ul>"},{"location":"Git/FR/#8-projets-pratiques","title":"8. Projets pratiques","text":"<ul> <li>Portfolio GitHub</li> <li>Projet collaboratif</li> <li>Gestion de scripts Python</li> <li>Documentation de projet</li> </ul>"},{"location":"Git/FR/#demarrage-rapide","title":"\ud83d\ude80 D\u00e9marrage rapide","text":""},{"location":"Git/FR/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Syst\u00e8me d'exploitation : Windows, Linux, ou macOS</li> <li>Connexion Internet : Pour GitHub/GitLab</li> <li>\u00c9diteur de texte : VS Code, Sublime, etc.</li> </ul>"},{"location":"Git/FR/#installation-rapide","title":"Installation rapide","text":"<p>Windows : 1. T\u00e9l\u00e9charger Git : https://git-scm.com/download/win 2. Installer avec les options par d\u00e9faut 3. V\u00e9rifier : <code>git --version</code></p> <p>Linux : <pre><code># Ubuntu/Debian\nsudo apt install git\n\n# CentOS/RHEL\nsudo yum install git\n\n# V\u00e9rifier\ngit --version\n</code></pre></p> <p>macOS : <pre><code># Avec Homebrew\nbrew install git\n\n# Ou t\u00e9l\u00e9charger\n# https://git-scm.com/download/mac\n\n# V\u00e9rifier\ngit --version\n</code></pre></p>"},{"location":"Git/FR/#configuration-initiale","title":"Configuration initiale","text":"<pre><code># Configurer votre nom\ngit config --global user.name \"Votre Nom\"\n\n# Configurer votre email\ngit config --global user.email \"votre.email@example.com\"\n\n# V\u00e9rifier la configuration\ngit config --list\n</code></pre>"},{"location":"Git/FR/#premier-depot","title":"Premier d\u00e9p\u00f4t","text":"<pre><code># Cr\u00e9er un nouveau d\u00e9p\u00f4t\nmkdir mon-projet\ncd mon-projet\ngit init\n\n# Cr\u00e9er un fichier\necho \"# Mon Projet\" &gt; README.md\n\n# Ajouter et commiter\ngit add README.md\ngit commit -m \"Premier commit\"\n</code></pre>"},{"location":"Git/FR/#cas-dusage-pour-data-analyst","title":"\ud83d\udcca Cas d'usage pour Data Analyst","text":"<ul> <li>Versioning : G\u00e9rer les versions de vos scripts Python/R</li> <li>Collaboration : Travailler en \u00e9quipe sur des projets</li> <li>Portfolio : Pr\u00e9senter vos projets sur GitHub</li> <li>Documentation : Versionner votre documentation</li> <li>Backup : Sauvegarder votre code en ligne</li> </ul>"},{"location":"Git/FR/#ressources-gratuites","title":"\ud83d\udcda Ressources gratuites","text":""},{"location":"Git/FR/#documentation-officielle","title":"Documentation officielle","text":"<ul> <li>Git Documentation : https://git-scm.com/doc</li> <li>GitHub Guides : https://guides.github.com/</li> <li>GitLab Documentation : https://docs.gitlab.com/</li> </ul>"},{"location":"Git/FR/#ressources-externes","title":"Ressources externes","text":"<ul> <li>GitHub Learning Lab : https://lab.github.com/</li> <li>Atlassian Git Tutorials : https://www.atlassian.com/git/tutorials</li> <li>YouTube : Tutoriels Git</li> </ul>"},{"location":"Git/FR/#certifications-optionnel","title":"\ud83c\udf93 Certifications (optionnel)","text":""},{"location":"Git/FR/#github-certifications","title":"GitHub Certifications","text":"<ul> <li>GitHub Actions : Gratuit</li> <li>GitHub Advanced Security : Formation gratuite</li> </ul>"},{"location":"Git/FR/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>Tous les exemples fonctionnent sur Windows, Linux, et macOS</li> <li>Les commandes sont identiques sur tous les syst\u00e8mes</li> <li>GitHub est utilis\u00e9 comme exemple principal</li> </ul>"},{"location":"Git/FR/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>Cette formation est con\u00e7ue pour \u00eatre \u00e9volutive. N'h\u00e9sitez pas \u00e0 proposer des am\u00e9liorations.</p>"},{"location":"Git/FR/#ressources-complementaires","title":"\ud83d\udcda Ressources compl\u00e9mentaires","text":"<ul> <li>Git Documentation</li> <li>GitHub</li> <li>GitLab</li> <li>Git Cheat Sheet</li> </ul>"},{"location":"Git/FR/01-getting-started/","title":"1. Prise en main Git","text":""},{"location":"Git/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Git et le contr\u00f4le de version</li> <li>Installer Git</li> <li>Configurer Git</li> <li>Cr\u00e9er votre premier d\u00e9p\u00f4t</li> <li>Comprendre les concepts de base</li> </ul>"},{"location":"Git/FR/01-getting-started/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Git</li> <li>Installation</li> <li>Configuration</li> <li>Premier d\u00e9p\u00f4t</li> <li>Concepts de base</li> </ol>"},{"location":"Git/FR/01-getting-started/#introduction-a-git","title":"Introduction \u00e0 Git","text":""},{"location":"Git/FR/01-getting-started/#quest-ce-que-git","title":"Qu'est-ce que Git ?","text":"<p>Git = Syst\u00e8me de contr\u00f4le de version distribu\u00e9</p> <ul> <li>Versioning : Suit les modifications de fichiers</li> <li>Distribu\u00e9 : Chaque d\u00e9veloppeur a une copie compl\u00e8te</li> <li>Collaboration : Facilite le travail en \u00e9quipe</li> <li>Historique : Conserve l'historique complet</li> </ul>"},{"location":"Git/FR/01-getting-started/#pourquoi-git-pour-data-analyst","title":"Pourquoi Git pour Data Analyst ?","text":"<ul> <li>Scripts : Versionner vos scripts Python/R</li> <li>Projets : G\u00e9rer vos projets de portfolio</li> <li>Collaboration : Travailler en \u00e9quipe</li> <li>Backup : Sauvegarder en ligne (GitHub)</li> <li>Documentation : Versionner votre documentation</li> </ul>"},{"location":"Git/FR/01-getting-started/#git-vs-autres-systemes","title":"Git vs Autres syst\u00e8mes","text":"<p>Git : - Distribu\u00e9 - Rapide - Gratuit et open-source - Standard de l'industrie</p> <p>Autres (SVN, CVS) : - Centralis\u00e9 - Plus lent - Moins utilis\u00e9</p>"},{"location":"Git/FR/01-getting-started/#installation","title":"Installation","text":""},{"location":"Git/FR/01-getting-started/#windows","title":"Windows","text":"<p>\u00c9tape 1 : T\u00e9l\u00e9charger Git</p> <ol> <li>Aller sur : https://git-scm.com/download/win</li> <li>T\u00e9l\u00e9charger l'installateur</li> <li>Ex\u00e9cuter l'installateur</li> <li>Accepter les options par d\u00e9faut</li> </ol> <p>\u00c9tape 2 : V\u00e9rifier l'installation</p> <pre><code># Ouvrir Git Bash ou PowerShell\ngit --version\n</code></pre>"},{"location":"Git/FR/01-getting-started/#linux","title":"Linux","text":"<p>Ubuntu/Debian :</p> <pre><code># Mettre \u00e0 jour les paquets\nsudo apt update\n\n# Installer Git\nsudo apt install git\n\n# V\u00e9rifier\ngit --version\n</code></pre> <p>CentOS/RHEL :</p> <pre><code># Installer Git\nsudo yum install git\n\n# V\u00e9rifier\ngit --version\n</code></pre>"},{"location":"Git/FR/01-getting-started/#macos","title":"macOS","text":"<p>Avec Homebrew :</p> <pre><code># Installer Git\nbrew install git\n\n# V\u00e9rifier\ngit --version\n</code></pre> <p>Ou t\u00e9l\u00e9charger :</p> <ol> <li>Aller sur : https://git-scm.com/download/mac</li> <li>T\u00e9l\u00e9charger et installer</li> </ol>"},{"location":"Git/FR/01-getting-started/#configuration","title":"Configuration","text":""},{"location":"Git/FR/01-getting-started/#configuration-globale","title":"Configuration globale","text":"<pre><code># Configurer votre nom\ngit config --global user.name \"Votre Nom\"\n\n# Configurer votre email\ngit config --global user.email \"votre.email@example.com\"\n\n# Configurer l'\u00e9diteur par d\u00e9faut\ngit config --global core.editor \"code --wait\"  # VS Code\n# ou\ngit config --global core.editor \"nano\"  # Nano\n</code></pre>"},{"location":"Git/FR/01-getting-started/#verifier-la-configuration","title":"V\u00e9rifier la configuration","text":"<pre><code># Voir toute la configuration\ngit config --list\n\n# Voir une configuration sp\u00e9cifique\ngit config user.name\ngit config user.email\n</code></pre>"},{"location":"Git/FR/01-getting-started/#configuration-par-depot","title":"Configuration par d\u00e9p\u00f4t","text":"<pre><code># Dans un d\u00e9p\u00f4t sp\u00e9cifique\ngit config user.name \"Nom pour ce projet\"\ngit config user.email \"email@example.com\"\n</code></pre>"},{"location":"Git/FR/01-getting-started/#premier-depot","title":"Premier d\u00e9p\u00f4t","text":""},{"location":"Git/FR/01-getting-started/#creer-un-nouveau-depot","title":"Cr\u00e9er un nouveau d\u00e9p\u00f4t","text":"<pre><code># Cr\u00e9er un r\u00e9pertoire\nmkdir mon-projet\ncd mon-projet\n\n# Initialiser Git\ngit init\n\n# V\u00e9rifier\nls -la  # Voir le dossier .git\n</code></pre>"},{"location":"Git/FR/01-getting-started/#premier-commit","title":"Premier commit","text":"<pre><code># Cr\u00e9er un fichier\necho \"# Mon Projet\" &gt; README.md\n\n# Voir le statut\ngit status\n\n# Ajouter le fichier\ngit add README.md\n\n# Commiter\ngit commit -m \"Premier commit : ajout README\"\n</code></pre>"},{"location":"Git/FR/01-getting-started/#voir-lhistorique","title":"Voir l'historique","text":"<pre><code># Voir les commits\ngit log\n\n# Voir les commits de mani\u00e8re compacte\ngit log --oneline\n\n# Voir les modifications\ngit show\n</code></pre>"},{"location":"Git/FR/01-getting-started/#concepts-de-base","title":"Concepts de base","text":""},{"location":"Git/FR/01-getting-started/#depot-repository","title":"D\u00e9p\u00f4t (Repository)","text":"<p>D\u00e9p\u00f4t = Dossier avec historique Git</p> <ul> <li>Local : Sur votre machine</li> <li>Distant : Sur GitHub/GitLab</li> <li>.git : Dossier cach\u00e9 contenant l'historique</li> </ul>"},{"location":"Git/FR/01-getting-started/#commit","title":"Commit","text":"<p>Commit = Point dans l'historique</p> <ul> <li>Snapshot : Capture de l'\u00e9tat des fichiers</li> <li>Message : Description des modifications</li> <li>Auteur : Nom et email</li> <li>Hash : Identifiant unique (SHA-1)</li> </ul>"},{"location":"Git/FR/01-getting-started/#branche-branch","title":"Branche (Branch)","text":"<p>Branche = Ligne de d\u00e9veloppement</p> <ul> <li>main/master : Branche principale</li> <li>Autres branches : Pour nouvelles fonctionnalit\u00e9s</li> <li>Isolation : Travail isol\u00e9</li> </ul>"},{"location":"Git/FR/01-getting-started/#staging-area","title":"Staging Area","text":"<p>Staging Area = Zone de pr\u00e9paration</p> <ul> <li>git add : Ajouter des fichiers</li> <li>git commit : Cr\u00e9er un commit</li> <li>git status : Voir l'\u00e9tat</li> </ul>"},{"location":"Git/FR/01-getting-started/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Git/FR/01-getting-started/#exemple-1-projet-python","title":"Exemple 1 : Projet Python","text":"<pre><code># Cr\u00e9er le projet\nmkdir data-analysis\ncd data-analysis\ngit init\n\n# Cr\u00e9er les fichiers\necho \"import pandas as pd\" &gt; script.py\necho \"# Data Analysis Project\" &gt; README.md\n\n# Ajouter et commiter\ngit add .\ngit commit -m \"Initial commit : projet d'analyse de donn\u00e9es\"\n</code></pre>"},{"location":"Git/FR/01-getting-started/#exemple-2-projet-avec-structure","title":"Exemple 2 : Projet avec structure","text":"<pre><code># Cr\u00e9er la structure\nmkdir my-project\ncd my-project\ngit init\n\n# Cr\u00e9er les dossiers\nmkdir src data docs\n\n# Cr\u00e9er des fichiers\necho \"# Mon Projet\" &gt; README.md\necho \"print('Hello')\" &gt; src/main.py\n\n# Ajouter tout\ngit add .\n\n# Commiter\ngit commit -m \"Structure initiale du projet\"\n</code></pre>"},{"location":"Git/FR/01-getting-started/#commandes-essentielles","title":"Commandes essentielles","text":""},{"location":"Git/FR/01-getting-started/#informations","title":"Informations","text":"<pre><code># Version Git\ngit --version\n\n# Statut du d\u00e9p\u00f4t\ngit status\n\n# Historique\ngit log\n\n# Configuration\ngit config --list\n</code></pre>"},{"location":"Git/FR/01-getting-started/#creation","title":"Cr\u00e9ation","text":"<pre><code># Initialiser un d\u00e9p\u00f4t\ngit init\n\n# Cloner un d\u00e9p\u00f4t\ngit clone &lt;url&gt;\n</code></pre>"},{"location":"Git/FR/01-getting-started/#depannage","title":"D\u00e9pannage","text":""},{"location":"Git/FR/01-getting-started/#probleme-git-non-trouve","title":"Probl\u00e8me : Git non trouv\u00e9","text":"<p>Solutions : 1. V\u00e9rifier l'installation : <code>git --version</code> 2. Ajouter Git au PATH (Windows) 3. R\u00e9installer Git</p>"},{"location":"Git/FR/01-getting-started/#probleme-erreur-de-configuration","title":"Probl\u00e8me : Erreur de configuration","text":"<p>Solutions : 1. V\u00e9rifier la configuration : <code>git config --list</code> 2. Reconfigurer : <code>git config --global user.name \"Nom\"</code></p>"},{"location":"Git/FR/01-getting-started/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Git = Contr\u00f4le de version distribu\u00e9</li> <li>D\u00e9p\u00f4t = Dossier avec historique</li> <li>Commit = Point dans l'historique</li> <li>Branche = Ligne de d\u00e9veloppement</li> <li>Staging = Zone de pr\u00e9paration</li> </ol>"},{"location":"Git/FR/01-getting-started/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 2. Commandes de base pour ma\u00eetriser les commandes essentielles.</p>"},{"location":"Git/FR/02-basic-commands/","title":"2. Commandes de base Git","text":""},{"location":"Git/FR/02-basic-commands/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er et g\u00e9rer un d\u00e9p\u00f4t</li> <li>Ajouter et commiter des fichiers</li> <li>Voir l'historique</li> <li>Annuler des modifications</li> <li>Ignorer des fichiers</li> </ul>"},{"location":"Git/FR/02-basic-commands/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Cr\u00e9er un d\u00e9p\u00f4t</li> <li>Ajouter et commiter</li> <li>Voir l'historique</li> <li>Annuler des modifications</li> <li>.gitignore</li> </ol>"},{"location":"Git/FR/02-basic-commands/#creer-un-depot","title":"Cr\u00e9er un d\u00e9p\u00f4t","text":""},{"location":"Git/FR/02-basic-commands/#initialiser-un-depot-local","title":"Initialiser un d\u00e9p\u00f4t local","text":"<pre><code># Cr\u00e9er un nouveau d\u00e9p\u00f4t\nmkdir mon-projet\ncd mon-projet\ngit init\n\n# V\u00e9rifier\nls -la  # Voir .git\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#cloner-un-depot-existant","title":"Cloner un d\u00e9p\u00f4t existant","text":"<pre><code># Cloner depuis GitHub\ngit clone https://github.com/username/repo.git\n\n# Cloner dans un dossier sp\u00e9cifique\ngit clone https://github.com/username/repo.git mon-dossier\n\n# Cloner avec SSH\ngit clone git@github.com:username/repo.git\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#ajouter-et-commiter","title":"Ajouter et commiter","text":""},{"location":"Git/FR/02-basic-commands/#workflow-de-base","title":"Workflow de base","text":"<pre><code># 1. Voir le statut\ngit status\n\n# 2. Ajouter des fichiers\ngit add fichier.py\ngit add .  # Tous les fichiers\n\n# 3. Commiter\ngit commit -m \"Message de commit\"\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#ajouter-des-fichiers","title":"Ajouter des fichiers","text":"<pre><code># Ajouter un fichier sp\u00e9cifique\ngit add script.py\n\n# Ajouter tous les fichiers\ngit add .\n\n# Ajouter tous les fichiers Python\ngit add *.py\n\n# Ajouter un dossier\ngit add src/\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#creer-un-commit","title":"Cr\u00e9er un commit","text":"<pre><code># Commit avec message\ngit commit -m \"Ajout du script d'analyse\"\n\n# Commit avec message d\u00e9taill\u00e9\ngit commit -m \"Titre\" -m \"Description d\u00e9taill\u00e9e\"\n\n# Commit en ajoutant automatiquement les fichiers modifi\u00e9s\ngit commit -am \"Message\"\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#messages-de-commit","title":"Messages de commit","text":"<p>Bon format : <pre><code>Type : Description courte (50 caract\u00e8res max)\n\nDescription d\u00e9taill\u00e9e si n\u00e9cessaire\n</code></pre></p> <p>Exemples : <pre><code>feat: Ajout fonction analyse de donn\u00e9es\nfix: Correction bug dans le calcul\ndocs: Mise \u00e0 jour README\nrefactor: R\u00e9organisation du code\n</code></pre></p>"},{"location":"Git/FR/02-basic-commands/#voir-lhistorique","title":"Voir l'historique","text":""},{"location":"Git/FR/02-basic-commands/#git-log","title":"git log","text":"<pre><code># Historique complet\ngit log\n\n# Historique compact\ngit log --oneline\n\n# Historique avec graphique\ngit log --graph --oneline --all\n\n# Limiter le nombre\ngit log -5  # 5 derniers commits\n\n# Par auteur\ngit log --author=\"Nom\"\n\n# Par date\ngit log --since=\"2024-01-01\"\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#voir-les-modifications","title":"Voir les modifications","text":"<pre><code># Dernier commit\ngit show\n\n# Commit sp\u00e9cifique\ngit show &lt;hash&gt;\n\n# Diff\u00e9rence entre commits\ngit diff HEAD~1 HEAD\n\n# Diff\u00e9rence avec staging\ngit diff --staged\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#statut","title":"Statut","text":"<pre><code># Statut d\u00e9taill\u00e9\ngit status\n\n# Statut court\ngit status -s\n\n# Voir les fichiers modifi\u00e9s\ngit status --short\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#annuler-des-modifications","title":"Annuler des modifications","text":""},{"location":"Git/FR/02-basic-commands/#annuler-dans-le-working-directory","title":"Annuler dans le working directory","text":"<pre><code># Annuler modifications d'un fichier\ngit checkout -- fichier.py\n\n# Annuler toutes les modifications\ngit checkout -- .\n\n# Nouvelle syntaxe (Git 2.23+)\ngit restore fichier.py\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#annuler-dans-staging","title":"Annuler dans staging","text":"<pre><code># Retirer un fichier du staging\ngit reset HEAD fichier.py\n\n# Retirer tous les fichiers\ngit reset HEAD\n\n# Nouvelle syntaxe\ngit restore --staged fichier.py\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#modifier-le-dernier-commit","title":"Modifier le dernier commit","text":"<pre><code># Modifier le message\ngit commit --amend -m \"Nouveau message\"\n\n# Ajouter des fichiers oubli\u00e9s\ngit add fichier_oublie.py\ngit commit --amend --no-edit\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#annuler-un-commit-garder-les-modifications","title":"Annuler un commit (garder les modifications)","text":"<pre><code># Annuler le dernier commit\ngit reset --soft HEAD~1\n\n# Annuler et supprimer les modifications\ngit reset --hard HEAD~1  # ATTENTION : destructif\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#gitignore","title":".gitignore","text":""},{"location":"Git/FR/02-basic-commands/#creer-un-gitignore","title":"Cr\u00e9er un .gitignore","text":"<p>Fichier <code>.gitignore</code> :</p> <pre><code># Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nvenv/\nenv/\n\n# Jupyter\n.ipynb_checkpoints\n*.ipynb\n\n# Data\n*.csv\n*.xlsx\n*.parquet\ndata/\n\n# IDE\n.vscode/\n.idea/\n*.swp\n\n# OS\n.DS_Store\nThumbs.db\n\n# Secrets\n.env\n*.key\nconfig.ini\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#utiliser-gitignore","title":"Utiliser .gitignore","text":"<pre><code># Cr\u00e9er le fichier\ntouch .gitignore\n\n# Ajouter au d\u00e9p\u00f4t\ngit add .gitignore\ngit commit -m \"Ajout .gitignore\"\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#patterns","title":"Patterns","text":"<pre><code># Ignorer un fichier\nfichier.txt\n\n# Ignorer un dossier\ndossier/\n\n# Ignorer tous les fichiers .log\n*.log\n\n# Ignorer sauf un fichier sp\u00e9cifique\n*.log\n!important.log\n\n# Ignorer dans un dossier sp\u00e9cifique\ndata/*.csv\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Git/FR/02-basic-commands/#exemple-1-projet-python","title":"Exemple 1 : Projet Python","text":"<pre><code># Cr\u00e9er le projet\nmkdir data-project\ncd data-project\ngit init\n\n# Cr\u00e9er .gitignore\necho \"__pycache__/\" &gt; .gitignore\necho \"*.pyc\" &gt;&gt; .gitignore\necho \"venv/\" &gt;&gt; .gitignore\n\n# Cr\u00e9er des fichiers\necho \"import pandas as pd\" &gt; main.py\necho \"# Data Project\" &gt; README.md\n\n# Ajouter et commiter\ngit add .\ngit commit -m \"Initial commit : projet Python\"\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#exemple-2-gestion-des-modifications","title":"Exemple 2 : Gestion des modifications","text":"<pre><code># Modifier un fichier\necho \"print('Hello')\" &gt;&gt; script.py\n\n# Voir les modifications\ngit diff\n\n# Ajouter au staging\ngit add script.py\n\n# Voir les modifications staged\ngit diff --staged\n\n# Commiter\ngit commit -m \"Ajout fonction hello\"\n</code></pre>"},{"location":"Git/FR/02-basic-commands/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>git add : Ajouter au staging</li> <li>git commit : Cr\u00e9er un commit</li> <li>git log : Voir l'historique</li> <li>git status : Voir l'\u00e9tat</li> <li>.gitignore : Exclure des fichiers</li> </ol>"},{"location":"Git/FR/02-basic-commands/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 3. Branches pour apprendre \u00e0 g\u00e9rer les branches.</p>"},{"location":"Git/FR/03-branching/","title":"3. Branches Git","text":""},{"location":"Git/FR/03-branching/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les branches</li> <li>Cr\u00e9er et g\u00e9rer des branches</li> <li>Fusionner des branches</li> <li>R\u00e9soudre les conflits</li> <li>Workflow avec branches</li> </ul>"},{"location":"Git/FR/03-branching/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction aux branches</li> <li>Cr\u00e9er des branches</li> <li>Fusionner des branches</li> <li>R\u00e9soudre les conflits</li> <li>Workflow</li> </ol>"},{"location":"Git/FR/03-branching/#introduction-aux-branches","title":"Introduction aux branches","text":""},{"location":"Git/FR/03-branching/#quest-ce-quune-branche","title":"Qu'est-ce qu'une branche ?","text":"<p>Branche = Ligne de d\u00e9veloppement ind\u00e9pendante</p> <ul> <li>Isolation : Travail isol\u00e9</li> <li>Parall\u00e8le : Plusieurs branches en m\u00eame temps</li> <li>Fusion : Combiner les modifications</li> <li>main/master : Branche principale</li> </ul>"},{"location":"Git/FR/03-branching/#pourquoi-utiliser-des-branches","title":"Pourquoi utiliser des branches ?","text":"<ul> <li>Nouvelles fonctionnalit\u00e9s : Une branche par fonctionnalit\u00e9</li> <li>Corrections : Branche pour les bugs</li> <li>Exp\u00e9rimentation : Tester sans risque</li> <li>Collaboration : Travail en parall\u00e8le</li> </ul>"},{"location":"Git/FR/03-branching/#creer-des-branches","title":"Cr\u00e9er des branches","text":""},{"location":"Git/FR/03-branching/#creer-une-nouvelle-branche","title":"Cr\u00e9er une nouvelle branche","text":"<pre><code># Cr\u00e9er une branche\ngit branch feature-analyse\n\n# Cr\u00e9er et basculer\ngit checkout -b feature-analyse\n\n# Nouvelle syntaxe (Git 2.23+)\ngit switch -c feature-analyse\n</code></pre>"},{"location":"Git/FR/03-branching/#basculer-entre-branches","title":"Basculer entre branches","text":"<pre><code># Basculer vers une branche\ngit checkout feature-analyse\n\n# Nouvelle syntaxe\ngit switch feature-analyse\n\n# Retourner \u00e0 main\ngit checkout main\ngit switch main\n</code></pre>"},{"location":"Git/FR/03-branching/#lister-les-branches","title":"Lister les branches","text":"<pre><code># Branches locales\ngit branch\n\n# Branches distantes\ngit branch -r\n\n# Toutes les branches\ngit branch -a\n\n# Avec dernier commit\ngit branch -v\n</code></pre>"},{"location":"Git/FR/03-branching/#supprimer-une-branche","title":"Supprimer une branche","text":"<pre><code># Supprimer une branche locale\ngit branch -d feature-analyse\n\n# Forcer la suppression\ngit branch -D feature-analyse\n\n# Supprimer une branche distante\ngit push origin --delete feature-analyse\n</code></pre>"},{"location":"Git/FR/03-branching/#fusionner-des-branches","title":"Fusionner des branches","text":""},{"location":"Git/FR/03-branching/#merge-fusion","title":"Merge (fusion)","text":"<pre><code># Basculer sur main\ngit checkout main\n\n# Fusionner la branche\ngit merge feature-analyse\n\n# Fusionner avec message\ngit merge feature-analyse -m \"Fusion feature analyse\"\n</code></pre>"},{"location":"Git/FR/03-branching/#types-de-merge","title":"Types de merge","text":"<p>Fast-forward : - Pas de conflit - Historique lin\u00e9aire - Simple</p> <p>Merge commit : - Cr\u00e9e un commit de fusion - Conserve l'historique - Plus complexe</p>"},{"location":"Git/FR/03-branching/#rebase-alternative","title":"Rebase (alternative)","text":"<pre><code># Rebase interactif\ngit rebase main\n\n# Rebase interactif avec \u00e9dition\ngit rebase -i HEAD~3\n</code></pre>"},{"location":"Git/FR/03-branching/#resoudre-les-conflits","title":"R\u00e9soudre les conflits","text":""},{"location":"Git/FR/03-branching/#quand-surviennent-les-conflits","title":"Quand surviennent les conflits ?","text":"<ul> <li>M\u00eame ligne modifi\u00e9e : Dans deux branches diff\u00e9rentes</li> <li>Fichier supprim\u00e9 : Dans une branche, modifi\u00e9 dans l'autre</li> <li>Fichier ajout\u00e9 : M\u00eame nom dans deux branches</li> </ul>"},{"location":"Git/FR/03-branching/#resoudre-un-conflit","title":"R\u00e9soudre un conflit","text":"<p>\u00c9tape 1 : Identifier le conflit</p> <pre><code># Voir les fichiers en conflit\ngit status\n</code></pre> <p>\u00c9tape 2 : Ouvrir le fichier</p> <pre><code># Fichier avec conflit\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nprint(\"Version main\")\n=======\nprint(\"Version feature\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-analyse\n</code></pre> <p>\u00c9tape 3 : R\u00e9soudre manuellement</p> <pre><code># Choisir la version ou combiner\nprint(\"Version combin\u00e9e\")\n</code></pre> <p>\u00c9tape 4 : Marquer comme r\u00e9solu</p> <pre><code># Ajouter le fichier r\u00e9solu\ngit add fichier.py\n\n# Finaliser le merge\ngit commit\n</code></pre>"},{"location":"Git/FR/03-branching/#outils-de-resolution","title":"Outils de r\u00e9solution","text":"<pre><code># Ouvrir un outil de merge\ngit mergetool\n\n# Voir les conflits\ngit diff\n</code></pre>"},{"location":"Git/FR/03-branching/#workflow","title":"Workflow","text":""},{"location":"Git/FR/03-branching/#workflow-simple","title":"Workflow simple","text":"<pre><code># 1. Cr\u00e9er une branche pour une fonctionnalit\u00e9\ngit checkout -b feature-nouvelle-fonction\n\n# 2. Travailler sur la branche\n# ... modifications ...\n\n# 3. Commiter\ngit add .\ngit commit -m \"Ajout nouvelle fonctionnalit\u00e9\"\n\n# 4. Fusionner dans main\ngit checkout main\ngit merge feature-nouvelle-fonction\n\n# 5. Supprimer la branche\ngit branch -d feature-nouvelle-fonction\n</code></pre>"},{"location":"Git/FR/03-branching/#git-flow-avance","title":"Git Flow (avanc\u00e9)","text":"<pre><code># Branches principales\nmain        # Production\ndevelop     # D\u00e9veloppement\n\n# Branches de support\nfeature/*   # Nouvelles fonctionnalit\u00e9s\nhotfix/*    # Corrections urgentes\nrelease/*   # Pr\u00e9paration release\n</code></pre>"},{"location":"Git/FR/03-branching/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Git/FR/03-branching/#exemple-1-nouvelle-fonctionnalite","title":"Exemple 1 : Nouvelle fonctionnalit\u00e9","text":"<pre><code># Cr\u00e9er branche\ngit checkout -b feature-analyse-donnees\n\n# Travailler\necho \"def analyse():\" &gt; analyse.py\ngit add analyse.py\ngit commit -m \"Ajout fonction analyse\"\n\n# Fusionner\ngit checkout main\ngit merge feature-analyse-donnees\n</code></pre>"},{"location":"Git/FR/03-branching/#exemple-2-correction-de-bug","title":"Exemple 2 : Correction de bug","text":"<pre><code># Cr\u00e9er branche hotfix\ngit checkout -b hotfix-bug-calcul\n\n# Corriger\n# ... modifications ...\n\n# Commiter\ngit add .\ngit commit -m \"Correction bug calcul\"\n\n# Fusionner rapidement\ngit checkout main\ngit merge hotfix-bug-calcul\n</code></pre>"},{"location":"Git/FR/03-branching/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Branches : Lignes de d\u00e9veloppement isol\u00e9es</li> <li>git branch : Cr\u00e9er/g\u00e9rer branches</li> <li>git merge : Fusionner branches</li> <li>Conflits : R\u00e9soudre manuellement</li> <li>Workflow : Une branche par fonctionnalit\u00e9</li> </ol>"},{"location":"Git/FR/03-branching/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 4. D\u00e9p\u00f4ts distants pour travailler avec GitHub/GitLab.</p>"},{"location":"Git/FR/04-remote-repositories/","title":"4. D\u00e9p\u00f4ts distants Git","text":""},{"location":"Git/FR/04-remote-repositories/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les d\u00e9p\u00f4ts distants</li> <li>Travailler avec GitHub/GitLab</li> <li>Cloner des d\u00e9p\u00f4ts</li> <li>Push et Pull</li> <li>Synchronisation</li> </ul>"},{"location":"Git/FR/04-remote-repositories/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction aux d\u00e9p\u00f4ts distants</li> <li>GitHub et GitLab</li> <li>Cloner un d\u00e9p\u00f4t</li> <li>Push et Pull</li> <li>Synchronisation</li> </ol>"},{"location":"Git/FR/04-remote-repositories/#introduction-aux-depots-distants","title":"Introduction aux d\u00e9p\u00f4ts distants","text":""},{"location":"Git/FR/04-remote-repositories/#quest-ce-quun-depot-distant","title":"Qu'est-ce qu'un d\u00e9p\u00f4t distant ?","text":"<p>D\u00e9p\u00f4t distant = Copie du d\u00e9p\u00f4t sur un serveur</p> <ul> <li>GitHub : Service populaire</li> <li>GitLab : Alternative open-source</li> <li>Bitbucket : Autre option</li> <li>Backup : Sauvegarde en ligne</li> </ul>"},{"location":"Git/FR/04-remote-repositories/#pourquoi-un-depot-distant","title":"Pourquoi un d\u00e9p\u00f4t distant ?","text":"<ul> <li>Backup : Sauvegarde automatique</li> <li>Collaboration : Travailler en \u00e9quipe</li> <li>Portfolio : Pr\u00e9senter vos projets</li> <li>CI/CD : Int\u00e9gration continue</li> </ul>"},{"location":"Git/FR/04-remote-repositories/#github-et-gitlab","title":"GitHub et GitLab","text":""},{"location":"Git/FR/04-remote-repositories/#creer-un-compte-github","title":"Cr\u00e9er un compte GitHub","text":"<ol> <li>Aller sur : https://github.com</li> <li>Cliquer sur \"Sign up\"</li> <li>Remplir le formulaire</li> <li>V\u00e9rifier l'email</li> </ol>"},{"location":"Git/FR/04-remote-repositories/#creer-un-depot-github","title":"Cr\u00e9er un d\u00e9p\u00f4t GitHub","text":"<ol> <li>Cliquer sur \"New repository\"</li> <li>Nommer le d\u00e9p\u00f4t</li> <li>Choisir public/private</li> <li>Ne pas initialiser avec README (si d\u00e9p\u00f4t local existe)</li> <li>Cliquer sur \"Create repository\"</li> </ol>"},{"location":"Git/FR/04-remote-repositories/#creer-un-compte-gitlab","title":"Cr\u00e9er un compte GitLab","text":"<ol> <li>Aller sur : https://gitlab.com</li> <li>Cliquer sur \"Register\"</li> <li>Remplir le formulaire</li> <li>V\u00e9rifier l'email</li> </ol>"},{"location":"Git/FR/04-remote-repositories/#cloner-un-depot","title":"Cloner un d\u00e9p\u00f4t","text":""},{"location":"Git/FR/04-remote-repositories/#cloner-depuis-github","title":"Cloner depuis GitHub","text":"<pre><code># Cloner avec HTTPS\ngit clone https://github.com/username/repo.git\n\n# Cloner avec SSH\ngit clone git@github.com:username/repo.git\n\n# Cloner dans un dossier sp\u00e9cifique\ngit clone https://github.com/username/repo.git mon-dossier\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#cloner-depuis-gitlab","title":"Cloner depuis GitLab","text":"<pre><code># Cloner avec HTTPS\ngit clone https://gitlab.com/username/repo.git\n\n# Cloner avec SSH\ngit clone git@gitlab.com:username/repo.git\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#push-et-pull","title":"Push et Pull","text":""},{"location":"Git/FR/04-remote-repositories/#ajouter-un-remote","title":"Ajouter un remote","text":"<pre><code># Ajouter un remote\ngit remote add origin https://github.com/username/repo.git\n\n# Voir les remotes\ngit remote -v\n\n# Renommer un remote\ngit remote rename origin upstream\n\n# Supprimer un remote\ngit remote remove origin\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#push-envoyer","title":"Push (envoyer)","text":"<pre><code># Premier push\ngit push -u origin main\n\n# Pushes suivants\ngit push\n\n# Push une branche sp\u00e9cifique\ngit push origin feature-branche\n\n# Force push (ATTENTION)\ngit push --force\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#pull-recuperer","title":"Pull (r\u00e9cup\u00e9rer)","text":"<pre><code># R\u00e9cup\u00e9rer et fusionner\ngit pull\n\n# R\u00e9cup\u00e9rer seulement\ngit fetch\n\n# Fusionner apr\u00e8s fetch\ngit merge origin/main\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#synchronisation","title":"Synchronisation","text":""},{"location":"Git/FR/04-remote-repositories/#workflow-de-base","title":"Workflow de base","text":"<pre><code># 1. R\u00e9cup\u00e9rer les derni\u00e8res modifications\ngit pull\n\n# 2. Travailler localement\n# ... modifications ...\n\n# 3. Ajouter et commiter\ngit add .\ngit commit -m \"Modifications\"\n\n# 4. Envoyer\ngit push\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#synchroniser-une-branche","title":"Synchroniser une branche","text":"<pre><code># Cr\u00e9er une branche locale\ngit checkout -b feature-branche\n\n# Pousser la branche\ngit push -u origin feature-branche\n\n# R\u00e9cup\u00e9rer une branche distante\ngit fetch origin\ngit checkout -b feature-branche origin/feature-branche\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#mettre-a-jour-main","title":"Mettre \u00e0 jour main","text":"<pre><code># R\u00e9cup\u00e9rer les modifications\ngit fetch origin\n\n# Fusionner\ngit merge origin/main\n\n# Ou en une commande\ngit pull origin main\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Git/FR/04-remote-repositories/#exemple-1-premier-push","title":"Exemple 1 : Premier push","text":"<pre><code># Cr\u00e9er un d\u00e9p\u00f4t local\nmkdir mon-projet\ncd mon-projet\ngit init\n\n# Cr\u00e9er des fichiers\necho \"# Mon Projet\" &gt; README.md\ngit add README.md\ngit commit -m \"Initial commit\"\n\n# Ajouter le remote\ngit remote add origin https://github.com/username/repo.git\n\n# Pousser\ngit push -u origin main\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#exemple-2-cloner-et-modifier","title":"Exemple 2 : Cloner et modifier","text":"<pre><code># Cloner un d\u00e9p\u00f4t\ngit clone https://github.com/username/repo.git\ncd repo\n\n# Cr\u00e9er une branche\ngit checkout -b ma-modification\n\n# Modifier\necho \"Nouvelle ligne\" &gt;&gt; README.md\ngit add README.md\ngit commit -m \"Ajout ligne\"\n\n# Pousser\ngit push -u origin ma-modification\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#authentification","title":"Authentification","text":""},{"location":"Git/FR/04-remote-repositories/#https","title":"HTTPS","text":"<pre><code># Premi\u00e8re fois : demandera credentials\ngit push\n\n# Stocker les credentials\ngit config --global credential.helper store\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#ssh","title":"SSH","text":"<p>G\u00e9n\u00e9rer une cl\u00e9 SSH :</p> <pre><code># G\u00e9n\u00e9rer une cl\u00e9\nssh-keygen -t ed25519 -C \"votre.email@example.com\"\n\n# Copier la cl\u00e9 publique\ncat ~/.ssh/id_ed25519.pub\n\n# Ajouter sur GitHub/GitLab\n# Settings &gt; SSH Keys &gt; New SSH Key\n</code></pre>"},{"location":"Git/FR/04-remote-repositories/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Remote : D\u00e9p\u00f4t sur serveur</li> <li>git clone : Copier un d\u00e9p\u00f4t</li> <li>git push : Envoyer les modifications</li> <li>git pull : R\u00e9cup\u00e9rer les modifications</li> <li>Synchronisation : Pull avant Push</li> </ol>"},{"location":"Git/FR/04-remote-repositories/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 5. Collaboration pour apprendre \u00e0 collaborer.</p>"},{"location":"Git/FR/05-collaboration/","title":"5. Collaboration avec Git","text":""},{"location":"Git/FR/05-collaboration/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Fork et Pull Requests</li> <li>Issues et Projects</li> <li>Code Review</li> <li>Workflow en \u00e9quipe</li> <li>Bonnes pratiques</li> </ul>"},{"location":"Git/FR/05-collaboration/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Fork</li> <li>Pull Requests</li> <li>Issues</li> <li>Code Review</li> <li>Workflow en \u00e9quipe</li> </ol>"},{"location":"Git/FR/05-collaboration/#fork","title":"Fork","text":""},{"location":"Git/FR/05-collaboration/#quest-ce-quun-fork","title":"Qu'est-ce qu'un Fork ?","text":"<p>Fork = Copie d'un d\u00e9p\u00f4t dans votre compte</p> <ul> <li>Copie compl\u00e8te : Tous les fichiers et historique</li> <li>Ind\u00e9pendant : Modifications sans affecter l'original</li> <li>Contribution : Proposer des modifications via PR</li> </ul>"},{"location":"Git/FR/05-collaboration/#forker-un-depot","title":"Forker un d\u00e9p\u00f4t","text":"<p>Sur GitHub :</p> <ol> <li>Aller sur le d\u00e9p\u00f4t</li> <li>Cliquer sur \"Fork\"</li> <li>Choisir votre compte</li> <li>Le d\u00e9p\u00f4t est copi\u00e9</li> </ol> <p>Cloner votre fork :</p> <pre><code># Cloner votre fork\ngit clone https://github.com/votre-username/repo.git\n\n# Ajouter l'original comme upstream\ngit remote add upstream https://github.com/original-owner/repo.git\n\n# Voir les remotes\ngit remote -v\n</code></pre>"},{"location":"Git/FR/05-collaboration/#synchroniser-avec-loriginal","title":"Synchroniser avec l'original","text":"<pre><code># R\u00e9cup\u00e9rer les modifications de l'original\ngit fetch upstream\n\n# Fusionner dans votre branche\ngit checkout main\ngit merge upstream/main\n\n# Pousser vers votre fork\ngit push origin main\n</code></pre>"},{"location":"Git/FR/05-collaboration/#pull-requests","title":"Pull Requests","text":""},{"location":"Git/FR/05-collaboration/#creer-une-pull-request","title":"Cr\u00e9er une Pull Request","text":"<p>\u00c9tape 1 : Cr\u00e9er une branche</p> <pre><code># Dans votre fork\ngit checkout -b feature-ma-contribution\n</code></pre> <p>\u00c9tape 2 : Faire des modifications</p> <pre><code># Modifier les fichiers\n# ... modifications ...\n\n# Commiter\ngit add .\ngit commit -m \"Ajout nouvelle fonctionnalit\u00e9\"\n</code></pre> <p>\u00c9tape 3 : Pousser la branche</p> <pre><code># Pousser vers votre fork\ngit push -u origin feature-ma-contribution\n</code></pre> <p>\u00c9tape 4 : Cr\u00e9er la PR sur GitHub</p> <ol> <li>Aller sur votre fork</li> <li>Cliquer sur \"Compare &amp; pull request\"</li> <li>Remplir le formulaire</li> <li>Cliquer sur \"Create pull request\"</li> </ol>"},{"location":"Git/FR/05-collaboration/#bonnes-pratiques-pour-pr","title":"Bonnes pratiques pour PR","text":"<p>Titre clair : <pre><code>feat: Ajout fonction analyse de donn\u00e9es\nfix: Correction bug calcul\n</code></pre></p> <p>Description d\u00e9taill\u00e9e : - Ce qui a \u00e9t\u00e9 fait - Pourquoi - Comment tester - Screenshots si applicable</p>"},{"location":"Git/FR/05-collaboration/#issues","title":"Issues","text":""},{"location":"Git/FR/05-collaboration/#creer-une-issue","title":"Cr\u00e9er une Issue","text":"<p>Sur GitHub :</p> <ol> <li>Aller sur le d\u00e9p\u00f4t</li> <li>Cliquer sur \"Issues\"</li> <li>Cliquer sur \"New Issue\"</li> <li>Remplir le formulaire</li> </ol>"},{"location":"Git/FR/05-collaboration/#types-dissues","title":"Types d'Issues","text":"<p>Bug Report : - Description du bug - \u00c9tapes pour reproduire - Comportement attendu - Environnement</p> <p>Feature Request : - Description de la fonctionnalit\u00e9 - Cas d'usage - Avantages</p> <p>Question : - Question claire - Contexte</p>"},{"location":"Git/FR/05-collaboration/#labels-et-milestones","title":"Labels et Milestones","text":"<p>Labels : - <code>bug</code> : Bug \u00e0 corriger - <code>enhancement</code> : Am\u00e9lioration - <code>documentation</code> : Documentation - <code>good first issue</code> : Pour d\u00e9butants</p> <p>Milestones : - Regrouper les issues - Suivre la progression</p>"},{"location":"Git/FR/05-collaboration/#code-review","title":"Code Review","text":""},{"location":"Git/FR/05-collaboration/#processus-de-review","title":"Processus de Review","text":"<ol> <li>Cr\u00e9er la PR : Avec description claire</li> <li>Attendre la review : Les maintainers v\u00e9rifient</li> <li>Corriger : Si demand\u00e9</li> <li>Approuver : Une fois valid\u00e9</li> <li>Fusionner : Par le maintainer</li> </ol>"},{"location":"Git/FR/05-collaboration/#repondre-aux-commentaires","title":"R\u00e9pondre aux commentaires","text":"<pre><code># Faire des modifications\n# ... modifications ...\n\n# Commiter\ngit add .\ngit commit -m \"Correction selon review\"\n\n# Pousser\ngit push\n</code></pre>"},{"location":"Git/FR/05-collaboration/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ul> <li>Code clair : Lisible et comment\u00e9</li> <li>Tests : Ajouter des tests</li> <li>Documentation : Mettre \u00e0 jour la doc</li> <li>Respecter le style : Suivre les conventions</li> </ul>"},{"location":"Git/FR/05-collaboration/#workflow-en-equipe","title":"Workflow en \u00e9quipe","text":""},{"location":"Git/FR/05-collaboration/#workflow-standard","title":"Workflow standard","text":"<pre><code># 1. R\u00e9cup\u00e9rer les derni\u00e8res modifications\ngit pull origin main\n\n# 2. Cr\u00e9er une branche\ngit checkout -b feature-nouvelle-fonction\n\n# 3. Travailler\n# ... modifications ...\n\n# 4. Commiter r\u00e9guli\u00e8rement\ngit add .\ngit commit -m \"Message clair\"\n\n# 5. Pousser la branche\ngit push -u origin feature-nouvelle-fonction\n\n# 6. Cr\u00e9er une PR\n# Sur GitHub/GitLab\n\n# 7. Apr\u00e8s fusion, nettoyer\ngit checkout main\ngit pull origin main\ngit branch -d feature-nouvelle-fonction\n</code></pre>"},{"location":"Git/FR/05-collaboration/#workflow-avec-plusieurs-contributeurs","title":"Workflow avec plusieurs contributeurs","text":"<pre><code># Synchroniser avant de travailler\ngit fetch origin\ngit merge origin/main\n\n# Travailler sur votre branche\ngit checkout -b ma-contribution\n\n# Pousser r\u00e9guli\u00e8rement\ngit push origin ma-contribution\n</code></pre>"},{"location":"Git/FR/05-collaboration/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Git/FR/05-collaboration/#exemple-1-contribuer-a-un-projet-open-source","title":"Exemple 1 : Contribuer \u00e0 un projet open-source","text":"<pre><code># 1. Forker le projet (sur GitHub)\n\n# 2. Cloner votre fork\ngit clone https://github.com/votre-username/projet.git\ncd projet\n\n# 3. Ajouter l'original\ngit remote add upstream https://github.com/original/projet.git\n\n# 4. Cr\u00e9er une branche\ngit checkout -b fix-bug-123\n\n# 5. Corriger\n# ... modifications ...\n\n# 6. Commiter et pousser\ngit add .\ngit commit -m \"fix: Correction bug #123\"\ngit push -u origin fix-bug-123\n\n# 7. Cr\u00e9er PR sur GitHub\n</code></pre>"},{"location":"Git/FR/05-collaboration/#exemple-2-travailler-en-equipe","title":"Exemple 2 : Travailler en \u00e9quipe","text":"<pre><code># R\u00e9cup\u00e9rer les modifications de l'\u00e9quipe\ngit pull origin main\n\n# Cr\u00e9er votre branche\ngit checkout -b feature-analyse\n\n# Travailler\n# ... modifications ...\n\n# Pousser\ngit push -u origin feature-analyse\n\n# Cr\u00e9er PR pour review\n</code></pre>"},{"location":"Git/FR/05-collaboration/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Fork : Copie d'un d\u00e9p\u00f4t</li> <li>Pull Request : Proposer des modifications</li> <li>Issues : Suivre les probl\u00e8mes</li> <li>Code Review : V\u00e9rification du code</li> <li>Workflow : Processus structur\u00e9</li> </ol>"},{"location":"Git/FR/05-collaboration/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 6. Fonctionnalit\u00e9s avanc\u00e9es pour approfondir.</p>"},{"location":"Git/FR/06-advanced/","title":"6. Fonctionnalit\u00e9s avanc\u00e9es Git","text":""},{"location":"Git/FR/06-advanced/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Utiliser Stash</li> <li>Comprendre Rebase</li> <li>G\u00e9rer les Tags</li> <li>Utiliser les Hooks</li> <li>Commandes avanc\u00e9es</li> </ul>"},{"location":"Git/FR/06-advanced/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Stash</li> <li>Rebase</li> <li>Tags</li> <li>Hooks</li> <li>Commandes avanc\u00e9es</li> </ol>"},{"location":"Git/FR/06-advanced/#stash","title":"Stash","text":""},{"location":"Git/FR/06-advanced/#quest-ce-que-stash","title":"Qu'est-ce que Stash ?","text":"<p>Stash = Sauvegarder temporairement des modifications</p> <ul> <li>Temporaire : Modifications non commit\u00e9es</li> <li>Rapide : Basculer de branche rapidement</li> <li>R\u00e9cup\u00e9rable : R\u00e9cup\u00e9rer plus tard</li> </ul>"},{"location":"Git/FR/06-advanced/#utiliser-stash","title":"Utiliser Stash","text":"<pre><code># Sauvegarder les modifications\ngit stash\n\n# Avec message\ngit stash save \"Message descriptif\"\n\n# Inclure les fichiers non track\u00e9s\ngit stash -u\n\n# Voir la liste\ngit stash list\n\n# Appliquer le dernier stash\ngit stash apply\n\n# Appliquer et supprimer\ngit stash pop\n\n# Appliquer un stash sp\u00e9cifique\ngit stash apply stash@{0}\n\n# Supprimer un stash\ngit stash drop stash@{0}\n\n# Supprimer tous les stashes\ngit stash clear\n</code></pre>"},{"location":"Git/FR/06-advanced/#exemple","title":"Exemple","text":"<pre><code># Travailler sur une branche\ngit checkout feature-analyse\n# ... modifications non commit\u00e9es ...\n\n# Besoin de basculer rapidement\ngit stash\n\n# Basculer sur main\ngit checkout main\n# ... faire quelque chose ...\n\n# Retourner et r\u00e9cup\u00e9rer\ngit checkout feature-analyse\ngit stash pop\n</code></pre>"},{"location":"Git/FR/06-advanced/#rebase","title":"Rebase","text":""},{"location":"Git/FR/06-advanced/#quest-ce-que-rebase","title":"Qu'est-ce que Rebase ?","text":"<p>Rebase = R\u00e9appliquer les commits sur une autre base</p> <ul> <li>Historique lin\u00e9aire : Plus propre</li> <li>R\u00e9\u00e9criture : Modifie l'historique</li> <li>Attention : Ne pas rebase sur branches partag\u00e9es</li> </ul>"},{"location":"Git/FR/06-advanced/#rebase-simple","title":"Rebase simple","text":"<pre><code># Rebase sur main\ngit checkout feature-branche\ngit rebase main\n\n# Rebase interactif\ngit rebase -i HEAD~3\n</code></pre>"},{"location":"Git/FR/06-advanced/#rebase-interactif","title":"Rebase interactif","text":"<pre><code># \u00c9diter les 3 derniers commits\ngit rebase -i HEAD~3\n\n# Options :\n# pick : Garder le commit\n# reword : Modifier le message\n# edit : Modifier le commit\n# squash : Fusionner avec le pr\u00e9c\u00e9dent\n# drop : Supprimer le commit\n</code></pre>"},{"location":"Git/FR/06-advanced/#resoudre-les-conflits-pendant-rebase","title":"R\u00e9soudre les conflits pendant rebase","text":"<pre><code># Si conflit pendant rebase\n# 1. R\u00e9soudre le conflit\n# 2. Ajouter le fichier\ngit add fichier.py\n\n# 3. Continuer le rebase\ngit rebase --continue\n\n# Ou annuler\ngit rebase --abort\n</code></pre>"},{"location":"Git/FR/06-advanced/#tags","title":"Tags","text":""},{"location":"Git/FR/06-advanced/#quest-ce-quun-tag","title":"Qu'est-ce qu'un Tag ?","text":"<p>Tag = Pointeur vers un commit sp\u00e9cifique</p> <ul> <li>Version : Marquer des versions</li> <li>Release : Points de release</li> <li>R\u00e9f\u00e9rence : R\u00e9f\u00e9rence stable</li> </ul>"},{"location":"Git/FR/06-advanced/#creer-un-tag","title":"Cr\u00e9er un Tag","text":"<pre><code># Tag l\u00e9ger\ngit tag v1.0.0\n\n# Tag annot\u00e9 (recommand\u00e9)\ngit tag -a v1.0.0 -m \"Version 1.0.0\"\n\n# Tag sur un commit sp\u00e9cifique\ngit tag -a v1.0.0 &lt;hash&gt; -m \"Message\"\n\n# Voir les tags\ngit tag\n\n# Voir les d\u00e9tails\ngit show v1.0.0\n</code></pre>"},{"location":"Git/FR/06-advanced/#pousser-les-tags","title":"Pousser les Tags","text":"<pre><code># Pousser un tag\ngit push origin v1.0.0\n\n# Pousser tous les tags\ngit push origin --tags\n</code></pre>"},{"location":"Git/FR/06-advanced/#supprimer-un-tag","title":"Supprimer un Tag","text":"<pre><code># Supprimer localement\ngit tag -d v1.0.0\n\n# Supprimer sur le remote\ngit push origin --delete v1.0.0\n</code></pre>"},{"location":"Git/FR/06-advanced/#hooks","title":"Hooks","text":""},{"location":"Git/FR/06-advanced/#quest-ce-quun-hook","title":"Qu'est-ce qu'un Hook ?","text":"<p>Hook = Script ex\u00e9cut\u00e9 \u00e0 certains \u00e9v\u00e9nements</p> <ul> <li>Automatisation : Ex\u00e9cuter des actions</li> <li>Validation : V\u00e9rifier avant commit</li> <li>Notification : Notifier apr\u00e8s push</li> </ul>"},{"location":"Git/FR/06-advanced/#hooks-disponibles","title":"Hooks disponibles","text":"<p>Pre-commit : - Avant le commit - Validation du code - Tests</p> <p>Post-commit : - Apr\u00e8s le commit - Notifications</p> <p>Pre-push : - Avant le push - Tests complets</p>"},{"location":"Git/FR/06-advanced/#exemple-de-hook","title":"Exemple de Hook","text":"<p><code>.git/hooks/pre-commit</code> :</p> <pre><code>#!/bin/bash\n# Ex\u00e9cuter les tests avant commit\npython -m pytest tests/\n\n# Si \u00e9chec, annuler le commit\nif [ $? -ne 0 ]; then\n    echo \"Tests \u00e9chou\u00e9s, commit annul\u00e9\"\n    exit 1\nfi\n</code></pre>"},{"location":"Git/FR/06-advanced/#commandes-avancees","title":"Commandes avanc\u00e9es","text":""},{"location":"Git/FR/06-advanced/#cherry-pick","title":"Cherry-pick","text":"<pre><code># Appliquer un commit sp\u00e9cifique\ngit cherry-pick &lt;hash&gt;\n\n# Appliquer plusieurs commits\ngit cherry-pick &lt;hash1&gt; &lt;hash2&gt;\n</code></pre>"},{"location":"Git/FR/06-advanced/#reflog","title":"Reflog","text":"<pre><code># Voir l'historique des actions\ngit reflog\n\n# R\u00e9cup\u00e9rer un commit perdu\ngit checkout &lt;hash&gt;\n</code></pre>"},{"location":"Git/FR/06-advanced/#bisect","title":"Bisect","text":"<pre><code># Trouver le commit qui a introduit un bug\ngit bisect start\ngit bisect bad  # Commit actuel est mauvais\ngit bisect good &lt;hash&gt;  # Commit connu bon\n\n# Git va tester des commits\n# Marquer comme good ou bad\ngit bisect good\ngit bisect bad\n\n# Terminer\ngit bisect reset\n</code></pre>"},{"location":"Git/FR/06-advanced/#submodules","title":"Submodules","text":"<pre><code># Ajouter un submodule\ngit submodule add https://github.com/user/repo.git path\n\n# Initialiser les submodules\ngit submodule init\ngit submodule update\n\n# En une commande\ngit submodule update --init --recursive\n</code></pre>"},{"location":"Git/FR/06-advanced/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Git/FR/06-advanced/#exemple-1-stash-pour-changement-urgent","title":"Exemple 1 : Stash pour changement urgent","text":"<pre><code># Travailler sur feature\ngit checkout feature-analyse\n# ... modifications ...\n\n# Bug urgent \u00e0 corriger\ngit stash\ngit checkout main\ngit checkout -b hotfix-bug\n\n# Corriger\n# ... modifications ...\ngit commit -m \"fix: Bug urgent\"\ngit checkout main\ngit merge hotfix-bug\n\n# Retourner au travail\ngit checkout feature-analyse\ngit stash pop\n</code></pre>"},{"location":"Git/FR/06-advanced/#exemple-2-tag-pour-release","title":"Exemple 2 : Tag pour release","text":"<pre><code># Pr\u00e9parer la release\ngit checkout main\ngit pull origin main\n\n# Cr\u00e9er le tag\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\n\n# Pousser\ngit push origin main\ngit push origin v1.0.0\n</code></pre>"},{"location":"Git/FR/06-advanced/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Stash : Sauvegarder temporairement</li> <li>Rebase : R\u00e9\u00e9crire l'historique</li> <li>Tags : Marquer des versions</li> <li>Hooks : Automatiser des actions</li> <li>Commandes avanc\u00e9es : Outils puissants</li> </ol>"},{"location":"Git/FR/06-advanced/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 7. Bonnes pratiques pour les meilleures pratiques.</p>"},{"location":"Git/FR/07-best-practices/","title":"7. Bonnes pratiques Git","text":""},{"location":"Git/FR/07-best-practices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Messages de commit efficaces</li> <li>Structure de projet</li> <li>.gitignore complet</li> <li>Documentation</li> <li>Workflow optimal</li> </ul>"},{"location":"Git/FR/07-best-practices/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Messages de commit</li> <li>Structure de projet</li> <li>.gitignore</li> <li>Documentation</li> <li>Workflow</li> </ol>"},{"location":"Git/FR/07-best-practices/#messages-de-commit","title":"Messages de commit","text":""},{"location":"Git/FR/07-best-practices/#format-recommande","title":"Format recommand\u00e9","text":"<pre><code>Type : Description courte (50 caract\u00e8res max)\n\nDescription d\u00e9taill\u00e9e si n\u00e9cessaire (72 caract\u00e8res par ligne)\n\n- Point 1\n- Point 2\n</code></pre>"},{"location":"Git/FR/07-best-practices/#types-de-commit","title":"Types de commit","text":"<ul> <li>feat : Nouvelle fonctionnalit\u00e9</li> <li>fix : Correction de bug</li> <li>docs : Documentation</li> <li>style : Formatage (pas de changement de code)</li> <li>refactor : Refactorisation</li> <li>test : Tests</li> <li>chore : T\u00e2ches de maintenance</li> </ul>"},{"location":"Git/FR/07-best-practices/#exemples","title":"Exemples","text":"<p>Bon : <pre><code>feat: Ajout fonction analyse de donn\u00e9es\n\nImpl\u00e9mentation d'une fonction pour analyser les donn\u00e9es CSV\navec support des colonnes multiples.\n\n- Lecture des fichiers CSV\n- Calcul des statistiques\n- Export des r\u00e9sultats\n</code></pre></p> <p>Mauvais : <pre><code>modifications\n</code></pre></p>"},{"location":"Git/FR/07-best-practices/#structure-de-projet","title":"Structure de projet","text":""},{"location":"Git/FR/07-best-practices/#structure-recommandee","title":"Structure recommand\u00e9e","text":"<pre><code>mon-projet/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_main.py\n\u251c\u2500\u2500 docs/\n\u2502   \u2514\u2500\u2500 guide.md\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 .gitkeep\n</code></pre>"},{"location":"Git/FR/07-best-practices/#readmemd","title":"README.md","text":"<p>Contenu essentiel :</p> <pre><code># Nom du Projet\n\nDescription courte du projet.\n\n## Installation\n\n```bash\npip install -r requirements.txt\n</code></pre>"},{"location":"Git/FR/07-best-practices/#usage","title":"Usage","text":"<pre><code>from src.main import fonction\nfonction()\n</code></pre>"},{"location":"Git/FR/07-best-practices/#contribution","title":"Contribution","text":"<p>Les contributions sont les bienvenues !</p>"},{"location":"Git/FR/07-best-practices/#license","title":"License","text":"<p>MIT <pre><code>---\n\n## .gitignore\n\n### .gitignore complet pour Python\n</code></pre></p>"},{"location":"Git/FR/07-best-practices/#byte-compiled-optimized-dll-files","title":"Byte-compiled / optimized / DLL files","text":"<p>pycache/ .py[cod] $py.class</p>"},{"location":"Git/FR/07-best-practices/#virtual-environments","title":"Virtual environments","text":"<p>venv/ env/ ENV/</p>"},{"location":"Git/FR/07-best-practices/#ides","title":"IDEs","text":"<p>.vscode/ .idea/ .swp .swo</p>"},{"location":"Git/FR/07-best-practices/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>.ipynb_checkpoints *.ipynb</p>"},{"location":"Git/FR/07-best-practices/#data-files","title":"Data files","text":"<p>.csv .xlsx .parquet data/ .db *.sqlite</p>"},{"location":"Git/FR/07-best-practices/#secrets","title":"Secrets","text":"<p>.env *.key config.ini secrets/</p>"},{"location":"Git/FR/07-best-practices/#os","title":"OS","text":"<p>.DS_Store Thumbs.db</p>"},{"location":"Git/FR/07-best-practices/#logs","title":"Logs","text":"<p>*.log logs/ <pre><code>### .gitignore pour Data Science\n</code></pre></p>"},{"location":"Git/FR/07-best-practices/#data","title":"Data","text":"<p>data/ .csv .xlsx .parquet .h5 *.hdf5</p>"},{"location":"Git/FR/07-best-practices/#models","title":"Models","text":"<p>models/ .pkl .joblib</p>"},{"location":"Git/FR/07-best-practices/#notebooks-optionnel","title":"Notebooks (optionnel)","text":"<p>*.ipynb</p>"},{"location":"Git/FR/07-best-practices/#results","title":"Results","text":"<p>results/ outputs/ <pre><code>---\n\n## Documentation\n\n### README.md\n\n**Sections essentielles :**\n\n1. **Titre et description**\n2. **Installation**\n3. **Usage**\n4. **Exemples**\n5. **Contribution**\n6. **License**\n\n### Documentation du code\n\n**Docstrings Python :**\n\n```python\ndef analyse_donnees(fichier):\n    \"\"\"\n    Analyse un fichier de donn\u00e9es CSV.\n\n    Args:\n        fichier (str): Chemin vers le fichier CSV\n\n    Returns:\n        dict: Dictionnaire avec les statistiques\n\n    Example:\n        &gt;&gt;&gt; stats = analyse_donnees('data.csv')\n        &gt;&gt;&gt; print(stats['moyenne'])\n    \"\"\"\n    # Code...\n</code></pre></p>"},{"location":"Git/FR/07-best-practices/#changelogmd","title":"CHANGELOG.md","text":"<pre><code># Changelog\n\n## [1.0.0] - 2024-01-15\n\n### Added\n- Fonction analyse de donn\u00e9es\n- Support CSV\n\n### Fixed\n- Bug dans le calcul de moyenne\n\n### Changed\n- Am\u00e9lioration de la documentation\n</code></pre>"},{"location":"Git/FR/07-best-practices/#workflow","title":"Workflow","text":""},{"location":"Git/FR/07-best-practices/#workflow-recommande","title":"Workflow recommand\u00e9","text":"<ol> <li>Cr\u00e9er une branche : Pour chaque fonctionnalit\u00e9</li> <li>Commiter r\u00e9guli\u00e8rement : Petits commits fr\u00e9quents</li> <li>Tester : Avant de pousser</li> <li>Pull Request : Pour review</li> <li>Fusionner : Apr\u00e8s approbation</li> </ol>"},{"location":"Git/FR/07-best-practices/#regles-dor","title":"R\u00e8gles d'or","text":"<ul> <li>Un commit = Une modification logique</li> <li>Messages clairs et descriptifs</li> <li>Tester avant de pousser</li> <li>Ne jamais force push sur main</li> <li>Synchroniser r\u00e9guli\u00e8rement</li> </ul>"},{"location":"Git/FR/07-best-practices/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"Git/FR/07-best-practices/#exemple-1-projet-python-structure","title":"Exemple 1 : Projet Python structur\u00e9","text":"<pre><code>data-analysis/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 data_loader.py\n\u2502   \u2514\u2500\u2500 analyzer.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_analyzer.py\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 guide.md\n</code></pre>"},{"location":"Git/FR/07-best-practices/#exemple-2-workflow-de-commit","title":"Exemple 2 : Workflow de commit","text":"<pre><code># 1. Cr\u00e9er une branche\ngit checkout -b feature-analyse\n\n# 2. Faire des modifications\n# ... code ...\n\n# 3. Tester\npython -m pytest tests/\n\n# 4. Commiter\ngit add src/analyzer.py\ngit commit -m \"feat: Ajout fonction analyse statistique\"\n\n# 5. Pousser\ngit push -u origin feature-analyse\n\n# 6. Cr\u00e9er PR\n</code></pre>"},{"location":"Git/FR/07-best-practices/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Messages : Clairs et structur\u00e9s</li> <li>Structure : Organis\u00e9e et logique</li> <li>.gitignore : Complet et adapt\u00e9</li> <li>Documentation : README et docstrings</li> <li>Workflow : R\u00e9gulier et coh\u00e9rent</li> </ol>"},{"location":"Git/FR/07-best-practices/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 8. Projets pratiques pour cr\u00e9er des projets complets.</p>"},{"location":"Git/FR/08-projets/","title":"8. Projets pratiques Git","text":""},{"location":"Git/FR/08-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er un portfolio GitHub</li> <li>G\u00e9rer un projet collaboratif</li> <li>Versionner des scripts Python</li> <li>Documenter un projet</li> <li>Projets pour portfolio</li> </ul>"},{"location":"Git/FR/08-projets/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Projet 1 : Portfolio GitHub</li> <li>Projet 2 : Projet collaboratif</li> <li>Projet 3 : Scripts Python versionn\u00e9s</li> <li>Projet 4 : Documentation de projet</li> </ol>"},{"location":"Git/FR/08-projets/#projet-1-portfolio-github","title":"Projet 1 : Portfolio GitHub","text":""},{"location":"Git/FR/08-projets/#objectif","title":"Objectif","text":"<p>Cr\u00e9er un portfolio professionnel sur GitHub.</p>"},{"location":"Git/FR/08-projets/#structure","title":"Structure","text":"<pre><code>portfolio/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 projects/\n\u2502   \u251c\u2500\u2500 project1/\n\u2502   \u251c\u2500\u2500 project2/\n\u2502   \u2514\u2500\u2500 project3/\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 utilities.py\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 resume.md\n</code></pre>"},{"location":"Git/FR/08-projets/#readmemd","title":"README.md","text":"<pre><code># Mon Portfolio Data Analyst\n\n## \u00c0 propos\n\nData Analyst passionn\u00e9 par l'analyse de donn\u00e9es et la visualisation.\n\n## Projets\n\n### [Projet 1 : Analyse de ventes](projects/project1/)\nAnalyse des ventes avec Python et pandas.\n\n### [Projet 2 : Dashboard PowerBI](projects/project2/)\nDashboard interactif pour la gestion.\n\n## Comp\u00e9tences\n\n- Python\n- SQL\n- PowerBI\n- Git/GitHub\n\n## Contact\n\nEmail: votre.email@example.com\nLinkedIn: linkedin.com/in/votre-profil\n</code></pre>"},{"location":"Git/FR/08-projets/#creer-le-depot","title":"Cr\u00e9er le d\u00e9p\u00f4t","text":"<pre><code># Cr\u00e9er le d\u00e9p\u00f4t local\nmkdir portfolio\ncd portfolio\ngit init\n\n# Cr\u00e9er la structure\nmkdir projects scripts docs\n\n# Cr\u00e9er README\necho \"# Mon Portfolio\" &gt; README.md\n\n# Premier commit\ngit add .\ngit commit -m \"Initial commit : portfolio\"\n\n# Cr\u00e9er sur GitHub et pousser\ngit remote add origin https://github.com/username/portfolio.git\ngit push -u origin main\n</code></pre>"},{"location":"Git/FR/08-projets/#projet-2-projet-collaboratif","title":"Projet 2 : Projet collaboratif","text":""},{"location":"Git/FR/08-projets/#objectif_1","title":"Objectif","text":"<p>G\u00e9rer un projet avec plusieurs contributeurs.</p>"},{"location":"Git/FR/08-projets/#workflow","title":"Workflow","text":"<pre><code># 1. Cloner le d\u00e9p\u00f4t\ngit clone https://github.com/team/projet.git\ncd projet\n\n# 2. Cr\u00e9er une branche\ngit checkout -b feature-ma-contribution\n\n# 3. Travailler\n# ... modifications ...\n\n# 4. Commiter\ngit add .\ngit commit -m \"feat: Ajout nouvelle fonctionnalit\u00e9\"\n\n# 5. Synchroniser avec main\ngit fetch origin\ngit rebase origin/main\n\n# 6. Pousser\ngit push -u origin feature-ma-contribution\n\n# 7. Cr\u00e9er Pull Request sur GitHub\n</code></pre>"},{"location":"Git/FR/08-projets/#gestion-des-conflits","title":"Gestion des conflits","text":"<pre><code># Si conflit apr\u00e8s rebase\n# 1. R\u00e9soudre le conflit\n# 2. Ajouter les fichiers\ngit add .\n\n# 3. Continuer le rebase\ngit rebase --continue\n\n# 4. Pousser (force n\u00e9cessaire apr\u00e8s rebase)\ngit push --force-with-lease\n</code></pre>"},{"location":"Git/FR/08-projets/#projet-3-scripts-python-versionnes","title":"Projet 3 : Scripts Python versionn\u00e9s","text":""},{"location":"Git/FR/08-projets/#objectif_2","title":"Objectif","text":"<p>Versionner des scripts Python pour l'analyse de donn\u00e9es.</p>"},{"location":"Git/FR/08-projets/#structure_1","title":"Structure","text":"<pre><code>data-scripts/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 data_loader.py\n\u2502   \u251c\u2500\u2500 analyzer.py\n\u2502   \u2514\u2500\u2500 visualizer.py\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 analysis.ipynb\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 .gitkeep\n</code></pre>"},{"location":"Git/FR/08-projets/#gitignore","title":".gitignore","text":"<pre><code>__pycache__/\n*.pyc\nvenv/\n.env\n*.csv\n*.xlsx\ndata/\n.ipynb_checkpoints\n</code></pre>"},{"location":"Git/FR/08-projets/#workflow_1","title":"Workflow","text":"<pre><code># Initialiser\ngit init\ngit add .\ngit commit -m \"Initial commit : scripts d'analyse\"\n\n# Cr\u00e9er une branche pour nouvelle fonctionnalit\u00e9\ngit checkout -b feature-nouvelle-analyse\n\n# D\u00e9velopper\n# ... code ...\n\n# Commiter\ngit add src/analyzer.py\ngit commit -m \"feat: Ajout analyse statistique avanc\u00e9e\"\n\n# Fusionner\ngit checkout main\ngit merge feature-nouvelle-analyse\n\n# Taguer une version\ngit tag -a v1.0.0 -m \"Version 1.0.0\"\ngit push origin main --tags\n</code></pre>"},{"location":"Git/FR/08-projets/#projet-4-documentation-de-projet","title":"Projet 4 : Documentation de projet","text":""},{"location":"Git/FR/08-projets/#objectif_3","title":"Objectif","text":"<p>Cr\u00e9er une documentation compl\u00e8te versionn\u00e9e.</p>"},{"location":"Git/FR/08-projets/#structure_2","title":"Structure","text":"<pre><code>project-docs/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u251c\u2500\u2500 usage.md\n\u2502   \u251c\u2500\u2500 api.md\n\u2502   \u2514\u2500\u2500 examples.md\n\u2514\u2500\u2500 CHANGELOG.md\n</code></pre>"},{"location":"Git/FR/08-projets/#readmemd-complet","title":"README.md complet","text":"<pre><code># Nom du Projet\n\nDescription d\u00e9taill\u00e9e du projet.\n\n## Table des mati\u00e8res\n\n- [Installation](#installation)\n- [Usage](#usage)\n- [Documentation](#documentation)\n- [Contribution](#contribution)\n\n## Installation\n\n\\`\\`\\`bash\npip install -r requirements.txt\n\\`\\`\\`\n\n## Usage\n\n\\`\\`\\`python\nfrom project import fonction\nresultat = fonction()\n\\`\\`\\`\n\n## Documentation\n\nVoir [docs/](docs/) pour la documentation compl\u00e8te.\n\n## Contribution\n\nLes contributions sont les bienvenues !\n\n## License\n\nMIT\n</code></pre>"},{"location":"Git/FR/08-projets/#workflow-de-documentation","title":"Workflow de documentation","text":"<pre><code># Cr\u00e9er une branche pour documentation\ngit checkout -b docs/ajout-guide-usage\n\n# Ajouter la documentation\n# ... \u00e9crire docs/usage.md ...\n\n# Commiter\ngit add docs/usage.md\ngit commit -m \"docs: Ajout guide d'utilisation\"\n\n# Pousser et cr\u00e9er PR\ngit push -u origin docs/ajout-guide-usage\n</code></pre>"},{"location":"Git/FR/08-projets/#exemples-de-projets-portfolio","title":"Exemples de projets portfolio","text":""},{"location":"Git/FR/08-projets/#projet-data-analysis","title":"Projet Data Analysis","text":"<pre><code># Structure\ndata-analysis-project/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 .gitkeep\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 analysis.ipynb\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 analysis.py\n\u2514\u2500\u2500 results/\n    \u2514\u2500\u2500 .gitkeep\n</code></pre>"},{"location":"Git/FR/08-projets/#projet-etl-pipeline","title":"Projet ETL Pipeline","text":"<pre><code># Structure\netl-pipeline/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 extract.py\n\u2502   \u251c\u2500\u2500 transform.py\n\u2502   \u2514\u2500\u2500 load.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_pipeline.py\n\u2514\u2500\u2500 config/\n    \u2514\u2500\u2500 config.yaml.example\n</code></pre>"},{"location":"Git/FR/08-projets/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Portfolio : Pr\u00e9senter vos projets</li> <li>Collaboration : Workflow structur\u00e9</li> <li>Versioning : G\u00e9rer les versions</li> <li>Documentation : Essentielle</li> <li>GitHub : Plateforme professionnelle</li> </ol>"},{"location":"Git/FR/08-projets/#ressources","title":"\ud83d\udd17 Ressources","text":"<ul> <li>GitHub Guides</li> <li>Git Documentation</li> <li>GitHub Student Pack</li> </ul> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation Git. Vous pouvez maintenant g\u00e9rer vos projets efficacement avec Git et GitHub.</p>"},{"location":"Git/PL/","title":"Szkolenie Git dla Data Analyst","text":""},{"location":"Git/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>To szkolenie poprowadzi Ci\u0119 przez nauk\u0119 Git jako Data Analyst. Git to rozproszony system kontroli wersji niezb\u0119dny do zarz\u0105dzania kodem, skryptami i dokumentacj\u0105 projekt\u00f3w.</p>"},{"location":"Git/PL/#cele-szkoleniowe","title":"\ud83c\udfaf Cele szkoleniowe","text":"<ul> <li>Zrozumie\u0107 Git i kontrol\u0119 wersji</li> <li>Zainstalowa\u0107 Git</li> <li>Opanowa\u0107 podstawowe polecenia</li> <li>Zarz\u0105dza\u0107 ga\u0142\u0119ziami</li> <li>Pracowa\u0107 z zdalnymi repozytoriami (GitHub, GitLab)</li> <li>Wsp\u00f3\u0142pracowa\u0107 nad projektami</li> <li>U\u017cywa\u0107 Git w przep\u0142ywach danych</li> <li>Tworzy\u0107 praktyczne projekty do portfolio</li> </ul>"},{"location":"Git/PL/#wszystko-jest-darmowe","title":"\ud83d\udcb0 Wszystko jest darmowe!","text":"<p>To szkolenie u\u017cywa tylko: - \u2705 Git : Open-source i darmowy - \u2705 GitHub : Darmowe konto (nieograniczone) - \u2705 GitLab : Darmowe konto - \u2705 Oficjalna dokumentacja : Kompletne darmowe przewodniki - \u2705 Tutoriale online : Darmowe zasoby</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"Git/PL/#struktura-szkolenia","title":"\ud83d\udcd6 Struktura szkolenia","text":""},{"location":"Git/PL/#1-rozpoczecie-z-git","title":"1. Rozpocz\u0119cie z Git","text":"<ul> <li>Zainstalowa\u0107 Git</li> <li>Konfiguracja pocz\u0105tkowa</li> <li>Pierwsze repozytoria</li> <li>Podstawowe koncepcje</li> </ul>"},{"location":"Git/PL/#2-podstawowe-polecenia","title":"2. Podstawowe polecenia","text":"<ul> <li>Utworzy\u0107 repozytorium</li> <li>Dodawa\u0107 i commitowa\u0107</li> <li>Widzie\u0107 histori\u0119</li> <li>Cofa\u0107 zmiany</li> </ul>"},{"location":"Git/PL/#3-gaezie","title":"3. Ga\u0142\u0119zie","text":"<ul> <li>Tworzy\u0107 i zarz\u0105dza\u0107 ga\u0142\u0119ziami</li> <li>\u0141\u0105czy\u0107 ga\u0142\u0119zie</li> <li>Rozwi\u0105zywa\u0107 konflikty</li> <li>Workflow z ga\u0142\u0119ziami</li> </ul>"},{"location":"Git/PL/#4-zdalne-repozytoria","title":"4. Zdalne repozytoria","text":"<ul> <li>GitHub i GitLab</li> <li>Klonowa\u0107 repozytorium</li> <li>Push i Pull</li> <li>Synchronizacja</li> </ul>"},{"location":"Git/PL/#5-wspopraca","title":"5. Wsp\u00f3\u0142praca","text":"<ul> <li>Fork i Pull Requests</li> <li>Issues i Projects</li> <li>Code Review</li> <li>Workflow w zespole</li> </ul>"},{"location":"Git/PL/#6-funkcje-zaawansowane","title":"6. Funkcje zaawansowane","text":"<ul> <li>Stash</li> <li>Rebase</li> <li>Tagi</li> <li>Hooki</li> </ul>"},{"location":"Git/PL/#7-dobre-praktyki","title":"7. Dobre praktyki","text":"<ul> <li>Wiadomo\u015bci commit</li> <li>Struktura projektu</li> <li>.gitignore</li> <li>Dokumentacja</li> </ul>"},{"location":"Git/PL/#8-projekty-praktyczne","title":"8. Projekty praktyczne","text":"<ul> <li>Portfolio GitHub</li> <li>Projekt wsp\u00f3\u0142pracuj\u0105cy</li> <li>Zarz\u0105dzanie skryptami Python</li> <li>Dokumentacja projektu</li> </ul>"},{"location":"Git/PL/#szybki-start","title":"\ud83d\ude80 Szybki start","text":""},{"location":"Git/PL/#wymagania-wstepne","title":"Wymagania wst\u0119pne","text":"<ul> <li>System operacyjny : Windows, Linux lub macOS</li> <li>Po\u0142\u0105czenie internetowe : Dla GitHub/GitLab</li> <li>Edytor tekstu : VS Code, Sublime, itp.</li> </ul>"},{"location":"Git/PL/#szybka-instalacja","title":"Szybka instalacja","text":"<p>Windows: 1. Pobra\u0107 Git: https://git-scm.com/download/win 2. Zainstalowa\u0107 z opcjami domy\u015blnymi 3. Sprawdzi\u0107: <code>git --version</code></p> <p>Linux: <pre><code># Ubuntu/Debian\nsudo apt install git\n\n# CentOS/RHEL\nsudo yum install git\n\n# Sprawdzi\u0107\ngit --version\n</code></pre></p> <p>macOS: <pre><code># Z Homebrew\nbrew install git\n\n# Sprawdzi\u0107\ngit --version\n</code></pre></p>"},{"location":"Git/PL/#konfiguracja-poczatkowa","title":"Konfiguracja pocz\u0105tkowa","text":"<pre><code># Skonfigurowa\u0107 imi\u0119\ngit config --global user.name \"Twoje Imi\u0119\"\n\n# Skonfigurowa\u0107 email\ngit config --global user.email \"twoj.email@example.com\"\n\n# Sprawdzi\u0107 konfiguracj\u0119\ngit config --list\n</code></pre>"},{"location":"Git/PL/#pierwsze-repozytorium","title":"Pierwsze repozytorium","text":"<pre><code># Utworzy\u0107 nowe repozytorium\nmkdir moj-projekt\ncd moj-projekt\ngit init\n\n# Utworzy\u0107 plik\necho \"# M\u00f3j Projekt\" &gt; README.md\n\n# Doda\u0107 i commitowa\u0107\ngit add README.md\ngit commit -m \"Pierwszy commit\"\n</code></pre>"},{"location":"Git/PL/#przypadki-uzycia-dla-data-analyst","title":"\ud83d\udcca Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Wersjonowanie : Zarz\u0105dza\u0107 wersjami skrypt\u00f3w Python/R</li> <li>Wsp\u00f3\u0142praca : Pracowa\u0107 w zespole nad projektami</li> <li>Portfolio : Prezentowa\u0107 projekty na GitHub</li> <li>Dokumentacja : Wersjonowa\u0107 dokumentacj\u0119</li> <li>Backup : Tworzy\u0107 kopi\u0119 zapasow\u0105 kodu online</li> </ul>"},{"location":"Git/PL/#darmowe-zasoby","title":"\ud83d\udcda Darmowe zasoby","text":""},{"location":"Git/PL/#oficjalna-dokumentacja","title":"Oficjalna dokumentacja","text":"<ul> <li>Dokumentacja Git : https://git-scm.com/doc</li> <li>Przewodniki GitHub : https://guides.github.com/</li> <li>Dokumentacja GitLab : https://docs.gitlab.com/</li> </ul>"},{"location":"Git/PL/01-getting-started/","title":"1. Rozpocz\u0119cie z Git","text":""},{"location":"Git/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Git i kontrol\u0119 wersji</li> <li>Zainstalowa\u0107 Git</li> <li>Skonfigurowa\u0107 Git</li> <li>Utworzy\u0107 pierwsze repozytorium</li> <li>Zrozumie\u0107 podstawowe koncepcje</li> </ul>"},{"location":"Git/PL/01-getting-started/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Git</li> <li>Instalacja</li> <li>Konfiguracja</li> <li>Pierwsze repozytorium</li> <li>Podstawowe koncepcje</li> </ol>"},{"location":"Git/PL/01-getting-started/#wprowadzenie-do-git","title":"Wprowadzenie do Git","text":""},{"location":"Git/PL/01-getting-started/#czym-jest-git","title":"Czym jest Git?","text":"<p>Git = Rozproszony system kontroli wersji</p> <ul> <li>Wersjonowanie : \u015aledzi zmiany plik\u00f3w</li> <li>Rozproszony : Ka\u017cdy developer ma kompletn\u0105 kopi\u0119</li> <li>Wsp\u00f3\u0142praca : U\u0142atwia prac\u0119 w zespole</li> <li>Historia : Zachowuje kompletn\u0105 histori\u0119</li> </ul>"},{"location":"Git/PL/01-getting-started/#dlaczego-git-dla-data-analyst","title":"Dlaczego Git dla Data Analyst?","text":"<ul> <li>Skrypty : Wersjonowa\u0107 skrypty Python/R</li> <li>Projekty : Zarz\u0105dza\u0107 projektami portfolio</li> <li>Wsp\u00f3\u0142praca : Pracowa\u0107 w zespo\u0142ach</li> <li>Backup : Tworzy\u0107 kopi\u0119 zapasow\u0105 online (GitHub)</li> <li>Dokumentacja : Wersjonowa\u0107 dokumentacj\u0119</li> </ul>"},{"location":"Git/PL/01-getting-started/#instalacja","title":"Instalacja","text":""},{"location":"Git/PL/01-getting-started/#windows","title":"Windows","text":"<ol> <li>Przej\u015b\u0107 do: https://git-scm.com/download/win</li> <li>Pobra\u0107 instalator</li> <li>Uruchomi\u0107 instalator</li> <li>Zaakceptowa\u0107 opcje domy\u015blne</li> </ol>"},{"location":"Git/PL/01-getting-started/#linux","title":"Linux","text":"<p>Ubuntu/Debian: <pre><code>sudo apt update\nsudo apt install git\ngit --version\n</code></pre></p>"},{"location":"Git/PL/01-getting-started/#macos","title":"macOS","text":"<p>Z Homebrew: <pre><code>brew install git\ngit --version\n</code></pre></p>"},{"location":"Git/PL/01-getting-started/#konfiguracja","title":"Konfiguracja","text":""},{"location":"Git/PL/01-getting-started/#konfiguracja-globalna","title":"Konfiguracja globalna","text":"<pre><code># Skonfigurowa\u0107 imi\u0119\ngit config --global user.name \"Twoje Imi\u0119\"\n\n# Skonfigurowa\u0107 email\ngit config --global user.email \"twoj.email@example.com\"\n\n# Skonfigurowa\u0107 edytor domy\u015blny\ngit config --global core.editor \"code --wait\"  # VS Code\n</code></pre>"},{"location":"Git/PL/01-getting-started/#sprawdzic-konfiguracje","title":"Sprawdzi\u0107 konfiguracj\u0119","text":"<pre><code># Zobaczy\u0107 ca\u0142\u0105 konfiguracj\u0119\ngit config --list\n\n# Zobaczy\u0107 konkretn\u0105 konfiguracj\u0119\ngit config user.name\n</code></pre>"},{"location":"Git/PL/01-getting-started/#pierwsze-repozytorium","title":"Pierwsze repozytorium","text":""},{"location":"Git/PL/01-getting-started/#utworzyc-nowe-repozytorium","title":"Utworzy\u0107 nowe repozytorium","text":"<pre><code># Utworzy\u0107 katalog\nmkdir moj-projekt\ncd moj-projekt\n\n# Zainicjalizowa\u0107 Git\ngit init\n\n# Sprawdzi\u0107\nls -la  # Zobaczy\u0107 folder .git\n</code></pre>"},{"location":"Git/PL/01-getting-started/#pierwszy-commit","title":"Pierwszy commit","text":"<pre><code># Utworzy\u0107 plik\necho \"# M\u00f3j Projekt\" &gt; README.md\n\n# Zobaczy\u0107 status\ngit status\n\n# Doda\u0107 plik\ngit add README.md\n\n# Commitowa\u0107\ngit commit -m \"Pierwszy commit: dodanie README\"\n</code></pre>"},{"location":"Git/PL/01-getting-started/#podstawowe-koncepcje","title":"Podstawowe koncepcje","text":""},{"location":"Git/PL/01-getting-started/#repozytorium","title":"Repozytorium","text":"<p>Repozytorium = Folder z histori\u0105 Git</p> <ul> <li>Lokalne : Na Twojej maszynie</li> <li>Zdalne : Na GitHub/GitLab</li> <li>.git : Ukryty folder zawieraj\u0105cy histori\u0119</li> </ul>"},{"location":"Git/PL/01-getting-started/#commit","title":"Commit","text":"<p>Commit = Punkt w historii</p> <ul> <li>Snapshot : Zapis stanu plik\u00f3w</li> <li>Wiadomo\u015b\u0107 : Opis zmian</li> <li>Autor : Imi\u0119 i email</li> <li>Hash : Unikalny identyfikator (SHA-1)</li> </ul>"},{"location":"Git/PL/01-getting-started/#gaaz","title":"Ga\u0142\u0105\u017a","text":"<p>Ga\u0142\u0105\u017a = Linia rozwoju</p> <ul> <li>main/master : G\u0142\u00f3wna ga\u0142\u0105\u017a</li> <li>Inne ga\u0142\u0119zie : Dla nowych funkcji</li> <li>Izolacja : Praca izolowana</li> </ul>"},{"location":"Git/PL/01-getting-started/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Git = Rozproszona kontrola wersji</li> <li>Repozytorium = Folder z histori\u0105</li> <li>Commit = Punkt w historii</li> <li>Ga\u0142\u0105\u017a = Linia rozwoju</li> <li>Staging = Obszar przygotowania</li> </ol>"},{"location":"Git/PL/01-getting-started/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 2. Podstawowe polecenia, aby opanowa\u0107 podstawowe polecenia.</p>"},{"location":"Git/PL/02-basic-commands/","title":"2. Podstawowe polecenia Git","text":""},{"location":"Git/PL/02-basic-commands/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 i zarz\u0105dza\u0107 repozytorium</li> <li>Dodawa\u0107 i commitowa\u0107 pliki</li> <li>Widzie\u0107 histori\u0119</li> <li>Cofa\u0107 zmiany</li> <li>Ignorowa\u0107 pliki</li> </ul>"},{"location":"Git/PL/02-basic-commands/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Utworzy\u0107 repozytorium</li> <li>Dodawa\u0107 i commitowa\u0107</li> <li>Widzie\u0107 histori\u0119</li> <li>Cofa\u0107 zmiany</li> <li>.gitignore</li> </ol>"},{"location":"Git/PL/02-basic-commands/#utworzyc-repozytorium","title":"Utworzy\u0107 repozytorium","text":""},{"location":"Git/PL/02-basic-commands/#zainicjalizowac-lokalne-repozytorium","title":"Zainicjalizowa\u0107 lokalne repozytorium","text":"<pre><code># Utworzy\u0107 nowe repozytorium\nmkdir moj-projekt\ncd moj-projekt\ngit init\n\n# Sprawdzi\u0107\nls -la  # Zobaczy\u0107 .git\n</code></pre>"},{"location":"Git/PL/02-basic-commands/#klonowac-istniejace-repozytorium","title":"Klonowa\u0107 istniej\u0105ce repozytorium","text":"<pre><code># Klonowa\u0107 z GitHub\ngit clone https://github.com/username/repo.git\n\n# Klonowa\u0107 do konkretnego folderu\ngit clone https://github.com/username/repo.git moj-folder\n\n# Klonowa\u0107 z SSH\ngit clone git@github.com:username/repo.git\n</code></pre>"},{"location":"Git/PL/02-basic-commands/#dodawac-i-commitowac","title":"Dodawa\u0107 i commitowa\u0107","text":""},{"location":"Git/PL/02-basic-commands/#podstawowy-workflow","title":"Podstawowy workflow","text":"<pre><code># 1. Zobaczy\u0107 status\ngit status\n\n# 2. Dodawa\u0107 pliki\ngit add plik.py\ngit add .  # Wszystkie pliki\n\n# 3. Commitowa\u0107\ngit commit -m \"Wiadomo\u015b\u0107 commit\"\n</code></pre>"},{"location":"Git/PL/02-basic-commands/#wiadomosci-commit","title":"Wiadomo\u015bci commit","text":"<p>Dobry format: <pre><code>Type: Kr\u00f3tki opis (max 50 znak\u00f3w)\n\nSzczeg\u00f3\u0142owy opis je\u015bli potrzebny\n</code></pre></p> <p>Przyk\u0142ady: <pre><code>feat: Dodanie funkcji analizy danych\nfix: Poprawka b\u0142\u0119du oblicze\u0144\ndocs: Aktualizacja README\n</code></pre></p>"},{"location":"Git/PL/02-basic-commands/#widziec-historie","title":"Widzie\u0107 histori\u0119","text":""},{"location":"Git/PL/02-basic-commands/#git-log","title":"git log","text":"<pre><code># Pe\u0142na historia\ngit log\n\n# Kompaktowa historia\ngit log --oneline\n\n# Historia z wykresem\ngit log --graph --oneline --all\n\n# Ograniczy\u0107 liczb\u0119\ngit log -5  # Ostatnie 5 commit\u00f3w\n</code></pre>"},{"location":"Git/PL/02-basic-commands/#cofac-zmiany","title":"Cofa\u0107 zmiany","text":""},{"location":"Git/PL/02-basic-commands/#cofac-w-working-directory","title":"Cofa\u0107 w working directory","text":"<pre><code># Cofa\u0107 modyfikacje pliku\ngit restore plik.py\n\n# Cofa\u0107 wszystkie modyfikacje\ngit restore .\n</code></pre>"},{"location":"Git/PL/02-basic-commands/#cofac-w-staging","title":"Cofa\u0107 w staging","text":"<pre><code># Usun\u0105\u0107 ze staging\ngit restore --staged plik.py\n</code></pre>"},{"location":"Git/PL/02-basic-commands/#modyfikowac-ostatni-commit","title":"Modyfikowa\u0107 ostatni commit","text":"<pre><code># Modyfikowa\u0107 wiadomo\u015b\u0107\ngit commit --amend -m \"Nowa wiadomo\u015b\u0107\"\n\n# Doda\u0107 zapomniane pliki\ngit add zapomniany_plik.py\ngit commit --amend --no-edit\n</code></pre>"},{"location":"Git/PL/02-basic-commands/#gitignore","title":".gitignore","text":""},{"location":"Git/PL/02-basic-commands/#utworzyc-gitignore","title":"Utworzy\u0107 .gitignore","text":"<pre><code># Python\n__pycache__/\n*.py[cod]\nvenv/\nenv/\n\n# Jupyter\n.ipynb_checkpoints\n*.ipynb\n\n# Dane\n*.csv\n*.xlsx\ndata/\n\n# IDE\n.vscode/\n.idea/\n\n# Sekrety\n.env\n*.key\n</code></pre>"},{"location":"Git/PL/02-basic-commands/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>git add : Dodawa\u0107 do staging</li> <li>git commit : Tworzy\u0107 commit</li> <li>git log : Widzie\u0107 histori\u0119</li> <li>git status : Widzie\u0107 status</li> <li>.gitignore : Wyklucza\u0107 pliki</li> </ol>"},{"location":"Git/PL/02-basic-commands/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 3. Ga\u0142\u0119zie, aby nauczy\u0107 si\u0119 zarz\u0105dzania ga\u0142\u0119ziami.</p>"},{"location":"Git/PL/03-branching/","title":"3. Ga\u0142\u0119zie Git","text":""},{"location":"Git/PL/03-branching/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 ga\u0142\u0119zie</li> <li>Tworzy\u0107 i zarz\u0105dza\u0107 ga\u0142\u0119ziami</li> <li>\u0141\u0105czy\u0107 ga\u0142\u0119zie</li> <li>Rozwi\u0105zywa\u0107 konflikty</li> <li>Workflow z ga\u0142\u0119ziami</li> </ul>"},{"location":"Git/PL/03-branching/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do ga\u0142\u0119zi</li> <li>Tworzy\u0107 ga\u0142\u0119zie</li> <li>\u0141\u0105czy\u0107 ga\u0142\u0119zie</li> <li>Rozwi\u0105zywa\u0107 konflikty</li> <li>Workflow</li> </ol>"},{"location":"Git/PL/03-branching/#wprowadzenie-do-gaezi","title":"Wprowadzenie do ga\u0142\u0119zi","text":""},{"location":"Git/PL/03-branching/#czym-jest-gaaz","title":"Czym jest ga\u0142\u0105\u017a?","text":"<p>Ga\u0142\u0105\u017a = Niezale\u017cna linia rozwoju</p> <ul> <li>Izolacja : Praca izolowana</li> <li>R\u00f3wnoleg\u0142a : Wiele ga\u0142\u0119zi jednocze\u015bnie</li> <li>\u0141\u0105czenie : \u0141\u0105czy\u0107 zmiany</li> <li>main/master : G\u0142\u00f3wna ga\u0142\u0105\u017a</li> </ul>"},{"location":"Git/PL/03-branching/#tworzyc-gaezie","title":"Tworzy\u0107 ga\u0142\u0119zie","text":""},{"location":"Git/PL/03-branching/#utworzyc-nowa-gaaz","title":"Utworzy\u0107 now\u0105 ga\u0142\u0105\u017a","text":"<pre><code># Utworzy\u0107 ga\u0142\u0105\u017a\ngit branch feature-analyse\n\n# Utworzy\u0107 i prze\u0142\u0105czy\u0107\ngit checkout -b feature-analyse\n\n# Nowa sk\u0142adnia (Git 2.23+)\ngit switch -c feature-analyse\n</code></pre>"},{"location":"Git/PL/03-branching/#przeaczac-miedzy-gaeziami","title":"Prze\u0142\u0105cza\u0107 mi\u0119dzy ga\u0142\u0119ziami","text":"<pre><code># Prze\u0142\u0105czy\u0107 na ga\u0142\u0105\u017a\ngit checkout feature-analyse\n\n# Nowa sk\u0142adnia\ngit switch feature-analyse\n\n# Wr\u00f3ci\u0107 do main\ngit checkout main\n</code></pre>"},{"location":"Git/PL/03-branching/#aczyc-gaezie","title":"\u0141\u0105czy\u0107 ga\u0142\u0119zie","text":""},{"location":"Git/PL/03-branching/#aczenie","title":"\u0141\u0105czenie","text":"<pre><code># Prze\u0142\u0105czy\u0107 na main\ngit checkout main\n\n# \u0141\u0105czy\u0107 ga\u0142\u0105\u017a\ngit merge feature-analyse\n</code></pre>"},{"location":"Git/PL/03-branching/#rozwiazywac-konflikty","title":"Rozwi\u0105zywa\u0107 konflikty","text":""},{"location":"Git/PL/03-branching/#kiedy-wystepuja-konflikty","title":"Kiedy wyst\u0119puj\u0105 konflikty?","text":"<ul> <li>Ta sama linia zmodyfikowana : W dw\u00f3ch r\u00f3\u017cnych ga\u0142\u0119ziach</li> <li>Plik usuni\u0119ty : W jednej ga\u0142\u0119zi, zmodyfikowany w drugiej</li> </ul>"},{"location":"Git/PL/03-branching/#rozwiazac-konflikt","title":"Rozwi\u0105za\u0107 konflikt","text":"<p>Krok 1 : Zidentyfikowa\u0107 konflikt</p> <pre><code>git status\n</code></pre> <p>Krok 2 : Otworzy\u0107 plik</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nprint(\"Wersja main\")\n=======\nprint(\"Wersja feature\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-analyse\n</code></pre> <p>Krok 3 : Rozwi\u0105za\u0107 r\u0119cznie</p> <pre><code>print(\"Wersja po\u0142\u0105czona\")\n</code></pre> <p>Krok 4 : Oznaczy\u0107 jako rozwi\u0105zane</p> <pre><code>git add plik.py\ngit commit\n</code></pre>"},{"location":"Git/PL/03-branching/#workflow","title":"Workflow","text":""},{"location":"Git/PL/03-branching/#prosty-workflow","title":"Prosty workflow","text":"<pre><code># 1. Utworzy\u0107 ga\u0142\u0105\u017a dla funkcji\ngit checkout -b feature-nowa-funkcja\n\n# 2. Pracowa\u0107 na ga\u0142\u0119zi\n# ... modyfikacje ...\n\n# 3. Commitowa\u0107\ngit add .\ngit commit -m \"Dodanie nowej funkcji\"\n\n# 4. \u0141\u0105czy\u0107 z main\ngit checkout main\ngit merge feature-nowa-funkcja\n\n# 5. Usun\u0105\u0107 ga\u0142\u0105\u017a\ngit branch -d feature-nowa-funkcja\n</code></pre>"},{"location":"Git/PL/03-branching/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Ga\u0142\u0119zie : Izolowane linie rozwoju</li> <li>git branch : Tworzy\u0107/zar\u0105dza\u0107 ga\u0142\u0119ziami</li> <li>git merge : \u0141\u0105czy\u0107 ga\u0142\u0119zie</li> <li>Konflikty : Rozwi\u0105zywa\u0107 r\u0119cznie</li> <li>Workflow : Jedna ga\u0142\u0105\u017a na funkcj\u0119</li> </ol>"},{"location":"Git/PL/03-branching/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 4. Zdalne repozytoria, aby pracowa\u0107 z GitHub/GitLab.</p>"},{"location":"Git/PL/04-remote-repositories/","title":"4. Zdalne repozytoria Git","text":""},{"location":"Git/PL/04-remote-repositories/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 zdalne repozytoria</li> <li>Pracowa\u0107 z GitHub/GitLab</li> <li>Klonowa\u0107 repozytoria</li> <li>Push i Pull</li> <li>Synchronizacja</li> </ul>"},{"location":"Git/PL/04-remote-repositories/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do zdalnych repozytori\u00f3w</li> <li>GitHub i GitLab</li> <li>Klonowa\u0107 repozytorium</li> <li>Push i Pull</li> <li>Synchronizacja</li> </ol>"},{"location":"Git/PL/04-remote-repositories/#wprowadzenie-do-zdalnych-repozytoriow","title":"Wprowadzenie do zdalnych repozytori\u00f3w","text":""},{"location":"Git/PL/04-remote-repositories/#czym-jest-zdalne-repozytorium","title":"Czym jest zdalne repozytorium?","text":"<p>Zdalne repozytorium = Kopia repozytorium na serwerze</p> <ul> <li>GitHub : Popularna us\u0142uga</li> <li>GitLab : Alternatywa open-source</li> <li>Backup : Kopia zapasowa online</li> </ul>"},{"location":"Git/PL/04-remote-repositories/#github-i-gitlab","title":"GitHub i GitLab","text":""},{"location":"Git/PL/04-remote-repositories/#utworzyc-konto-github","title":"Utworzy\u0107 konto GitHub","text":"<ol> <li>Przej\u015b\u0107 do: https://github.com</li> <li>Klikn\u0105\u0107 \"Sign up\"</li> <li>Wype\u0142ni\u0107 formularz</li> <li>Zweryfikowa\u0107 email</li> </ol>"},{"location":"Git/PL/04-remote-repositories/#utworzyc-repozytorium-github","title":"Utworzy\u0107 repozytorium GitHub","text":"<ol> <li>Klikn\u0105\u0107 \"New repository\"</li> <li>Nazwa\u0107 repozytorium</li> <li>Wybra\u0107 public/private</li> <li>Klikn\u0105\u0107 \"Create repository\"</li> </ol>"},{"location":"Git/PL/04-remote-repositories/#klonowac-repozytorium","title":"Klonowa\u0107 repozytorium","text":""},{"location":"Git/PL/04-remote-repositories/#klonowac-z-github","title":"Klonowa\u0107 z GitHub","text":"<pre><code># Klonowa\u0107 z HTTPS\ngit clone https://github.com/username/repo.git\n\n# Klonowa\u0107 z SSH\ngit clone git@github.com:username/repo.git\n</code></pre>"},{"location":"Git/PL/04-remote-repositories/#push-i-pull","title":"Push i Pull","text":""},{"location":"Git/PL/04-remote-repositories/#dodac-remote","title":"Doda\u0107 remote","text":"<pre><code># Doda\u0107 remote\ngit remote add origin https://github.com/username/repo.git\n\n# Zobaczy\u0107 remotes\ngit remote -v\n</code></pre>"},{"location":"Git/PL/04-remote-repositories/#push-wysac","title":"Push (wys\u0142a\u0107)","text":"<pre><code># Pierwszy push\ngit push -u origin main\n\n# Kolejne pushy\ngit push\n</code></pre>"},{"location":"Git/PL/04-remote-repositories/#pull-pobrac","title":"Pull (pobra\u0107)","text":"<pre><code># Pobra\u0107 i po\u0142\u0105czy\u0107\ngit pull\n\n# Pobra\u0107 tylko\ngit fetch\n\n# Po\u0142\u0105czy\u0107 po fetch\ngit merge origin/main\n</code></pre>"},{"location":"Git/PL/04-remote-repositories/#synchronizacja","title":"Synchronizacja","text":""},{"location":"Git/PL/04-remote-repositories/#podstawowy-workflow","title":"Podstawowy workflow","text":"<pre><code># 1. Pobra\u0107 ostatnie zmiany\ngit pull\n\n# 2. Pracowa\u0107 lokalnie\n# ... modyfikacje ...\n\n# 3. Doda\u0107 i commitowa\u0107\ngit add .\ngit commit -m \"Modyfikacje\"\n\n# 4. Wys\u0142a\u0107\ngit push\n</code></pre>"},{"location":"Git/PL/04-remote-repositories/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Remote : Repozytorium na serwerze</li> <li>git clone : Kopiowa\u0107 repozytorium</li> <li>git push : Wysy\u0142a\u0107 zmiany</li> <li>git pull : Pobiera\u0107 zmiany</li> <li>Synchronizacja : Pull przed Push</li> </ol>"},{"location":"Git/PL/04-remote-repositories/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 5. Wsp\u00f3\u0142praca, aby nauczy\u0107 si\u0119 wsp\u00f3\u0142pracy.</p>"},{"location":"Git/PL/05-collaboration/","title":"5. Wsp\u00f3\u0142praca z Git","text":""},{"location":"Git/PL/05-collaboration/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Fork i Pull Requests</li> <li>Issues i Projects</li> <li>Code Review</li> <li>Workflow w zespole</li> <li>Dobre praktyki</li> </ul>"},{"location":"Git/PL/05-collaboration/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Fork</li> <li>Pull Requests</li> <li>Issues</li> <li>Code Review</li> <li>Workflow w zespole</li> </ol>"},{"location":"Git/PL/05-collaboration/#fork","title":"Fork","text":""},{"location":"Git/PL/05-collaboration/#czym-jest-fork","title":"Czym jest Fork?","text":"<p>Fork = Kopia repozytorium w Twoim koncie</p> <ul> <li>Kompletna kopia : Wszystkie pliki i historia</li> <li>Niezale\u017cna : Zmiany bez wp\u0142ywu na orygina\u0142</li> <li>Wk\u0142ad : Proponowa\u0107 zmiany przez PR</li> </ul>"},{"location":"Git/PL/05-collaboration/#forkowac-repozytorium","title":"Forkowa\u0107 repozytorium","text":"<p>Na GitHub:</p> <ol> <li>Przej\u015b\u0107 do repozytorium</li> <li>Klikn\u0105\u0107 \"Fork\"</li> <li>Wybra\u0107 swoje konto</li> <li>Repozytorium jest skopiowane</li> </ol> <p>Klonowa\u0107 sw\u00f3j fork:</p> <pre><code># Klonowa\u0107 sw\u00f3j fork\ngit clone https://github.com/twoj-username/repo.git\n\n# Doda\u0107 orygina\u0142 jako upstream\ngit remote add upstream https://github.com/original-owner/repo.git\n</code></pre>"},{"location":"Git/PL/05-collaboration/#pull-requests","title":"Pull Requests","text":""},{"location":"Git/PL/05-collaboration/#utworzyc-pull-request","title":"Utworzy\u0107 Pull Request","text":"<p>Krok 1 : Utworzy\u0107 ga\u0142\u0105\u017a</p> <pre><code>git checkout -b feature-moj-wklad\n</code></pre> <p>Krok 2 : Wprowadzi\u0107 modyfikacje</p> <pre><code># ... modyfikacje ...\ngit add .\ngit commit -m \"Dodanie nowej funkcji\"\n</code></pre> <p>Krok 3 : Wypchn\u0105\u0107 ga\u0142\u0105\u017a</p> <pre><code>git push -u origin feature-moj-wklad\n</code></pre> <p>Krok 4 : Utworzy\u0107 PR na GitHub</p> <ol> <li>Przej\u015b\u0107 do swojego forka</li> <li>Klikn\u0105\u0107 \"Compare &amp; pull request\"</li> <li>Wype\u0142ni\u0107 formularz</li> <li>Klikn\u0105\u0107 \"Create pull request\"</li> </ol>"},{"location":"Git/PL/05-collaboration/#issues","title":"Issues","text":""},{"location":"Git/PL/05-collaboration/#utworzyc-issue","title":"Utworzy\u0107 Issue","text":"<p>Na GitHub:</p> <ol> <li>Przej\u015b\u0107 do repozytorium</li> <li>Klikn\u0105\u0107 \"Issues\"</li> <li>Klikn\u0105\u0107 \"New Issue\"</li> <li>Wype\u0142ni\u0107 formularz</li> </ol>"},{"location":"Git/PL/05-collaboration/#typy-issues","title":"Typy Issues","text":"<p>Bug Report: - Opis b\u0142\u0119du - Kroki do reprodukcji - Oczekiwane zachowanie</p> <p>Feature Request: - Opis funkcji - Przypadki u\u017cycia</p>"},{"location":"Git/PL/05-collaboration/#code-review","title":"Code Review","text":""},{"location":"Git/PL/05-collaboration/#proces-review","title":"Proces Review","text":"<ol> <li>Utworzy\u0107 PR : Z jasnym opisem</li> <li>Czeka\u0107 na review : Maintainerzy sprawdzaj\u0105</li> <li>Poprawi\u0107 : Je\u015bli wymagane</li> <li>Zatwierdzi\u0107 : Po walidacji</li> <li>Po\u0142\u0105czy\u0107 : Przez maintainera</li> </ol>"},{"location":"Git/PL/05-collaboration/#workflow-w-zespole","title":"Workflow w zespole","text":""},{"location":"Git/PL/05-collaboration/#standardowy-workflow","title":"Standardowy workflow","text":"<pre><code># 1. Pobra\u0107 ostatnie zmiany\ngit pull origin main\n\n# 2. Utworzy\u0107 ga\u0142\u0105\u017a\ngit checkout -b feature-nowa-funkcja\n\n# 3. Pracowa\u0107\n# ... modyfikacje ...\n\n# 4. Commitowa\u0107 regularnie\ngit add .\ngit commit -m \"Jasna wiadomo\u015b\u0107\"\n\n# 5. Wypchn\u0105\u0107 ga\u0142\u0105\u017a\ngit push -u origin feature-nowa-funkcja\n\n# 6. Utworzy\u0107 PR\n# Na GitHub/GitLab\n\n# 7. Po po\u0142\u0105czeniu, wyczy\u015bci\u0107\ngit checkout main\ngit pull origin main\ngit branch -d feature-nowa-funkcja\n</code></pre>"},{"location":"Git/PL/05-collaboration/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Fork : Kopia repozytorium</li> <li>Pull Request : Proponowa\u0107 zmiany</li> <li>Issues : \u015aledzi\u0107 problemy</li> <li>Code Review : Weryfikacja kodu</li> <li>Workflow : Ustrukturyzowany proces</li> </ol>"},{"location":"Git/PL/05-collaboration/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 6. Funkcje zaawansowane, aby pog\u0142\u0119bi\u0107.</p>"},{"location":"Git/PL/06-advanced/","title":"6. Funkcje zaawansowane Git","text":""},{"location":"Git/PL/06-advanced/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>U\u017cywa\u0107 Stash</li> <li>Zrozumie\u0107 Rebase</li> <li>Zarz\u0105dza\u0107 Tagami</li> <li>U\u017cywa\u0107 Hook\u00f3w</li> <li>Polecenia zaawansowane</li> </ul>"},{"location":"Git/PL/06-advanced/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Stash</li> <li>Rebase</li> <li>Tagi</li> <li>Hooki</li> <li>Polecenia zaawansowane</li> </ol>"},{"location":"Git/PL/06-advanced/#stash","title":"Stash","text":""},{"location":"Git/PL/06-advanced/#czym-jest-stash","title":"Czym jest Stash?","text":"<p>Stash = Tymczasowo zapisa\u0107 zmiany</p> <ul> <li>Tymczasowy : Niecommitowane zmiany</li> <li>Szybki : Szybko prze\u0142\u0105cza\u0107 ga\u0142\u0119zie</li> <li>Odzyskiwalny : Odzyska\u0107 p\u00f3\u017aniej</li> </ul>"},{"location":"Git/PL/06-advanced/#uzywac-stash","title":"U\u017cywa\u0107 Stash","text":"<pre><code># Zapisa\u0107 zmiany\ngit stash\n\n# Z wiadomo\u015bci\u0105\ngit stash save \"Opisowa wiadomo\u015b\u0107\"\n\n# Zastosowa\u0107 ostatni stash\ngit stash apply\n\n# Zastosowa\u0107 i usun\u0105\u0107\ngit stash pop\n\n# Listowa\u0107 stashy\ngit stash list\n</code></pre>"},{"location":"Git/PL/06-advanced/#rebase","title":"Rebase","text":""},{"location":"Git/PL/06-advanced/#czym-jest-rebase","title":"Czym jest Rebase?","text":"<p>Rebase = Ponownie zastosowa\u0107 commity na innej bazie</p> <ul> <li>Liniowa historia : Czystsza</li> <li>Przepisywanie : Modyfikuje histori\u0119</li> <li>Uwaga : Nie rebase'owa\u0107 na wsp\u00f3\u0142dzielonych ga\u0142\u0119ziach</li> </ul>"},{"location":"Git/PL/06-advanced/#prosty-rebase","title":"Prosty Rebase","text":"<pre><code># Rebase na main\ngit checkout feature-branche\ngit rebase main\n\n# Rebase interaktywny\ngit rebase -i HEAD~3\n</code></pre>"},{"location":"Git/PL/06-advanced/#tagi","title":"Tagi","text":""},{"location":"Git/PL/06-advanced/#czym-jest-tag","title":"Czym jest Tag?","text":"<p>Tag = Wska\u017anik do konkretnego commita</p> <ul> <li>Wersja : Oznacza\u0107 wersje</li> <li>Release : Punkty release</li> <li>Referencja : Stabilna referencja</li> </ul>"},{"location":"Git/PL/06-advanced/#utworzyc-tag","title":"Utworzy\u0107 Tag","text":"<pre><code># Tag lekki\ngit tag v1.0.0\n\n# Tag annotowany (zalecany)\ngit tag -a v1.0.0 -m \"Wersja 1.0.0\"\n\n# Wypchn\u0105\u0107 tag\ngit push origin v1.0.0\n</code></pre>"},{"location":"Git/PL/06-advanced/#hooki","title":"Hooki","text":""},{"location":"Git/PL/06-advanced/#czym-jest-hook","title":"Czym jest Hook?","text":"<p>Hook = Skrypt wykonywany przy okre\u015blonych zdarzeniach</p> <ul> <li>Automatyzacja : Wykonywa\u0107 akcje</li> <li>Walidacja : Sprawdza\u0107 przed commitem</li> <li>Powiadomienia : Powiadamia\u0107 po push</li> </ul>"},{"location":"Git/PL/06-advanced/#przykad-hooka","title":"Przyk\u0142ad Hooka","text":"<p><code>.git/hooks/pre-commit</code>:</p> <pre><code>#!/bin/bash\n# Uruchomi\u0107 testy przed commitem\npython -m pytest tests/\n\nif [ $? -ne 0 ]; then\n    echo \"Testy nieudane, commit anulowany\"\n    exit 1\nfi\n</code></pre>"},{"location":"Git/PL/06-advanced/#polecenia-zaawansowane","title":"Polecenia zaawansowane","text":""},{"location":"Git/PL/06-advanced/#cherry-pick","title":"Cherry-pick","text":"<pre><code># Zastosowa\u0107 konkretny commit\ngit cherry-pick &lt;hash&gt;\n</code></pre>"},{"location":"Git/PL/06-advanced/#reflog","title":"Reflog","text":"<pre><code># Zobaczy\u0107 histori\u0119 akcji\ngit reflog\n\n# Odzyska\u0107 utracony commit\ngit checkout &lt;hash&gt;\n</code></pre>"},{"location":"Git/PL/06-advanced/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Stash : Tymczasowo zapisa\u0107</li> <li>Rebase : Przepisa\u0107 histori\u0119</li> <li>Tagi : Oznacza\u0107 wersje</li> <li>Hooki : Automatyzowa\u0107 akcje</li> <li>Polecenia zaawansowane : Pot\u0119\u017cne narz\u0119dzia</li> </ol>"},{"location":"Git/PL/06-advanced/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 7. Dobre praktyki, aby pozna\u0107 najlepsze praktyki.</p>"},{"location":"Git/PL/07-best-practices/","title":"7. Dobre praktyki Git","text":""},{"location":"Git/PL/07-best-practices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Skuteczne wiadomo\u015bci commit</li> <li>Struktura projektu</li> <li>Kompletny .gitignore</li> <li>Dokumentacja</li> <li>Optymalny workflow</li> </ul>"},{"location":"Git/PL/07-best-practices/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wiadomo\u015bci commit</li> <li>Struktura projektu</li> <li>.gitignore</li> <li>Dokumentacja</li> <li>Workflow</li> </ol>"},{"location":"Git/PL/07-best-practices/#wiadomosci-commit","title":"Wiadomo\u015bci commit","text":""},{"location":"Git/PL/07-best-practices/#zalecany-format","title":"Zalecany format","text":"<pre><code>Type: Kr\u00f3tki opis (max 50 znak\u00f3w)\n\nSzczeg\u00f3\u0142owy opis je\u015bli potrzebny (72 znaki na lini\u0119)\n\n- Punkt 1\n- Punkt 2\n</code></pre>"},{"location":"Git/PL/07-best-practices/#typy-commitow","title":"Typy commit\u00f3w","text":"<ul> <li>feat : Nowa funkcja</li> <li>fix : Poprawka b\u0142\u0119du</li> <li>docs : Dokumentacja</li> <li>style : Formatowanie</li> <li>refactor : Refaktoryzacja</li> <li>test : Testy</li> <li>chore : Zadania konserwacyjne</li> </ul>"},{"location":"Git/PL/07-best-practices/#struktura-projektu","title":"Struktura projektu","text":""},{"location":"Git/PL/07-best-practices/#zalecana-struktura","title":"Zalecana struktura","text":"<pre><code>moj-projekt/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_main.py\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 guide.md\n</code></pre>"},{"location":"Git/PL/07-best-practices/#gitignore","title":".gitignore","text":""},{"location":"Git/PL/07-best-practices/#kompletny-gitignore-dla-pythona","title":"Kompletny .gitignore dla Pythona","text":"<pre><code># Python\n__pycache__/\n*.py[cod]\nvenv/\nenv/\n\n# Jupyter\n.ipynb_checkpoints\n*.ipynb\n\n# Dane\n*.csv\n*.xlsx\ndata/\n\n# IDE\n.vscode/\n.idea/\n\n# Sekrety\n.env\n*.key\n</code></pre>"},{"location":"Git/PL/07-best-practices/#dokumentacja","title":"Dokumentacja","text":""},{"location":"Git/PL/07-best-practices/#dokumentacja-kodu","title":"Dokumentacja kodu","text":"<p>Docstringi Pythona:</p> <pre><code>def analizuj_dane(plik):\n    \"\"\"\n    Analizuje plik danych CSV.\n\n    Args:\n        plik (str): \u015acie\u017cka do pliku CSV\n\n    Returns:\n        dict: S\u0142ownik ze statystykami\n    \"\"\"\n    # Kod...\n</code></pre>"},{"location":"Git/PL/07-best-practices/#workflow","title":"Workflow","text":""},{"location":"Git/PL/07-best-practices/#zalecany-workflow","title":"Zalecany workflow","text":"<ol> <li>Utworzy\u0107 ga\u0142\u0105\u017a : Dla ka\u017cdej funkcji</li> <li>Commitowa\u0107 regularnie : Ma\u0142e cz\u0119ste commity</li> <li>Testowa\u0107 : Przed wypchni\u0119ciem</li> <li>Pull Request : Do review</li> <li>\u0141\u0105czy\u0107 : Po zatwierdzeniu</li> </ol>"},{"location":"Git/PL/07-best-practices/#zote-zasady","title":"Z\u0142ote zasady","text":"<ul> <li>Jeden commit = Jedna logiczna zmiana</li> <li>Jasne i opisowe wiadomo\u015bci</li> <li>Testowa\u0107 przed wypchni\u0119ciem</li> <li>Nigdy nie force push na main</li> <li>Synchronizowa\u0107 regularnie</li> </ul>"},{"location":"Git/PL/07-best-practices/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Wiadomo\u015bci : Jasne i ustrukturyzowane</li> <li>Struktura : Zorganizowana i logiczna</li> <li>.gitignore : Kompletny i dostosowany</li> <li>Dokumentacja : README i docstringi</li> <li>Workflow : Regularny i sp\u00f3jny</li> </ol>"},{"location":"Git/PL/07-best-practices/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 8. Projekty praktyczne, aby tworzy\u0107 kompletne projekty.</p>"},{"location":"Git/PL/08-projets/","title":"8. Projekty praktyczne Git","text":""},{"location":"Git/PL/08-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Utworzy\u0107 portfolio GitHub</li> <li>Zarz\u0105dza\u0107 projektem wsp\u00f3\u0142pracuj\u0105cym</li> <li>Wersjonowa\u0107 skrypty Python</li> <li>Dokumentowa\u0107 projekt</li> <li>Projekty do portfolio</li> </ul>"},{"location":"Git/PL/08-projets/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Projekt 1 : Portfolio GitHub</li> <li>Projekt 2 : Projekt wsp\u00f3\u0142pracuj\u0105cy</li> <li>Projekt 3 : Wersjonowane skrypty Python</li> <li>Projekt 4 : Dokumentacja projektu</li> </ol>"},{"location":"Git/PL/08-projets/#projekt-1-portfolio-github","title":"Projekt 1 : Portfolio GitHub","text":""},{"location":"Git/PL/08-projets/#cel","title":"Cel","text":"<p>Utworzy\u0107 profesjonalne portfolio na GitHub.</p>"},{"location":"Git/PL/08-projets/#struktura","title":"Struktura","text":"<pre><code>portfolio/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 projects/\n\u2502   \u251c\u2500\u2500 project1/\n\u2502   \u251c\u2500\u2500 project2/\n\u2502   \u2514\u2500\u2500 project3/\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 utilities.py\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 resume.md\n</code></pre>"},{"location":"Git/PL/08-projets/#utworzyc-repozytorium","title":"Utworzy\u0107 repozytorium","text":"<pre><code># Utworzy\u0107 lokalne repozytorium\nmkdir portfolio\ncd portfolio\ngit init\n\n# Utworzy\u0107 struktur\u0119\nmkdir projects scripts docs\n\n# Utworzy\u0107 README\necho \"# Moje Portfolio\" &gt; README.md\n\n# Pierwszy commit\ngit add .\ngit commit -m \"Initial commit: portfolio\"\n\n# Utworzy\u0107 na GitHub i wypchn\u0105\u0107\ngit remote add origin https://github.com/username/portfolio.git\ngit push -u origin main\n</code></pre>"},{"location":"Git/PL/08-projets/#projekt-2-projekt-wspopracujacy","title":"Projekt 2 : Projekt wsp\u00f3\u0142pracuj\u0105cy","text":""},{"location":"Git/PL/08-projets/#workflow","title":"Workflow","text":"<pre><code># 1. Klonowa\u0107 repozytorium\ngit clone https://github.com/team/projekt.git\ncd projekt\n\n# 2. Utworzy\u0107 ga\u0142\u0105\u017a\ngit checkout -b feature-moj-wklad\n\n# 3. Pracowa\u0107\n# ... modyfikacje ...\n\n# 4. Commitowa\u0107\ngit add .\ngit commit -m \"feat: Dodanie nowej funkcji\"\n\n# 5. Synchronizowa\u0107 z main\ngit fetch origin\ngit rebase origin/main\n\n# 6. Wypchn\u0105\u0107\ngit push -u origin feature-moj-wklad\n\n# 7. Utworzy\u0107 Pull Request na GitHub\n</code></pre>"},{"location":"Git/PL/08-projets/#projekt-3-wersjonowane-skrypty-python","title":"Projekt 3 : Wersjonowane skrypty Python","text":""},{"location":"Git/PL/08-projets/#struktura_1","title":"Struktura","text":"<pre><code>data-scripts/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 data_loader.py\n\u2502   \u251c\u2500\u2500 analyzer.py\n\u2502   \u2514\u2500\u2500 visualizer.py\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 .gitkeep\n</code></pre>"},{"location":"Git/PL/08-projets/#workflow_1","title":"Workflow","text":"<pre><code># Zainicjalizowa\u0107\ngit init\ngit add .\ngit commit -m \"Initial commit: skrypty analizy\"\n\n# Utworzy\u0107 ga\u0142\u0105\u017a dla nowej funkcji\ngit checkout -b feature-nowa-analiza\n\n# Rozwija\u0107\n# ... kod ...\n\n# Commitowa\u0107\ngit add src/analyzer.py\ngit commit -m \"feat: Dodanie zaawansowanej analizy statystycznej\"\n\n# \u0141\u0105czy\u0107\ngit checkout main\ngit merge feature-nowa-analiza\n\n# Oznaczy\u0107 wersj\u0119\ngit tag -a v1.0.0 -m \"Wersja 1.0.0\"\ngit push origin main --tags\n</code></pre>"},{"location":"Git/PL/08-projets/#projekt-4-dokumentacja-projektu","title":"Projekt 4 : Dokumentacja projektu","text":""},{"location":"Git/PL/08-projets/#struktura_2","title":"Struktura","text":"<pre><code>project-docs/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u251c\u2500\u2500 usage.md\n\u2502   \u2514\u2500\u2500 api.md\n\u2514\u2500\u2500 CHANGELOG.md\n</code></pre>"},{"location":"Git/PL/08-projets/#workflow-dokumentacji","title":"Workflow dokumentacji","text":"<pre><code># Utworzy\u0107 ga\u0142\u0105\u017a dla dokumentacji\ngit checkout -b docs/dodanie-guide-usage\n\n# Doda\u0107 dokumentacj\u0119\n# ... pisa\u0107 docs/usage.md ...\n\n# Commitowa\u0107\ngit add docs/usage.md\ngit commit -m \"docs: Dodanie guide u\u017cycia\"\n\n# Wypchn\u0105\u0107 i utworzy\u0107 PR\ngit push -u origin docs/dodanie-guide-usage\n</code></pre>"},{"location":"Git/PL/08-projets/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Portfolio : Prezentowa\u0107 projekty</li> <li>Wsp\u00f3\u0142praca : Ustrukturyzowany workflow</li> <li>Wersjonowanie : Zarz\u0105dza\u0107 wersjami</li> <li>Dokumentacja : Niezb\u0119dna</li> <li>GitHub : Profesjonalna platforma</li> </ol>"},{"location":"Git/PL/08-projets/#zasoby","title":"\ud83d\udd17 Zasoby","text":"<ul> <li>Przewodniki GitHub</li> <li>Dokumentacja Git</li> </ul> <p>Gratulacje ! Uko\u0144czy\u0142e\u015b szkolenie Git. Mo\u017cesz teraz zarz\u0105dza\u0107 projektami efektywnie z Git i GitHub.</p>"},{"location":"Kubernetes/EN/","title":"Kubernetes Training for Data Analyst","text":""},{"location":"Kubernetes/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through learning Kubernetes as a Data Analyst. Kubernetes is an open-source platform for orchestrating and managing containers at scale.</p>"},{"location":"Kubernetes/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand Kubernetes and container orchestration</li> <li>Install Kubernetes (locally)</li> <li>Create and manage Pods and Deployments</li> <li>Configure Services and Ingress</li> <li>Manage ConfigMaps and Secrets</li> <li>Use Persistent Volumes</li> <li>Create practical projects for your portfolio</li> </ul>"},{"location":"Kubernetes/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 Minikube / Kind : Free local Kubernetes - \u2705 kubectl : Free Kubernetes CLI - \u2705 Official Documentation : Complete free guides - \u2705 Online Tutorials : Free resources</p> <p>Total Budget: $0</p>"},{"location":"Kubernetes/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"Kubernetes/EN/#1-kubernetes-getting-started","title":"1. Kubernetes Getting Started","text":"<ul> <li>Install Kubernetes locally</li> <li>Basic concepts</li> <li>First Pods</li> <li>Essential commands</li> </ul>"},{"location":"Kubernetes/EN/#2-fundamental-concepts","title":"2. Fundamental Concepts","text":"<ul> <li>Kubernetes architecture</li> <li>Pods, Nodes, Clusters</li> <li>Controllers and ReplicaSets</li> <li>Namespaces</li> </ul>"},{"location":"Kubernetes/EN/#3-pods-and-deployments","title":"3. Pods and Deployments","text":"<ul> <li>Create Pods</li> <li>Manage Deployments</li> <li>Scaling and Rolling Updates</li> <li>Health Checks</li> </ul>"},{"location":"Kubernetes/EN/#4-services","title":"4. Services","text":"<ul> <li>Service types</li> <li>Service Discovery</li> <li>Load Balancing</li> <li>Ingress</li> </ul>"},{"location":"Kubernetes/EN/#5-configmaps-and-secrets","title":"5. ConfigMaps and Secrets","text":"<ul> <li>Manage configuration</li> <li>Secrets management</li> <li>Environment variables</li> <li>Best practices</li> </ul>"},{"location":"Kubernetes/EN/#6-persistent-volumes","title":"6. Persistent Volumes","text":"<ul> <li>Kubernetes Volumes</li> <li>Persistent Volumes</li> <li>Storage Classes</li> <li>StatefulSets</li> </ul>"},{"location":"Kubernetes/EN/#7-best-practices","title":"7. Best Practices","text":"<ul> <li>Security</li> <li>Performance</li> <li>Organization</li> <li>Monitoring</li> </ul>"},{"location":"Kubernetes/EN/#8-practical-projects","title":"8. Practical Projects","text":"<ul> <li>Deploy a web application</li> <li>Data pipeline with Kubernetes</li> <li>Complete stack</li> <li>Portfolio projects</li> </ul>"},{"location":"Kubernetes/EN/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"Kubernetes/EN/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker : Installed and working</li> <li>4 GB RAM : Minimum recommended</li> <li>Disk Space : 20 GB free</li> </ul>"},{"location":"Kubernetes/EN/#quick-installation","title":"Quick Installation","text":"<p>Minikube (recommended):</p> <pre><code># Install Minikube\n# Windows\nchoco install minikube\n\n# Linux\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# macOS\nbrew install minikube\n\n# Start Minikube\nminikube start\n\n# Verify\nkubectl get nodes\n</code></pre>"},{"location":"Kubernetes/EN/#first-pod","title":"First Pod","text":"<pre><code># Create a Pod\nkubectl run nginx --image=nginx\n\n# See Pods\nkubectl get pods\n\n# Describe a Pod\nkubectl describe pod nginx\n</code></pre>"},{"location":"Kubernetes/EN/#use-cases-for-data-analyst","title":"\ud83d\udcca Use Cases for Data Analyst","text":"<ul> <li>Orchestration : Manage multiple containers</li> <li>Scaling : Scale automatically</li> <li>Deployment : Deploy easily</li> <li>Resilience : Auto-healing</li> <li>Data Pipelines : Orchestrate pipelines</li> </ul>"},{"location":"Kubernetes/EN/#free-resources","title":"\ud83d\udcda Free Resources","text":""},{"location":"Kubernetes/EN/#official-documentation","title":"Official Documentation","text":"<ul> <li>Kubernetes Documentation : https://kubernetes.io/docs/</li> <li>Kubernetes Playground : https://www.katacoda.com/courses/kubernetes</li> <li>Minikube : https://minikube.sigs.k8s.io/</li> </ul>"},{"location":"Kubernetes/EN/01-getting-started/","title":"1. Kubernetes Getting Started","text":""},{"location":"Kubernetes/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Kubernetes</li> <li>Install Kubernetes locally</li> <li>Understand basic concepts</li> <li>Create your first Pod</li> </ul>"},{"location":"Kubernetes/EN/01-getting-started/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Kubernetes</li> <li>Installation</li> <li>Basic Concepts</li> <li>First Pods</li> <li>Essential Commands</li> </ol>"},{"location":"Kubernetes/EN/01-getting-started/#introduction-to-kubernetes","title":"Introduction to Kubernetes","text":""},{"location":"Kubernetes/EN/01-getting-started/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes (K8s) = Container orchestration platform</p> <ul> <li>Orchestration : Manages multiple containers</li> <li>Scaling : Automatic scaling</li> <li>Auto-healing : Restarts failed containers</li> <li>Load Balancing : Traffic distribution</li> <li>Rolling Updates : Zero-downtime updates</li> </ul>"},{"location":"Kubernetes/EN/01-getting-started/#why-kubernetes-for-data-analyst","title":"Why Kubernetes for Data Analyst?","text":"<ul> <li>Orchestration : Manage multiple services</li> <li>Scaling : Adapt to needs</li> <li>Resilience : Auto-healing</li> <li>Deployment : Deploy easily</li> </ul>"},{"location":"Kubernetes/EN/01-getting-started/#installation","title":"Installation","text":""},{"location":"Kubernetes/EN/01-getting-started/#minikube-recommended","title":"Minikube (recommended)","text":"<p>Windows: <pre><code># With Chocolatey\nchoco install minikube\n\n# Start\nminikube start\n\n# Verify\nkubectl get nodes\n</code></pre></p> <p>Linux: <pre><code># Install Minikube\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# Start\nminikube start\n\n# Verify\nkubectl get nodes\n</code></pre></p> <p>macOS: <pre><code># With Homebrew\nbrew install minikube\n\n# Start\nminikube start\n\n# Verify\nkubectl get nodes\n</code></pre></p>"},{"location":"Kubernetes/EN/01-getting-started/#install-kubectl","title":"Install kubectl","text":"<p>kubectl = CLI for Kubernetes</p> <p>Windows: <pre><code># With Chocolatey\nchoco install kubernetes-cli\n</code></pre></p> <p>Linux: <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre></p> <p>macOS: <pre><code># With Homebrew\nbrew install kubectl\n</code></pre></p>"},{"location":"Kubernetes/EN/01-getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"Kubernetes/EN/01-getting-started/#cluster","title":"Cluster","text":"<p>Cluster = Set of machines (nodes)</p> <ul> <li>Master Node : Manages the cluster</li> <li>Worker Nodes : Run containers</li> </ul>"},{"location":"Kubernetes/EN/01-getting-started/#pod","title":"Pod","text":"<p>Pod = Smallest deployable unit in Kubernetes</p> <ul> <li>Containers : One or more containers</li> <li>Shared resources : Network and storage</li> <li>Ephemeral : Can be created/destroyed</li> </ul>"},{"location":"Kubernetes/EN/01-getting-started/#first-pods","title":"First Pods","text":""},{"location":"Kubernetes/EN/01-getting-started/#create-a-simple-pod","title":"Create a simple Pod","text":"<pre><code># Create a Pod\nkubectl run nginx --image=nginx\n\n# See Pods\nkubectl get pods\n\n# Describe a Pod\nkubectl describe pod nginx\n\n# Pod logs\nkubectl logs nginx\n</code></pre>"},{"location":"Kubernetes/EN/01-getting-started/#pod-with-yaml","title":"Pod with YAML","text":"<p>nginx-pod.yaml: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n</code></pre></p> <p>Create the Pod: <pre><code>kubectl apply -f nginx-pod.yaml\n\n# See the Pod\nkubectl get pods\n\n# Delete the Pod\nkubectl delete -f nginx-pod.yaml\n</code></pre></p>"},{"location":"Kubernetes/EN/01-getting-started/#essential-commands","title":"Essential Commands","text":""},{"location":"Kubernetes/EN/01-getting-started/#pod-management","title":"Pod Management","text":"<pre><code># List Pods\nkubectl get pods\n\n# Describe a Pod\nkubectl describe pod pod-name\n\n# Pod logs\nkubectl logs pod-name\n\n# Execute a command in a Pod\nkubectl exec -it pod-name -- bash\n\n# Delete a Pod\nkubectl delete pod pod-name\n</code></pre>"},{"location":"Kubernetes/EN/01-getting-started/#information","title":"Information","text":"<pre><code># See nodes\nkubectl get nodes\n\n# See namespaces\nkubectl get namespaces\n\n# Cluster information\nkubectl cluster-info\n\n# Version\nkubectl version\n</code></pre>"},{"location":"Kubernetes/EN/01-getting-started/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Kubernetes orchestrates containers</li> <li>Pods are the smallest units</li> <li>kubectl is the main CLI</li> <li>Minikube/Kind for local Kubernetes</li> <li>YAML to define resources</li> </ol>"},{"location":"Kubernetes/EN/01-getting-started/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 2. Fundamental Concepts to deepen.</p>"},{"location":"Kubernetes/EN/02-concepts/","title":"2. Kubernetes Fundamental Concepts","text":""},{"location":"Kubernetes/EN/02-concepts/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Kubernetes architecture</li> <li>Master Pods, Nodes, Clusters</li> <li>Understand Controllers and ReplicaSets</li> <li>Use Namespaces</li> </ul>"},{"location":"Kubernetes/EN/02-concepts/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Architecture</li> <li>Pods</li> <li>Nodes</li> <li>Controllers</li> <li>Namespaces</li> </ol>"},{"location":"Kubernetes/EN/02-concepts/#architecture","title":"Architecture","text":""},{"location":"Kubernetes/EN/02-concepts/#main-components","title":"Main Components","text":"<p>Master Node: - API Server : Entry point - etcd : Database - Scheduler : Schedules Pods - Controller Manager : Manages controllers</p> <p>Worker Node: - kubelet : Agent on each node - kube-proxy : Network - Container Runtime : Docker/containerd</p>"},{"location":"Kubernetes/EN/02-concepts/#pods","title":"Pods","text":""},{"location":"Kubernetes/EN/02-concepts/#what-is-a-pod","title":"What is a Pod?","text":"<p>Pod = Smallest deployable unit</p> <ul> <li>One or more containers : Share network/storage</li> <li>Ephemeral : Can be recreated</li> <li>Unique IP : Each Pod has an IP</li> </ul>"},{"location":"Kubernetes/EN/02-concepts/#pod-example","title":"Pod Example","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    ports:\n    - containerPort: 80\n</code></pre>"},{"location":"Kubernetes/EN/02-concepts/#nodes","title":"Nodes","text":""},{"location":"Kubernetes/EN/02-concepts/#node-types","title":"Node Types","text":"<p>Master Node: - Manages cluster - Schedules Pods - Manages state</p> <p>Worker Node: - Runs Pods - Provides resources</p>"},{"location":"Kubernetes/EN/02-concepts/#controllers","title":"Controllers","text":""},{"location":"Kubernetes/EN/02-concepts/#controller-types","title":"Controller Types","text":"<p>ReplicaSet: - Maintains number of Pods - Auto-healing</p> <p>Deployment: - Manages ReplicaSets - Rolling updates</p> <p>StatefulSet: - For stateful applications - Stable identity</p>"},{"location":"Kubernetes/EN/02-concepts/#namespaces","title":"Namespaces","text":""},{"location":"Kubernetes/EN/02-concepts/#use-namespaces","title":"Use Namespaces","text":"<pre><code># List namespaces\nkubectl get namespaces\n\n# Create a namespace\nkubectl create namespace my-namespace\n\n# Use a namespace\nkubectl get pods -n my-namespace\n</code></pre>"},{"location":"Kubernetes/EN/02-concepts/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Architecture : Master and Worker Nodes</li> <li>Pods : Smallest units</li> <li>Controllers : Manage Pods</li> <li>Namespaces : Logical isolation</li> </ol>"},{"location":"Kubernetes/EN/02-concepts/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 3. Pods and Deployments.</p>"},{"location":"Kubernetes/EN/03-pods-deployments/","title":"3. Pods and Deployments","text":""},{"location":"Kubernetes/EN/03-pods-deployments/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create Pods</li> <li>Manage Deployments</li> <li>Scaling and Rolling Updates</li> <li>Health Checks</li> </ul>"},{"location":"Kubernetes/EN/03-pods-deployments/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Pods</li> <li>Deployments</li> <li>Scaling</li> <li>Rolling Updates</li> <li>Health Checks</li> </ol>"},{"location":"Kubernetes/EN/03-pods-deployments/#pods","title":"Pods","text":""},{"location":"Kubernetes/EN/03-pods-deployments/#create-a-pod","title":"Create a Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f pod.yaml\n</code></pre>"},{"location":"Kubernetes/EN/03-pods-deployments/#deployments","title":"Deployments","text":""},{"location":"Kubernetes/EN/03-pods-deployments/#create-a-deployment","title":"Create a Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f deployment.yaml\nkubectl get deployments\n</code></pre>"},{"location":"Kubernetes/EN/03-pods-deployments/#scaling","title":"Scaling","text":""},{"location":"Kubernetes/EN/03-pods-deployments/#manual-scaling","title":"Manual Scaling","text":"<pre><code># Scale\nkubectl scale deployment nginx-deployment --replicas=5\n\n# See Pods\nkubectl get pods\n</code></pre>"},{"location":"Kubernetes/EN/03-pods-deployments/#auto-scaling","title":"Auto-scaling","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nginx-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n</code></pre>"},{"location":"Kubernetes/EN/03-pods-deployments/#rolling-updates","title":"Rolling Updates","text":""},{"location":"Kubernetes/EN/03-pods-deployments/#update","title":"Update","text":"<pre><code># Update image\nkubectl set image deployment/nginx-deployment nginx=nginx:1.22\n\n# See status\nkubectl rollout status deployment/nginx-deployment\n\n# Rollback\nkubectl rollout undo deployment/nginx-deployment\n</code></pre>"},{"location":"Kubernetes/EN/03-pods-deployments/#health-checks","title":"Health Checks","text":""},{"location":"Kubernetes/EN/03-pods-deployments/#liveness-probe","title":"Liveness Probe","text":"<pre><code>containers:\n- name: nginx\n  image: nginx\n  livenessProbe:\n    httpGet:\n      path: /\n      port: 80\n    initialDelaySeconds: 30\n    periodSeconds: 10\n</code></pre>"},{"location":"Kubernetes/EN/03-pods-deployments/#readiness-probe","title":"Readiness Probe","text":"<pre><code>readinessProbe:\n  httpGet:\n    path: /health\n    port: 80\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>"},{"location":"Kubernetes/EN/03-pods-deployments/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Pods : Basic units</li> <li>Deployments : Manage Pods</li> <li>Scaling : Manual or automatic</li> <li>Rolling Updates : Zero-downtime</li> <li>Health Checks : Monitoring</li> </ol>"},{"location":"Kubernetes/EN/03-pods-deployments/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 4. Services.</p>"},{"location":"Kubernetes/EN/04-services/","title":"4. Kubernetes Services","text":""},{"location":"Kubernetes/EN/04-services/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Services</li> <li>Service types</li> <li>Service Discovery</li> <li>Load Balancing</li> <li>Ingress</li> </ul>"},{"location":"Kubernetes/EN/04-services/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Services</li> <li>Service Types</li> <li>Service Discovery</li> <li>Load Balancing</li> <li>Ingress</li> </ol>"},{"location":"Kubernetes/EN/04-services/#introduction-to-services","title":"Introduction to Services","text":""},{"location":"Kubernetes/EN/04-services/#what-is-a-service","title":"What is a Service?","text":"<p>Service = Stable access point to Pods</p> <ul> <li>Stable IP : Even if Pods change</li> <li>Load Balancing : Distributes traffic</li> <li>Service Discovery : Finds Pods automatically</li> </ul>"},{"location":"Kubernetes/EN/04-services/#service-types","title":"Service Types","text":""},{"location":"Kubernetes/EN/04-services/#clusterip-default","title":"ClusterIP (default)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"Kubernetes/EN/04-services/#nodeport","title":"NodePort","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: NodePort\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 8080\n    nodePort: 30080\n</code></pre>"},{"location":"Kubernetes/EN/04-services/#loadbalancer","title":"LoadBalancer","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n</code></pre>"},{"location":"Kubernetes/EN/04-services/#service-discovery","title":"Service Discovery","text":""},{"location":"Kubernetes/EN/04-services/#dns","title":"DNS","text":"<p>Services are accessible by name:</p> <pre><code># In a Pod\nimport requests\nresponse = requests.get('http://my-service:80')\n</code></pre>"},{"location":"Kubernetes/EN/04-services/#ingress","title":"Ingress","text":""},{"location":"Kubernetes/EN/04-services/#create-an-ingress","title":"Create an Ingress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\nspec:\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-service\n            port:\n              number: 80\n</code></pre>"},{"location":"Kubernetes/EN/04-services/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Services : Stable access points</li> <li>Types : ClusterIP, NodePort, LoadBalancer</li> <li>Service Discovery : By DNS name</li> <li>Ingress : HTTP/HTTPS routing</li> </ol>"},{"location":"Kubernetes/EN/04-services/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 5. ConfigMaps and Secrets.</p>"},{"location":"Kubernetes/EN/05-configmaps-secrets/","title":"5. ConfigMaps and Secrets","text":""},{"location":"Kubernetes/EN/05-configmaps-secrets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Manage configuration with ConfigMaps</li> <li>Manage secrets</li> <li>Environment variables</li> <li>Best practices</li> </ul>"},{"location":"Kubernetes/EN/05-configmaps-secrets/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>ConfigMaps</li> <li>Secrets</li> <li>Usage</li> <li>Best Practices</li> </ol>"},{"location":"Kubernetes/EN/05-configmaps-secrets/#configmaps","title":"ConfigMaps","text":""},{"location":"Kubernetes/EN/05-configmaps-secrets/#create-a-configmap","title":"Create a ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-config\ndata:\n  config.yaml: |\n    database:\n      host: localhost\n      port: 5432\n  app.properties: |\n    debug=true\n    log_level=info\n</code></pre> <pre><code>kubectl apply -f configmap.yaml\n</code></pre>"},{"location":"Kubernetes/EN/05-configmaps-secrets/#use-in-a-pod","title":"Use in a Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    envFrom:\n    - configMapRef:\n        name: my-config\n</code></pre>"},{"location":"Kubernetes/EN/05-configmaps-secrets/#secrets","title":"Secrets","text":""},{"location":"Kubernetes/EN/05-configmaps-secrets/#create-a-secret","title":"Create a Secret","text":"<pre><code># Create from command line\nkubectl create secret generic my-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=secret123\n</code></pre>"},{"location":"Kubernetes/EN/05-configmaps-secrets/#use-in-a-pod_1","title":"Use in a Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n</code></pre>"},{"location":"Kubernetes/EN/05-configmaps-secrets/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>ConfigMaps : Non-sensitive configuration</li> <li>Secrets : Sensitive data</li> <li>Environment variables : Injection into Pods</li> <li>Security : Don't commit secrets</li> </ol>"},{"location":"Kubernetes/EN/05-configmaps-secrets/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 6. Persistent Volumes.</p>"},{"location":"Kubernetes/EN/06-persistent-volumes/","title":"6. Persistent Volumes","text":""},{"location":"Kubernetes/EN/06-persistent-volumes/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Kubernetes Volumes</li> <li>Persistent Volumes and Claims</li> <li>Storage Classes</li> <li>StatefulSets</li> </ul>"},{"location":"Kubernetes/EN/06-persistent-volumes/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Volumes</li> <li>Persistent Volumes</li> <li>Storage Classes</li> <li>StatefulSets</li> </ol>"},{"location":"Kubernetes/EN/06-persistent-volumes/#volumes","title":"Volumes","text":""},{"location":"Kubernetes/EN/06-persistent-volumes/#volume-types","title":"Volume Types","text":"<ul> <li>emptyDir : Temporary</li> <li>hostPath : Host directory</li> <li>PersistentVolume : Persistent storage</li> </ul>"},{"location":"Kubernetes/EN/06-persistent-volumes/#persistent-volumes","title":"Persistent Volumes","text":""},{"location":"Kubernetes/EN/06-persistent-volumes/#create-a-persistentvolume","title":"Create a PersistentVolume","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /data\n</code></pre>"},{"location":"Kubernetes/EN/06-persistent-volumes/#persistentvolumeclaim","title":"PersistentVolumeClaim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Kubernetes/EN/06-persistent-volumes/#use-in-a-pod","title":"Use in a Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    volumeMounts:\n    - name: storage\n      mountPath: /data\n  volumes:\n  - name: storage\n    persistentVolumeClaim:\n      claimName: my-pvc\n</code></pre>"},{"location":"Kubernetes/EN/06-persistent-volumes/#storage-classes","title":"Storage Classes","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-ssd\nprovisioner: kubernetes.io/gce-pd\nparameters:\n  type: pd-ssd\n</code></pre>"},{"location":"Kubernetes/EN/06-persistent-volumes/#statefulsets","title":"StatefulSets","text":"<pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"nginx\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre>"},{"location":"Kubernetes/EN/06-persistent-volumes/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Volumes : Temporary storage</li> <li>Persistent Volumes : Persistent storage</li> <li>Storage Classes : Dynamic provisioning</li> <li>StatefulSets : Stateful applications</li> </ol>"},{"location":"Kubernetes/EN/06-persistent-volumes/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 7. Best Practices.</p>"},{"location":"Kubernetes/EN/07-best-practices/","title":"7. Kubernetes Best Practices","text":""},{"location":"Kubernetes/EN/07-best-practices/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Security</li> <li>Performance</li> <li>Organization</li> <li>Monitoring</li> </ul>"},{"location":"Kubernetes/EN/07-best-practices/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Security</li> <li>Performance</li> <li>Organization</li> <li>Monitoring</li> </ol>"},{"location":"Kubernetes/EN/07-best-practices/#security","title":"Security","text":""},{"location":"Kubernetes/EN/07-best-practices/#resource-limits","title":"Resource Limits","text":"<pre><code>resources:\n  requests:\n    memory: \"64Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n</code></pre>"},{"location":"Kubernetes/EN/07-best-practices/#security-context","title":"Security Context","text":"<pre><code>securityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  fsGroup: 2000\n</code></pre>"},{"location":"Kubernetes/EN/07-best-practices/#performance","title":"Performance","text":""},{"location":"Kubernetes/EN/07-best-practices/#resource-requests","title":"Resource Requests","text":"<pre><code>resources:\n  requests:\n    memory: \"256Mi\"\n    cpu: \"100m\"\n</code></pre>"},{"location":"Kubernetes/EN/07-best-practices/#affinity-rules","title":"Affinity Rules","text":"<pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: disktype\n          operator: In\n          values:\n          - ssd\n</code></pre>"},{"location":"Kubernetes/EN/07-best-practices/#organization","title":"Organization","text":""},{"location":"Kubernetes/EN/07-best-practices/#labels","title":"Labels","text":"<pre><code>metadata:\n  labels:\n    app: my-app\n    version: v1\n    environment: production\n</code></pre>"},{"location":"Kubernetes/EN/07-best-practices/#namespaces","title":"Namespaces","text":"<pre><code>kubectl create namespace production\nkubectl create namespace development\n</code></pre>"},{"location":"Kubernetes/EN/07-best-practices/#monitoring","title":"Monitoring","text":""},{"location":"Kubernetes/EN/07-best-practices/#health-checks","title":"Health Checks","text":"<pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n</code></pre>"},{"location":"Kubernetes/EN/07-best-practices/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Security : Resource limits, security context</li> <li>Performance : Resource requests</li> <li>Organization : Labels, namespaces</li> <li>Monitoring : Health checks</li> </ol>"},{"location":"Kubernetes/EN/07-best-practices/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 8. Practical Projects.</p>"},{"location":"Kubernetes/EN/08-projets/","title":"8. Kubernetes Practical Projects","text":""},{"location":"Kubernetes/EN/08-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Deploy a web application</li> <li>Data pipeline with Kubernetes</li> <li>Complete stack</li> <li>Portfolio projects</li> </ul>"},{"location":"Kubernetes/EN/08-projets/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Project 1 : Web Application</li> <li>Project 2 : Data Pipeline</li> <li>Project 3 : Complete Stack</li> </ol>"},{"location":"Kubernetes/EN/08-projets/#project-1-web-application","title":"Project 1 : Web Application","text":""},{"location":"Kubernetes/EN/08-projets/#deployment","title":"Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: app\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"Kubernetes/EN/08-projets/#service","title":"Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\nspec:\n  selector:\n    app: web\n  ports:\n  - port: 80\n    targetPort: 80\n  type: LoadBalancer\n</code></pre>"},{"location":"Kubernetes/EN/08-projets/#project-2-data-pipeline","title":"Project 2 : Data Pipeline","text":""},{"location":"Kubernetes/EN/08-projets/#deployment-with-configmap","title":"Deployment with ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: pipeline-config\ndata:\n  config.yaml: |\n    input_path: /data/input\n    output_path: /data/output\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: data-pipeline\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: pipeline\n  template:\n    metadata:\n      labels:\n        app: pipeline\n    spec:\n      containers:\n      - name: pipeline\n        image: python:3.11\n        command: [\"python\", \"pipeline.py\"]\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: config\n        configMap:\n          name: pipeline-config\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-pvc\n</code></pre>"},{"location":"Kubernetes/EN/08-projets/#project-3-complete-stack","title":"Project 3 : Complete Stack","text":""},{"location":"Kubernetes/EN/08-projets/#application-database","title":"Application + Database","text":"<pre><code># Deployment app\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: app\n        image: myapp:latest\n        env:\n        - name: DB_HOST\n          value: \"db-service\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: password\n\n---\n# Service app\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-service\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n\n---\n# Deployment DB\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: db\nspec:\n  serviceName: db-service\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:15\n        volumeMounts:\n        - name: db-storage\n          mountPath: /var/lib/postgresql/data\n  volumeClaimTemplates:\n  - metadata:\n      name: db-storage\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre>"},{"location":"Kubernetes/EN/08-projets/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Deployments : Manage applications</li> <li>Services : Access points</li> <li>ConfigMaps/Secrets : Configuration</li> <li>Volumes : Persistence</li> <li>Portfolio : Demonstrable projects</li> </ol>"},{"location":"Kubernetes/EN/08-projets/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>Kubernetes Examples</li> <li>Kubernetes Documentation</li> </ul> <p>Congratulations! You have completed the Kubernetes training.</p>"},{"location":"Kubernetes/FR/","title":"Formation Kubernetes pour Data Analyst","text":""},{"location":"Kubernetes/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'apprentissage de Kubernetes en tant que Data Analyst. Kubernetes est une plateforme open-source pour orchestrer et g\u00e9rer des conteneurs \u00e0 grande \u00e9chelle.</p>"},{"location":"Kubernetes/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre Kubernetes et l'orchestration de conteneurs</li> <li>Installer Kubernetes (localement)</li> <li>Cr\u00e9er et g\u00e9rer des Pods et Deployments</li> <li>Configurer des Services et Ingress</li> <li>G\u00e9rer les ConfigMaps et Secrets</li> <li>Utiliser les Persistent Volumes</li> <li>Cr\u00e9er des projets pratiques pour votre portfolio</li> </ul>"},{"location":"Kubernetes/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 Minikube / Kind : Kubernetes local gratuit - \u2705 kubectl : CLI Kubernetes gratuite - \u2705 Documentation officielle : Guides complets gratuits - \u2705 Tutoriels en ligne : Ressources gratuites</p> <p>Budget total : 0\u20ac</p>"},{"location":"Kubernetes/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"Kubernetes/FR/#1-prise-en-main-kubernetes","title":"1. Prise en main Kubernetes","text":"<ul> <li>Installer Kubernetes localement</li> <li>Concepts de base</li> <li>Premiers Pods</li> <li>Commandes essentielles</li> </ul>"},{"location":"Kubernetes/FR/#2-concepts-fondamentaux","title":"2. Concepts fondamentaux","text":"<ul> <li>Architecture Kubernetes</li> <li>Pods, Nodes, Clusters</li> <li>Controllers et ReplicaSets</li> <li>Namespaces</li> </ul>"},{"location":"Kubernetes/FR/#3-pods-et-deployments","title":"3. Pods et Deployments","text":"<ul> <li>Cr\u00e9er des Pods</li> <li>G\u00e9rer les Deployments</li> <li>Scaling et Rolling Updates</li> <li>Health Checks</li> </ul>"},{"location":"Kubernetes/FR/#4-services","title":"4. Services","text":"<ul> <li>Types de Services</li> <li>Service Discovery</li> <li>Load Balancing</li> <li>Ingress</li> </ul>"},{"location":"Kubernetes/FR/#5-configmaps-et-secrets","title":"5. ConfigMaps et Secrets","text":"<ul> <li>G\u00e9rer la configuration</li> <li>Secrets management</li> <li>Variables d'environnement</li> <li>Bonnes pratiques</li> </ul>"},{"location":"Kubernetes/FR/#6-persistent-volumes","title":"6. Persistent Volumes","text":"<ul> <li>Volumes Kubernetes</li> <li>Persistent Volumes</li> <li>Storage Classes</li> <li>StatefulSets</li> </ul>"},{"location":"Kubernetes/FR/#7-bonnes-pratiques","title":"7. Bonnes pratiques","text":"<ul> <li>S\u00e9curit\u00e9</li> <li>Performance</li> <li>Organisation</li> <li>Monitoring</li> </ul>"},{"location":"Kubernetes/FR/#8-projets-pratiques","title":"8. Projets pratiques","text":"<ul> <li>D\u00e9ployer une application web</li> <li>Pipeline de donn\u00e9es avec Kubernetes</li> <li>Stack compl\u00e8te</li> <li>Projets pour portfolio</li> </ul>"},{"location":"Kubernetes/FR/#demarrage-rapide","title":"\ud83d\ude80 D\u00e9marrage rapide","text":""},{"location":"Kubernetes/FR/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Docker : Install\u00e9 et fonctionnel</li> <li>4 Go RAM : Minimum recommand\u00e9</li> <li>Espace disque : 20 Go libres</li> </ul>"},{"location":"Kubernetes/FR/#installation-rapide","title":"Installation rapide","text":"<p>Minikube (recommand\u00e9) :</p> <pre><code># Installer Minikube\n# Windows\nchoco install minikube\n\n# Linux\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# macOS\nbrew install minikube\n\n# D\u00e9marrer Minikube\nminikube start\n\n# V\u00e9rifier\nkubectl get nodes\n</code></pre> <p>Kind (alternative) :</p> <pre><code># Installer Kind\n# Windows\nchoco install kind\n\n# Linux/macOS\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Cr\u00e9er un cluster\nkind create cluster\n\n# V\u00e9rifier\nkubectl get nodes\n</code></pre>"},{"location":"Kubernetes/FR/#premier-pod","title":"Premier Pod","text":"<pre><code># Cr\u00e9er un Pod\nkubectl run nginx --image=nginx\n\n# Voir les Pods\nkubectl get pods\n\n# D\u00e9crire un Pod\nkubectl describe pod nginx\n</code></pre>"},{"location":"Kubernetes/FR/#cas-dusage-pour-data-analyst","title":"\ud83d\udcca Cas d'usage pour Data Analyst","text":"<ul> <li>Orchestration : G\u00e9rer plusieurs conteneurs</li> <li>Scaling : Mettre \u00e0 l'\u00e9chelle automatiquement</li> <li>D\u00e9ploiement : D\u00e9ployer facilement</li> <li>R\u00e9silience : Auto-healing</li> <li>Data Pipelines : Orchestrer des pipelines</li> </ul>"},{"location":"Kubernetes/FR/#ressources-gratuites","title":"\ud83d\udcda Ressources gratuites","text":""},{"location":"Kubernetes/FR/#documentation-officielle","title":"Documentation officielle","text":"<ul> <li>Kubernetes Documentation : https://kubernetes.io/docs/</li> <li>Kubernetes Playground : https://www.katacoda.com/courses/kubernetes</li> <li>Minikube : https://minikube.sigs.k8s.io/</li> </ul>"},{"location":"Kubernetes/FR/#certifications-optionnel","title":"\ud83c\udf93 Certifications (optionnel)","text":""},{"location":"Kubernetes/FR/#certified-kubernetes-administrator-cka","title":"Certified Kubernetes Administrator (CKA)","text":"<ul> <li>Co\u00fbt : ~$395</li> <li>Pr\u00e9paration : Documentation gratuite</li> <li>Dur\u00e9e : 2-3 mois</li> <li>Niveau : Avanc\u00e9</li> </ul>"},{"location":"Kubernetes/FR/01-getting-started/","title":"1. Prise en main Kubernetes","text":""},{"location":"Kubernetes/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Kubernetes</li> <li>Installer Kubernetes localement</li> <li>Comprendre les concepts de base</li> <li>Cr\u00e9er votre premier Pod</li> </ul>"},{"location":"Kubernetes/FR/01-getting-started/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Kubernetes</li> <li>Installation</li> <li>Concepts de base</li> <li>Premiers Pods</li> <li>Commandes essentielles</li> </ol>"},{"location":"Kubernetes/FR/01-getting-started/#introduction-a-kubernetes","title":"Introduction \u00e0 Kubernetes","text":""},{"location":"Kubernetes/FR/01-getting-started/#quest-ce-que-kubernetes","title":"Qu'est-ce que Kubernetes ?","text":"<p>Kubernetes (K8s) = Plateforme d'orchestration de conteneurs</p> <ul> <li>Orchestration : G\u00e8re plusieurs conteneurs</li> <li>Scaling : Mise \u00e0 l'\u00e9chelle automatique</li> <li>Auto-healing : Red\u00e9marre les conteneurs d\u00e9faillants</li> <li>Load Balancing : R\u00e9partition de charge</li> <li>Rolling Updates : Mises \u00e0 jour sans interruption</li> </ul>"},{"location":"Kubernetes/FR/01-getting-started/#pourquoi-kubernetes-pour-data-analyst","title":"Pourquoi Kubernetes pour Data Analyst ?","text":"<ul> <li>Orchestration : G\u00e9rer plusieurs services</li> <li>Scaling : Adapter aux besoins</li> <li>R\u00e9silience : Auto-healing</li> <li>D\u00e9ploiement : D\u00e9ployer facilement</li> </ul>"},{"location":"Kubernetes/FR/01-getting-started/#installation","title":"Installation","text":""},{"location":"Kubernetes/FR/01-getting-started/#minikube-recommande","title":"Minikube (recommand\u00e9)","text":"<p>Windows : <pre><code># Avec Chocolatey\nchoco install minikube\n\n# Ou t\u00e9l\u00e9charger\n# https://minikube.sigs.k8s.io/docs/start/\n\n# D\u00e9marrer\nminikube start\n\n# V\u00e9rifier\nkubectl get nodes\n</code></pre></p> <p>Linux : <pre><code># Installer Minikube\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# D\u00e9marrer\nminikube start\n\n# V\u00e9rifier\nkubectl get nodes\n</code></pre></p> <p>macOS : <pre><code># Avec Homebrew\nbrew install minikube\n\n# D\u00e9marrer\nminikube start\n\n# V\u00e9rifier\nkubectl get nodes\n</code></pre></p>"},{"location":"Kubernetes/FR/01-getting-started/#kind-alternative","title":"Kind (alternative)","text":"<pre><code># Installer Kind\n# Windows\nchoco install kind\n\n# Linux/macOS\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Cr\u00e9er un cluster\nkind create cluster --name my-cluster\n\n# V\u00e9rifier\nkubectl get nodes\n</code></pre>"},{"location":"Kubernetes/FR/01-getting-started/#installer-kubectl","title":"Installer kubectl","text":"<p>kubectl = CLI pour Kubernetes</p> <p>Windows : <pre><code># Avec Chocolatey\nchoco install kubernetes-cli\n\n# Ou t\u00e9l\u00e9charger\n# https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/\n</code></pre></p> <p>Linux : <pre><code># Installer kubectl\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre></p> <p>macOS : <pre><code># Avec Homebrew\nbrew install kubectl\n</code></pre></p>"},{"location":"Kubernetes/FR/01-getting-started/#concepts-de-base","title":"Concepts de base","text":""},{"location":"Kubernetes/FR/01-getting-started/#cluster","title":"Cluster","text":"<p>Cluster = Ensemble de machines (nodes)</p> <ul> <li>Master Node : G\u00e8re le cluster</li> <li>Worker Nodes : Ex\u00e9cutent les conteneurs</li> </ul>"},{"location":"Kubernetes/FR/01-getting-started/#pod","title":"Pod","text":"<p>Pod = Plus petit d\u00e9ploiement dans Kubernetes</p> <ul> <li>Conteneurs : Un ou plusieurs conteneurs</li> <li>Ressources partag\u00e9es : R\u00e9seau et stockage</li> <li>\u00c9ph\u00e9m\u00e8re : Peut \u00eatre cr\u00e9\u00e9/d\u00e9truit</li> </ul>"},{"location":"Kubernetes/FR/01-getting-started/#node","title":"Node","text":"<p>Node = Machine dans le cluster</p> <ul> <li>Worker Node : Ex\u00e9cute les Pods</li> <li>Master Node : G\u00e8re le cluster</li> </ul>"},{"location":"Kubernetes/FR/01-getting-started/#premiers-pods","title":"Premiers Pods","text":""},{"location":"Kubernetes/FR/01-getting-started/#creer-un-pod-simple","title":"Cr\u00e9er un Pod simple","text":"<pre><code># Cr\u00e9er un Pod\nkubectl run nginx --image=nginx\n\n# Voir les Pods\nkubectl get pods\n\n# D\u00e9crire un Pod\nkubectl describe pod nginx\n\n# Logs d'un Pod\nkubectl logs nginx\n</code></pre>"},{"location":"Kubernetes/FR/01-getting-started/#pod-avec-yaml","title":"Pod avec YAML","text":"<p>nginx-pod.yaml : <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n</code></pre></p> <p>Cr\u00e9er le Pod : <pre><code>kubectl apply -f nginx-pod.yaml\n\n# Voir le Pod\nkubectl get pods\n\n# Supprimer le Pod\nkubectl delete -f nginx-pod.yaml\n</code></pre></p>"},{"location":"Kubernetes/FR/01-getting-started/#commandes-essentielles","title":"Commandes essentielles","text":""},{"location":"Kubernetes/FR/01-getting-started/#gestion-des-pods","title":"Gestion des Pods","text":"<pre><code># Lister les Pods\nkubectl get pods\n\n# D\u00e9crire un Pod\nkubectl describe pod pod-name\n\n# Logs d'un Pod\nkubectl logs pod-name\n\n# Ex\u00e9cuter une commande dans un Pod\nkubectl exec -it pod-name -- bash\n\n# Supprimer un Pod\nkubectl delete pod pod-name\n</code></pre>"},{"location":"Kubernetes/FR/01-getting-started/#informations","title":"Informations","text":"<pre><code># Voir les nodes\nkubectl get nodes\n\n# Voir les namespaces\nkubectl get namespaces\n\n# Informations du cluster\nkubectl cluster-info\n\n# Version\nkubectl version\n</code></pre>"},{"location":"Kubernetes/FR/01-getting-started/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Kubernetes orchestre les conteneurs</li> <li>Pods sont les plus petites unit\u00e9s</li> <li>kubectl est la CLI principale</li> <li>Minikube/Kind pour Kubernetes local</li> <li>YAML pour d\u00e9finir les ressources</li> </ol>"},{"location":"Kubernetes/FR/01-getting-started/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 2. Concepts fondamentaux pour approfondir.</p>"},{"location":"Kubernetes/FR/02-concepts/","title":"2. Concepts fondamentaux Kubernetes","text":""},{"location":"Kubernetes/FR/02-concepts/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre l'architecture Kubernetes</li> <li>Ma\u00eetriser Pods, Nodes, Clusters</li> <li>Comprendre Controllers et ReplicaSets</li> <li>Utiliser les Namespaces</li> </ul>"},{"location":"Kubernetes/FR/02-concepts/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Architecture</li> <li>Pods</li> <li>Nodes</li> <li>Controllers</li> <li>Namespaces</li> </ol>"},{"location":"Kubernetes/FR/02-concepts/#architecture","title":"Architecture","text":""},{"location":"Kubernetes/FR/02-concepts/#composants-principaux","title":"Composants principaux","text":"<p>Master Node : - API Server : Point d'entr\u00e9e - etcd : Base de donn\u00e9es - Scheduler : Planifie les Pods - Controller Manager : G\u00e8re les controllers</p> <p>Worker Node : - kubelet : Agent sur chaque node - kube-proxy : R\u00e9seau - Container Runtime : Docker/containerd</p>"},{"location":"Kubernetes/FR/02-concepts/#pods","title":"Pods","text":""},{"location":"Kubernetes/FR/02-concepts/#quest-ce-quun-pod","title":"Qu'est-ce qu'un Pod ?","text":"<p>Pod = Plus petite unit\u00e9 d\u00e9ployable</p> <ul> <li>Un ou plusieurs conteneurs : Partagent r\u00e9seau/stockage</li> <li>\u00c9ph\u00e9m\u00e8re : Peut \u00eatre recr\u00e9\u00e9</li> <li>IP unique : Chaque Pod a une IP</li> </ul>"},{"location":"Kubernetes/FR/02-concepts/#exemple-de-pod","title":"Exemple de Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    ports:\n    - containerPort: 80\n</code></pre>"},{"location":"Kubernetes/FR/02-concepts/#nodes","title":"Nodes","text":""},{"location":"Kubernetes/FR/02-concepts/#types-de-nodes","title":"Types de Nodes","text":"<p>Master Node : - G\u00e8re le cluster - Planifie les Pods - G\u00e8re l'\u00e9tat</p> <p>Worker Node : - Ex\u00e9cute les Pods - Fournit les ressources</p>"},{"location":"Kubernetes/FR/02-concepts/#controllers","title":"Controllers","text":""},{"location":"Kubernetes/FR/02-concepts/#types-de-controllers","title":"Types de Controllers","text":"<p>ReplicaSet : - Maintient un nombre de Pods - Auto-healing</p> <p>Deployment : - G\u00e8re les ReplicaSets - Rolling updates</p> <p>StatefulSet : - Pour applications stateful - Identit\u00e9 stable</p>"},{"location":"Kubernetes/FR/02-concepts/#namespaces","title":"Namespaces","text":""},{"location":"Kubernetes/FR/02-concepts/#utiliser-les-namespaces","title":"Utiliser les Namespaces","text":"<pre><code># Lister les namespaces\nkubectl get namespaces\n\n# Cr\u00e9er un namespace\nkubectl create namespace my-namespace\n\n# Utiliser un namespace\nkubectl get pods -n my-namespace\n</code></pre>"},{"location":"Kubernetes/FR/02-concepts/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Architecture : Master et Worker Nodes</li> <li>Pods : Plus petites unit\u00e9s</li> <li>Controllers : G\u00e8rent les Pods</li> <li>Namespaces : Isolation logique</li> </ol>"},{"location":"Kubernetes/FR/02-concepts/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 3. Pods et Deployments.</p>"},{"location":"Kubernetes/FR/03-pods-deployments/","title":"3. Pods et Deployments","text":""},{"location":"Kubernetes/FR/03-pods-deployments/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er des Pods</li> <li>G\u00e9rer les Deployments</li> <li>Scaling et Rolling Updates</li> <li>Health Checks</li> </ul>"},{"location":"Kubernetes/FR/03-pods-deployments/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Pods</li> <li>Deployments</li> <li>Scaling</li> <li>Rolling Updates</li> <li>Health Checks</li> </ol>"},{"location":"Kubernetes/FR/03-pods-deployments/#pods","title":"Pods","text":""},{"location":"Kubernetes/FR/03-pods-deployments/#creer-un-pod","title":"Cr\u00e9er un Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f pod.yaml\n</code></pre>"},{"location":"Kubernetes/FR/03-pods-deployments/#deployments","title":"Deployments","text":""},{"location":"Kubernetes/FR/03-pods-deployments/#creer-un-deployment","title":"Cr\u00e9er un Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f deployment.yaml\nkubectl get deployments\n</code></pre>"},{"location":"Kubernetes/FR/03-pods-deployments/#scaling","title":"Scaling","text":""},{"location":"Kubernetes/FR/03-pods-deployments/#scaling-manuel","title":"Scaling manuel","text":"<pre><code># Mettre \u00e0 l'\u00e9chelle\nkubectl scale deployment nginx-deployment --replicas=5\n\n# Voir les Pods\nkubectl get pods\n</code></pre>"},{"location":"Kubernetes/FR/03-pods-deployments/#auto-scaling","title":"Auto-scaling","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nginx-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n</code></pre>"},{"location":"Kubernetes/FR/03-pods-deployments/#rolling-updates","title":"Rolling Updates","text":""},{"location":"Kubernetes/FR/03-pods-deployments/#mettre-a-jour","title":"Mettre \u00e0 jour","text":"<pre><code># Mettre \u00e0 jour l'image\nkubectl set image deployment/nginx-deployment nginx=nginx:1.22\n\n# Voir le statut\nkubectl rollout status deployment/nginx-deployment\n\n# Rollback\nkubectl rollout undo deployment/nginx-deployment\n</code></pre>"},{"location":"Kubernetes/FR/03-pods-deployments/#health-checks","title":"Health Checks","text":""},{"location":"Kubernetes/FR/03-pods-deployments/#liveness-probe","title":"Liveness Probe","text":"<pre><code>containers:\n- name: nginx\n  image: nginx\n  livenessProbe:\n    httpGet:\n      path: /\n      port: 80\n    initialDelaySeconds: 30\n    periodSeconds: 10\n</code></pre>"},{"location":"Kubernetes/FR/03-pods-deployments/#readiness-probe","title":"Readiness Probe","text":"<pre><code>readinessProbe:\n  httpGet:\n    path: /health\n    port: 80\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>"},{"location":"Kubernetes/FR/03-pods-deployments/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Pods : Unit\u00e9s de base</li> <li>Deployments : G\u00e8rent les Pods</li> <li>Scaling : Manuel ou automatique</li> <li>Rolling Updates : Sans interruption</li> <li>Health Checks : Monitoring</li> </ol>"},{"location":"Kubernetes/FR/03-pods-deployments/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 4. Services.</p>"},{"location":"Kubernetes/FR/04-services/","title":"4. Services Kubernetes","text":""},{"location":"Kubernetes/FR/04-services/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les Services</li> <li>Types de Services</li> <li>Service Discovery</li> <li>Load Balancing</li> <li>Ingress</li> </ul>"},{"location":"Kubernetes/FR/04-services/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction aux Services</li> <li>Types de Services</li> <li>Service Discovery</li> <li>Load Balancing</li> <li>Ingress</li> </ol>"},{"location":"Kubernetes/FR/04-services/#introduction-aux-services","title":"Introduction aux Services","text":""},{"location":"Kubernetes/FR/04-services/#quest-ce-quun-service","title":"Qu'est-ce qu'un Service ?","text":"<p>Service = Point d'acc\u00e8s stable aux Pods</p> <ul> <li>IP stable : M\u00eame si les Pods changent</li> <li>Load Balancing : R\u00e9partit le trafic</li> <li>Service Discovery : Trouve les Pods automatiquement</li> </ul>"},{"location":"Kubernetes/FR/04-services/#types-de-services","title":"Types de Services","text":""},{"location":"Kubernetes/FR/04-services/#clusterip-par-defaut","title":"ClusterIP (par d\u00e9faut)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"Kubernetes/FR/04-services/#nodeport","title":"NodePort","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: NodePort\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 8080\n    nodePort: 30080\n</code></pre>"},{"location":"Kubernetes/FR/04-services/#loadbalancer","title":"LoadBalancer","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n</code></pre>"},{"location":"Kubernetes/FR/04-services/#service-discovery","title":"Service Discovery","text":""},{"location":"Kubernetes/FR/04-services/#dns","title":"DNS","text":"<p>Les Services sont accessibles par nom :</p> <pre><code># Dans un Pod\nimport requests\nresponse = requests.get('http://my-service:80')\n</code></pre>"},{"location":"Kubernetes/FR/04-services/#ingress","title":"Ingress","text":""},{"location":"Kubernetes/FR/04-services/#creer-un-ingress","title":"Cr\u00e9er un Ingress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\nspec:\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-service\n            port:\n              number: 80\n</code></pre>"},{"location":"Kubernetes/FR/04-services/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Services : Points d'acc\u00e8s stables</li> <li>Types : ClusterIP, NodePort, LoadBalancer</li> <li>Service Discovery : Par nom DNS</li> <li>Ingress : Routage HTTP/HTTPS</li> </ol>"},{"location":"Kubernetes/FR/04-services/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 5. ConfigMaps et Secrets.</p>"},{"location":"Kubernetes/FR/05-configmaps-secrets/","title":"5. ConfigMaps et Secrets","text":""},{"location":"Kubernetes/FR/05-configmaps-secrets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>G\u00e9rer la configuration avec ConfigMaps</li> <li>G\u00e9rer les secrets</li> <li>Variables d'environnement</li> <li>Bonnes pratiques</li> </ul>"},{"location":"Kubernetes/FR/05-configmaps-secrets/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>ConfigMaps</li> <li>Secrets</li> <li>Utilisation</li> <li>Bonnes pratiques</li> </ol>"},{"location":"Kubernetes/FR/05-configmaps-secrets/#configmaps","title":"ConfigMaps","text":""},{"location":"Kubernetes/FR/05-configmaps-secrets/#creer-un-configmap","title":"Cr\u00e9er un ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-config\ndata:\n  config.yaml: |\n    database:\n      host: localhost\n      port: 5432\n  app.properties: |\n    debug=true\n    log_level=info\n</code></pre> <pre><code>kubectl apply -f configmap.yaml\n</code></pre>"},{"location":"Kubernetes/FR/05-configmaps-secrets/#utiliser-dans-un-pod","title":"Utiliser dans un Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    envFrom:\n    - configMapRef:\n        name: my-config\n</code></pre>"},{"location":"Kubernetes/FR/05-configmaps-secrets/#secrets","title":"Secrets","text":""},{"location":"Kubernetes/FR/05-configmaps-secrets/#creer-un-secret","title":"Cr\u00e9er un Secret","text":"<pre><code># Cr\u00e9er depuis la ligne de commande\nkubectl create secret generic my-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=secret123\n</code></pre>"},{"location":"Kubernetes/FR/05-configmaps-secrets/#utiliser-dans-un-pod_1","title":"Utiliser dans un Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n</code></pre>"},{"location":"Kubernetes/FR/05-configmaps-secrets/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>ConfigMaps : Configuration non sensible</li> <li>Secrets : Donn\u00e9es sensibles</li> <li>Variables d'environnement : Injection dans Pods</li> <li>S\u00e9curit\u00e9 : Ne pas commiter les secrets</li> </ol>"},{"location":"Kubernetes/FR/05-configmaps-secrets/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 6. Persistent Volumes.</p>"},{"location":"Kubernetes/FR/06-persistent-volumes/","title":"6. Persistent Volumes","text":""},{"location":"Kubernetes/FR/06-persistent-volumes/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les Volumes Kubernetes</li> <li>Persistent Volumes et Claims</li> <li>Storage Classes</li> <li>StatefulSets</li> </ul>"},{"location":"Kubernetes/FR/06-persistent-volumes/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Volumes</li> <li>Persistent Volumes</li> <li>Storage Classes</li> <li>StatefulSets</li> </ol>"},{"location":"Kubernetes/FR/06-persistent-volumes/#volumes","title":"Volumes","text":""},{"location":"Kubernetes/FR/06-persistent-volumes/#types-de-volumes","title":"Types de Volumes","text":"<ul> <li>emptyDir : Temporaire</li> <li>hostPath : R\u00e9pertoire h\u00f4te</li> <li>PersistentVolume : Stockage persistant</li> </ul>"},{"location":"Kubernetes/FR/06-persistent-volumes/#persistent-volumes","title":"Persistent Volumes","text":""},{"location":"Kubernetes/FR/06-persistent-volumes/#creer-un-persistentvolume","title":"Cr\u00e9er un PersistentVolume","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /data\n</code></pre>"},{"location":"Kubernetes/FR/06-persistent-volumes/#persistentvolumeclaim","title":"PersistentVolumeClaim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Kubernetes/FR/06-persistent-volumes/#utiliser-dans-un-pod","title":"Utiliser dans un Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    volumeMounts:\n    - name: storage\n      mountPath: /data\n  volumes:\n  - name: storage\n    persistentVolumeClaim:\n      claimName: my-pvc\n</code></pre>"},{"location":"Kubernetes/FR/06-persistent-volumes/#storage-classes","title":"Storage Classes","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-ssd\nprovisioner: kubernetes.io/gce-pd\nparameters:\n  type: pd-ssd\n</code></pre>"},{"location":"Kubernetes/FR/06-persistent-volumes/#statefulsets","title":"StatefulSets","text":"<pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"nginx\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre>"},{"location":"Kubernetes/FR/06-persistent-volumes/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Volumes : Stockage temporaire</li> <li>Persistent Volumes : Stockage persistant</li> <li>Storage Classes : Provisionnement dynamique</li> <li>StatefulSets : Applications stateful</li> </ol>"},{"location":"Kubernetes/FR/06-persistent-volumes/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 7. Bonnes pratiques.</p>"},{"location":"Kubernetes/FR/07-best-practices/","title":"7. Bonnes pratiques Kubernetes","text":""},{"location":"Kubernetes/FR/07-best-practices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>S\u00e9curit\u00e9</li> <li>Performance</li> <li>Organisation</li> <li>Monitoring</li> </ul>"},{"location":"Kubernetes/FR/07-best-practices/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>S\u00e9curit\u00e9</li> <li>Performance</li> <li>Organisation</li> <li>Monitoring</li> </ol>"},{"location":"Kubernetes/FR/07-best-practices/#securite","title":"S\u00e9curit\u00e9","text":""},{"location":"Kubernetes/FR/07-best-practices/#resource-limits","title":"Resource Limits","text":"<pre><code>resources:\n  requests:\n    memory: \"64Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n</code></pre>"},{"location":"Kubernetes/FR/07-best-practices/#security-context","title":"Security Context","text":"<pre><code>securityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  fsGroup: 2000\n</code></pre>"},{"location":"Kubernetes/FR/07-best-practices/#performance","title":"Performance","text":""},{"location":"Kubernetes/FR/07-best-practices/#resource-requests","title":"Resource Requests","text":"<pre><code>resources:\n  requests:\n    memory: \"256Mi\"\n    cpu: \"100m\"\n</code></pre>"},{"location":"Kubernetes/FR/07-best-practices/#affinity-rules","title":"Affinity Rules","text":"<pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: disktype\n          operator: In\n          values:\n          - ssd\n</code></pre>"},{"location":"Kubernetes/FR/07-best-practices/#organisation","title":"Organisation","text":""},{"location":"Kubernetes/FR/07-best-practices/#labels","title":"Labels","text":"<pre><code>metadata:\n  labels:\n    app: my-app\n    version: v1\n    environment: production\n</code></pre>"},{"location":"Kubernetes/FR/07-best-practices/#namespaces","title":"Namespaces","text":"<pre><code>kubectl create namespace production\nkubectl create namespace development\n</code></pre>"},{"location":"Kubernetes/FR/07-best-practices/#monitoring","title":"Monitoring","text":""},{"location":"Kubernetes/FR/07-best-practices/#health-checks","title":"Health Checks","text":"<pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n</code></pre>"},{"location":"Kubernetes/FR/07-best-practices/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>S\u00e9curit\u00e9 : Resource limits, security context</li> <li>Performance : Resource requests</li> <li>Organisation : Labels, namespaces</li> <li>Monitoring : Health checks</li> </ol>"},{"location":"Kubernetes/FR/07-best-practices/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 8. Projets pratiques.</p>"},{"location":"Kubernetes/FR/08-projets/","title":"8. Projets pratiques Kubernetes","text":""},{"location":"Kubernetes/FR/08-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>D\u00e9ployer une application web</li> <li>Pipeline de donn\u00e9es avec Kubernetes</li> <li>Stack compl\u00e8te</li> <li>Projets pour portfolio</li> </ul>"},{"location":"Kubernetes/FR/08-projets/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Projet 1 : Application web</li> <li>Projet 2 : Pipeline de donn\u00e9es</li> <li>Projet 3 : Stack compl\u00e8te</li> </ol>"},{"location":"Kubernetes/FR/08-projets/#projet-1-application-web","title":"Projet 1 : Application web","text":""},{"location":"Kubernetes/FR/08-projets/#deployment","title":"Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: app\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"Kubernetes/FR/08-projets/#service","title":"Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\nspec:\n  selector:\n    app: web\n  ports:\n  - port: 80\n    targetPort: 80\n  type: LoadBalancer\n</code></pre>"},{"location":"Kubernetes/FR/08-projets/#projet-2-pipeline-de-donnees","title":"Projet 2 : Pipeline de donn\u00e9es","text":""},{"location":"Kubernetes/FR/08-projets/#deployment-avec-configmap","title":"Deployment avec ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: pipeline-config\ndata:\n  config.yaml: |\n    input_path: /data/input\n    output_path: /data/output\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: data-pipeline\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: pipeline\n  template:\n    metadata:\n      labels:\n        app: pipeline\n    spec:\n      containers:\n      - name: pipeline\n        image: python:3.11\n        command: [\"python\", \"pipeline.py\"]\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: config\n        configMap:\n          name: pipeline-config\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-pvc\n</code></pre>"},{"location":"Kubernetes/FR/08-projets/#projet-3-stack-complete","title":"Projet 3 : Stack compl\u00e8te","text":""},{"location":"Kubernetes/FR/08-projets/#application-base-de-donnees","title":"Application + Base de donn\u00e9es","text":"<pre><code># Deployment app\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: app\n        image: myapp:latest\n        env:\n        - name: DB_HOST\n          value: \"db-service\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: password\n\n---\n# Service app\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-service\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n\n---\n# Deployment DB\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: db\nspec:\n  serviceName: db-service\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:15\n        volumeMounts:\n        - name: db-storage\n          mountPath: /var/lib/postgresql/data\n  volumeClaimTemplates:\n  - metadata:\n      name: db-storage\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre>"},{"location":"Kubernetes/FR/08-projets/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Deployments : G\u00e8rent les applications</li> <li>Services : Points d'acc\u00e8s</li> <li>ConfigMaps/Secrets : Configuration</li> <li>Volumes : Persistance</li> <li>Portfolio : Projets d\u00e9montrables</li> </ol>"},{"location":"Kubernetes/FR/08-projets/#ressources","title":"\ud83d\udd17 Ressources","text":"<ul> <li>Kubernetes Examples</li> <li>Kubernetes Documentation</li> </ul> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation Kubernetes.</p>"},{"location":"Kubernetes/PL/","title":"Szkolenie Kubernetes dla Data Analyst","text":""},{"location":"Kubernetes/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>To szkolenie poprowadzi Ci\u0119 przez nauk\u0119 Kubernetes jako Data Analyst. Kubernetes to platforma open-source do orkiestracji i zarz\u0105dzania kontenerami na du\u017c\u0105 skal\u0119.</p>"},{"location":"Kubernetes/PL/#cele-szkoleniowe","title":"\ud83c\udfaf Cele szkoleniowe","text":"<ul> <li>Zrozumie\u0107 Kubernetes i orkiestracj\u0119 kontener\u00f3w</li> <li>Zainstalowa\u0107 Kubernetes (lokalnie)</li> <li>Tworzy\u0107 i zarz\u0105dza\u0107 Podami i Deploymentami</li> <li>Konfigurowa\u0107 Serwisy i Ingress</li> <li>Zarz\u0105dza\u0107 ConfigMapami i Secretami</li> <li>U\u017cywa\u0107 Persistent Volumes</li> <li>Tworzy\u0107 praktyczne projekty do portfolio</li> </ul>"},{"location":"Kubernetes/PL/#wszystko-jest-darmowe","title":"\ud83d\udcb0 Wszystko jest darmowe!","text":"<p>To szkolenie u\u017cywa tylko: - \u2705 Minikube / Kind : Darmowy lokalny Kubernetes - \u2705 kubectl : Darmowa CLI Kubernetes - \u2705 Oficjalna dokumentacja : Kompletne darmowe przewodniki - \u2705 Tutoriale online : Darmowe zasoby</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"Kubernetes/PL/#struktura-szkolenia","title":"\ud83d\udcd6 Struktura szkolenia","text":""},{"location":"Kubernetes/PL/#1-rozpoczecie-z-kubernetes","title":"1. Rozpocz\u0119cie z Kubernetes","text":"<ul> <li>Zainstalowa\u0107 Kubernetes lokalnie</li> <li>Podstawowe koncepcje</li> <li>Pierwsze Pody</li> <li>Podstawowe polecenia</li> </ul>"},{"location":"Kubernetes/PL/#2-podstawowe-koncepcje","title":"2. Podstawowe koncepcje","text":"<ul> <li>Architektura Kubernetes</li> <li>Pody, Nody, Klastry</li> <li>Kontrolery i ReplicaSety</li> <li>Namespace'y</li> </ul>"},{"location":"Kubernetes/PL/#3-pody-i-deploymenty","title":"3. Pody i Deploymenty","text":"<ul> <li>Tworzy\u0107 Pody</li> <li>Zarz\u0105dza\u0107 Deploymentami</li> <li>Skalowanie i Rolling Updates</li> <li>Health Checks</li> </ul>"},{"location":"Kubernetes/PL/#4-serwisy","title":"4. Serwisy","text":"<ul> <li>Typy serwis\u00f3w</li> <li>Service Discovery</li> <li>Load Balancing</li> <li>Ingress</li> </ul>"},{"location":"Kubernetes/PL/#5-configmaps-i-secrety","title":"5. ConfigMaps i Secrety","text":"<ul> <li>Zarz\u0105dza\u0107 konfiguracj\u0105</li> <li>Zarz\u0105dzanie secretami</li> <li>Zmienne \u015brodowiskowe</li> <li>Dobre praktyki</li> </ul>"},{"location":"Kubernetes/PL/#6-persistent-volumes","title":"6. Persistent Volumes","text":"<ul> <li>Wolumeny Kubernetes</li> <li>Persistent Volumes</li> <li>Storage Classes</li> <li>StatefulSety</li> </ul>"},{"location":"Kubernetes/PL/#7-dobre-praktyki","title":"7. Dobre praktyki","text":"<ul> <li>Bezpiecze\u0144stwo</li> <li>Wydajno\u015b\u0107</li> <li>Organizacja</li> <li>Monitorowanie</li> </ul>"},{"location":"Kubernetes/PL/#8-projekty-praktyczne","title":"8. Projekty praktyczne","text":"<ul> <li>Wdro\u017cy\u0107 aplikacj\u0119 web</li> <li>Pipeline danych z Kubernetes</li> <li>Kompletny stack</li> <li>Projekty do portfolio</li> </ul>"},{"location":"Kubernetes/PL/#szybki-start","title":"\ud83d\ude80 Szybki start","text":""},{"location":"Kubernetes/PL/#wymagania-wstepne","title":"Wymagania wst\u0119pne","text":"<ul> <li>Docker : Zainstalowany i dzia\u0142aj\u0105cy</li> <li>4 GB RAM : Minimum zalecane</li> <li>Miejsce na dysku : 20 GB wolne</li> </ul>"},{"location":"Kubernetes/PL/#szybka-instalacja","title":"Szybka instalacja","text":"<p>Minikube (zalecane):</p> <pre><code># Zainstalowa\u0107 Minikube\n# Windows\nchoco install minikube\n\n# Linux\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# macOS\nbrew install minikube\n\n# Uruchomi\u0107 Minikube\nminikube start\n\n# Sprawdzi\u0107\nkubectl get nodes\n</code></pre>"},{"location":"Kubernetes/PL/#pierwszy-pod","title":"Pierwszy Pod","text":"<pre><code># Utworzy\u0107 Pod\nkubectl run nginx --image=nginx\n\n# Zobaczy\u0107 Pody\nkubectl get pods\n\n# Opisa\u0107 Pod\nkubectl describe pod nginx\n</code></pre>"},{"location":"Kubernetes/PL/#przypadki-uzycia-dla-data-analyst","title":"\ud83d\udcca Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Orkiestracja : Zarz\u0105dza\u0107 wieloma kontenerami</li> <li>Skalowanie : Skalowa\u0107 automatycznie</li> <li>Wdra\u017canie : \u0141atwo wdra\u017ca\u0107</li> <li>Odporno\u015b\u0107 : Auto-healing</li> <li>Pipeline'y danych : Orkiestrowa\u0107 pipeline'y</li> </ul>"},{"location":"Kubernetes/PL/#darmowe-zasoby","title":"\ud83d\udcda Darmowe zasoby","text":""},{"location":"Kubernetes/PL/#oficjalna-dokumentacja","title":"Oficjalna dokumentacja","text":"<ul> <li>Dokumentacja Kubernetes : https://kubernetes.io/docs/</li> <li>Kubernetes Playground : https://www.katacoda.com/courses/kubernetes</li> <li>Minikube : https://minikube.sigs.k8s.io/</li> </ul>"},{"location":"Kubernetes/PL/01-getting-started/","title":"1. Rozpocz\u0119cie z Kubernetes","text":""},{"location":"Kubernetes/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Kubernetes</li> <li>Zainstalowa\u0107 Kubernetes lokalnie</li> <li>Zrozumie\u0107 podstawowe koncepcje</li> <li>Utworzy\u0107 pierwszy Pod</li> </ul>"},{"location":"Kubernetes/PL/01-getting-started/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Kubernetes</li> <li>Instalacja</li> <li>Podstawowe koncepcje</li> <li>Pierwsze Pody</li> <li>Podstawowe polecenia</li> </ol>"},{"location":"Kubernetes/PL/01-getting-started/#wprowadzenie-do-kubernetes","title":"Wprowadzenie do Kubernetes","text":""},{"location":"Kubernetes/PL/01-getting-started/#czym-jest-kubernetes","title":"Czym jest Kubernetes?","text":"<p>Kubernetes (K8s) = Platforma orkiestracji kontener\u00f3w</p> <ul> <li>Orkiestracja : Zarz\u0105dza wieloma kontenerami</li> <li>Skalowanie : Automatyczne skalowanie</li> <li>Auto-healing : Uruchamia ponownie nieudane kontenery</li> <li>Load Balancing : Rozk\u0142ad ruchu</li> <li>Rolling Updates : Aktualizacje bez przestoj\u00f3w</li> </ul>"},{"location":"Kubernetes/PL/01-getting-started/#dlaczego-kubernetes-dla-data-analyst","title":"Dlaczego Kubernetes dla Data Analyst?","text":"<ul> <li>Orkiestracja : Zarz\u0105dza\u0107 wieloma us\u0142ugami</li> <li>Skalowanie : Dostosowa\u0107 do potrzeb</li> <li>Odporno\u015b\u0107 : Auto-healing</li> <li>Wdra\u017canie : \u0141atwo wdra\u017ca\u0107</li> </ul>"},{"location":"Kubernetes/PL/01-getting-started/#instalacja","title":"Instalacja","text":""},{"location":"Kubernetes/PL/01-getting-started/#minikube-zalecane","title":"Minikube (zalecane)","text":"<p>Windows: <pre><code># Z Chocolatey\nchoco install minikube\n\n# Uruchomi\u0107\nminikube start\n\n# Sprawdzi\u0107\nkubectl get nodes\n</code></pre></p> <p>Linux: <pre><code># Zainstalowa\u0107 Minikube\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# Uruchomi\u0107\nminikube start\n\n# Sprawdzi\u0107\nkubectl get nodes\n</code></pre></p> <p>macOS: <pre><code># Z Homebrew\nbrew install minikube\n\n# Uruchomi\u0107\nminikube start\n\n# Sprawdzi\u0107\nkubectl get nodes\n</code></pre></p>"},{"location":"Kubernetes/PL/01-getting-started/#zainstalowac-kubectl","title":"Zainstalowa\u0107 kubectl","text":"<p>kubectl = CLI dla Kubernetes</p> <p>Windows: <pre><code># Z Chocolatey\nchoco install kubernetes-cli\n</code></pre></p> <p>Linux: <pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre></p> <p>macOS: <pre><code># Z Homebrew\nbrew install kubectl\n</code></pre></p>"},{"location":"Kubernetes/PL/01-getting-started/#podstawowe-koncepcje","title":"Podstawowe koncepcje","text":""},{"location":"Kubernetes/PL/01-getting-started/#klaster","title":"Klaster","text":"<p>Klaster = Zbi\u00f3r maszyn (nod\u00f3w)</p> <ul> <li>Master Node : Zarz\u0105dza klastrem</li> <li>Worker Nodes : Uruchamiaj\u0105 kontenery</li> </ul>"},{"location":"Kubernetes/PL/01-getting-started/#pod","title":"Pod","text":"<p>Pod = Najmniejsza jednostka wdro\u017ceniowa w Kubernetes</p> <ul> <li>Kontenery : Jeden lub wi\u0119cej kontener\u00f3w</li> <li>Wsp\u00f3lne zasoby : Sie\u0107 i magazyn</li> <li>Efemeryczny : Mo\u017ce by\u0107 tworzony/usuwany</li> </ul>"},{"location":"Kubernetes/PL/01-getting-started/#pierwsze-pody","title":"Pierwsze Pody","text":""},{"location":"Kubernetes/PL/01-getting-started/#utworzyc-prosty-pod","title":"Utworzy\u0107 prosty Pod","text":"<pre><code># Utworzy\u0107 Pod\nkubectl run nginx --image=nginx\n\n# Zobaczy\u0107 Pody\nkubectl get pods\n\n# Opisa\u0107 Pod\nkubectl describe pod nginx\n\n# Logi Poda\nkubectl logs nginx\n</code></pre>"},{"location":"Kubernetes/PL/01-getting-started/#pod-z-yaml","title":"Pod z YAML","text":"<p>nginx-pod.yaml: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n</code></pre></p> <p>Utworzy\u0107 Pod: <pre><code>kubectl apply -f nginx-pod.yaml\n\n# Zobaczy\u0107 Pod\nkubectl get pods\n\n# Usun\u0105\u0107 Pod\nkubectl delete -f nginx-pod.yaml\n</code></pre></p>"},{"location":"Kubernetes/PL/01-getting-started/#podstawowe-polecenia","title":"Podstawowe polecenia","text":""},{"location":"Kubernetes/PL/01-getting-started/#zarzadzanie-podami","title":"Zarz\u0105dzanie Podami","text":"<pre><code># Listowa\u0107 Pody\nkubectl get pods\n\n# Opisa\u0107 Pod\nkubectl describe pod pod-name\n\n# Logi Poda\nkubectl logs pod-name\n\n# Wykona\u0107 polecenie w Podzie\nkubectl exec -it pod-name -- bash\n\n# Usun\u0105\u0107 Pod\nkubectl delete pod pod-name\n</code></pre>"},{"location":"Kubernetes/PL/01-getting-started/#informacje","title":"Informacje","text":"<pre><code># Zobaczy\u0107 nody\nkubectl get nodes\n\n# Zobaczy\u0107 namespace'y\nkubectl get namespaces\n\n# Informacje klastra\nkubectl cluster-info\n\n# Wersja\nkubectl version\n</code></pre>"},{"location":"Kubernetes/PL/01-getting-started/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Kubernetes orkiestruje kontenery</li> <li>Pody to najmniejsze jednostki</li> <li>kubectl to g\u0142\u00f3wna CLI</li> <li>Minikube/Kind dla lokalnego Kubernetes</li> <li>YAML do definiowania zasob\u00f3w</li> </ol>"},{"location":"Kubernetes/PL/01-getting-started/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 2. Podstawowe koncepcje, aby pog\u0142\u0119bi\u0107.</p>"},{"location":"Kubernetes/PL/02-concepts/","title":"2. Podstawowe koncepcje Kubernetes","text":""},{"location":"Kubernetes/PL/02-concepts/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 architektur\u0119 Kubernetes</li> <li>Opanowa\u0107 Pody, Nody, Klastry</li> <li>Zrozumie\u0107 Kontrolery i ReplicaSety</li> <li>U\u017cywa\u0107 Namespace'\u00f3w</li> </ul>"},{"location":"Kubernetes/PL/02-concepts/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Architektura</li> <li>Pody</li> <li>Nody</li> <li>Kontrolery</li> <li>Namespace'y</li> </ol>"},{"location":"Kubernetes/PL/02-concepts/#architektura","title":"Architektura","text":""},{"location":"Kubernetes/PL/02-concepts/#gowne-komponenty","title":"G\u0142\u00f3wne komponenty","text":"<p>Master Node: - API Server : Punkt wej\u015bcia - etcd : Baza danych - Scheduler : Planuje Pody - Controller Manager : Zarz\u0105dza kontrolerami</p> <p>Worker Node: - kubelet : Agent na ka\u017cdym nodzie - kube-proxy : Sie\u0107 - Container Runtime : Docker/containerd</p>"},{"location":"Kubernetes/PL/02-concepts/#pody","title":"Pody","text":""},{"location":"Kubernetes/PL/02-concepts/#czym-jest-pod","title":"Czym jest Pod?","text":"<p>Pod = Najmniejsza jednostka wdro\u017ceniowa</p> <ul> <li>Jeden lub wi\u0119cej kontener\u00f3w : Dziel\u0105 sie\u0107/magazyn</li> <li>Efemeryczny : Mo\u017ce by\u0107 odtworzony</li> <li>Unikalne IP : Ka\u017cdy Pod ma IP</li> </ul>"},{"location":"Kubernetes/PL/02-concepts/#przykad-poda","title":"Przyk\u0142ad Poda","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    ports:\n    - containerPort: 80\n</code></pre>"},{"location":"Kubernetes/PL/02-concepts/#nody","title":"Nody","text":""},{"location":"Kubernetes/PL/02-concepts/#typy-nodow","title":"Typy nod\u00f3w","text":"<p>Master Node: - Zarz\u0105dza klastrem - Planuje Pody - Zarz\u0105dza stanem</p> <p>Worker Node: - Uruchamia Pody - Dostarcza zasoby</p>"},{"location":"Kubernetes/PL/02-concepts/#kontrolery","title":"Kontrolery","text":""},{"location":"Kubernetes/PL/02-concepts/#typy-kontrolerow","title":"Typy kontroler\u00f3w","text":"<p>ReplicaSet: - Utrzymuje liczb\u0119 Pod\u00f3w - Auto-healing</p> <p>Deployment: - Zarz\u0105dza ReplicaSetami - Rolling updates</p> <p>StatefulSet: - Dla aplikacji stateful - Stabilna to\u017csamo\u015b\u0107</p>"},{"location":"Kubernetes/PL/02-concepts/#namespacey","title":"Namespace'y","text":""},{"location":"Kubernetes/PL/02-concepts/#uzywac-namespaceow","title":"U\u017cywa\u0107 Namespace'\u00f3w","text":"<pre><code># Listowa\u0107 namespace'y\nkubectl get namespaces\n\n# Utworzy\u0107 namespace\nkubectl create namespace my-namespace\n\n# U\u017cywa\u0107 namespace\nkubectl get pods -n my-namespace\n</code></pre>"},{"location":"Kubernetes/PL/02-concepts/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Architektura : Master i Worker Nodes</li> <li>Pody : Najmniejsze jednostki</li> <li>Kontrolery : Zarz\u0105dzaj\u0105 Podami</li> <li>Namespace'y : Izolacja logiczna</li> </ol>"},{"location":"Kubernetes/PL/02-concepts/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 3. Pody i Deploymenty.</p>"},{"location":"Kubernetes/PL/03-pods-deployments/","title":"3. Pody i Deploymenty","text":""},{"location":"Kubernetes/PL/03-pods-deployments/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 Pody</li> <li>Zarz\u0105dza\u0107 Deploymentami</li> <li>Skalowanie i Rolling Updates</li> <li>Health Checks</li> </ul>"},{"location":"Kubernetes/PL/03-pods-deployments/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Pody</li> <li>Deploymenty</li> <li>Skalowanie</li> <li>Rolling Updates</li> <li>Health Checks</li> </ol>"},{"location":"Kubernetes/PL/03-pods-deployments/#pody","title":"Pody","text":""},{"location":"Kubernetes/PL/03-pods-deployments/#utworzyc-pod","title":"Utworzy\u0107 Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f pod.yaml\n</code></pre>"},{"location":"Kubernetes/PL/03-pods-deployments/#deploymenty","title":"Deploymenty","text":""},{"location":"Kubernetes/PL/03-pods-deployments/#utworzyc-deployment","title":"Utworzy\u0107 Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21\n        ports:\n        - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f deployment.yaml\nkubectl get deployments\n</code></pre>"},{"location":"Kubernetes/PL/03-pods-deployments/#skalowanie","title":"Skalowanie","text":""},{"location":"Kubernetes/PL/03-pods-deployments/#skalowanie-reczne","title":"Skalowanie r\u0119czne","text":"<pre><code># Skalowa\u0107\nkubectl scale deployment nginx-deployment --replicas=5\n\n# Zobaczy\u0107 Pody\nkubectl get pods\n</code></pre>"},{"location":"Kubernetes/PL/03-pods-deployments/#auto-skaling","title":"Auto-skaling","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nginx-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n</code></pre>"},{"location":"Kubernetes/PL/03-pods-deployments/#rolling-updates","title":"Rolling Updates","text":""},{"location":"Kubernetes/PL/03-pods-deployments/#aktualizowac","title":"Aktualizowa\u0107","text":"<pre><code># Aktualizowa\u0107 obraz\nkubectl set image deployment/nginx-deployment nginx=nginx:1.22\n\n# Zobaczy\u0107 status\nkubectl rollout status deployment/nginx-deployment\n\n# Rollback\nkubectl rollout undo deployment/nginx-deployment\n</code></pre>"},{"location":"Kubernetes/PL/03-pods-deployments/#health-checks","title":"Health Checks","text":""},{"location":"Kubernetes/PL/03-pods-deployments/#liveness-probe","title":"Liveness Probe","text":"<pre><code>containers:\n- name: nginx\n  image: nginx\n  livenessProbe:\n    httpGet:\n      path: /\n      port: 80\n    initialDelaySeconds: 30\n    periodSeconds: 10\n</code></pre>"},{"location":"Kubernetes/PL/03-pods-deployments/#readiness-probe","title":"Readiness Probe","text":"<pre><code>readinessProbe:\n  httpGet:\n    path: /health\n    port: 80\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>"},{"location":"Kubernetes/PL/03-pods-deployments/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Pody : Podstawowe jednostki</li> <li>Deploymenty : Zarz\u0105dzaj\u0105 Podami</li> <li>Skalowanie : R\u0119czne lub automatyczne</li> <li>Rolling Updates : Bez przestoj\u00f3w</li> <li>Health Checks : Monitorowanie</li> </ol>"},{"location":"Kubernetes/PL/03-pods-deployments/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 4. Serwisy.</p>"},{"location":"Kubernetes/PL/04-services/","title":"4. Serwisy Kubernetes","text":""},{"location":"Kubernetes/PL/04-services/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Serwisy</li> <li>Typy serwis\u00f3w</li> <li>Service Discovery</li> <li>Load Balancing</li> <li>Ingress</li> </ul>"},{"location":"Kubernetes/PL/04-services/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do Serwis\u00f3w</li> <li>Typy serwis\u00f3w</li> <li>Service Discovery</li> <li>Load Balancing</li> <li>Ingress</li> </ol>"},{"location":"Kubernetes/PL/04-services/#wprowadzenie-do-serwisow","title":"Wprowadzenie do Serwis\u00f3w","text":""},{"location":"Kubernetes/PL/04-services/#czym-jest-serwis","title":"Czym jest Serwis?","text":"<p>Serwis = Stabilny punkt dost\u0119pu do Pod\u00f3w</p> <ul> <li>Stabilne IP : Nawet je\u015bli Pody si\u0119 zmieniaj\u0105</li> <li>Load Balancing : Rozk\u0142ada ruch</li> <li>Service Discovery : Znajduje Pody automatycznie</li> </ul>"},{"location":"Kubernetes/PL/04-services/#typy-serwisow","title":"Typy serwis\u00f3w","text":""},{"location":"Kubernetes/PL/04-services/#clusterip-domyslny","title":"ClusterIP (domy\u015blny)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"Kubernetes/PL/04-services/#nodeport","title":"NodePort","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: NodePort\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n    targetPort: 8080\n    nodePort: 30080\n</code></pre>"},{"location":"Kubernetes/PL/04-services/#loadbalancer","title":"LoadBalancer","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: LoadBalancer\n  selector:\n    app: nginx\n  ports:\n  - port: 80\n</code></pre>"},{"location":"Kubernetes/PL/04-services/#service-discovery","title":"Service Discovery","text":""},{"location":"Kubernetes/PL/04-services/#dns","title":"DNS","text":"<p>Serwisy s\u0105 dost\u0119pne po nazwie:</p> <pre><code># W Podzie\nimport requests\nresponse = requests.get('http://my-service:80')\n</code></pre>"},{"location":"Kubernetes/PL/04-services/#ingress","title":"Ingress","text":""},{"location":"Kubernetes/PL/04-services/#utworzyc-ingress","title":"Utworzy\u0107 Ingress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\nspec:\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-service\n            port:\n              number: 80\n</code></pre>"},{"location":"Kubernetes/PL/04-services/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Serwisy : Stabilne punkty dost\u0119pu</li> <li>Typy : ClusterIP, NodePort, LoadBalancer</li> <li>Service Discovery : Po nazwie DNS</li> <li>Ingress : Routing HTTP/HTTPS</li> </ol>"},{"location":"Kubernetes/PL/04-services/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 5. ConfigMaps i Secrety.</p>"},{"location":"Kubernetes/PL/05-configmaps-secrets/","title":"5. ConfigMaps i Secrety","text":""},{"location":"Kubernetes/PL/05-configmaps-secrets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zarz\u0105dza\u0107 konfiguracj\u0105 z ConfigMapami</li> <li>Zarz\u0105dza\u0107 secretami</li> <li>Zmienne \u015brodowiskowe</li> <li>Dobre praktyki</li> </ul>"},{"location":"Kubernetes/PL/05-configmaps-secrets/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>ConfigMaps</li> <li>Secrety</li> <li>U\u017cycie</li> <li>Dobre praktyki</li> </ol>"},{"location":"Kubernetes/PL/05-configmaps-secrets/#configmaps","title":"ConfigMaps","text":""},{"location":"Kubernetes/PL/05-configmaps-secrets/#utworzyc-configmap","title":"Utworzy\u0107 ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-config\ndata:\n  config.yaml: |\n    database:\n      host: localhost\n      port: 5432\n  app.properties: |\n    debug=true\n    log_level=info\n</code></pre> <pre><code>kubectl apply -f configmap.yaml\n</code></pre>"},{"location":"Kubernetes/PL/05-configmaps-secrets/#uzywac-w-podzie","title":"U\u017cywa\u0107 w Podzie","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    envFrom:\n    - configMapRef:\n        name: my-config\n</code></pre>"},{"location":"Kubernetes/PL/05-configmaps-secrets/#secrety","title":"Secrety","text":""},{"location":"Kubernetes/PL/05-configmaps-secrets/#utworzyc-secret","title":"Utworzy\u0107 Secret","text":"<pre><code># Utworzy\u0107 z linii polece\u0144\nkubectl create secret generic my-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=secret123\n</code></pre>"},{"location":"Kubernetes/PL/05-configmaps-secrets/#uzywac-w-podzie_1","title":"U\u017cywa\u0107 w Podzie","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-secret\n          key: password\n</code></pre>"},{"location":"Kubernetes/PL/05-configmaps-secrets/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>ConfigMaps : Konfiguracja niewra\u017cliwa</li> <li>Secrety : Dane wra\u017cliwe</li> <li>Zmienne \u015brodowiskowe : Wstrzykiwanie do Pod\u00f3w</li> <li>Bezpiecze\u0144stwo : Nie committowa\u0107 secret\u00f3w</li> </ol>"},{"location":"Kubernetes/PL/05-configmaps-secrets/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 6. Persistent Volumes.</p>"},{"location":"Kubernetes/PL/06-persistent-volumes/","title":"6. Persistent Volumes","text":""},{"location":"Kubernetes/PL/06-persistent-volumes/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Wolumeny Kubernetes</li> <li>Persistent Volumes i Claims</li> <li>Storage Classes</li> <li>StatefulSety</li> </ul>"},{"location":"Kubernetes/PL/06-persistent-volumes/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wolumeny</li> <li>Persistent Volumes</li> <li>Storage Classes</li> <li>StatefulSety</li> </ol>"},{"location":"Kubernetes/PL/06-persistent-volumes/#wolumeny","title":"Wolumeny","text":""},{"location":"Kubernetes/PL/06-persistent-volumes/#typy-wolumenow","title":"Typy wolumen\u00f3w","text":"<ul> <li>emptyDir : Tymczasowy</li> <li>hostPath : Katalog hosta</li> <li>PersistentVolume : Magazyn trwa\u0142y</li> </ul>"},{"location":"Kubernetes/PL/06-persistent-volumes/#persistent-volumes","title":"Persistent Volumes","text":""},{"location":"Kubernetes/PL/06-persistent-volumes/#utworzyc-persistentvolume","title":"Utworzy\u0107 PersistentVolume","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /data\n</code></pre>"},{"location":"Kubernetes/PL/06-persistent-volumes/#persistentvolumeclaim","title":"PersistentVolumeClaim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre>"},{"location":"Kubernetes/PL/06-persistent-volumes/#uzywac-w-podzie","title":"U\u017cywa\u0107 w Podzie","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: app\n    image: nginx\n    volumeMounts:\n    - name: storage\n      mountPath: /data\n  volumes:\n  - name: storage\n    persistentVolumeClaim:\n      claimName: my-pvc\n</code></pre>"},{"location":"Kubernetes/PL/06-persistent-volumes/#storage-classes","title":"Storage Classes","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-ssd\nprovisioner: kubernetes.io/gce-pd\nparameters:\n  type: pd-ssd\n</code></pre>"},{"location":"Kubernetes/PL/06-persistent-volumes/#statefulsety","title":"StatefulSety","text":"<pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: \"nginx\"\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre>"},{"location":"Kubernetes/PL/06-persistent-volumes/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Wolumeny : Magazyn tymczasowy</li> <li>Persistent Volumes : Magazyn trwa\u0142y</li> <li>Storage Classes : Provisioning dynamiczny</li> <li>StatefulSety : Aplikacje stateful</li> </ol>"},{"location":"Kubernetes/PL/06-persistent-volumes/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 7. Dobre praktyki.</p>"},{"location":"Kubernetes/PL/07-best-practices/","title":"7. Dobre praktyki Kubernetes","text":""},{"location":"Kubernetes/PL/07-best-practices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Bezpiecze\u0144stwo</li> <li>Wydajno\u015b\u0107</li> <li>Organizacja</li> <li>Monitorowanie</li> </ul>"},{"location":"Kubernetes/PL/07-best-practices/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Bezpiecze\u0144stwo</li> <li>Wydajno\u015b\u0107</li> <li>Organizacja</li> <li>Monitorowanie</li> </ol>"},{"location":"Kubernetes/PL/07-best-practices/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":""},{"location":"Kubernetes/PL/07-best-practices/#limity-zasobow","title":"Limity zasob\u00f3w","text":"<pre><code>resources:\n  requests:\n    memory: \"64Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n</code></pre>"},{"location":"Kubernetes/PL/07-best-practices/#security-context","title":"Security Context","text":"<pre><code>securityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  fsGroup: 2000\n</code></pre>"},{"location":"Kubernetes/PL/07-best-practices/#wydajnosc","title":"Wydajno\u015b\u0107","text":""},{"location":"Kubernetes/PL/07-best-practices/#zadania-zasobow","title":"\u017b\u0105dania zasob\u00f3w","text":"<pre><code>resources:\n  requests:\n    memory: \"256Mi\"\n    cpu: \"100m\"\n</code></pre>"},{"location":"Kubernetes/PL/07-best-practices/#reguy-affinity","title":"Regu\u0142y Affinity","text":"<pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: disktype\n          operator: In\n          values:\n          - ssd\n</code></pre>"},{"location":"Kubernetes/PL/07-best-practices/#organizacja","title":"Organizacja","text":""},{"location":"Kubernetes/PL/07-best-practices/#etykiety","title":"Etykiety","text":"<pre><code>metadata:\n  labels:\n    app: my-app\n    version: v1\n    environment: production\n</code></pre>"},{"location":"Kubernetes/PL/07-best-practices/#namespacey","title":"Namespace'y","text":"<pre><code>kubectl create namespace production\nkubectl create namespace development\n</code></pre>"},{"location":"Kubernetes/PL/07-best-practices/#monitorowanie","title":"Monitorowanie","text":""},{"location":"Kubernetes/PL/07-best-practices/#health-checks","title":"Health Checks","text":"<pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n</code></pre>"},{"location":"Kubernetes/PL/07-best-practices/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Bezpiecze\u0144stwo : Limity zasob\u00f3w, security context</li> <li>Wydajno\u015b\u0107 : \u017b\u0105dania zasob\u00f3w</li> <li>Organizacja : Etykiety, namespace'y</li> <li>Monitorowanie : Health checks</li> </ol>"},{"location":"Kubernetes/PL/07-best-practices/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 8. Projekty praktyczne.</p>"},{"location":"Kubernetes/PL/08-projets/","title":"8. Projekty praktyczne Kubernetes","text":""},{"location":"Kubernetes/PL/08-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Wdro\u017cy\u0107 aplikacj\u0119 web</li> <li>Pipeline danych z Kubernetes</li> <li>Kompletny stack</li> <li>Projekty do portfolio</li> </ul>"},{"location":"Kubernetes/PL/08-projets/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Projekt 1 : Aplikacja web</li> <li>Projekt 2 : Pipeline danych</li> <li>Projekt 3 : Kompletny stack</li> </ol>"},{"location":"Kubernetes/PL/08-projets/#projekt-1-aplikacja-web","title":"Projekt 1 : Aplikacja web","text":""},{"location":"Kubernetes/PL/08-projets/#deployment","title":"Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: app\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"Kubernetes/PL/08-projets/#serwis","title":"Serwis","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\nspec:\n  selector:\n    app: web\n  ports:\n  - port: 80\n    targetPort: 80\n  type: LoadBalancer\n</code></pre>"},{"location":"Kubernetes/PL/08-projets/#projekt-2-pipeline-danych","title":"Projekt 2 : Pipeline danych","text":""},{"location":"Kubernetes/PL/08-projets/#deployment-z-configmap","title":"Deployment z ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: pipeline-config\ndata:\n  config.yaml: |\n    input_path: /data/input\n    output_path: /data/output\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: data-pipeline\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: pipeline\n  template:\n    metadata:\n      labels:\n        app: pipeline\n    spec:\n      containers:\n      - name: pipeline\n        image: python:3.11\n        command: [\"python\", \"pipeline.py\"]\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: config\n        configMap:\n          name: pipeline-config\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-pvc\n</code></pre>"},{"location":"Kubernetes/PL/08-projets/#projekt-3-kompletny-stack","title":"Projekt 3 : Kompletny stack","text":""},{"location":"Kubernetes/PL/08-projets/#aplikacja-baza-danych","title":"Aplikacja + Baza danych","text":"<pre><code># Deployment app\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: app\n        image: myapp:latest\n        env:\n        - name: DB_HOST\n          value: \"db-service\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: password\n\n---\n# Serwis app\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-service\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n\n---\n# Deployment DB\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: db\nspec:\n  serviceName: db-service\n  replicas: 1\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:15\n        volumeMounts:\n        - name: db-storage\n          mountPath: /var/lib/postgresql/data\n  volumeClaimTemplates:\n  - metadata:\n      name: db-storage\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre>"},{"location":"Kubernetes/PL/08-projets/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Deploymenty : Zarz\u0105dzaj\u0105 aplikacjami</li> <li>Serwisy : Punkty dost\u0119pu</li> <li>ConfigMaps/Secrety : Konfiguracja</li> <li>Wolumeny : Trwa\u0142o\u015b\u0107</li> <li>Portfolio : Projekty demonstrowalne</li> </ol>"},{"location":"Kubernetes/PL/08-projets/#zasoby","title":"\ud83d\udd17 Zasoby","text":"<ul> <li>Przyk\u0142ady Kubernetes</li> <li>Dokumentacja Kubernetes</li> </ul> <p>Gratulacje ! Uko\u0144czy\u0142e\u015b szkolenie Kubernetes.</p>"},{"location":"MongoDB/EN/","title":"MongoDB Training for Data Analyst","text":""},{"location":"MongoDB/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through learning MongoDB as a Data Analyst. MongoDB is a NoSQL document-oriented database, ideal for managing unstructured and semi-structured data.</p>"},{"location":"MongoDB/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand MongoDB and NoSQL</li> <li>Install MongoDB</li> <li>Master CRUD operations</li> <li>Use queries and aggregations</li> <li>Optimize with indexes</li> <li>Model data</li> <li>Integrate MongoDB into your workflows</li> <li>Create practical projects for your portfolio</li> </ul>"},{"location":"MongoDB/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 MongoDB Community Server : Free and open-source - \u2705 MongoDB Compass : Free graphical interface - \u2705 MongoDB Atlas : Free cluster (512 MB) - \u2705 Official Documentation : Complete free guides - \u2705 Online Tutorials : Free resources</p> <p>Total Budget: $0</p>"},{"location":"MongoDB/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"MongoDB/EN/#1-mongodb-getting-started","title":"1. MongoDB Getting Started","text":"<ul> <li>Install MongoDB</li> <li>Basic concepts</li> <li>First operations</li> <li>MongoDB Compass interface</li> </ul>"},{"location":"MongoDB/EN/#2-basic-operations","title":"2. Basic Operations","text":"<ul> <li>CRUD (Create, Read, Update, Delete)</li> <li>Collections and Documents</li> <li>Data types</li> <li>Query operators</li> </ul>"},{"location":"MongoDB/EN/#3-queries-and-aggregation","title":"3. Queries and Aggregation","text":"<ul> <li>Advanced queries</li> <li>Aggregation pipeline</li> <li>Aggregation operators</li> <li>Grouping and calculations</li> </ul>"},{"location":"MongoDB/EN/#4-indexes-and-performance","title":"4. Indexes and Performance","text":"<ul> <li>Create indexes</li> <li>Index types</li> <li>Performance analysis</li> <li>Query optimization</li> </ul>"},{"location":"MongoDB/EN/#5-data-modeling","title":"5. Data Modeling","text":"<ul> <li>Data models</li> <li>Relations (Embedded vs References)</li> <li>Flexible schemas</li> <li>Best practices</li> </ul>"},{"location":"MongoDB/EN/#6-advanced-features","title":"6. Advanced Features","text":"<ul> <li>Transactions</li> <li>Replication</li> <li>Sharding</li> <li>Text Search</li> </ul>"},{"location":"MongoDB/EN/#7-best-practices","title":"7. Best Practices","text":"<ul> <li>Security</li> <li>Performance</li> <li>Maintenance</li> <li>Backup and Restore</li> </ul>"},{"location":"MongoDB/EN/#8-practical-projects","title":"8. Practical Projects","text":"<ul> <li>Python application with MongoDB</li> <li>Data pipeline</li> <li>Data analysis</li> <li>Portfolio projects</li> </ul>"},{"location":"MongoDB/EN/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"MongoDB/EN/#prerequisites","title":"Prerequisites","text":"<ul> <li>Operating System : Windows, Linux, or macOS</li> <li>4 GB RAM : Minimum recommended</li> <li>Disk Space : 5 GB free</li> </ul>"},{"location":"MongoDB/EN/#quick-installation","title":"Quick Installation","text":"<p>Windows: 1. Download MongoDB: https://www.mongodb.com/try/download/community 2. Install with default options 3. Verify: <code>mongod --version</code></p> <p>Linux: <pre><code>wget -qO - https://www.mongodb.org/static/pgp/server-7.0.asc | sudo apt-key add -\necho \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org\nsudo systemctl start mongod\n</code></pre></p> <p>macOS: <pre><code>brew tap mongodb/brew\nbrew install mongodb-community\nbrew services start mongodb-community\n</code></pre></p>"},{"location":"MongoDB/EN/#first-test","title":"First Test","text":"<pre><code>mongod\nmongosh\nuse test\ndb.collection.insertOne({name: \"test\"})\ndb.collection.find()\n</code></pre>"},{"location":"MongoDB/EN/#use-cases-for-data-analyst","title":"\ud83d\udcca Use Cases for Data Analyst","text":"<ul> <li>Unstructured data : JSON, logs, APIs</li> <li>Flexibility : Evolving schemas</li> <li>Aggregation : Powerful pipeline for analysis</li> <li>Integration : With Python, R, PowerBI</li> <li>Big Data : Horizontal scalability</li> </ul>"},{"location":"MongoDB/EN/#free-resources","title":"\ud83d\udcda Free Resources","text":""},{"location":"MongoDB/EN/#official-documentation","title":"Official Documentation","text":"<ul> <li>MongoDB Documentation : https://docs.mongodb.com/</li> <li>MongoDB University : https://university.mongodb.com/</li> <li>MongoDB Compass : https://www.mongodb.com/products/compass</li> </ul>"},{"location":"MongoDB/EN/01-getting-started/","title":"1. MongoDB Getting Started","text":""},{"location":"MongoDB/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand MongoDB and NoSQL</li> <li>Install MongoDB</li> <li>Understand basic concepts</li> <li>Use MongoDB Compass</li> <li>First operations</li> </ul>"},{"location":"MongoDB/EN/01-getting-started/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to MongoDB</li> <li>Installation</li> <li>Basic Concepts</li> <li>MongoDB Compass</li> <li>First Operations</li> </ol>"},{"location":"MongoDB/EN/01-getting-started/#introduction-to-mongodb","title":"Introduction to MongoDB","text":""},{"location":"MongoDB/EN/01-getting-started/#what-is-mongodb","title":"What is MongoDB?","text":"<p>MongoDB = NoSQL document-oriented database</p> <ul> <li>NoSQL : Non-relational</li> <li>Documents : Storage in JSON format (BSON)</li> <li>Flexible : Evolving schemas</li> <li>Scalable : Horizontal scalability</li> <li>Open-source : Free and open-source</li> </ul>"},{"location":"MongoDB/EN/01-getting-started/#why-mongodb-for-data-analyst","title":"Why MongoDB for Data Analyst?","text":"<ul> <li>Unstructured data : JSON, logs, APIs</li> <li>Flexibility : Evolving schemas</li> <li>Aggregation : Powerful pipeline for analysis</li> <li>Integration : With Python, R, PowerBI</li> <li>Performance : Fast for complex queries</li> </ul>"},{"location":"MongoDB/EN/01-getting-started/#installation","title":"Installation","text":""},{"location":"MongoDB/EN/01-getting-started/#windows","title":"Windows","text":"<ol> <li>Go to: https://www.mongodb.com/try/download/community</li> <li>Select Windows</li> <li>Download MSI installer</li> <li>Run installer</li> <li>Choose \"Complete\" installation</li> </ol>"},{"location":"MongoDB/EN/01-getting-started/#linux","title":"Linux","text":"<p>Ubuntu/Debian: <pre><code>wget -qO - https://www.mongodb.org/static/pgp/server-7.0.asc | sudo apt-key add -\necho \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org\nsudo systemctl start mongod\n</code></pre></p>"},{"location":"MongoDB/EN/01-getting-started/#macos","title":"macOS","text":"<p>With Homebrew: <pre><code>brew tap mongodb/brew\nbrew install mongodb-community\nbrew services start mongodb-community\n</code></pre></p>"},{"location":"MongoDB/EN/01-getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"MongoDB/EN/01-getting-started/#database","title":"Database","text":"<p>Database = Container of collections</p> <ul> <li>Auto-creation : Created on first use</li> <li>Name : Unique identifier</li> <li>Collections : Contains collections</li> </ul>"},{"location":"MongoDB/EN/01-getting-started/#collection","title":"Collection","text":"<p>Collection = Group of documents</p> <ul> <li>Equivalent : Table in SQL</li> <li>Flexible : No imposed schema</li> <li>Documents : Contains documents</li> </ul>"},{"location":"MongoDB/EN/01-getting-started/#document","title":"Document","text":"<p>Document = Record in JSON format</p> <ul> <li>Format : BSON (Binary JSON)</li> <li>Flexible : Variable structure</li> <li>Fields : Key-value pairs</li> </ul>"},{"location":"MongoDB/EN/01-getting-started/#mongodb-compass","title":"MongoDB Compass","text":""},{"location":"MongoDB/EN/01-getting-started/#what-is-compass","title":"What is Compass?","text":"<p>MongoDB Compass = Graphical interface</p> <ul> <li>Visualization : View data</li> <li>Queries : Execute queries</li> <li>Analysis : Analyze performance</li> <li>Management : Manage indexes</li> </ul>"},{"location":"MongoDB/EN/01-getting-started/#installation_1","title":"Installation","text":"<ol> <li>Download: https://www.mongodb.com/products/compass</li> <li>Install</li> <li>Launch Compass</li> <li>Connect to <code>mongodb://localhost:27017</code></li> </ol>"},{"location":"MongoDB/EN/01-getting-started/#first-operations","title":"First Operations","text":""},{"location":"MongoDB/EN/01-getting-started/#connect-with-mongosh","title":"Connect with mongosh","text":"<pre><code># Launch mongosh\nmongosh\n\n# See databases\nshow dbs\n\n# Use a database\nuse mydb\n\n# See collections\nshow collections\n\n# Insert a document\ndb.users.insertOne({name: \"John\", age: 30, city: \"Paris\"})\n\n# Find documents\ndb.users.find()\n</code></pre>"},{"location":"MongoDB/EN/01-getting-started/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>MongoDB = NoSQL document-oriented database</li> <li>Documents = JSON format (BSON)</li> <li>Collections = Groups of documents</li> <li>Databases = Containers of collections</li> <li>Compass = Graphical interface</li> </ol>"},{"location":"MongoDB/EN/01-getting-started/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 2. Basic Operations to master CRUD.</p>"},{"location":"MongoDB/EN/02-basic-operations/","title":"2. MongoDB Basic Operations","text":""},{"location":"MongoDB/EN/02-basic-operations/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Master CRUD operations</li> <li>Understand collections and documents</li> <li>Use data types</li> <li>Apply query operators</li> <li>Manage updates</li> </ul>"},{"location":"MongoDB/EN/02-basic-operations/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>CRUD Operations</li> <li>Collections and Documents</li> <li>Data Types</li> <li>Query Operators</li> <li>Updates</li> </ol>"},{"location":"MongoDB/EN/02-basic-operations/#crud-operations","title":"CRUD Operations","text":""},{"location":"MongoDB/EN/02-basic-operations/#create","title":"Create","text":"<pre><code>// Insert one document\ndb.users.insertOne({\n  name: \"John\",\n  age: 30,\n  email: \"john@example.com\"\n})\n\n// Insert many documents\ndb.users.insertMany([\n  {name: \"Alice\", age: 28},\n  {name: \"Bob\", age: 32}\n])\n</code></pre>"},{"location":"MongoDB/EN/02-basic-operations/#read","title":"Read","text":"<pre><code>// Find all documents\ndb.users.find()\n\n// Find with filter\ndb.users.find({age: 30})\n\n// Find one document\ndb.users.findOne({name: \"John\"})\n\n// Limit results\ndb.users.find().limit(5)\n\n// Sort\ndb.users.find().sort({age: 1})\n</code></pre>"},{"location":"MongoDB/EN/02-basic-operations/#update","title":"Update","text":"<pre><code>// Update one document\ndb.users.updateOne(\n  {name: \"John\"},\n  {$set: {age: 31}}\n)\n\n// Update many documents\ndb.users.updateMany(\n  {age: {$lt: 30}},\n  {$set: {status: \"young\"}}\n)\n</code></pre>"},{"location":"MongoDB/EN/02-basic-operations/#delete","title":"Delete","text":"<pre><code>// Delete one document\ndb.users.deleteOne({name: \"John\"})\n\n// Delete many documents\ndb.users.deleteMany({age: {$lt: 18}})\n</code></pre>"},{"location":"MongoDB/EN/02-basic-operations/#data-types","title":"Data Types","text":""},{"location":"MongoDB/EN/02-basic-operations/#basic-types","title":"Basic Types","text":"<pre><code>// String\n{name: \"John\"}\n\n// Number\n{age: 30}\n{price: 99.99}\n\n// Boolean\n{active: true}\n\n// Date\n{created_at: new Date()}\n\n// Array\n{hobbies: [\"reading\", \"coding\"]}\n\n// Object (Embedded)\n{address: {street: \"123 Main St\", city: \"Paris\"}}\n</code></pre>"},{"location":"MongoDB/EN/02-basic-operations/#query-operators","title":"Query Operators","text":""},{"location":"MongoDB/EN/02-basic-operations/#comparison-operators","title":"Comparison Operators","text":"<pre><code>// Equal\ndb.users.find({age: 30})\n\n// Greater than\ndb.users.find({age: {$gt: 30}})\n\n// Less than\ndb.users.find({age: {$lt: 30}})\n\n// In a list\ndb.users.find({age: {$in: [25, 30, 35]}})\n</code></pre>"},{"location":"MongoDB/EN/02-basic-operations/#logical-operators","title":"Logical Operators","text":"<pre><code>// AND\ndb.users.find({\n  $and: [\n    {age: {$gt: 25}},\n    {age: {$lt: 35}}\n  ]\n})\n\n// OR\ndb.users.find({\n  $or: [\n    {age: {$lt: 25}},\n    {age: {$gt: 35}}\n  ]\n})\n</code></pre>"},{"location":"MongoDB/EN/02-basic-operations/#updates","title":"Updates","text":""},{"location":"MongoDB/EN/02-basic-operations/#update-operators","title":"Update Operators","text":"<pre><code>// $set: Set a field\ndb.users.updateOne(\n  {name: \"John\"},\n  {$set: {age: 31}}\n)\n\n// $inc: Increment\ndb.users.updateOne(\n  {name: \"John\"},\n  {$inc: {age: 1}}\n)\n\n// $push: Add to array\ndb.users.updateOne(\n  {name: \"John\"},\n  {$push: {hobbies: \"swimming\"}}\n)\n</code></pre>"},{"location":"MongoDB/EN/02-basic-operations/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>CRUD : Create, Read, Update, Delete</li> <li>Documents : Flexible JSON format</li> <li>Collections : Groups of documents</li> <li>Operators : For filtering and updating</li> <li>Types : Various data types supported</li> </ol>"},{"location":"MongoDB/EN/02-basic-operations/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 3. Queries and Aggregation for advanced queries.</p>"},{"location":"MongoDB/EN/03-queries-aggregation/","title":"3. MongoDB Queries and Aggregation","text":""},{"location":"MongoDB/EN/03-queries-aggregation/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Master advanced queries</li> <li>Use aggregation pipeline</li> <li>Apply aggregation operators</li> <li>Perform grouping and calculations</li> <li>Analyze complex data</li> </ul>"},{"location":"MongoDB/EN/03-queries-aggregation/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Advanced Queries</li> <li>Aggregation Pipeline</li> <li>Aggregation Operators</li> <li>Grouping and Calculations</li> <li>Practical Examples</li> </ol>"},{"location":"MongoDB/EN/03-queries-aggregation/#advanced-queries","title":"Advanced Queries","text":""},{"location":"MongoDB/EN/03-queries-aggregation/#projection","title":"Projection","text":"<pre><code>// Select specific fields\ndb.users.find({}, {name: 1, email: 1, _id: 0})\n</code></pre>"},{"location":"MongoDB/EN/03-queries-aggregation/#sort-and-limit","title":"Sort and Limit","text":"<pre><code>// Sort by age (ascending)\ndb.users.find().sort({age: 1})\n\n// Limit results\ndb.users.find().limit(10)\n</code></pre>"},{"location":"MongoDB/EN/03-queries-aggregation/#aggregation-pipeline","title":"Aggregation Pipeline","text":""},{"location":"MongoDB/EN/03-queries-aggregation/#what-is-a-pipeline","title":"What is a Pipeline?","text":"<p>Pipeline = Series of transformation steps</p> <ul> <li>Steps : Each step transforms data</li> <li>Sequential : Result of one step = input of next</li> <li>Powerful : For complex analysis</li> </ul>"},{"location":"MongoDB/EN/03-queries-aggregation/#basic-structure","title":"Basic Structure","text":"<pre><code>db.collection.aggregate([\n  { $match: { ... } },      // Filter\n  { $group: { ... } },       // Group\n  { $sort: { ... } },        // Sort\n  { $project: { ... } }      // Select\n])\n</code></pre>"},{"location":"MongoDB/EN/03-queries-aggregation/#aggregation-operators","title":"Aggregation Operators","text":""},{"location":"MongoDB/EN/03-queries-aggregation/#match","title":"$match","text":"<pre><code>db.sales.aggregate([\n  {$match: {amount: {$gt: 500}}}\n])\n</code></pre>"},{"location":"MongoDB/EN/03-queries-aggregation/#group","title":"$group","text":"<pre><code>db.products.aggregate([\n  {\n    $group: {\n      _id: \"$category\",\n      total: {$sum: \"$price\"},\n      count: {$sum: 1},\n      average: {$avg: \"$price\"}\n    }\n  }\n])\n</code></pre>"},{"location":"MongoDB/EN/03-queries-aggregation/#project","title":"$project","text":"<pre><code>db.users.aggregate([\n  {\n    $project: {\n      name: 1,\n      age: 1,\n      isAdult: {$gte: [\"$age\", 18]}\n    }\n  }\n])\n</code></pre>"},{"location":"MongoDB/EN/03-queries-aggregation/#grouping-and-calculations","title":"Grouping and Calculations","text":""},{"location":"MongoDB/EN/03-queries-aggregation/#accumulation-operators","title":"Accumulation Operators","text":"<pre><code>// Sum\n{$sum: \"$amount\"}\n\n// Average\n{$avg: \"$price\"}\n\n// Minimum\n{$min: \"$price\"}\n\n// Maximum\n{$max: \"$price\"}\n\n// Count\n{$sum: 1}\n</code></pre>"},{"location":"MongoDB/EN/03-queries-aggregation/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Pipeline : Series of transformation steps</li> <li>$match : Filter documents</li> <li>$group : Group and calculate</li> <li>$project : Select and transform</li> <li>Aggregation : Powerful for analysis</li> </ol>"},{"location":"MongoDB/EN/03-queries-aggregation/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 4. Indexes and Performance to optimize performance.</p>"},{"location":"MongoDB/EN/04-indexes-performance/","title":"4. MongoDB Indexes and Performance","text":""},{"location":"MongoDB/EN/04-indexes-performance/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand indexes</li> <li>Create different index types</li> <li>Analyze performance</li> <li>Optimize queries</li> <li>Use explain()</li> </ul>"},{"location":"MongoDB/EN/04-indexes-performance/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction to Indexes</li> <li>Index Types</li> <li>Create Indexes</li> <li>Analyze Performance</li> <li>Optimization</li> </ol>"},{"location":"MongoDB/EN/04-indexes-performance/#introduction-to-indexes","title":"Introduction to Indexes","text":""},{"location":"MongoDB/EN/04-indexes-performance/#what-is-an-index","title":"What is an Index?","text":"<p>Index = Data structure to speed up queries</p> <ul> <li>Performance : Faster search</li> <li>Cost : Additional disk space</li> <li>Maintenance : Automatic updates</li> </ul>"},{"location":"MongoDB/EN/04-indexes-performance/#index-types","title":"Index Types","text":""},{"location":"MongoDB/EN/04-indexes-performance/#simple-index","title":"Simple Index","text":"<pre><code>// Index on one field\ndb.users.createIndex({email: 1})\n</code></pre>"},{"location":"MongoDB/EN/04-indexes-performance/#compound-index","title":"Compound Index","text":"<pre><code>// Index on multiple fields\ndb.users.createIndex({name: 1, age: -1})\n</code></pre>"},{"location":"MongoDB/EN/04-indexes-performance/#unique-index","title":"Unique Index","text":"<pre><code>// Ensure uniqueness\ndb.users.createIndex({email: 1}, {unique: true})\n</code></pre>"},{"location":"MongoDB/EN/04-indexes-performance/#create-indexes","title":"Create Indexes","text":""},{"location":"MongoDB/EN/04-indexes-performance/#creation-methods","title":"Creation Methods","text":"<pre><code>// Create an index\ndb.collection.createIndex({field: 1})\n\n// See indexes\ndb.collection.getIndexes()\n\n// Drop an index\ndb.collection.dropIndex({field: 1})\n</code></pre>"},{"location":"MongoDB/EN/04-indexes-performance/#analyze-performance","title":"Analyze Performance","text":""},{"location":"MongoDB/EN/04-indexes-performance/#explain","title":"explain()","text":"<pre><code>// Execution plan\ndb.users.find({email: \"john@example.com\"}).explain()\n\n// Detailed statistics\ndb.users.find({email: \"john@example.com\"}).explain(\"executionStats\")\n</code></pre>"},{"location":"MongoDB/EN/04-indexes-performance/#optimization","title":"Optimization","text":""},{"location":"MongoDB/EN/04-indexes-performance/#best-practices","title":"Best Practices","text":"<p>1. Index frequently searched fields:</p> <pre><code>db.users.createIndex({email: 1})\n</code></pre> <p>2. Compound index for multiple queries:</p> <pre><code>db.users.createIndex({name: 1, age: 1})\n</code></pre> <p>3. Avoid too many indexes:</p> <ul> <li>Each index slows writes</li> <li>Use only necessary indexes</li> </ul>"},{"location":"MongoDB/EN/04-indexes-performance/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Indexes : Speed up searches</li> <li>Types : Simple, compound, unique</li> <li>explain() : Analyze performance</li> <li>Optimization : Index frequent fields</li> <li>Balance : Not too many indexes</li> </ol>"},{"location":"MongoDB/EN/04-indexes-performance/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 5. Data Modeling to learn modeling.</p>"},{"location":"MongoDB/EN/05-data-modeling/","title":"5. MongoDB Data Modeling","text":""},{"location":"MongoDB/EN/05-data-modeling/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand data models</li> <li>Choose between Embedded and References</li> <li>Design flexible schemas</li> <li>Apply best practices</li> <li>Optimize structure</li> </ul>"},{"location":"MongoDB/EN/05-data-modeling/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Data Models</li> <li>Embedded vs References</li> <li>Flexible Schemas</li> <li>Best Practices</li> <li>Practical Examples</li> </ol>"},{"location":"MongoDB/EN/05-data-modeling/#data-models","title":"Data Models","text":""},{"location":"MongoDB/EN/05-data-modeling/#embedded-model","title":"Embedded Model","text":"<p>Everything in one document:</p> <pre><code>{\n  _id: ObjectId(\"...\"),\n  name: \"John\",\n  address: {\n    street: \"123 Main St\",\n    city: \"Paris\"\n  }\n}\n</code></pre> <p>Advantages: - Fast access (single document) - No joins - Consistent data</p>"},{"location":"MongoDB/EN/05-data-modeling/#references-model","title":"References Model","text":"<p>Separate documents with references:</p> <pre><code>// Collection users\n{_id: ObjectId(\"...\"), name: \"John\"}\n\n// Collection addresses\n{_id: ObjectId(\"...\"), user_id: ObjectId(\"...\"), street: \"123 Main St\"}\n</code></pre> <p>Advantages: - No size limit - No duplication - Flexibility</p>"},{"location":"MongoDB/EN/05-data-modeling/#embedded-vs-references","title":"Embedded vs References","text":""},{"location":"MongoDB/EN/05-data-modeling/#when-to-use-embedded","title":"When to Use Embedded?","text":"<p>Use cases: - Data often accessed together - Small amounts of data - 1:1 or 1:few relationship - Data that rarely changes</p>"},{"location":"MongoDB/EN/05-data-modeling/#when-to-use-references","title":"When to Use References?","text":"<p>Use cases: - Large amounts of data - 1:many or many:many relationship - Shared data - Data that changes often</p>"},{"location":"MongoDB/EN/05-data-modeling/#flexible-schemas","title":"Flexible Schemas","text":""},{"location":"MongoDB/EN/05-data-modeling/#schema-evolution","title":"Schema Evolution","text":"<pre><code>// Initial document\n{name: \"John\", age: 30}\n\n// Add field later\n{name: \"John\", age: 30, email: \"john@example.com\"}\n</code></pre>"},{"location":"MongoDB/EN/05-data-modeling/#best-practices","title":"Best Practices","text":""},{"location":"MongoDB/EN/05-data-modeling/#modeling-patterns","title":"Modeling Patterns","text":"<p>One-to-Few: <pre><code>// Embedded\n{name: \"John\", addresses: [{street: \"123 Main St\"}]}\n</code></pre></p> <p>One-to-Many: <pre><code>// References\n// Collection users\n{_id: ObjectId(\"...\"), name: \"John\"}\n\n// Collection orders\n{user_id: ObjectId(\"...\"), items: [...]}\n</code></pre></p>"},{"location":"MongoDB/EN/05-data-modeling/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Embedded : For data often accessed together</li> <li>References : For large amounts or complex relations</li> <li>Flexibility : Evolving schemas</li> <li>Patterns : One-to-Few, One-to-Many, Many-to-Many</li> <li>Performance : Balance access and consistency</li> </ol>"},{"location":"MongoDB/EN/05-data-modeling/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 6. Advanced Features to deepen.</p>"},{"location":"MongoDB/EN/06-advanced/","title":"6. MongoDB Advanced Features","text":""},{"location":"MongoDB/EN/06-advanced/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Use transactions</li> <li>Understand replication</li> <li>Master sharding</li> <li>Use text search</li> <li>Advanced features</li> </ul>"},{"location":"MongoDB/EN/06-advanced/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Transactions</li> <li>Replication</li> <li>Sharding</li> <li>Text Search</li> <li>Other Features</li> </ol>"},{"location":"MongoDB/EN/06-advanced/#transactions","title":"Transactions","text":""},{"location":"MongoDB/EN/06-advanced/#what-is-a-transaction","title":"What is a Transaction?","text":"<p>Transaction = Group of atomic operations</p> <ul> <li>Atomic : All or nothing</li> <li>Consistency : Consistent data</li> <li>Isolation : Isolated operations</li> <li>Durability : Persistent changes</li> </ul>"},{"location":"MongoDB/EN/06-advanced/#use-transactions","title":"Use Transactions","text":"<pre><code>const session = db.getMongo().startSession()\nsession.startTransaction()\n\ntry {\n  db.users.insertOne({name: \"John\"}, {session})\n  db.orders.insertOne({user_id: \"...\", items: [...]}, {session})\n  session.commitTransaction()\n} catch (error) {\n  session.abortTransaction()\n} finally {\n  session.endSession()\n}\n</code></pre>"},{"location":"MongoDB/EN/06-advanced/#replication","title":"Replication","text":""},{"location":"MongoDB/EN/06-advanced/#what-is-replication","title":"What is Replication?","text":"<p>Replication = Multiple copies of data</p> <ul> <li>High availability : No single point of failure</li> <li>Redundancy : Automatic backup</li> <li>Performance : Read from multiple servers</li> </ul>"},{"location":"MongoDB/EN/06-advanced/#sharding","title":"Sharding","text":""},{"location":"MongoDB/EN/06-advanced/#what-is-sharding","title":"What is Sharding?","text":"<p>Sharding = Horizontal partitioning</p> <ul> <li>Scalability : Distribute data</li> <li>Performance : Process in parallel</li> <li>Storage : More capacity</li> </ul>"},{"location":"MongoDB/EN/06-advanced/#text-search","title":"Text Search","text":""},{"location":"MongoDB/EN/06-advanced/#text-index","title":"Text Index","text":"<pre><code>// Create text index\ndb.articles.createIndex({\n  title: \"text\",\n  content: \"text\"\n})\n\n// Search\ndb.articles.find({\n  $text: {$search: \"mongodb tutorial\"}\n})\n</code></pre>"},{"location":"MongoDB/EN/06-advanced/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Transactions : Atomic operations</li> <li>Replication : High availability</li> <li>Sharding : Horizontal scalability</li> <li>Text Search : Text search</li> <li>Validation : Optional schemas</li> </ol>"},{"location":"MongoDB/EN/06-advanced/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 7. Best Practices for best practices.</p>"},{"location":"MongoDB/EN/07-best-practices/","title":"7. MongoDB Best Practices","text":""},{"location":"MongoDB/EN/07-best-practices/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Security</li> <li>Performance</li> <li>Maintenance</li> <li>Backup and Restore</li> <li>Monitoring</li> </ul>"},{"location":"MongoDB/EN/07-best-practices/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Security</li> <li>Performance</li> <li>Maintenance</li> <li>Backup and Restore</li> <li>Monitoring</li> </ol>"},{"location":"MongoDB/EN/07-best-practices/#security","title":"Security","text":""},{"location":"MongoDB/EN/07-best-practices/#authentication","title":"Authentication","text":"<pre><code>// Create admin user\nuse admin\ndb.createUser({\n  user: \"admin\",\n  pwd: \"secure_password\",\n  roles: [\"root\"]\n})\n</code></pre>"},{"location":"MongoDB/EN/07-best-practices/#secure-connection","title":"Secure Connection","text":"<pre><code>mongosh -u admin -p secure_password --authenticationDatabase admin\n</code></pre>"},{"location":"MongoDB/EN/07-best-practices/#performance","title":"Performance","text":""},{"location":"MongoDB/EN/07-best-practices/#indexes","title":"Indexes","text":"<pre><code>// Index frequently searched fields\ndb.users.createIndex({email: 1})\n\n// Compound index for multiple queries\ndb.orders.createIndex({customer: 1, date: -1})\n</code></pre>"},{"location":"MongoDB/EN/07-best-practices/#queries","title":"Queries","text":"<pre><code>// Use projection to limit data\ndb.users.find({}, {name: 1, email: 1})\n\n// Limit results\ndb.users.find().limit(100)\n</code></pre>"},{"location":"MongoDB/EN/07-best-practices/#maintenance","title":"Maintenance","text":""},{"location":"MongoDB/EN/07-best-practices/#cleanup","title":"Cleanup","text":"<pre><code>// Delete obsolete documents\ndb.logs.deleteMany({\n  created_at: {$lt: new Date(\"2024-01-01\")}\n})\n</code></pre>"},{"location":"MongoDB/EN/07-best-practices/#backup-and-restore","title":"Backup and Restore","text":""},{"location":"MongoDB/EN/07-best-practices/#backup-mongodump","title":"Backup (mongodump)","text":"<pre><code># Backup a database\nmongodump --db mydb --out /backup/\n</code></pre>"},{"location":"MongoDB/EN/07-best-practices/#restore-mongorestore","title":"Restore (mongorestore)","text":"<pre><code># Restore a database\nmongorestore --db mydb /backup/mydb/\n</code></pre>"},{"location":"MongoDB/EN/07-best-practices/#monitoring","title":"Monitoring","text":""},{"location":"MongoDB/EN/07-best-practices/#server-status","title":"Server Status","text":"<pre><code>// Server status\ndb.serverStatus()\n\n// Current operations\ndb.currentOp()\n</code></pre>"},{"location":"MongoDB/EN/07-best-practices/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Security : Authentication and authorization</li> <li>Performance : Indexes and optimized queries</li> <li>Maintenance : Regular cleanup</li> <li>Backup : Regular backups</li> <li>Monitoring : Monitor performance</li> </ol>"},{"location":"MongoDB/EN/07-best-practices/#next-module","title":"\ud83d\udd17 Next Module","text":"<p>Proceed to module 8. Practical Projects to create complete projects.</p>"},{"location":"MongoDB/EN/08-projets/","title":"8. MongoDB Practical Projects","text":""},{"location":"MongoDB/EN/08-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create Python application with MongoDB</li> <li>Data pipeline with MongoDB</li> <li>Data analysis</li> <li>Portfolio projects</li> </ul>"},{"location":"MongoDB/EN/08-projets/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Project 1 : Python Application</li> <li>Project 2 : Data Pipeline</li> <li>Project 3 : Data Analysis</li> <li>Project 4 : REST API</li> </ol>"},{"location":"MongoDB/EN/08-projets/#project-1-python-application","title":"Project 1 : Python Application","text":""},{"location":"MongoDB/EN/08-projets/#objective","title":"Objective","text":"<p>Create a Python application using MongoDB.</p>"},{"location":"MongoDB/EN/08-projets/#python-code","title":"Python Code","text":"<pre><code>from pymongo import MongoClient\nfrom datetime import datetime\n\n# Connection\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['mydb']\ncollection = db['users']\n\n# Insert user\ndef create_user(name, email):\n    user = {\n        'name': name,\n        'email': email,\n        'created_at': datetime.now()\n    }\n    result = collection.insert_one(user)\n    return result.inserted_id\n\n# Find user\ndef find_user(email):\n    return collection.find_one({'email': email})\n\n# Usage\ncreate_user('John', 'john@example.com')\nuser = find_user('john@example.com')\nprint(user)\n</code></pre>"},{"location":"MongoDB/EN/08-projets/#project-2-data-pipeline","title":"Project 2 : Data Pipeline","text":""},{"location":"MongoDB/EN/08-projets/#objective_1","title":"Objective","text":"<p>Create an ETL pipeline with MongoDB.</p>"},{"location":"MongoDB/EN/08-projets/#code","title":"Code","text":"<pre><code>from pymongo import MongoClient\nimport pandas as pd\n\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['etl_db']\n\n# Extract\ndef extract():\n    df = pd.read_csv('data.csv')\n    return df\n\n# Transform\ndef transform(df):\n    df['processed_at'] = pd.Timestamp.now()\n    df = df.dropna()\n    return df\n\n# Load\ndef load(df):\n    records = df.to_dict('records')\n    db.data.insert_many(records)\n\n# Complete pipeline\ndf = extract()\ndf = transform(df)\nload(df)\n</code></pre>"},{"location":"MongoDB/EN/08-projets/#project-3-data-analysis","title":"Project 3 : Data Analysis","text":""},{"location":"MongoDB/EN/08-projets/#objective_2","title":"Objective","text":"<p>Analyze data with MongoDB Aggregation.</p>"},{"location":"MongoDB/EN/08-projets/#code_1","title":"Code","text":"<pre><code>from pymongo import MongoClient\n\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['analytics_db']\n\n# Aggregation pipeline\npipeline = [\n    {'$match': {'date': {'$gte': datetime(2024, 1, 1)}}},\n    {'$group': {\n        '_id': '$product',\n        'total_sales': {'$sum': '$amount'},\n        'count': {'$sum': 1}\n    }},\n    {'$sort': {'total_sales': -1}},\n    {'$limit': 10}\n]\n\n# Execute\nresults = db.sales.aggregate(pipeline)\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"MongoDB/EN/08-projets/#project-4-rest-api","title":"Project 4 : REST API","text":""},{"location":"MongoDB/EN/08-projets/#objective_3","title":"Objective","text":"<p>Create a REST API with Flask and MongoDB.</p>"},{"location":"MongoDB/EN/08-projets/#code_2","title":"Code","text":"<pre><code>from flask import Flask, request, jsonify\nfrom pymongo import MongoClient\n\napp = Flask(__name__)\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['api_db']\ncollection = db['items']\n\n@app.route('/items', methods=['GET'])\ndef get_items():\n    items = list(collection.find({}, {'_id': 0}))\n    return jsonify(items)\n\n@app.route('/items', methods=['POST'])\ndef create_item():\n    data = request.json\n    result = collection.insert_one(data)\n    return jsonify({'id': str(result.inserted_id)})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"MongoDB/EN/08-projets/#key-takeaways","title":"\ud83d\udcca Key Takeaways","text":"<ol> <li>Python : Integration with pymongo</li> <li>Pipeline : ETL with MongoDB</li> <li>Aggregation : Data analysis</li> <li>API : REST with MongoDB</li> <li>Portfolio : Demonstrable projects</li> </ol>"},{"location":"MongoDB/EN/08-projets/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>PyMongo Documentation</li> <li>MongoDB Examples</li> </ul> <p>Congratulations! You have completed the MongoDB training. You can now use MongoDB in your Data Analyst projects.</p>"},{"location":"MongoDB/FR/","title":"Formation MongoDB pour Data Analyst","text":""},{"location":"MongoDB/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'apprentissage de MongoDB en tant que Data Analyst. MongoDB est une base de donn\u00e9es NoSQL orient\u00e9e documents, id\u00e9ale pour g\u00e9rer des donn\u00e9es non structur\u00e9es et semi-structur\u00e9es.</p>"},{"location":"MongoDB/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre MongoDB et NoSQL</li> <li>Installer MongoDB</li> <li>Ma\u00eetriser les op\u00e9rations CRUD</li> <li>Utiliser les requ\u00eates et agr\u00e9gations</li> <li>Optimiser avec les index</li> <li>Mod\u00e9liser les donn\u00e9es</li> <li>Int\u00e9grer MongoDB dans vos workflows</li> <li>Cr\u00e9er des projets pratiques pour votre portfolio</li> </ul>"},{"location":"MongoDB/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 MongoDB Community Server : Gratuit et open-source - \u2705 MongoDB Compass : Interface graphique gratuite - \u2705 MongoDB Atlas : Cluster gratuit (512 MB) - \u2705 Documentation officielle : Guides complets gratuits - \u2705 Tutoriels en ligne : Ressources gratuites</p> <p>Budget total : 0\u20ac</p>"},{"location":"MongoDB/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"MongoDB/FR/#1-prise-en-main-mongodb","title":"1. Prise en main MongoDB","text":"<ul> <li>Installer MongoDB</li> <li>Concepts de base</li> <li>Premi\u00e8res op\u00e9rations</li> <li>Interface MongoDB Compass</li> </ul>"},{"location":"MongoDB/FR/#2-operations-de-base","title":"2. Op\u00e9rations de base","text":"<ul> <li>CRUD (Create, Read, Update, Delete)</li> <li>Collections et Documents</li> <li>Types de donn\u00e9es</li> <li>Op\u00e9rateurs de requ\u00eate</li> </ul>"},{"location":"MongoDB/FR/#3-requetes-et-agregation","title":"3. Requ\u00eates et Agr\u00e9gation","text":"<ul> <li>Requ\u00eates avanc\u00e9es</li> <li>Pipeline d'agr\u00e9gation</li> <li>Op\u00e9rateurs d'agr\u00e9gation</li> <li>Groupement et calculs</li> </ul>"},{"location":"MongoDB/FR/#4-index-et-performance","title":"4. Index et Performance","text":"<ul> <li>Cr\u00e9er des index</li> <li>Types d'index</li> <li>Analyse de performance</li> <li>Optimisation des requ\u00eates</li> </ul>"},{"location":"MongoDB/FR/#5-modelisation-des-donnees","title":"5. Mod\u00e9lisation des donn\u00e9es","text":"<ul> <li>Mod\u00e8les de donn\u00e9es</li> <li>Relations (Embedded vs References)</li> <li>Sch\u00e9mas flexibles</li> <li>Bonnes pratiques</li> </ul>"},{"location":"MongoDB/FR/#6-fonctionnalites-avancees","title":"6. Fonctionnalit\u00e9s avanc\u00e9es","text":"<ul> <li>Transactions</li> <li>R\u00e9plication</li> <li>Sharding</li> <li>Text Search</li> </ul>"},{"location":"MongoDB/FR/#7-bonnes-pratiques","title":"7. Bonnes pratiques","text":"<ul> <li>S\u00e9curit\u00e9</li> <li>Performance</li> <li>Maintenance</li> <li>Backup et Restore</li> </ul>"},{"location":"MongoDB/FR/#8-projets-pratiques","title":"8. Projets pratiques","text":"<ul> <li>Application Python avec MongoDB</li> <li>Pipeline de donn\u00e9es</li> <li>Analyse de donn\u00e9es</li> <li>Projets pour portfolio</li> </ul>"},{"location":"MongoDB/FR/#demarrage-rapide","title":"\ud83d\ude80 D\u00e9marrage rapide","text":""},{"location":"MongoDB/FR/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Syst\u00e8me d'exploitation : Windows, Linux, ou macOS</li> <li>4 Go RAM : Minimum recommand\u00e9</li> <li>Espace disque : 5 Go libres</li> </ul>"},{"location":"MongoDB/FR/#installation-rapide","title":"Installation rapide","text":"<p>Windows : 1. T\u00e9l\u00e9charger MongoDB : https://www.mongodb.com/try/download/community 2. Installer avec les options par d\u00e9faut 3. V\u00e9rifier : <code>mongod --version</code></p> <p>Linux : <pre><code># Ubuntu/Debian\nwget -qO - https://www.mongodb.org/static/pgp/server-7.0.asc | sudo apt-key add -\necho \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org\n\n# D\u00e9marrer MongoDB\nsudo systemctl start mongod\nsudo systemctl enable mongod\n</code></pre></p> <p>macOS : <pre><code># Avec Homebrew\nbrew tap mongodb/brew\nbrew install mongodb-community\nbrew services start mongodb-community\n</code></pre></p>"},{"location":"MongoDB/FR/#premier-test","title":"Premier test","text":"<pre><code># D\u00e9marrer MongoDB\nmongod\n\n# Dans un autre terminal, se connecter\nmongosh\n\n# Tester\nuse test\ndb.collection.insertOne({name: \"test\"})\ndb.collection.find()\n</code></pre>"},{"location":"MongoDB/FR/#cas-dusage-pour-data-analyst","title":"\ud83d\udcca Cas d'usage pour Data Analyst","text":"<ul> <li>Donn\u00e9es non structur\u00e9es : JSON, logs, APIs</li> <li>Flexibilit\u00e9 : Sch\u00e9mas \u00e9volutifs</li> <li>Agr\u00e9gation : Pipeline puissant pour l'analyse</li> <li>Int\u00e9gration : Avec Python, R, PowerBI</li> <li>Big Data : Scalabilit\u00e9 horizontale</li> </ul>"},{"location":"MongoDB/FR/#ressources-gratuites","title":"\ud83d\udcda Ressources gratuites","text":""},{"location":"MongoDB/FR/#documentation-officielle","title":"Documentation officielle","text":"<ul> <li>MongoDB Documentation : https://docs.mongodb.com/</li> <li>MongoDB University : https://university.mongodb.com/ (cours gratuits)</li> <li>MongoDB Compass : https://www.mongodb.com/products/compass</li> </ul>"},{"location":"MongoDB/FR/#ressources-externes","title":"Ressources externes","text":"<ul> <li>MongoDB Atlas : Cluster gratuit 512 MB</li> <li>YouTube : Tutoriels MongoDB</li> <li>GitHub : Exemples MongoDB</li> </ul>"},{"location":"MongoDB/FR/#certifications-optionnel","title":"\ud83c\udf93 Certifications (optionnel)","text":""},{"location":"MongoDB/FR/#mongodb-certified-associate-developer","title":"MongoDB Certified Associate Developer","text":"<ul> <li>Co\u00fbt : ~$150</li> <li>Pr\u00e9paration : Documentation gratuite</li> <li>Dur\u00e9e : 1-2 mois</li> <li>Niveau : Interm\u00e9diaire</li> </ul>"},{"location":"MongoDB/FR/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>Tous les exemples utilisent MongoDB 7.0+</li> <li>Les commandes fonctionnent avec <code>mongosh</code> (nouvelle CLI)</li> <li>Python avec <code>pymongo</code> pour les exemples</li> <li>Les donn\u00e9es sont en format JSON</li> </ul>"},{"location":"MongoDB/FR/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>Cette formation est con\u00e7ue pour \u00eatre \u00e9volutive. N'h\u00e9sitez pas \u00e0 proposer des am\u00e9liorations.</p>"},{"location":"MongoDB/FR/#ressources-complementaires","title":"\ud83d\udcda Ressources compl\u00e9mentaires","text":"<ul> <li>MongoDB Documentation</li> <li>MongoDB University</li> <li>MongoDB Atlas</li> <li>PyMongo Documentation</li> </ul>"},{"location":"MongoDB/FR/01-getting-started/","title":"1. Prise en main MongoDB","text":""},{"location":"MongoDB/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre MongoDB et NoSQL</li> <li>Installer MongoDB</li> <li>Comprendre les concepts de base</li> <li>Utiliser MongoDB Compass</li> <li>Premi\u00e8res op\u00e9rations</li> </ul>"},{"location":"MongoDB/FR/01-getting-started/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 MongoDB</li> <li>Installation</li> <li>Concepts de base</li> <li>MongoDB Compass</li> <li>Premi\u00e8res op\u00e9rations</li> </ol>"},{"location":"MongoDB/FR/01-getting-started/#introduction-a-mongodb","title":"Introduction \u00e0 MongoDB","text":""},{"location":"MongoDB/FR/01-getting-started/#quest-ce-que-mongodb","title":"Qu'est-ce que MongoDB ?","text":"<p>MongoDB = Base de donn\u00e9es NoSQL orient\u00e9e documents</p> <ul> <li>NoSQL : Non relationnel</li> <li>Documents : Stockage en format JSON (BSON)</li> <li>Flexible : Sch\u00e9mas \u00e9volutifs</li> <li>Scalable : Scalabilit\u00e9 horizontale</li> <li>Open-source : Gratuit et open-source</li> </ul>"},{"location":"MongoDB/FR/01-getting-started/#pourquoi-mongodb-pour-data-analyst","title":"Pourquoi MongoDB pour Data Analyst ?","text":"<ul> <li>Donn\u00e9es non structur\u00e9es : JSON, logs, APIs</li> <li>Flexibilit\u00e9 : Sch\u00e9mas qui \u00e9voluent</li> <li>Agr\u00e9gation : Pipeline puissant pour l'analyse</li> <li>Int\u00e9gration : Avec Python, R, PowerBI</li> <li>Performance : Rapide pour les requ\u00eates complexes</li> </ul>"},{"location":"MongoDB/FR/01-getting-started/#mongodb-vs-sql","title":"MongoDB vs SQL","text":"<p>MongoDB (NoSQL) : - Documents JSON - Sch\u00e9ma flexible - Scalabilit\u00e9 horizontale - Id\u00e9al pour donn\u00e9es non structur\u00e9es</p> <p>SQL (Relationnel) : - Tables structur\u00e9es - Sch\u00e9ma fixe - Relations complexes - Id\u00e9al pour donn\u00e9es structur\u00e9es</p>"},{"location":"MongoDB/FR/01-getting-started/#installation","title":"Installation","text":""},{"location":"MongoDB/FR/01-getting-started/#windows","title":"Windows","text":"<p>\u00c9tape 1 : T\u00e9l\u00e9charger MongoDB</p> <ol> <li>Aller sur : https://www.mongodb.com/try/download/community</li> <li>S\u00e9lectionner Windows</li> <li>T\u00e9l\u00e9charger l'installateur MSI</li> <li>Ex\u00e9cuter l'installateur</li> <li>Choisir \"Complete\" installation</li> </ol> <p>\u00c9tape 2 : V\u00e9rifier l'installation</p> <pre><code># V\u00e9rifier la version\nmongod --version\n\n# D\u00e9marrer MongoDB\nmongod\n</code></pre> <p>\u00c9tape 3 : Installer MongoDB Compass (optionnel)</p> <ol> <li>T\u00e9l\u00e9charger : https://www.mongodb.com/products/compass</li> <li>Installer avec l'installateur</li> </ol>"},{"location":"MongoDB/FR/01-getting-started/#linux","title":"Linux","text":"<p>Ubuntu/Debian :</p> <pre><code># Importer la cl\u00e9 GPG\nwget -qO - https://www.mongodb.org/static/pgp/server-7.0.asc | sudo apt-key add -\n\n# Ajouter le repository\necho \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\n\n# Installer\nsudo apt-get update\nsudo apt-get install -y mongodb-org\n\n# D\u00e9marrer MongoDB\nsudo systemctl start mongod\nsudo systemctl enable mongod\n\n# V\u00e9rifier\nsudo systemctl status mongod\n</code></pre>"},{"location":"MongoDB/FR/01-getting-started/#macos","title":"macOS","text":"<p>Avec Homebrew :</p> <pre><code># Ajouter le tap\nbrew tap mongodb/brew\n\n# Installer MongoDB\nbrew install mongodb-community\n\n# D\u00e9marrer MongoDB\nbrew services start mongodb-community\n\n# V\u00e9rifier\nbrew services list\n</code></pre>"},{"location":"MongoDB/FR/01-getting-started/#concepts-de-base","title":"Concepts de base","text":""},{"location":"MongoDB/FR/01-getting-started/#base-de-donnees-database","title":"Base de donn\u00e9es (Database)","text":"<p>Base de donn\u00e9es = Conteneur de collections</p> <ul> <li>Cr\u00e9ation automatique : Cr\u00e9\u00e9e \u00e0 la premi\u00e8re utilisation</li> <li>Nom : Identifiant unique</li> <li>Collections : Contient des collections</li> </ul>"},{"location":"MongoDB/FR/01-getting-started/#collection","title":"Collection","text":"<p>Collection = Groupe de documents</p> <ul> <li>\u00c9quivalent : Table en SQL</li> <li>Flexible : Pas de sch\u00e9ma impos\u00e9</li> <li>Documents : Contient des documents</li> </ul>"},{"location":"MongoDB/FR/01-getting-started/#document","title":"Document","text":"<p>Document = Enregistrement en format JSON</p> <ul> <li>Format : BSON (Binary JSON)</li> <li>Flexible : Structure variable</li> <li>Champs : Paires cl\u00e9-valeur</li> </ul>"},{"location":"MongoDB/FR/01-getting-started/#exemple-de-structure","title":"Exemple de structure","text":"<pre><code>Database: mydb\n  \u2514\u2500\u2500 Collection: users\n        \u251c\u2500\u2500 Document 1: {_id: 1, name: \"John\", age: 30}\n        \u251c\u2500\u2500 Document 2: {_id: 2, name: \"Jane\", age: 25}\n        \u2514\u2500\u2500 Document 3: {_id: 3, name: \"Bob\", age: 35}\n</code></pre>"},{"location":"MongoDB/FR/01-getting-started/#mongodb-compass","title":"MongoDB Compass","text":""},{"location":"MongoDB/FR/01-getting-started/#quest-ce-que-compass","title":"Qu'est-ce que Compass ?","text":"<p>MongoDB Compass = Interface graphique</p> <ul> <li>Visualisation : Voir les donn\u00e9es</li> <li>Requ\u00eates : Ex\u00e9cuter des requ\u00eates</li> <li>Analyse : Analyser les performances</li> <li>Gestion : G\u00e9rer les index</li> </ul>"},{"location":"MongoDB/FR/01-getting-started/#installation_1","title":"Installation","text":"<ol> <li>T\u00e9l\u00e9charger : https://www.mongodb.com/products/compass</li> <li>Installer</li> <li>Lancer Compass</li> <li>Se connecter \u00e0 <code>mongodb://localhost:27017</code></li> </ol>"},{"location":"MongoDB/FR/01-getting-started/#utilisation-de-base","title":"Utilisation de base","text":"<p>Se connecter : - Host : <code>localhost</code> - Port : <code>27017</code> - Pas d'authentification (par d\u00e9faut)</p> <p>Naviguer : - Voir les bases de donn\u00e9es - Voir les collections - Voir les documents</p>"},{"location":"MongoDB/FR/01-getting-started/#premieres-operations","title":"Premi\u00e8res op\u00e9rations","text":""},{"location":"MongoDB/FR/01-getting-started/#se-connecter-avec-mongosh","title":"Se connecter avec mongosh","text":"<pre><code># Lancer mongosh\nmongosh\n\n# Voir les bases de donn\u00e9es\nshow dbs\n\n# Utiliser une base de donn\u00e9es\nuse mydb\n\n# Voir les collections\nshow collections\n\n# Ins\u00e9rer un document\ndb.users.insertOne({name: \"John\", age: 30, city: \"Paris\"})\n\n# Trouver des documents\ndb.users.find()\n\n# Trouver un document sp\u00e9cifique\ndb.users.findOne({name: \"John\"})\n</code></pre>"},{"location":"MongoDB/FR/01-getting-started/#exemple-complet","title":"Exemple complet","text":"<pre><code>// Se connecter\nmongosh\n\n// Utiliser une base de donn\u00e9es\nuse testdb\n\n// Ins\u00e9rer plusieurs documents\ndb.products.insertMany([\n  {name: \"Laptop\", price: 999, category: \"Electronics\"},\n  {name: \"Book\", price: 19, category: \"Education\"},\n  {name: \"Phone\", price: 699, category: \"Electronics\"}\n])\n\n// Trouver tous les produits\ndb.products.find()\n\n// Trouver par cat\u00e9gorie\ndb.products.find({category: \"Electronics\"})\n\n// Compter les documents\ndb.products.countDocuments()\n</code></pre>"},{"location":"MongoDB/FR/01-getting-started/#commandes-essentielles","title":"Commandes essentielles","text":""},{"location":"MongoDB/FR/01-getting-started/#gestion-des-bases-de-donnees","title":"Gestion des bases de donn\u00e9es","text":"<pre><code>// Voir les bases de donn\u00e9es\nshow dbs\n\n// Utiliser une base de donn\u00e9es\nuse mydb\n\n// Voir la base de donn\u00e9es actuelle\ndb\n\n// Supprimer une base de donn\u00e9es\ndb.dropDatabase()\n</code></pre>"},{"location":"MongoDB/FR/01-getting-started/#gestion-des-collections","title":"Gestion des collections","text":"<pre><code>// Voir les collections\nshow collections\n\n// Cr\u00e9er une collection (automatique \u00e0 l'insertion)\ndb.mycollection.insertOne({test: \"data\"})\n\n// Supprimer une collection\ndb.mycollection.drop()\n\n// Renommer une collection\ndb.mycollection.renameCollection(\"newcollection\")\n</code></pre>"},{"location":"MongoDB/FR/01-getting-started/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"MongoDB/FR/01-getting-started/#exemple-1-gestion-dutilisateurs","title":"Exemple 1 : Gestion d'utilisateurs","text":"<pre><code>use userdb\n\n// Ins\u00e9rer des utilisateurs\ndb.users.insertMany([\n  {name: \"Alice\", email: \"alice@example.com\", age: 28},\n  {name: \"Bob\", email: \"bob@example.com\", age: 32},\n  {name: \"Charlie\", email: \"charlie@example.com\", age: 25}\n])\n\n// Trouver tous les utilisateurs\ndb.users.find()\n\n// Trouver les utilisateurs de plus de 30 ans\ndb.users.find({age: {$gt: 30}})\n</code></pre>"},{"location":"MongoDB/FR/01-getting-started/#exemple-2-donnees-de-ventes","title":"Exemple 2 : Donn\u00e9es de ventes","text":"<pre><code>use salesdb\n\n// Ins\u00e9rer des ventes\ndb.sales.insertMany([\n  {product: \"Laptop\", amount: 999, date: new Date(\"2024-01-15\")},\n  {product: \"Phone\", amount: 699, date: new Date(\"2024-01-16\")},\n  {product: \"Tablet\", amount: 399, date: new Date(\"2024-01-17\")}\n])\n\n// Trouver toutes les ventes\ndb.sales.find()\n\n// Trouver les ventes sup\u00e9rieures \u00e0 500\ndb.sales.find({amount: {$gt: 500}})\n</code></pre>"},{"location":"MongoDB/FR/01-getting-started/#depannage","title":"D\u00e9pannage","text":""},{"location":"MongoDB/FR/01-getting-started/#probleme-mongodb-ne-demarre-pas","title":"Probl\u00e8me : MongoDB ne d\u00e9marre pas","text":"<p>Solutions : 1. V\u00e9rifier les logs : <code>/var/log/mongodb/mongod.log</code> (Linux) 2. V\u00e9rifier les permissions 3. V\u00e9rifier que le port 27017 est libre 4. Red\u00e9marrer le service : <code>sudo systemctl restart mongod</code></p>"},{"location":"MongoDB/FR/01-getting-started/#probleme-connexion-refusee","title":"Probl\u00e8me : Connexion refus\u00e9e","text":"<p>Solutions : 1. V\u00e9rifier que MongoDB est d\u00e9marr\u00e9 2. V\u00e9rifier le port : <code>netstat -an | grep 27017</code> 3. V\u00e9rifier le firewall</p>"},{"location":"MongoDB/FR/01-getting-started/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>MongoDB = Base de donn\u00e9es NoSQL orient\u00e9e documents</li> <li>Documents = Format JSON (BSON)</li> <li>Collections = Groupes de documents</li> <li>Bases de donn\u00e9es = Conteneurs de collections</li> <li>Compass = Interface graphique</li> </ol>"},{"location":"MongoDB/FR/01-getting-started/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 2. Op\u00e9rations de base pour ma\u00eetriser le CRUD.</p>"},{"location":"MongoDB/FR/02-basic-operations/","title":"2. Op\u00e9rations de base MongoDB","text":""},{"location":"MongoDB/FR/02-basic-operations/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Ma\u00eetriser les op\u00e9rations CRUD</li> <li>Comprendre les collections et documents</li> <li>Utiliser les types de donn\u00e9es</li> <li>Appliquer les op\u00e9rateurs de requ\u00eate</li> <li>G\u00e9rer les mises \u00e0 jour</li> </ul>"},{"location":"MongoDB/FR/02-basic-operations/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>CRUD Operations</li> <li>Collections et Documents</li> <li>Types de donn\u00e9es</li> <li>Op\u00e9rateurs de requ\u00eate</li> <li>Mises \u00e0 jour</li> </ol>"},{"location":"MongoDB/FR/02-basic-operations/#crud-operations","title":"CRUD Operations","text":""},{"location":"MongoDB/FR/02-basic-operations/#create-creer","title":"Create (Cr\u00e9er)","text":"<p>insertOne :</p> <pre><code>// Ins\u00e9rer un document\ndb.users.insertOne({\n  name: \"John\",\n  age: 30,\n  email: \"john@example.com\"\n})\n</code></pre> <p>insertMany :</p> <pre><code>// Ins\u00e9rer plusieurs documents\ndb.users.insertMany([\n  {name: \"Alice\", age: 28, email: \"alice@example.com\"},\n  {name: \"Bob\", age: 32, email: \"bob@example.com\"},\n  {name: \"Charlie\", age: 25, email: \"charlie@example.com\"}\n])\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#read-lire","title":"Read (Lire)","text":"<p>find :</p> <pre><code>// Trouver tous les documents\ndb.users.find()\n\n// Trouver avec filtre\ndb.users.find({age: 30})\n\n// Trouver un seul document\ndb.users.findOne({name: \"John\"})\n\n// Limiter les r\u00e9sultats\ndb.users.find().limit(5)\n\n// Trier\ndb.users.find().sort({age: 1})  // 1 = ascendant, -1 = descendant\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#update-mettre-a-jour","title":"Update (Mettre \u00e0 jour)","text":"<p>updateOne :</p> <pre><code>// Mettre \u00e0 jour un document\ndb.users.updateOne(\n  {name: \"John\"},\n  {$set: {age: 31}}\n)\n</code></pre> <p>updateMany :</p> <pre><code>// Mettre \u00e0 jour plusieurs documents\ndb.users.updateMany(\n  {age: {$lt: 30}},\n  {$set: {status: \"young\"}}\n)\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#delete-supprimer","title":"Delete (Supprimer)","text":"<p>deleteOne :</p> <pre><code>// Supprimer un document\ndb.users.deleteOne({name: \"John\"})\n</code></pre> <p>deleteMany :</p> <pre><code>// Supprimer plusieurs documents\ndb.users.deleteMany({age: {$lt: 18}})\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#collections-et-documents","title":"Collections et Documents","text":""},{"location":"MongoDB/FR/02-basic-operations/#creer-une-collection","title":"Cr\u00e9er une collection","text":"<pre><code>// Cr\u00e9ation automatique \u00e0 la premi\u00e8re insertion\ndb.mycollection.insertOne({test: \"data\"})\n\n// Cr\u00e9ation explicite\ndb.createCollection(\"mycollection\")\n\n// Avec options\ndb.createCollection(\"mycollection\", {\n  capped: true,\n  size: 100000,\n  max: 5000\n})\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#structure-dun-document","title":"Structure d'un document","text":"<pre><code>{\n  _id: ObjectId(\"...\"),  // Identifiant unique (auto-g\u00e9n\u00e9r\u00e9)\n  name: \"John\",\n  age: 30,\n  address: {\n    street: \"123 Main St\",\n    city: \"Paris\",\n    zip: \"75001\"\n  },\n  hobbies: [\"reading\", \"coding\", \"traveling\"],\n  created_at: new Date()\n}\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#types-de-donnees","title":"Types de donn\u00e9es","text":""},{"location":"MongoDB/FR/02-basic-operations/#types-de-base","title":"Types de base","text":"<pre><code>// String\n{name: \"John\"}\n\n// Number\n{age: 30}\n{price: 99.99}\n\n// Boolean\n{active: true}\n\n// Date\n{created_at: new Date()}\n{birthday: new Date(\"1990-01-15\")}\n\n// Array\n{hobbies: [\"reading\", \"coding\"]}\n\n// Object (Embedded)\n{address: {street: \"123 Main St\", city: \"Paris\"}}\n\n// Null\n{description: null}\n\n// ObjectId\n{_id: ObjectId(\"507f1f77bcf86cd799439011\")}\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#exemple-complet","title":"Exemple complet","text":"<pre><code>db.products.insertOne({\n  name: \"Laptop\",\n  price: 999.99,\n  inStock: true,\n  tags: [\"electronics\", \"computers\"],\n  specifications: {\n    cpu: \"Intel i7\",\n    ram: \"16GB\",\n    storage: \"512GB SSD\"\n  },\n  created_at: new Date()\n})\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#operateurs-de-requete","title":"Op\u00e9rateurs de requ\u00eate","text":""},{"location":"MongoDB/FR/02-basic-operations/#operateurs-de-comparaison","title":"Op\u00e9rateurs de comparaison","text":"<pre><code>// \u00c9gal\ndb.users.find({age: 30})\n\n// Plus grand que\ndb.users.find({age: {$gt: 30}})\n\n// Plus grand ou \u00e9gal\ndb.users.find({age: {$gte: 30}})\n\n// Plus petit que\ndb.users.find({age: {$lt: 30}})\n\n// Plus petit ou \u00e9gal\ndb.users.find({age: {$lte: 30}})\n\n// Diff\u00e9rent\ndb.users.find({age: {$ne: 30}})\n\n// Dans une liste\ndb.users.find({age: {$in: [25, 30, 35]}})\n\n// Pas dans une liste\ndb.users.find({age: {$nin: [25, 30, 35]}})\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#operateurs-logiques","title":"Op\u00e9rateurs logiques","text":"<pre><code>// ET (AND)\ndb.users.find({\n  $and: [\n    {age: {$gt: 25}},\n    {age: {$lt: 35}}\n  ]\n})\n\n// OU (OR)\ndb.users.find({\n  $or: [\n    {age: {$lt: 25}},\n    {age: {$gt: 35}}\n  ]\n})\n\n// NON (NOT)\ndb.users.find({\n  age: {$not: {$gt: 30}}\n})\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#operateurs-de-tableau","title":"Op\u00e9rateurs de tableau","text":"<pre><code>// Contient un \u00e9l\u00e9ment\ndb.users.find({hobbies: \"reading\"})\n\n// Contient tous les \u00e9l\u00e9ments\ndb.users.find({hobbies: {$all: [\"reading\", \"coding\"]}})\n\n// Taille du tableau\ndb.users.find({hobbies: {$size: 3}})\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#mises-a-jour","title":"Mises \u00e0 jour","text":""},{"location":"MongoDB/FR/02-basic-operations/#operateurs-de-mise-a-jour","title":"Op\u00e9rateurs de mise \u00e0 jour","text":"<p>$set :</p> <pre><code>// D\u00e9finir un champ\ndb.users.updateOne(\n  {name: \"John\"},\n  {$set: {age: 31, city: \"Lyon\"}}\n)\n</code></pre> <p>$unset :</p> <pre><code>// Supprimer un champ\ndb.users.updateOne(\n  {name: \"John\"},\n  {$unset: {city: \"\"}}\n)\n</code></pre> <p>$inc :</p> <pre><code>// Incr\u00e9menter\ndb.users.updateOne(\n  {name: \"John\"},\n  {$inc: {age: 1}}\n)\n</code></pre> <p>$push :</p> <pre><code>// Ajouter \u00e0 un tableau\ndb.users.updateOne(\n  {name: \"John\"},\n  {$push: {hobbies: \"swimming\"}}\n)\n</code></pre> <p>$pull :</p> <pre><code>// Retirer d'un tableau\ndb.users.updateOne(\n  {name: \"John\"},\n  {$pull: {hobbies: \"swimming\"}}\n)\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"MongoDB/FR/02-basic-operations/#exemple-1-gestion-de-commandes","title":"Exemple 1 : Gestion de commandes","text":"<pre><code>use shopdb\n\n// Cr\u00e9er une commande\ndb.orders.insertOne({\n  order_id: \"ORD001\",\n  customer: \"John Doe\",\n  items: [\n    {product: \"Laptop\", quantity: 1, price: 999},\n    {product: \"Mouse\", quantity: 2, price: 25}\n  ],\n  total: 1049,\n  status: \"pending\",\n  created_at: new Date()\n})\n\n// Trouver les commandes en attente\ndb.orders.find({status: \"pending\"})\n\n// Mettre \u00e0 jour le statut\ndb.orders.updateOne(\n  {order_id: \"ORD001\"},\n  {$set: {status: \"completed\"}}\n)\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#exemple-2-analyse-de-donnees","title":"Exemple 2 : Analyse de donn\u00e9es","text":"<pre><code>use analyticsdb\n\n// Ins\u00e9rer des \u00e9v\u00e9nements\ndb.events.insertMany([\n  {event: \"page_view\", user: \"user1\", timestamp: new Date()},\n  {event: \"click\", user: \"user1\", timestamp: new Date()},\n  {event: \"page_view\", user: \"user2\", timestamp: new Date()}\n])\n\n// Trouver les \u00e9v\u00e9nements d'un utilisateur\ndb.events.find({user: \"user1\"})\n\n// Compter les \u00e9v\u00e9nements\ndb.events.countDocuments({event: \"page_view\"})\n</code></pre>"},{"location":"MongoDB/FR/02-basic-operations/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>CRUD : Create, Read, Update, Delete</li> <li>Documents : Format JSON flexible</li> <li>Collections : Groupes de documents</li> <li>Op\u00e9rateurs : Pour filtrer et mettre \u00e0 jour</li> <li>Types : Donn\u00e9es vari\u00e9es support\u00e9es</li> </ol>"},{"location":"MongoDB/FR/02-basic-operations/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 3. Requ\u00eates et Agr\u00e9gation pour les requ\u00eates avanc\u00e9es.</p>"},{"location":"MongoDB/FR/03-queries-aggregation/","title":"3. Requ\u00eates et Agr\u00e9gation MongoDB","text":""},{"location":"MongoDB/FR/03-queries-aggregation/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Ma\u00eetriser les requ\u00eates avanc\u00e9es</li> <li>Utiliser le pipeline d'agr\u00e9gation</li> <li>Appliquer les op\u00e9rateurs d'agr\u00e9gation</li> <li>Effectuer des groupements et calculs</li> <li>Analyser des donn\u00e9es complexes</li> </ul>"},{"location":"MongoDB/FR/03-queries-aggregation/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Requ\u00eates avanc\u00e9es</li> <li>Pipeline d'agr\u00e9gation</li> <li>Op\u00e9rateurs d'agr\u00e9gation</li> <li>Groupement et calculs</li> <li>Exemples pratiques</li> </ol>"},{"location":"MongoDB/FR/03-queries-aggregation/#requetes-avancees","title":"Requ\u00eates avanc\u00e9es","text":""},{"location":"MongoDB/FR/03-queries-aggregation/#projection","title":"Projection","text":"<pre><code>// S\u00e9lectionner des champs sp\u00e9cifiques\ndb.users.find({}, {name: 1, email: 1, _id: 0})\n\n// Exclure des champs\ndb.users.find({}, {password: 0, secret: 0})\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#tri-et-limite","title":"Tri et limite","text":"<pre><code>// Trier par \u00e2ge (croissant)\ndb.users.find().sort({age: 1})\n\n// Trier par \u00e2ge (d\u00e9croissant)\ndb.users.find().sort({age: -1})\n\n// Trier par plusieurs champs\ndb.users.find().sort({age: 1, name: 1})\n\n// Limiter les r\u00e9sultats\ndb.users.find().limit(10)\n\n// Sauter des r\u00e9sultats\ndb.users.find().skip(10).limit(10)\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#requetes-sur-tableaux","title":"Requ\u00eates sur tableaux","text":"<pre><code>// \u00c9l\u00e9ment dans un tableau\ndb.users.find({hobbies: \"reading\"})\n\n// Tous les \u00e9l\u00e9ments\ndb.users.find({hobbies: {$all: [\"reading\", \"coding\"]}})\n\n// Taille du tableau\ndb.users.find({hobbies: {$size: 3}})\n\n// \u00c9l\u00e9ment \u00e0 une position\ndb.users.find({\"hobbies.0\": \"reading\"})\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#requetes-sur-objets-imbriques","title":"Requ\u00eates sur objets imbriqu\u00e9s","text":"<pre><code>// Acc\u00e9der \u00e0 un champ imbriqu\u00e9\ndb.users.find({\"address.city\": \"Paris\"})\n\n// Requ\u00eate sur objet complet\ndb.users.find({address: {street: \"123 Main St\", city: \"Paris\"}})\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#pipeline-dagregation","title":"Pipeline d'agr\u00e9gation","text":""},{"location":"MongoDB/FR/03-queries-aggregation/#quest-ce-quun-pipeline","title":"Qu'est-ce qu'un pipeline ?","text":"<p>Pipeline = S\u00e9rie d'\u00e9tapes de transformation</p> <ul> <li>\u00c9tapes : Chaque \u00e9tape transforme les donn\u00e9es</li> <li>S\u00e9quentiel : R\u00e9sultat d'une \u00e9tape = entr\u00e9e de la suivante</li> <li>Puissant : Pour analyses complexes</li> </ul>"},{"location":"MongoDB/FR/03-queries-aggregation/#structure-de-base","title":"Structure de base","text":"<pre><code>db.collection.aggregate([\n  { $match: { ... } },      // Filtrer\n  { $group: { ... } },       // Grouper\n  { $sort: { ... } },        // Trier\n  { $project: { ... } }      // S\u00e9lectionner\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#operateurs-dagregation","title":"Op\u00e9rateurs d'agr\u00e9gation","text":""},{"location":"MongoDB/FR/03-queries-aggregation/#match","title":"$match","text":"<p>Filtrer les documents :</p> <pre><code>db.sales.aggregate([\n  {$match: {amount: {$gt: 500}}}\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#group","title":"$group","text":"<p>Grouper et calculer :</p> <pre><code>// Grouper par cat\u00e9gorie et calculer la somme\ndb.products.aggregate([\n  {\n    $group: {\n      _id: \"$category\",\n      total: {$sum: \"$price\"},\n      count: {$sum: 1},\n      average: {$avg: \"$price\"}\n    }\n  }\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#project","title":"$project","text":"<p>S\u00e9lectionner et transformer :</p> <pre><code>db.users.aggregate([\n  {\n    $project: {\n      name: 1,\n      age: 1,\n      isAdult: {$gte: [\"$age\", 18]}\n    }\n  }\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#sort","title":"$sort","text":"<p>Trier :</p> <pre><code>db.sales.aggregate([\n  {$sort: {amount: -1}}\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#limit","title":"$limit","text":"<p>Limiter :</p> <pre><code>db.sales.aggregate([\n  {$sort: {amount: -1}},\n  {$limit: 10}\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#lookup","title":"$lookup","text":"<p>Jointure (comme SQL JOIN) :</p> <pre><code>db.orders.aggregate([\n  {\n    $lookup: {\n      from: \"products\",\n      localField: \"product_id\",\n      foreignField: \"_id\",\n      as: \"product_details\"\n    }\n  }\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#groupement-et-calculs","title":"Groupement et calculs","text":""},{"location":"MongoDB/FR/03-queries-aggregation/#operateurs-daccumulation","title":"Op\u00e9rateurs d'accumulation","text":"<pre><code>// Somme\n{$sum: \"$amount\"}\n\n// Moyenne\n{$avg: \"$price\"}\n\n// Minimum\n{$min: \"$price\"}\n\n// Maximum\n{$max: \"$price\"}\n\n// Premier\n{$first: \"$name\"}\n\n// Dernier\n{$last: \"$name\"}\n\n// Compter\n{$sum: 1}\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#exemple-analyse-de-ventes","title":"Exemple : Analyse de ventes","text":"<pre><code>db.sales.aggregate([\n  // Filtrer par date\n  {\n    $match: {\n      date: {\n        $gte: new Date(\"2024-01-01\"),\n        $lt: new Date(\"2024-02-01\")\n      }\n    }\n  },\n  // Grouper par produit\n  {\n    $group: {\n      _id: \"$product\",\n      total_sales: {$sum: \"$amount\"},\n      count: {$sum: 1},\n      average: {$avg: \"$amount\"}\n    }\n  },\n  // Trier par total\n  {\n    $sort: {total_sales: -1}\n  },\n  // Limiter aux 10 premiers\n  {\n    $limit: 10\n  }\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"MongoDB/FR/03-queries-aggregation/#exemple-1-analyse-de-donnees-utilisateurs","title":"Exemple 1 : Analyse de donn\u00e9es utilisateurs","text":"<pre><code>db.users.aggregate([\n  // Filtrer les utilisateurs actifs\n  {\n    $match: {active: true}\n  },\n  // Grouper par ville\n  {\n    $group: {\n      _id: \"$address.city\",\n      users: {$sum: 1},\n      avgAge: {$avg: \"$age\"}\n    }\n  },\n  // Trier par nombre d'utilisateurs\n  {\n    $sort: {users: -1}\n  }\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#exemple-2-analyse-de-logs","title":"Exemple 2 : Analyse de logs","text":"<pre><code>db.logs.aggregate([\n  // Filtrer par type\n  {\n    $match: {type: \"error\"}\n  },\n  // Grouper par heure\n  {\n    $group: {\n      _id: {\n        $dateToString: {\n          format: \"%Y-%m-%d %H:00:00\",\n          date: \"$timestamp\"\n        }\n      },\n      count: {$sum: 1}\n    }\n  },\n  // Trier par date\n  {\n    $sort: {_id: 1}\n  }\n])\n</code></pre>"},{"location":"MongoDB/FR/03-queries-aggregation/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Pipeline : S\u00e9rie d'\u00e9tapes de transformation</li> <li>$match : Filtrer les documents</li> <li>$group : Grouper et calculer</li> <li>$project : S\u00e9lectionner et transformer</li> <li>Agr\u00e9gation : Puissant pour l'analyse</li> </ol>"},{"location":"MongoDB/FR/03-queries-aggregation/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 4. Index et Performance pour optimiser les performances.</p>"},{"location":"MongoDB/FR/04-indexes-performance/","title":"4. Index et Performance MongoDB","text":""},{"location":"MongoDB/FR/04-indexes-performance/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les index</li> <li>Cr\u00e9er diff\u00e9rents types d'index</li> <li>Analyser les performances</li> <li>Optimiser les requ\u00eates</li> <li>Utiliser explain()</li> </ul>"},{"location":"MongoDB/FR/04-indexes-performance/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction aux index</li> <li>Types d'index</li> <li>Cr\u00e9er des index</li> <li>Analyser les performances</li> <li>Optimisation</li> </ol>"},{"location":"MongoDB/FR/04-indexes-performance/#introduction-aux-index","title":"Introduction aux index","text":""},{"location":"MongoDB/FR/04-indexes-performance/#quest-ce-quun-index","title":"Qu'est-ce qu'un index ?","text":"<p>Index = Structure de donn\u00e9es pour acc\u00e9l\u00e9rer les requ\u00eates</p> <ul> <li>Performance : Recherche plus rapide</li> <li>Co\u00fbt : Espace disque suppl\u00e9mentaire</li> <li>Maintenance : Mise \u00e0 jour automatique</li> <li>Similaire : Index dans un livre</li> </ul>"},{"location":"MongoDB/FR/04-indexes-performance/#pourquoi-des-index","title":"Pourquoi des index ?","text":"<ul> <li>Recherche rapide : Trouver rapidement</li> <li>Tri rapide : Trier efficacement</li> <li>Unicit\u00e9 : Garantir l'unicit\u00e9</li> <li>Performance : Am\u00e9liorer les requ\u00eates</li> </ul>"},{"location":"MongoDB/FR/04-indexes-performance/#types-dindex","title":"Types d'index","text":""},{"location":"MongoDB/FR/04-indexes-performance/#index-simple","title":"Index simple","text":"<pre><code>// Index sur un champ\ndb.users.createIndex({email: 1})\n\n// 1 = croissant, -1 = d\u00e9croissant\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#index-compose","title":"Index compos\u00e9","text":"<pre><code>// Index sur plusieurs champs\ndb.users.createIndex({name: 1, age: -1})\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#index-unique","title":"Index unique","text":"<pre><code>// Garantir l'unicit\u00e9\ndb.users.createIndex({email: 1}, {unique: true})\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#index-texte","title":"Index texte","text":"<pre><code>// Pour recherche de texte\ndb.articles.createIndex({title: \"text\", content: \"text\"})\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#index-geospatial","title":"Index g\u00e9ospatial","text":"<pre><code>// Pour donn\u00e9es g\u00e9ographiques\ndb.places.createIndex({location: \"2dsphere\"})\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#creer-des-index","title":"Cr\u00e9er des index","text":""},{"location":"MongoDB/FR/04-indexes-performance/#methodes-de-creation","title":"M\u00e9thodes de cr\u00e9ation","text":"<pre><code>// Cr\u00e9er un index\ndb.collection.createIndex({field: 1})\n\n// Cr\u00e9er avec options\ndb.collection.createIndex(\n  {field: 1},\n  {unique: true, sparse: true}\n)\n\n// Voir les index\ndb.collection.getIndexes()\n\n// Supprimer un index\ndb.collection.dropIndex({field: 1})\n\n// Supprimer tous les index (sauf _id)\ndb.collection.dropIndexes()\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#index-par-defaut","title":"Index par d\u00e9faut","text":"<p>Index _id : - Cr\u00e9\u00e9 automatiquement - Unique - Ne peut pas \u00eatre supprim\u00e9</p>"},{"location":"MongoDB/FR/04-indexes-performance/#analyser-les-performances","title":"Analyser les performances","text":""},{"location":"MongoDB/FR/04-indexes-performance/#explain","title":"explain()","text":"<p>Voir le plan d'ex\u00e9cution :</p> <pre><code>// Plan d'ex\u00e9cution\ndb.users.find({email: \"john@example.com\"}).explain()\n\n// Statistiques d\u00e9taill\u00e9es\ndb.users.find({email: \"john@example.com\"}).explain(\"executionStats\")\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#metriques-importantes","title":"M\u00e9triques importantes","text":"<p>executionStats : - executionTimeMillis : Temps d'ex\u00e9cution - totalDocsExamined : Documents examin\u00e9s - totalKeysExamined : Cl\u00e9s examin\u00e9es - nReturned : Documents retourn\u00e9s</p>"},{"location":"MongoDB/FR/04-indexes-performance/#exemple","title":"Exemple","text":"<pre><code>// Sans index\ndb.users.find({email: \"john@example.com\"}).explain(\"executionStats\")\n// totalDocsExamined: 10000 (scan complet)\n\n// Avec index\ndb.users.createIndex({email: 1})\ndb.users.find({email: \"john@example.com\"}).explain(\"executionStats\")\n// totalDocsExamined: 1 (utilisation de l'index)\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#optimisation","title":"Optimisation","text":""},{"location":"MongoDB/FR/04-indexes-performance/#bonnes-pratiques","title":"Bonnes pratiques","text":"<p>1. Indexer les champs fr\u00e9quemment utilis\u00e9s :</p> <pre><code>// Si souvent recherch\u00e9 par email\ndb.users.createIndex({email: 1})\n</code></pre> <p>2. Index compos\u00e9 pour requ\u00eates multiples :</p> <pre><code>// Si recherche par name ET age\ndb.users.createIndex({name: 1, age: 1})\n</code></pre> <p>3. \u00c9viter trop d'index :</p> <ul> <li>Chaque index ralentit les \u00e9critures</li> <li>Utiliser seulement les index n\u00e9cessaires</li> </ul> <p>4. Analyser les requ\u00eates lentes :</p> <pre><code>// Activer le profiler\ndb.setProfilingLevel(1, {slowms: 100})\n\n// Voir les requ\u00eates lentes\ndb.system.profile.find().sort({ts: -1}).limit(10)\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"MongoDB/FR/04-indexes-performance/#exemple-1-optimiser-une-requete","title":"Exemple 1 : Optimiser une requ\u00eate","text":"<pre><code>// Requ\u00eate lente\ndb.orders.find({customer: \"John\", status: \"pending\"})\n\n// Cr\u00e9er un index compos\u00e9\ndb.orders.createIndex({customer: 1, status: 1})\n\n// V\u00e9rifier l'utilisation\ndb.orders.find({customer: \"John\", status: \"pending\"}).explain(\"executionStats\")\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#exemple-2-index-pour-tri","title":"Exemple 2 : Index pour tri","text":"<pre><code>// Trier par date\ndb.sales.find().sort({date: -1})\n\n// Cr\u00e9er un index pour le tri\ndb.sales.createIndex({date: -1})\n\n// V\u00e9rifier\ndb.sales.find().sort({date: -1}).explain(\"executionStats\")\n</code></pre>"},{"location":"MongoDB/FR/04-indexes-performance/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Index : Acc\u00e9l\u00e8rent les recherches</li> <li>Types : Simple, compos\u00e9, unique, texte</li> <li>explain() : Analyser les performances</li> <li>Optimisation : Indexer les champs fr\u00e9quents</li> <li>\u00c9quilibre : Pas trop d'index</li> </ol>"},{"location":"MongoDB/FR/04-indexes-performance/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 5. Mod\u00e9lisation des donn\u00e9es pour apprendre \u00e0 mod\u00e9liser.</p>"},{"location":"MongoDB/FR/05-data-modeling/","title":"5. Mod\u00e9lisation des donn\u00e9es MongoDB","text":""},{"location":"MongoDB/FR/05-data-modeling/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les mod\u00e8les de donn\u00e9es</li> <li>Choisir entre Embedded et References</li> <li>Concevoir des sch\u00e9mas flexibles</li> <li>Appliquer les bonnes pratiques</li> <li>Optimiser la structure</li> </ul>"},{"location":"MongoDB/FR/05-data-modeling/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Mod\u00e8les de donn\u00e9es</li> <li>Embedded vs References</li> <li>Sch\u00e9mas flexibles</li> <li>Bonnes pratiques</li> <li>Exemples pratiques</li> </ol>"},{"location":"MongoDB/FR/05-data-modeling/#modeles-de-donnees","title":"Mod\u00e8les de donn\u00e9es","text":""},{"location":"MongoDB/FR/05-data-modeling/#modele-embedded-imbrique","title":"Mod\u00e8le Embedded (Imbriqu\u00e9)","text":"<p>Tout dans un document :</p> <pre><code>// Utilisateur avec adresse imbriqu\u00e9e\n{\n  _id: ObjectId(\"...\"),\n  name: \"John\",\n  email: \"john@example.com\",\n  address: {\n    street: \"123 Main St\",\n    city: \"Paris\",\n    zip: \"75001\"\n  }\n}\n</code></pre> <p>Avantages : - Acc\u00e8s rapide (un seul document) - Pas de jointure - Donn\u00e9es coh\u00e9rentes</p> <p>Inconv\u00e9nients : - Taille limit\u00e9e (16 MB par document) - Duplication possible</p>"},{"location":"MongoDB/FR/05-data-modeling/#modele-references-references","title":"Mod\u00e8le References (R\u00e9f\u00e9rences)","text":"<p>Documents s\u00e9par\u00e9s avec r\u00e9f\u00e9rences :</p> <pre><code>// Collection users\n{\n  _id: ObjectId(\"...\"),\n  name: \"John\",\n  email: \"john@example.com\"\n}\n\n// Collection addresses\n{\n  _id: ObjectId(\"...\"),\n  user_id: ObjectId(\"...\"),\n  street: \"123 Main St\",\n  city: \"Paris\"\n}\n</code></pre> <p>Avantages : - Pas de limite de taille - Pas de duplication - Flexibilit\u00e9</p> <p>Inconv\u00e9nients : - Requiert des jointures ($lookup) - Plus de requ\u00eates</p>"},{"location":"MongoDB/FR/05-data-modeling/#embedded-vs-references","title":"Embedded vs References","text":""},{"location":"MongoDB/FR/05-data-modeling/#quand-utiliser-embedded","title":"Quand utiliser Embedded ?","text":"<p>Cas d'usage : - Donn\u00e9es souvent acc\u00e9d\u00e9es ensemble - Petites quantit\u00e9s de donn\u00e9es - Relation 1:1 ou 1:peu - Donn\u00e9es qui changent rarement</p> <p>Exemple :</p> <pre><code>// Adresse d'un utilisateur (1:1)\n{\n  name: \"John\",\n  address: {\n    street: \"123 Main St\",\n    city: \"Paris\"\n  }\n}\n</code></pre>"},{"location":"MongoDB/FR/05-data-modeling/#quand-utiliser-references","title":"Quand utiliser References ?","text":"<p>Cas d'usage : - Grandes quantit\u00e9s de donn\u00e9es - Relation 1:beaucoup ou beaucoup:beaucoup - Donn\u00e9es partag\u00e9es - Donn\u00e9es qui changent souvent</p> <p>Exemple :</p> <pre><code>// Articles d'un blog (1:beaucoup)\n// Collection authors\n{_id: ObjectId(\"...\"), name: \"John\"}\n\n// Collection articles\n{\n  _id: ObjectId(\"...\"),\n  title: \"Article\",\n  author_id: ObjectId(\"...\")\n}\n</code></pre>"},{"location":"MongoDB/FR/05-data-modeling/#schemas-flexibles","title":"Sch\u00e9mas flexibles","text":""},{"location":"MongoDB/FR/05-data-modeling/#avantages-de-la-flexibilite","title":"Avantages de la flexibilit\u00e9","text":"<p>\u00c9volution du sch\u00e9ma :</p> <pre><code>// Document initial\n{\n  name: \"John\",\n  age: 30\n}\n\n// Ajouter un champ plus tard\n{\n  name: \"John\",\n  age: 30,\n  email: \"john@example.com\"  // Nouveau champ\n}\n</code></pre>"},{"location":"MongoDB/FR/05-data-modeling/#gerer-les-variations","title":"G\u00e9rer les variations","text":"<pre><code>// Documents avec structures diff\u00e9rentes\ndb.products.insertMany([\n  {name: \"Laptop\", price: 999, specs: {...}},\n  {name: \"Book\", author: \"Author\", pages: 300},\n  {name: \"Service\", duration: \"1 hour\", price: 50}\n])\n</code></pre>"},{"location":"MongoDB/FR/05-data-modeling/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"MongoDB/FR/05-data-modeling/#1-normalisation-vs-denormalisation","title":"1. Normalisation vs D\u00e9normalisation","text":"<p>Normalisation (SQL style) : - Donn\u00e9es s\u00e9par\u00e9es - R\u00e9f\u00e9rences - Coh\u00e9rence</p> <p>D\u00e9normalisation (NoSQL style) : - Donn\u00e9es dupliqu\u00e9es - Acc\u00e8s rapide - Performance</p>"},{"location":"MongoDB/FR/05-data-modeling/#2-patterns-de-modelisation","title":"2. Patterns de mod\u00e9lisation","text":"<p>One-to-Few : <pre><code>// Embedded\n{\n  name: \"John\",\n  addresses: [\n    {street: \"123 Main St\"},\n    {street: \"456 Oak Ave\"}\n  ]\n}\n</code></pre></p> <p>One-to-Many : <pre><code>// References\n// Collection users\n{_id: ObjectId(\"...\"), name: \"John\"}\n\n// Collection orders\n{user_id: ObjectId(\"...\"), items: [...]}\n</code></pre></p> <p>Many-to-Many : <pre><code>// References avec tableau\n// Collection students\n{_id: ObjectId(\"...\"), courses: [ObjectId(\"...\"), ObjectId(\"...\")]}\n\n// Collection courses\n{_id: ObjectId(\"...\"), students: [ObjectId(\"...\"), ObjectId(\"...\")]}\n</code></pre></p>"},{"location":"MongoDB/FR/05-data-modeling/#exemples-pratiques","title":"Exemples pratiques","text":""},{"location":"MongoDB/FR/05-data-modeling/#exemple-1-e-commerce","title":"Exemple 1 : E-commerce","text":"<pre><code>// Produit avec variantes (Embedded)\n{\n  _id: ObjectId(\"...\"),\n  name: \"T-Shirt\",\n  price: 29.99,\n  variants: [\n    {size: \"S\", color: \"Red\", stock: 10},\n    {size: \"M\", color: \"Blue\", stock: 15}\n  ]\n}\n\n// Commandes (References)\n// Collection orders\n{\n  _id: ObjectId(\"...\"),\n  user_id: ObjectId(\"...\"),\n  items: [\n    {product_id: ObjectId(\"...\"), quantity: 2}\n  ]\n}\n</code></pre>"},{"location":"MongoDB/FR/05-data-modeling/#exemple-2-blog","title":"Exemple 2 : Blog","text":"<pre><code>// Article avec commentaires (Embedded pour r\u00e9cents)\n{\n  _id: ObjectId(\"...\"),\n  title: \"Article\",\n  content: \"...\",\n  comments: [\n    {author: \"User1\", text: \"Great!\", date: new Date()}\n  ]\n}\n\n// Auteurs (References)\n// Collection authors\n{_id: ObjectId(\"...\"), name: \"John\"}\n\n// Collection articles\n{\n  _id: ObjectId(\"...\"),\n  title: \"Article\",\n  author_id: ObjectId(\"...\")\n}\n</code></pre>"},{"location":"MongoDB/FR/05-data-modeling/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Embedded : Pour donn\u00e9es souvent acc\u00e9d\u00e9es ensemble</li> <li>References : Pour grandes quantit\u00e9s ou relations complexes</li> <li>Flexibilit\u00e9 : Sch\u00e9mas \u00e9volutifs</li> <li>Patterns : One-to-Few, One-to-Many, Many-to-Many</li> <li>Performance : \u00c9quilibrer acc\u00e8s et coh\u00e9rence</li> </ol>"},{"location":"MongoDB/FR/05-data-modeling/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 6. Fonctionnalit\u00e9s avanc\u00e9es pour approfondir.</p>"},{"location":"MongoDB/FR/06-advanced/","title":"6. Fonctionnalit\u00e9s avanc\u00e9es MongoDB","text":""},{"location":"MongoDB/FR/06-advanced/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Utiliser les transactions</li> <li>Comprendre la r\u00e9plication</li> <li>Ma\u00eetriser le sharding</li> <li>Utiliser la recherche de texte</li> <li>Fonctionnalit\u00e9s avanc\u00e9es</li> </ul>"},{"location":"MongoDB/FR/06-advanced/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Transactions</li> <li>R\u00e9plication</li> <li>Sharding</li> <li>Text Search</li> <li>Autres fonctionnalit\u00e9s</li> </ol>"},{"location":"MongoDB/FR/06-advanced/#transactions","title":"Transactions","text":""},{"location":"MongoDB/FR/06-advanced/#quest-ce-quune-transaction","title":"Qu'est-ce qu'une transaction ?","text":"<p>Transaction = Groupe d'op\u00e9rations atomiques</p> <ul> <li>Atomique : Tout ou rien</li> <li>Coh\u00e9rence : Donn\u00e9es coh\u00e9rentes</li> <li>Isolation : Op\u00e9rations isol\u00e9es</li> <li>Durabilit\u00e9 : Changements persistants</li> </ul>"},{"location":"MongoDB/FR/06-advanced/#utiliser-les-transactions","title":"Utiliser les transactions","text":"<pre><code>// D\u00e9marrer une session\nconst session = db.getMongo().startSession()\n\n// D\u00e9marrer une transaction\nsession.startTransaction()\n\ntry {\n  // Op\u00e9rations\n  db.users.insertOne({name: \"John\"}, {session})\n  db.orders.insertOne({user_id: \"...\", items: [...]}, {session})\n\n  // Valider\n  session.commitTransaction()\n} catch (error) {\n  // Annuler\n  session.abortTransaction()\n} finally {\n  session.endSession()\n}\n</code></pre>"},{"location":"MongoDB/FR/06-advanced/#replication","title":"R\u00e9plication","text":""},{"location":"MongoDB/FR/06-advanced/#quest-ce-que-la-replication","title":"Qu'est-ce que la r\u00e9plication ?","text":"<p>R\u00e9plication = Copies multiples des donn\u00e9es</p> <ul> <li>Haute disponibilit\u00e9 : Pas de point de d\u00e9faillance unique</li> <li>Redondance : Sauvegarde automatique</li> <li>Performance : Lecture depuis plusieurs serveurs</li> </ul>"},{"location":"MongoDB/FR/06-advanced/#replica-set","title":"Replica Set","text":"<p>Configuration de base :</p> <pre><code>// 3 serveurs : 1 Primary + 2 Secondaries\n// Primary : \u00c9critures\n// Secondaries : Lectures et backup\n</code></pre>"},{"location":"MongoDB/FR/06-advanced/#sharding","title":"Sharding","text":""},{"location":"MongoDB/FR/06-advanced/#quest-ce-que-le-sharding","title":"Qu'est-ce que le sharding ?","text":"<p>Sharding = Partitionnement horizontal</p> <ul> <li>Scalabilit\u00e9 : Distribuer les donn\u00e9es</li> <li>Performance : Traiter en parall\u00e8le</li> <li>Stockage : Plus de capacit\u00e9</li> </ul>"},{"location":"MongoDB/FR/06-advanced/#configuration","title":"Configuration","text":"<pre><code>// Shard key : Cl\u00e9 de partitionnement\ndb.collection.createIndex({shard_key: 1})\n\n// Shard la collection\nsh.shardCollection(\"mydb.mycollection\", {shard_key: 1})\n</code></pre>"},{"location":"MongoDB/FR/06-advanced/#text-search","title":"Text Search","text":""},{"location":"MongoDB/FR/06-advanced/#index-de-texte","title":"Index de texte","text":"<pre><code>// Cr\u00e9er un index de texte\ndb.articles.createIndex({\n  title: \"text\",\n  content: \"text\"\n})\n\n// Rechercher\ndb.articles.find({\n  $text: {$search: \"mongodb tutorial\"}\n})\n\n// Score de pertinence\ndb.articles.find(\n  {$text: {$search: \"mongodb\"}},\n  {score: {$meta: \"textScore\"}}\n).sort({score: {$meta: \"textScore\"}})\n</code></pre>"},{"location":"MongoDB/FR/06-advanced/#autres-fonctionnalites","title":"Autres fonctionnalit\u00e9s","text":""},{"location":"MongoDB/FR/06-advanced/#validation-de-schema","title":"Validation de sch\u00e9ma","text":"<pre><code>// D\u00e9finir un sch\u00e9ma de validation\ndb.createCollection(\"users\", {\n  validator: {\n    $jsonSchema: {\n      bsonType: \"object\",\n      required: [\"name\", \"email\"],\n      properties: {\n        name: {\n          bsonType: \"string\",\n          description: \"must be a string\"\n        },\n        email: {\n          bsonType: \"string\",\n          pattern: \"^.+@.+$\"\n        }\n      }\n    }\n  }\n})\n</code></pre>"},{"location":"MongoDB/FR/06-advanced/#ttl-index","title":"TTL Index","text":"<pre><code>// Index avec expiration automatique\ndb.sessions.createIndex(\n  {created_at: 1},\n  {expireAfterSeconds: 3600}  // Expire apr\u00e8s 1 heure\n)\n</code></pre>"},{"location":"MongoDB/FR/06-advanced/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Transactions : Op\u00e9rations atomiques</li> <li>R\u00e9plication : Haute disponibilit\u00e9</li> <li>Sharding : Scalabilit\u00e9 horizontale</li> <li>Text Search : Recherche de texte</li> <li>Validation : Sch\u00e9mas optionnels</li> </ol>"},{"location":"MongoDB/FR/06-advanced/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 7. Bonnes pratiques pour les meilleures pratiques.</p>"},{"location":"MongoDB/FR/07-best-practices/","title":"7. Bonnes pratiques MongoDB","text":""},{"location":"MongoDB/FR/07-best-practices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>S\u00e9curit\u00e9</li> <li>Performance</li> <li>Maintenance</li> <li>Backup et Restore</li> <li>Monitoring</li> </ul>"},{"location":"MongoDB/FR/07-best-practices/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>S\u00e9curit\u00e9</li> <li>Performance</li> <li>Maintenance</li> <li>Backup et Restore</li> <li>Monitoring</li> </ol>"},{"location":"MongoDB/FR/07-best-practices/#securite","title":"S\u00e9curit\u00e9","text":""},{"location":"MongoDB/FR/07-best-practices/#authentification","title":"Authentification","text":"<pre><code>// Cr\u00e9er un utilisateur admin\nuse admin\ndb.createUser({\n  user: \"admin\",\n  pwd: \"secure_password\",\n  roles: [\"root\"]\n})\n\n// Cr\u00e9er un utilisateur pour une base\nuse mydb\ndb.createUser({\n  user: \"app_user\",\n  pwd: \"app_password\",\n  roles: [\"readWrite\"]\n})\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#connexion-securisee","title":"Connexion s\u00e9curis\u00e9e","text":"<pre><code># Se connecter avec authentification\nmongosh -u admin -p secure_password --authenticationDatabase admin\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#bonnes-pratiques-securite","title":"Bonnes pratiques s\u00e9curit\u00e9","text":"<ul> <li>Authentification : Toujours activer</li> <li>Autorisation : Principe du moindre privil\u00e8ge</li> <li>Chiffrement : Pour donn\u00e9es sensibles</li> <li>R\u00e9seau : Limiter l'acc\u00e8s r\u00e9seau</li> </ul>"},{"location":"MongoDB/FR/07-best-practices/#performance","title":"Performance","text":""},{"location":"MongoDB/FR/07-best-practices/#index","title":"Index","text":"<pre><code>// Indexer les champs de recherche fr\u00e9quents\ndb.users.createIndex({email: 1})\n\n// Index compos\u00e9 pour requ\u00eates multiples\ndb.orders.createIndex({customer: 1, date: -1})\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#requetes","title":"Requ\u00eates","text":"<pre><code>// Utiliser projection pour limiter les donn\u00e9es\ndb.users.find({}, {name: 1, email: 1})\n\n// Limiter les r\u00e9sultats\ndb.users.find().limit(100)\n\n// \u00c9viter les scans complets\n// Toujours utiliser des index\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#write-concern","title":"Write Concern","text":"<pre><code>// Contr\u00f4ler la confirmation d'\u00e9criture\ndb.collection.insertOne(\n  {data: \"value\"},\n  {writeConcern: {w: 1, j: true}}\n)\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#maintenance","title":"Maintenance","text":""},{"location":"MongoDB/FR/07-best-practices/#nettoyage","title":"Nettoyage","text":"<pre><code>// Supprimer les documents obsol\u00e8tes\ndb.logs.deleteMany({\n  created_at: {$lt: new Date(\"2024-01-01\")}\n})\n\n// Compacter la collection\ndb.runCommand({compact: \"collection_name\"})\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#statistiques","title":"Statistiques","text":"<pre><code>// Statistiques d'une collection\ndb.collection.stats()\n\n// Statistiques de la base\ndb.stats()\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#backup-et-restore","title":"Backup et Restore","text":""},{"location":"MongoDB/FR/07-best-practices/#backup-mongodump","title":"Backup (mongodump)","text":"<pre><code># Backup d'une base de donn\u00e9es\nmongodump --db mydb --out /backup/\n\n# Backup d'une collection\nmongodump --db mydb --collection users --out /backup/\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#restore-mongorestore","title":"Restore (mongorestore)","text":"<pre><code># Restaurer une base de donn\u00e9es\nmongorestore --db mydb /backup/mydb/\n\n# Restaurer une collection\nmongorestore --db mydb --collection users /backup/mydb/users.bson\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#monitoring","title":"Monitoring","text":""},{"location":"MongoDB/FR/07-best-practices/#server-status","title":"Server Status","text":"<pre><code>// Statut du serveur\ndb.serverStatus()\n\n// Informations sur les op\u00e9rations\ndb.currentOp()\n\n// Statistiques de r\u00e9plication\nrs.status()\n</code></pre>"},{"location":"MongoDB/FR/07-best-practices/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>S\u00e9curit\u00e9 : Authentification et autorisation</li> <li>Performance : Index et requ\u00eates optimis\u00e9es</li> <li>Maintenance : Nettoyage r\u00e9gulier</li> <li>Backup : Sauvegardes r\u00e9guli\u00e8res</li> <li>Monitoring : Surveiller les performances</li> </ol>"},{"location":"MongoDB/FR/07-best-practices/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 8. Projets pratiques pour cr\u00e9er des projets complets.</p>"},{"location":"MongoDB/FR/08-projets/","title":"8. Projets pratiques MongoDB","text":""},{"location":"MongoDB/FR/08-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er une application Python avec MongoDB</li> <li>Pipeline de donn\u00e9es avec MongoDB</li> <li>Analyse de donn\u00e9es</li> <li>Projets pour portfolio</li> </ul>"},{"location":"MongoDB/FR/08-projets/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Projet 1 : Application Python</li> <li>Projet 2 : Pipeline de donn\u00e9es</li> <li>Projet 3 : Analyse de donn\u00e9es</li> <li>Projet 4 : API REST</li> </ol>"},{"location":"MongoDB/FR/08-projets/#projet-1-application-python","title":"Projet 1 : Application Python","text":""},{"location":"MongoDB/FR/08-projets/#objectif","title":"Objectif","text":"<p>Cr\u00e9er une application Python qui utilise MongoDB.</p>"},{"location":"MongoDB/FR/08-projets/#structure","title":"Structure","text":"<pre><code>mongodb-app/\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 app.py\n\u2514\u2500\u2500 config.py\n</code></pre>"},{"location":"MongoDB/FR/08-projets/#code-python","title":"Code Python","text":"<pre><code>from pymongo import MongoClient\nfrom datetime import datetime\n\n# Connexion\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['mydb']\ncollection = db['users']\n\n# Ins\u00e9rer un utilisateur\ndef create_user(name, email):\n    user = {\n        'name': name,\n        'email': email,\n        'created_at': datetime.now()\n    }\n    result = collection.insert_one(user)\n    return result.inserted_id\n\n# Trouver un utilisateur\ndef find_user(email):\n    return collection.find_one({'email': email})\n\n# Mettre \u00e0 jour\ndef update_user(email, updates):\n    return collection.update_one(\n        {'email': email},\n        {'$set': updates}\n    )\n\n# Utilisation\ncreate_user('John', 'john@example.com')\nuser = find_user('john@example.com')\nprint(user)\n</code></pre>"},{"location":"MongoDB/FR/08-projets/#requirementstxt","title":"requirements.txt","text":"<pre><code>pymongo==4.6.0\n</code></pre>"},{"location":"MongoDB/FR/08-projets/#projet-2-pipeline-de-donnees","title":"Projet 2 : Pipeline de donn\u00e9es","text":""},{"location":"MongoDB/FR/08-projets/#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er un pipeline ETL avec MongoDB.</p>"},{"location":"MongoDB/FR/08-projets/#code","title":"Code","text":"<pre><code>from pymongo import MongoClient\nimport pandas as pd\n\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['etl_db']\n\n# Extract : Lire depuis CSV\ndef extract():\n    df = pd.read_csv('data.csv')\n    return df\n\n# Transform : Transformer les donn\u00e9es\ndef transform(df):\n    df['processed_at'] = pd.Timestamp.now()\n    df = df.dropna()\n    return df\n\n# Load : Charger dans MongoDB\ndef load(df):\n    records = df.to_dict('records')\n    db.data.insert_many(records)\n\n# Pipeline complet\ndf = extract()\ndf = transform(df)\nload(df)\n</code></pre>"},{"location":"MongoDB/FR/08-projets/#projet-3-analyse-de-donnees","title":"Projet 3 : Analyse de donn\u00e9es","text":""},{"location":"MongoDB/FR/08-projets/#objectif_2","title":"Objectif","text":"<p>Analyser des donn\u00e9es avec MongoDB Aggregation.</p>"},{"location":"MongoDB/FR/08-projets/#code_1","title":"Code","text":"<pre><code>from pymongo import MongoClient\n\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['analytics_db']\n\n# Pipeline d'agr\u00e9gation\npipeline = [\n    # Filtrer par date\n    {\n        '$match': {\n            'date': {\n                '$gte': datetime(2024, 1, 1),\n                '$lt': datetime(2024, 2, 1)\n            }\n        }\n    },\n    # Grouper par produit\n    {\n        '$group': {\n            '_id': '$product',\n            'total_sales': {'$sum': '$amount'},\n            'count': {'$sum': 1},\n            'average': {'$avg': '$amount'}\n        }\n    },\n    # Trier\n    {\n        '$sort': {'total_sales': -1}\n    },\n    # Limiter\n    {\n        '$limit': 10\n    }\n]\n\n# Ex\u00e9cuter\nresults = db.sales.aggregate(pipeline)\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"MongoDB/FR/08-projets/#projet-4-api-rest","title":"Projet 4 : API REST","text":""},{"location":"MongoDB/FR/08-projets/#objectif_3","title":"Objectif","text":"<p>Cr\u00e9er une API REST avec Flask et MongoDB.</p>"},{"location":"MongoDB/FR/08-projets/#code_2","title":"Code","text":"<pre><code>from flask import Flask, request, jsonify\nfrom pymongo import MongoClient\n\napp = Flask(__name__)\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['api_db']\ncollection = db['items']\n\n@app.route('/items', methods=['GET'])\ndef get_items():\n    items = list(collection.find({}, {'_id': 0}))\n    return jsonify(items)\n\n@app.route('/items', methods=['POST'])\ndef create_item():\n    data = request.json\n    result = collection.insert_one(data)\n    return jsonify({'id': str(result.inserted_id)})\n\n@app.route('/items/&lt;item_id&gt;', methods=['GET'])\ndef get_item(item_id):\n    item = collection.find_one({'_id': ObjectId(item_id)}, {'_id': 0})\n    return jsonify(item)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"MongoDB/FR/08-projets/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Python : Int\u00e9gration avec pymongo</li> <li>Pipeline : ETL avec MongoDB</li> <li>Agr\u00e9gation : Analyse de donn\u00e9es</li> <li>API : REST avec MongoDB</li> <li>Portfolio : Projets d\u00e9montrables</li> </ol>"},{"location":"MongoDB/FR/08-projets/#ressources","title":"\ud83d\udd17 Ressources","text":"<ul> <li>PyMongo Documentation</li> <li>MongoDB Examples</li> </ul> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation MongoDB. Vous pouvez maintenant utiliser MongoDB dans vos projets de Data Analyst.</p>"},{"location":"MongoDB/PL/","title":"Szkolenie MongoDB dla Data Analyst","text":""},{"location":"MongoDB/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>To szkolenie poprowadzi Ci\u0119 przez nauk\u0119 MongoDB jako Data Analyst. MongoDB to baza danych NoSQL zorientowana na dokumenty, idealna do zarz\u0105dzania danymi nieustrukturyzowanymi i cz\u0119\u015bciowo ustrukturyzowanymi.</p>"},{"location":"MongoDB/PL/#cele-szkoleniowe","title":"\ud83c\udfaf Cele szkoleniowe","text":"<ul> <li>Zrozumie\u0107 MongoDB i NoSQL</li> <li>Zainstalowa\u0107 MongoDB</li> <li>Opanowa\u0107 operacje CRUD</li> <li>U\u017cywa\u0107 zapyta\u0144 i agregacji</li> <li>Optymalizowa\u0107 z indeksami</li> <li>Modelowa\u0107 dane</li> <li>Integrowa\u0107 MongoDB w przep\u0142ywy danych</li> <li>Tworzy\u0107 praktyczne projekty do portfolio</li> </ul>"},{"location":"MongoDB/PL/#wszystko-jest-darmowe","title":"\ud83d\udcb0 Wszystko jest darmowe!","text":"<p>To szkolenie u\u017cywa tylko: - \u2705 MongoDB Community Server : Darmowy i open-source - \u2705 MongoDB Compass : Darmowy interfejs graficzny - \u2705 MongoDB Atlas : Darmowy klaster (512 MB) - \u2705 Oficjalna dokumentacja : Kompletne darmowe przewodniki - \u2705 Tutoriale online : Darmowe zasoby</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"MongoDB/PL/#struktura-szkolenia","title":"\ud83d\udcd6 Struktura szkolenia","text":""},{"location":"MongoDB/PL/#1-rozpoczecie-z-mongodb","title":"1. Rozpocz\u0119cie z MongoDB","text":"<ul> <li>Zainstalowa\u0107 MongoDB</li> <li>Podstawowe koncepcje</li> <li>Pierwsze operacje</li> <li>Interfejs MongoDB Compass</li> </ul>"},{"location":"MongoDB/PL/#2-operacje-podstawowe","title":"2. Operacje podstawowe","text":"<ul> <li>CRUD (Create, Read, Update, Delete)</li> <li>Kolekcje i Dokumenty</li> <li>Typy danych</li> <li>Operatory zapyta\u0144</li> </ul>"},{"location":"MongoDB/PL/#3-zapytania-i-agregacja","title":"3. Zapytania i Agregacja","text":"<ul> <li>Zaawansowane zapytania</li> <li>Pipeline agregacji</li> <li>Operatory agregacji</li> <li>Grupowanie i obliczenia</li> </ul>"},{"location":"MongoDB/PL/#4-indeksy-i-wydajnosc","title":"4. Indeksy i Wydajno\u015b\u0107","text":"<ul> <li>Tworzy\u0107 indeksy</li> <li>Typy indeks\u00f3w</li> <li>Analiza wydajno\u015bci</li> <li>Optymalizacja zapyta\u0144</li> </ul>"},{"location":"MongoDB/PL/#5-modelowanie-danych","title":"5. Modelowanie danych","text":"<ul> <li>Modele danych</li> <li>Relacje (Embedded vs References)</li> <li>Elastyczne schematy</li> <li>Dobre praktyki</li> </ul>"},{"location":"MongoDB/PL/#6-funkcje-zaawansowane","title":"6. Funkcje zaawansowane","text":"<ul> <li>Transakcje</li> <li>Replikacja</li> <li>Sharding</li> <li>Wyszukiwanie tekstu</li> </ul>"},{"location":"MongoDB/PL/#7-dobre-praktyki","title":"7. Dobre praktyki","text":"<ul> <li>Bezpiecze\u0144stwo</li> <li>Wydajno\u015b\u0107</li> <li>Konserwacja</li> <li>Backup i Restore</li> </ul>"},{"location":"MongoDB/PL/#8-projekty-praktyczne","title":"8. Projekty praktyczne","text":"<ul> <li>Aplikacja Python z MongoDB</li> <li>Pipeline danych</li> <li>Analiza danych</li> <li>Projekty do portfolio</li> </ul>"},{"location":"MongoDB/PL/#szybki-start","title":"\ud83d\ude80 Szybki start","text":""},{"location":"MongoDB/PL/#wymagania-wstepne","title":"Wymagania wst\u0119pne","text":"<ul> <li>System operacyjny : Windows, Linux lub macOS</li> <li>4 GB RAM : Minimum zalecane</li> <li>Miejsce na dysku : 5 GB wolne</li> </ul>"},{"location":"MongoDB/PL/#szybka-instalacja","title":"Szybka instalacja","text":"<p>Windows: 1. Pobra\u0107 MongoDB: https://www.mongodb.com/try/download/community 2. Zainstalowa\u0107 z opcjami domy\u015blnymi 3. Sprawdzi\u0107: <code>mongod --version</code></p> <p>Linux: <pre><code>wget -qO - https://www.mongodb.org/static/pgp/server-7.0.asc | sudo apt-key add -\necho \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org\nsudo systemctl start mongod\n</code></pre></p> <p>macOS: <pre><code>brew tap mongodb/brew\nbrew install mongodb-community\nbrew services start mongodb-community\n</code></pre></p>"},{"location":"MongoDB/PL/#pierwszy-test","title":"Pierwszy test","text":"<pre><code>mongod\nmongosh\nuse test\ndb.collection.insertOne({name: \"test\"})\ndb.collection.find()\n</code></pre>"},{"location":"MongoDB/PL/#przypadki-uzycia-dla-data-analyst","title":"\ud83d\udcca Przypadki u\u017cycia dla Data Analyst","text":"<ul> <li>Dane nieustrukturyzowane : JSON, logi, API</li> <li>Elastyczno\u015b\u0107 : Ewoluuj\u0105ce schematy</li> <li>Agregacja : Pot\u0119\u017cny pipeline do analizy</li> <li>Integracja : Z Python, R, PowerBI</li> <li>Big Data : Skalowalno\u015b\u0107 pozioma</li> </ul>"},{"location":"MongoDB/PL/#darmowe-zasoby","title":"\ud83d\udcda Darmowe zasoby","text":""},{"location":"MongoDB/PL/#oficjalna-dokumentacja","title":"Oficjalna dokumentacja","text":"<ul> <li>Dokumentacja MongoDB : https://docs.mongodb.com/</li> <li>MongoDB University : https://university.mongodb.com/</li> <li>MongoDB Compass : https://www.mongodb.com/products/compass</li> </ul>"},{"location":"MongoDB/PL/01-getting-started/","title":"1. Rozpocz\u0119cie z MongoDB","text":""},{"location":"MongoDB/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 MongoDB i NoSQL</li> <li>Zainstalowa\u0107 MongoDB</li> <li>Zrozumie\u0107 podstawowe koncepcje</li> <li>U\u017cywa\u0107 MongoDB Compass</li> <li>Pierwsze operacje</li> </ul>"},{"location":"MongoDB/PL/01-getting-started/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do MongoDB</li> <li>Instalacja</li> <li>Podstawowe koncepcje</li> <li>MongoDB Compass</li> <li>Pierwsze operacje</li> </ol>"},{"location":"MongoDB/PL/01-getting-started/#wprowadzenie-do-mongodb","title":"Wprowadzenie do MongoDB","text":""},{"location":"MongoDB/PL/01-getting-started/#czym-jest-mongodb","title":"Czym jest MongoDB?","text":"<p>MongoDB = Baza danych NoSQL zorientowana na dokumenty</p> <ul> <li>NoSQL : Nierelacyjna</li> <li>Dokumenty : Przechowywanie w formacie JSON (BSON)</li> <li>Elastyczna : Ewoluuj\u0105ce schematy</li> <li>Skalowalna : Skalowalno\u015b\u0107 pozioma</li> <li>Open-source : Darmowa i open-source</li> </ul>"},{"location":"MongoDB/PL/01-getting-started/#dlaczego-mongodb-dla-data-analyst","title":"Dlaczego MongoDB dla Data Analyst?","text":"<ul> <li>Dane nieustrukturyzowane : JSON, logi, API</li> <li>Elastyczno\u015b\u0107 : Ewoluuj\u0105ce schematy</li> <li>Agregacja : Pot\u0119\u017cny pipeline do analizy</li> <li>Integracja : Z Python, R, PowerBI</li> <li>Wydajno\u015b\u0107 : Szybka dla z\u0142o\u017conych zapyta\u0144</li> </ul>"},{"location":"MongoDB/PL/01-getting-started/#instalacja","title":"Instalacja","text":""},{"location":"MongoDB/PL/01-getting-started/#windows","title":"Windows","text":"<ol> <li>Przej\u015b\u0107 do: https://www.mongodb.com/try/download/community</li> <li>Wybra\u0107 Windows</li> <li>Pobra\u0107 instalator MSI</li> <li>Uruchomi\u0107 instalator</li> <li>Wybra\u0107 instalacj\u0119 \"Complete\"</li> </ol>"},{"location":"MongoDB/PL/01-getting-started/#linux","title":"Linux","text":"<p>Ubuntu/Debian: <pre><code>wget -qO - https://www.mongodb.org/static/pgp/server-7.0.asc | sudo apt-key add -\necho \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org\nsudo systemctl start mongod\n</code></pre></p>"},{"location":"MongoDB/PL/01-getting-started/#macos","title":"macOS","text":"<p>Z Homebrew: <pre><code>brew tap mongodb/brew\nbrew install mongodb-community\nbrew services start mongodb-community\n</code></pre></p>"},{"location":"MongoDB/PL/01-getting-started/#podstawowe-koncepcje","title":"Podstawowe koncepcje","text":""},{"location":"MongoDB/PL/01-getting-started/#baza-danych","title":"Baza danych","text":"<p>Baza danych = Kontener kolekcji</p> <ul> <li>Auto-tworzenie : Tworzona przy pierwszym u\u017cyciu</li> <li>Nazwa : Unikalny identyfikator</li> <li>Kolekcje : Zawiera kolekcje</li> </ul>"},{"location":"MongoDB/PL/01-getting-started/#kolekcja","title":"Kolekcja","text":"<p>Kolekcja = Grupa dokument\u00f3w</p> <ul> <li>R\u00f3wnowa\u017cna : Tabela w SQL</li> <li>Elastyczna : Brak narzuconego schematu</li> <li>Dokumenty : Zawiera dokumenty</li> </ul>"},{"location":"MongoDB/PL/01-getting-started/#dokument","title":"Dokument","text":"<p>Dokument = Rekord w formacie JSON</p> <ul> <li>Format : BSON (Binary JSON)</li> <li>Elastyczny : Zmienna struktura</li> <li>Pola : Pary klucz-warto\u015b\u0107</li> </ul>"},{"location":"MongoDB/PL/01-getting-started/#mongodb-compass","title":"MongoDB Compass","text":""},{"location":"MongoDB/PL/01-getting-started/#czym-jest-compass","title":"Czym jest Compass?","text":"<p>MongoDB Compass = Interfejs graficzny</p> <ul> <li>Wizualizacja : Widzie\u0107 dane</li> <li>Zapytania : Wykonywa\u0107 zapytania</li> <li>Analiza : Analizowa\u0107 wydajno\u015b\u0107</li> <li>Zarz\u0105dzanie : Zarz\u0105dza\u0107 indeksami</li> </ul>"},{"location":"MongoDB/PL/01-getting-started/#pierwsze-operacje","title":"Pierwsze operacje","text":""},{"location":"MongoDB/PL/01-getting-started/#poaczyc-z-mongosh","title":"Po\u0142\u0105czy\u0107 z mongosh","text":"<pre><code># Uruchomi\u0107 mongosh\nmongosh\n\n# Zobaczy\u0107 bazy danych\nshow dbs\n\n# U\u017cywa\u0107 bazy danych\nuse mydb\n\n# Zobaczy\u0107 kolekcje\nshow collections\n\n# Wstawi\u0107 dokument\ndb.users.insertOne({name: \"John\", age: 30, city: \"Warsaw\"})\n\n# Znale\u017a\u0107 dokumenty\ndb.users.find()\n</code></pre>"},{"location":"MongoDB/PL/01-getting-started/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>MongoDB = Baza danych NoSQL zorientowana na dokumenty</li> <li>Dokumenty = Format JSON (BSON)</li> <li>Kolekcje = Grupy dokument\u00f3w</li> <li>Bazy danych = Kontenery kolekcji</li> <li>Compass = Interfejs graficzny</li> </ol>"},{"location":"MongoDB/PL/01-getting-started/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 2. Operacje podstawowe, aby opanowa\u0107 CRUD.</p>"},{"location":"MongoDB/PL/02-basic-operations/","title":"2. Operacje podstawowe MongoDB","text":""},{"location":"MongoDB/PL/02-basic-operations/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Opanowa\u0107 operacje CRUD</li> <li>Zrozumie\u0107 kolekcje i dokumenty</li> <li>U\u017cywa\u0107 typ\u00f3w danych</li> <li>Stosowa\u0107 operatory zapyta\u0144</li> <li>Zarz\u0105dza\u0107 aktualizacjami</li> </ul>"},{"location":"MongoDB/PL/02-basic-operations/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Operacje CRUD</li> <li>Kolekcje i Dokumenty</li> <li>Typy danych</li> <li>Operatory zapyta\u0144</li> <li>Aktualizacje</li> </ol>"},{"location":"MongoDB/PL/02-basic-operations/#operacje-crud","title":"Operacje CRUD","text":""},{"location":"MongoDB/PL/02-basic-operations/#create-tworzyc","title":"Create (Tworzy\u0107)","text":"<pre><code>// Wstawi\u0107 jeden dokument\ndb.users.insertOne({\n  name: \"John\",\n  age: 30,\n  email: \"john@example.com\"\n})\n\n// Wstawi\u0107 wiele dokument\u00f3w\ndb.users.insertMany([\n  {name: \"Alice\", age: 28},\n  {name: \"Bob\", age: 32}\n])\n</code></pre>"},{"location":"MongoDB/PL/02-basic-operations/#read-czytac","title":"Read (Czyta\u0107)","text":"<pre><code>// Znale\u017a\u0107 wszystkie dokumenty\ndb.users.find()\n\n// Znale\u017a\u0107 z filtrem\ndb.users.find({age: 30})\n\n// Znale\u017a\u0107 jeden dokument\ndb.users.findOne({name: \"John\"})\n\n// Ograniczy\u0107 wyniki\ndb.users.find().limit(5)\n\n// Sortowa\u0107\ndb.users.find().sort({age: 1})\n</code></pre>"},{"location":"MongoDB/PL/02-basic-operations/#update-aktualizowac","title":"Update (Aktualizowa\u0107)","text":"<pre><code>// Aktualizowa\u0107 jeden dokument\ndb.users.updateOne(\n  {name: \"John\"},\n  {$set: {age: 31}}\n)\n\n// Aktualizowa\u0107 wiele dokument\u00f3w\ndb.users.updateMany(\n  {age: {$lt: 30}},\n  {$set: {status: \"young\"}}\n)\n</code></pre>"},{"location":"MongoDB/PL/02-basic-operations/#delete-usuwac","title":"Delete (Usuwa\u0107)","text":"<pre><code>// Usun\u0105\u0107 jeden dokument\ndb.users.deleteOne({name: \"John\"})\n\n// Usun\u0105\u0107 wiele dokument\u00f3w\ndb.users.deleteMany({age: {$lt: 18}})\n</code></pre>"},{"location":"MongoDB/PL/02-basic-operations/#typy-danych","title":"Typy danych","text":""},{"location":"MongoDB/PL/02-basic-operations/#typy-podstawowe","title":"Typy podstawowe","text":"<pre><code>// String\n{name: \"John\"}\n\n// Number\n{age: 30}\n{price: 99.99}\n\n// Boolean\n{active: true}\n\n// Date\n{created_at: new Date()}\n\n// Array\n{hobbies: [\"reading\", \"coding\"]}\n\n// Object (Zagnie\u017cd\u017cony)\n{address: {street: \"123 Main St\", city: \"Warsaw\"}}\n</code></pre>"},{"location":"MongoDB/PL/02-basic-operations/#operatory-zapytan","title":"Operatory zapyta\u0144","text":""},{"location":"MongoDB/PL/02-basic-operations/#operatory-porownania","title":"Operatory por\u00f3wnania","text":"<pre><code>// R\u00f3wny\ndb.users.find({age: 30})\n\n// Wi\u0119kszy ni\u017c\ndb.users.find({age: {$gt: 30}})\n\n// Mniejszy ni\u017c\ndb.users.find({age: {$lt: 30}})\n\n// W li\u015bcie\ndb.users.find({age: {$in: [25, 30, 35]}})\n</code></pre>"},{"location":"MongoDB/PL/02-basic-operations/#operatory-logiczne","title":"Operatory logiczne","text":"<pre><code>// I (AND)\ndb.users.find({\n  $and: [\n    {age: {$gt: 25}},\n    {age: {$lt: 35}}\n  ]\n})\n\n// LUB (OR)\ndb.users.find({\n  $or: [\n    {age: {$lt: 25}},\n    {age: {$gt: 35}}\n  ]\n})\n</code></pre>"},{"location":"MongoDB/PL/02-basic-operations/#aktualizacje","title":"Aktualizacje","text":""},{"location":"MongoDB/PL/02-basic-operations/#operatory-aktualizacji","title":"Operatory aktualizacji","text":"<pre><code>// $set: Ustawi\u0107 pole\ndb.users.updateOne(\n  {name: \"John\"},\n  {$set: {age: 31}}\n)\n\n// $inc: Zwi\u0119kszy\u0107\ndb.users.updateOne(\n  {name: \"John\"},\n  {$inc: {age: 1}}\n)\n\n// $push: Doda\u0107 do tablicy\ndb.users.updateOne(\n  {name: \"John\"},\n  {$push: {hobbies: \"swimming\"}}\n)\n</code></pre>"},{"location":"MongoDB/PL/02-basic-operations/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>CRUD : Create, Read, Update, Delete</li> <li>Dokumenty : Elastyczny format JSON</li> <li>Kolekcje : Grupy dokument\u00f3w</li> <li>Operatory : Do filtrowania i aktualizacji</li> <li>Typy : R\u00f3\u017cne typy danych wspierane</li> </ol>"},{"location":"MongoDB/PL/02-basic-operations/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 3. Zapytania i Agregacja, aby pozna\u0107 zaawansowane zapytania.</p>"},{"location":"MongoDB/PL/03-queries-aggregation/","title":"3. Zapytania i Agregacja MongoDB","text":""},{"location":"MongoDB/PL/03-queries-aggregation/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Opanowa\u0107 zaawansowane zapytania</li> <li>U\u017cywa\u0107 pipeline agregacji</li> <li>Stosowa\u0107 operatory agregacji</li> <li>Wykonywa\u0107 grupowanie i obliczenia</li> <li>Analizowa\u0107 z\u0142o\u017cone dane</li> </ul>"},{"location":"MongoDB/PL/03-queries-aggregation/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Zaawansowane zapytania</li> <li>Pipeline agregacji</li> <li>Operatory agregacji</li> <li>Grupowanie i obliczenia</li> <li>Przyk\u0142ady praktyczne</li> </ol>"},{"location":"MongoDB/PL/03-queries-aggregation/#zaawansowane-zapytania","title":"Zaawansowane zapytania","text":""},{"location":"MongoDB/PL/03-queries-aggregation/#projekcja","title":"Projekcja","text":"<pre><code>// Wybra\u0107 konkretne pola\ndb.users.find({}, {name: 1, email: 1, _id: 0})\n</code></pre>"},{"location":"MongoDB/PL/03-queries-aggregation/#sortowanie-i-limit","title":"Sortowanie i limit","text":"<pre><code>// Sortowa\u0107 wed\u0142ug wieku (rosn\u0105co)\ndb.users.find().sort({age: 1})\n\n// Ograniczy\u0107 wyniki\ndb.users.find().limit(10)\n</code></pre>"},{"location":"MongoDB/PL/03-queries-aggregation/#pipeline-agregacji","title":"Pipeline agregacji","text":""},{"location":"MongoDB/PL/03-queries-aggregation/#czym-jest-pipeline","title":"Czym jest Pipeline?","text":"<p>Pipeline = Seria krok\u00f3w transformacji</p> <ul> <li>Kroki : Ka\u017cdy krok przekszta\u0142ca dane</li> <li>Sekwencyjny : Wynik jednego kroku = wej\u015bcie nast\u0119pnego</li> <li>Pot\u0119\u017cny : Do z\u0142o\u017conej analizy</li> </ul>"},{"location":"MongoDB/PL/03-queries-aggregation/#podstawowa-struktura","title":"Podstawowa struktura","text":"<pre><code>db.collection.aggregate([\n  { $match: { ... } },      // Filtrowa\u0107\n  { $group: { ... } },       // Grupowa\u0107\n  { $sort: { ... } },        // Sortowa\u0107\n  { $project: { ... } }      // Wybiera\u0107\n])\n</code></pre>"},{"location":"MongoDB/PL/03-queries-aggregation/#operatory-agregacji","title":"Operatory agregacji","text":""},{"location":"MongoDB/PL/03-queries-aggregation/#match","title":"$match","text":"<pre><code>db.sales.aggregate([\n  {$match: {amount: {$gt: 500}}}\n])\n</code></pre>"},{"location":"MongoDB/PL/03-queries-aggregation/#group","title":"$group","text":"<pre><code>db.products.aggregate([\n  {\n    $group: {\n      _id: \"$category\",\n      total: {$sum: \"$price\"},\n      count: {$sum: 1},\n      average: {$avg: \"$price\"}\n    }\n  }\n])\n</code></pre>"},{"location":"MongoDB/PL/03-queries-aggregation/#project","title":"$project","text":"<pre><code>db.users.aggregate([\n  {\n    $project: {\n      name: 1,\n      age: 1,\n      isAdult: {$gte: [\"$age\", 18]}\n    }\n  }\n])\n</code></pre>"},{"location":"MongoDB/PL/03-queries-aggregation/#grupowanie-i-obliczenia","title":"Grupowanie i obliczenia","text":""},{"location":"MongoDB/PL/03-queries-aggregation/#operatory-akumulacji","title":"Operatory akumulacji","text":"<pre><code>// Suma\n{$sum: \"$amount\"}\n\n// \u015arednia\n{$avg: \"$price\"}\n\n// Minimum\n{$min: \"$price\"}\n\n// Maximum\n{$max: \"$price\"}\n\n// Liczenie\n{$sum: 1}\n</code></pre>"},{"location":"MongoDB/PL/03-queries-aggregation/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Pipeline : Seria krok\u00f3w transformacji</li> <li>$match : Filtrowa\u0107 dokumenty</li> <li>$group : Grupowa\u0107 i oblicza\u0107</li> <li>$project : Wybiera\u0107 i przekszta\u0142ca\u0107</li> <li>Agregacja : Pot\u0119\u017cna do analizy</li> </ol>"},{"location":"MongoDB/PL/03-queries-aggregation/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 4. Indeksy i Wydajno\u015b\u0107, aby optymalizowa\u0107 wydajno\u015b\u0107.</p>"},{"location":"MongoDB/PL/04-indexes-performance/","title":"4. Indeksy i Wydajno\u015b\u0107 MongoDB","text":""},{"location":"MongoDB/PL/04-indexes-performance/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 indeksy</li> <li>Tworzy\u0107 r\u00f3\u017cne typy indeks\u00f3w</li> <li>Analizowa\u0107 wydajno\u015b\u0107</li> <li>Optymalizowa\u0107 zapytania</li> <li>U\u017cywa\u0107 explain()</li> </ul>"},{"location":"MongoDB/PL/04-indexes-performance/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Wprowadzenie do indeks\u00f3w</li> <li>Typy indeks\u00f3w</li> <li>Tworzy\u0107 indeksy</li> <li>Analizowa\u0107 wydajno\u015b\u0107</li> <li>Optymalizacja</li> </ol>"},{"location":"MongoDB/PL/04-indexes-performance/#wprowadzenie-do-indeksow","title":"Wprowadzenie do indeks\u00f3w","text":""},{"location":"MongoDB/PL/04-indexes-performance/#czym-jest-indeks","title":"Czym jest indeks?","text":"<p>Indeks = Struktura danych do przyspieszenia zapyta\u0144</p> <ul> <li>Wydajno\u015b\u0107 : Szybsze wyszukiwanie</li> <li>Koszt : Dodatkowa przestrze\u0144 dyskowa</li> <li>Konserwacja : Automatyczne aktualizacje</li> </ul>"},{"location":"MongoDB/PL/04-indexes-performance/#typy-indeksow","title":"Typy indeks\u00f3w","text":""},{"location":"MongoDB/PL/04-indexes-performance/#indeks-prosty","title":"Indeks prosty","text":"<pre><code>// Indeks na jednym polu\ndb.users.createIndex({email: 1})\n</code></pre>"},{"location":"MongoDB/PL/04-indexes-performance/#indeks-zozony","title":"Indeks z\u0142o\u017cony","text":"<pre><code>// Indeks na wielu polach\ndb.users.createIndex({name: 1, age: -1})\n</code></pre>"},{"location":"MongoDB/PL/04-indexes-performance/#indeks-unikalny","title":"Indeks unikalny","text":"<pre><code>// Zapewni\u0107 unikalno\u015b\u0107\ndb.users.createIndex({email: 1}, {unique: true})\n</code></pre>"},{"location":"MongoDB/PL/04-indexes-performance/#tworzyc-indeksy","title":"Tworzy\u0107 indeksy","text":""},{"location":"MongoDB/PL/04-indexes-performance/#metody-tworzenia","title":"Metody tworzenia","text":"<pre><code>// Utworzy\u0107 indeks\ndb.collection.createIndex({field: 1})\n\n// Zobaczy\u0107 indeksy\ndb.collection.getIndexes()\n\n// Usun\u0105\u0107 indeks\ndb.collection.dropIndex({field: 1})\n</code></pre>"},{"location":"MongoDB/PL/04-indexes-performance/#analizowac-wydajnosc","title":"Analizowa\u0107 wydajno\u015b\u0107","text":""},{"location":"MongoDB/PL/04-indexes-performance/#explain","title":"explain()","text":"<pre><code>// Plan wykonania\ndb.users.find({email: \"john@example.com\"}).explain()\n\n// Szczeg\u00f3\u0142owe statystyki\ndb.users.find({email: \"john@example.com\"}).explain(\"executionStats\")\n</code></pre>"},{"location":"MongoDB/PL/04-indexes-performance/#optymalizacja","title":"Optymalizacja","text":""},{"location":"MongoDB/PL/04-indexes-performance/#dobre-praktyki","title":"Dobre praktyki","text":"<p>1. Indeksowa\u0107 cz\u0119sto wyszukiwane pola:</p> <pre><code>db.users.createIndex({email: 1})\n</code></pre> <p>2. Indeks z\u0142o\u017cony dla wielu zapyta\u0144:</p> <pre><code>db.users.createIndex({name: 1, age: 1})\n</code></pre> <p>3. Unika\u0107 zbyt wielu indeks\u00f3w:</p> <ul> <li>Ka\u017cdy indeks spowalnia zapisy</li> <li>U\u017cywa\u0107 tylko niezb\u0119dnych indeks\u00f3w</li> </ul>"},{"location":"MongoDB/PL/04-indexes-performance/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Indeksy : Przyspieszaj\u0105 wyszukiwania</li> <li>Typy : Prosty, z\u0142o\u017cony, unikalny</li> <li>explain() : Analizowa\u0107 wydajno\u015b\u0107</li> <li>Optymalizacja : Indeksowa\u0107 cz\u0119ste pola</li> <li>R\u00f3wnowaga : Nie za du\u017co indeks\u00f3w</li> </ol>"},{"location":"MongoDB/PL/04-indexes-performance/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 5. Modelowanie danych, aby nauczy\u0107 si\u0119 modelowania.</p>"},{"location":"MongoDB/PL/05-data-modeling/","title":"5. Modelowanie danych MongoDB","text":""},{"location":"MongoDB/PL/05-data-modeling/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 modele danych</li> <li>Wybiera\u0107 mi\u0119dzy Embedded i References</li> <li>Projektowa\u0107 elastyczne schematy</li> <li>Stosowa\u0107 dobre praktyki</li> <li>Optymalizowa\u0107 struktur\u0119</li> </ul>"},{"location":"MongoDB/PL/05-data-modeling/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Modele danych</li> <li>Embedded vs References</li> <li>Elastyczne schematy</li> <li>Dobre praktyki</li> <li>Przyk\u0142ady praktyczne</li> </ol>"},{"location":"MongoDB/PL/05-data-modeling/#modele-danych","title":"Modele danych","text":""},{"location":"MongoDB/PL/05-data-modeling/#model-embedded-zagniezdzony","title":"Model Embedded (Zagnie\u017cd\u017cony)","text":"<p>Wszystko w jednym dokumencie:</p> <pre><code>{\n  _id: ObjectId(\"...\"),\n  name: \"John\",\n  address: {\n    street: \"123 Main St\",\n    city: \"Warsaw\"\n  }\n}\n</code></pre> <p>Zalety: - Szybki dost\u0119p (jeden dokument) - Brak join\u00f3w - Sp\u00f3jne dane</p>"},{"location":"MongoDB/PL/05-data-modeling/#model-references-referencje","title":"Model References (Referencje)","text":"<p>Osobne dokumenty z referencjami:</p> <pre><code>// Kolekcja users\n{_id: ObjectId(\"...\"), name: \"John\"}\n\n// Kolekcja addresses\n{_id: ObjectId(\"...\"), user_id: ObjectId(\"...\"), street: \"123 Main St\"}\n</code></pre> <p>Zalety: - Brak limitu rozmiaru - Brak duplikacji - Elastyczno\u015b\u0107</p>"},{"location":"MongoDB/PL/05-data-modeling/#embedded-vs-references","title":"Embedded vs References","text":""},{"location":"MongoDB/PL/05-data-modeling/#kiedy-uzywac-embedded","title":"Kiedy u\u017cywa\u0107 Embedded?","text":"<p>Przypadki u\u017cycia: - Dane cz\u0119sto dost\u0119pne razem - Ma\u0142e ilo\u015bci danych - Relacja 1:1 lub 1:kilka - Dane rzadko si\u0119 zmieniaj\u0105</p>"},{"location":"MongoDB/PL/05-data-modeling/#kiedy-uzywac-references","title":"Kiedy u\u017cywa\u0107 References?","text":"<p>Przypadki u\u017cycia: - Du\u017ce ilo\u015bci danych - Relacja 1:wiele lub wiele:wiele - Dane wsp\u00f3\u0142dzielone - Dane cz\u0119sto si\u0119 zmieniaj\u0105</p>"},{"location":"MongoDB/PL/05-data-modeling/#elastyczne-schematy","title":"Elastyczne schematy","text":""},{"location":"MongoDB/PL/05-data-modeling/#ewolucja-schematu","title":"Ewolucja schematu","text":"<pre><code>// Dokument pocz\u0105tkowy\n{name: \"John\", age: 30}\n\n// Doda\u0107 pole p\u00f3\u017aniej\n{name: \"John\", age: 30, email: \"john@example.com\"}\n</code></pre>"},{"location":"MongoDB/PL/05-data-modeling/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"MongoDB/PL/05-data-modeling/#wzorce-modelowania","title":"Wzorce modelowania","text":"<p>One-to-Few: <pre><code>// Embedded\n{name: \"John\", addresses: [{street: \"123 Main St\"}]}\n</code></pre></p> <p>One-to-Many: <pre><code>// References\n// Kolekcja users\n{_id: ObjectId(\"...\"), name: \"John\"}\n\n// Kolekcja orders\n{user_id: ObjectId(\"...\"), items: [...]}\n</code></pre></p>"},{"location":"MongoDB/PL/05-data-modeling/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Embedded : Dla danych cz\u0119sto dost\u0119pnych razem</li> <li>References : Dla du\u017cych ilo\u015bci lub z\u0142o\u017conych relacji</li> <li>Elastyczno\u015b\u0107 : Ewoluuj\u0105ce schematy</li> <li>Wzorce : One-to-Few, One-to-Many, Many-to-Many</li> <li>Wydajno\u015b\u0107 : R\u00f3wnowa\u017cy\u0107 dost\u0119p i sp\u00f3jno\u015b\u0107</li> </ol>"},{"location":"MongoDB/PL/05-data-modeling/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 6. Funkcje zaawansowane, aby pog\u0142\u0119bi\u0107.</p>"},{"location":"MongoDB/PL/06-advanced/","title":"6. Funkcje zaawansowane MongoDB","text":""},{"location":"MongoDB/PL/06-advanced/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>U\u017cywa\u0107 transakcji</li> <li>Zrozumie\u0107 replikacj\u0119</li> <li>Opanowa\u0107 sharding</li> <li>U\u017cywa\u0107 wyszukiwania tekstu</li> <li>Funkcje zaawansowane</li> </ul>"},{"location":"MongoDB/PL/06-advanced/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Transakcje</li> <li>Replikacja</li> <li>Sharding</li> <li>Wyszukiwanie tekstu</li> <li>Inne funkcje</li> </ol>"},{"location":"MongoDB/PL/06-advanced/#transakcje","title":"Transakcje","text":""},{"location":"MongoDB/PL/06-advanced/#czym-jest-transakcja","title":"Czym jest transakcja?","text":"<p>Transakcja = Grupa operacji atomowych</p> <ul> <li>Atomowa : Wszystko lub nic</li> <li>Sp\u00f3jno\u015b\u0107 : Sp\u00f3jne dane</li> <li>Izolacja : Izolowane operacje</li> <li>Trwa\u0142o\u015b\u0107 : Trwa\u0142e zmiany</li> </ul>"},{"location":"MongoDB/PL/06-advanced/#uzywac-transakcji","title":"U\u017cywa\u0107 transakcji","text":"<pre><code>const session = db.getMongo().startSession()\nsession.startTransaction()\n\ntry {\n  db.users.insertOne({name: \"John\"}, {session})\n  db.orders.insertOne({user_id: \"...\", items: [...]}, {session})\n  session.commitTransaction()\n} catch (error) {\n  session.abortTransaction()\n} finally {\n  session.endSession()\n}\n</code></pre>"},{"location":"MongoDB/PL/06-advanced/#replikacja","title":"Replikacja","text":""},{"location":"MongoDB/PL/06-advanced/#czym-jest-replikacja","title":"Czym jest replikacja?","text":"<p>Replikacja = Wiele kopii danych</p> <ul> <li>Wysoka dost\u0119pno\u015b\u0107 : Brak pojedynczego punktu awarii</li> <li>Nadmiarowo\u015b\u0107 : Automatyczna kopia zapasowa</li> <li>Wydajno\u015b\u0107 : Odczyt z wielu serwer\u00f3w</li> </ul>"},{"location":"MongoDB/PL/06-advanced/#sharding","title":"Sharding","text":""},{"location":"MongoDB/PL/06-advanced/#czym-jest-sharding","title":"Czym jest sharding?","text":"<p>Sharding = Partycjonowanie poziome</p> <ul> <li>Skalowalno\u015b\u0107 : Rozk\u0142ada\u0107 dane</li> <li>Wydajno\u015b\u0107 : Przetwarza\u0107 r\u00f3wnolegle</li> <li>Magazyn : Wi\u0119cej pojemno\u015bci</li> </ul>"},{"location":"MongoDB/PL/06-advanced/#wyszukiwanie-tekstu","title":"Wyszukiwanie tekstu","text":""},{"location":"MongoDB/PL/06-advanced/#indeks-tekstu","title":"Indeks tekstu","text":"<pre><code>// Utworzy\u0107 indeks tekstu\ndb.articles.createIndex({\n  title: \"text\",\n  content: \"text\"\n})\n\n// Wyszukiwa\u0107\ndb.articles.find({\n  $text: {$search: \"mongodb tutorial\"}\n})\n</code></pre>"},{"location":"MongoDB/PL/06-advanced/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Transakcje : Operacje atomowe</li> <li>Replikacja : Wysoka dost\u0119pno\u015b\u0107</li> <li>Sharding : Skalowalno\u015b\u0107 pozioma</li> <li>Wyszukiwanie tekstu : Wyszukiwanie tekstu</li> <li>Walidacja : Opcjonalne schematy</li> </ol>"},{"location":"MongoDB/PL/06-advanced/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 7. Dobre praktyki, aby pozna\u0107 najlepsze praktyki.</p>"},{"location":"MongoDB/PL/07-best-practices/","title":"7. Dobre praktyki MongoDB","text":""},{"location":"MongoDB/PL/07-best-practices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Bezpiecze\u0144stwo</li> <li>Wydajno\u015b\u0107</li> <li>Konserwacja</li> <li>Backup i Restore</li> <li>Monitorowanie</li> </ul>"},{"location":"MongoDB/PL/07-best-practices/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Bezpiecze\u0144stwo</li> <li>Wydajno\u015b\u0107</li> <li>Konserwacja</li> <li>Backup i Restore</li> <li>Monitorowanie</li> </ol>"},{"location":"MongoDB/PL/07-best-practices/#bezpieczenstwo","title":"Bezpiecze\u0144stwo","text":""},{"location":"MongoDB/PL/07-best-practices/#uwierzytelnianie","title":"Uwierzytelnianie","text":"<pre><code>// Utworzy\u0107 u\u017cytkownika admin\nuse admin\ndb.createUser({\n  user: \"admin\",\n  pwd: \"secure_password\",\n  roles: [\"root\"]\n})\n</code></pre>"},{"location":"MongoDB/PL/07-best-practices/#bezpieczne-poaczenie","title":"Bezpieczne po\u0142\u0105czenie","text":"<pre><code>mongosh -u admin -p secure_password --authenticationDatabase admin\n</code></pre>"},{"location":"MongoDB/PL/07-best-practices/#wydajnosc","title":"Wydajno\u015b\u0107","text":""},{"location":"MongoDB/PL/07-best-practices/#indeksy","title":"Indeksy","text":"<pre><code>// Indeksowa\u0107 cz\u0119sto wyszukiwane pola\ndb.users.createIndex({email: 1})\n\n// Indeks z\u0142o\u017cony dla wielu zapyta\u0144\ndb.orders.createIndex({customer: 1, date: -1})\n</code></pre>"},{"location":"MongoDB/PL/07-best-practices/#zapytania","title":"Zapytania","text":"<pre><code>// U\u017cywa\u0107 projekcji do ograniczenia danych\ndb.users.find({}, {name: 1, email: 1})\n\n// Ogranicza\u0107 wyniki\ndb.users.find().limit(100)\n</code></pre>"},{"location":"MongoDB/PL/07-best-practices/#konserwacja","title":"Konserwacja","text":""},{"location":"MongoDB/PL/07-best-practices/#czyszczenie","title":"Czyszczenie","text":"<pre><code>// Usun\u0105\u0107 przestarza\u0142e dokumenty\ndb.logs.deleteMany({\n  created_at: {$lt: new Date(\"2024-01-01\")}\n})\n</code></pre>"},{"location":"MongoDB/PL/07-best-practices/#backup-i-restore","title":"Backup i Restore","text":""},{"location":"MongoDB/PL/07-best-practices/#backup-mongodump","title":"Backup (mongodump)","text":"<pre><code># Backup bazy danych\nmongodump --db mydb --out /backup/\n</code></pre>"},{"location":"MongoDB/PL/07-best-practices/#restore-mongorestore","title":"Restore (mongorestore)","text":"<pre><code># Przywr\u00f3ci\u0107 baz\u0119 danych\nmongorestore --db mydb /backup/mydb/\n</code></pre>"},{"location":"MongoDB/PL/07-best-practices/#monitorowanie","title":"Monitorowanie","text":""},{"location":"MongoDB/PL/07-best-practices/#status-serwera","title":"Status serwera","text":"<pre><code>// Status serwera\ndb.serverStatus()\n\n// Bie\u017c\u0105ce operacje\ndb.currentOp()\n</code></pre>"},{"location":"MongoDB/PL/07-best-practices/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Bezpiecze\u0144stwo : Uwierzytelnianie i autoryzacja</li> <li>Wydajno\u015b\u0107 : Indeksy i zoptymalizowane zapytania</li> <li>Konserwacja : Regularne czyszczenie</li> <li>Backup : Regularne kopie zapasowe</li> <li>Monitorowanie : Monitorowa\u0107 wydajno\u015b\u0107</li> </ol>"},{"location":"MongoDB/PL/07-best-practices/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 8. Projekty praktyczne, aby tworzy\u0107 kompletne projekty.</p>"},{"location":"MongoDB/PL/08-projets/","title":"8. Projekty praktyczne MongoDB","text":""},{"location":"MongoDB/PL/08-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 aplikacj\u0119 Python z MongoDB</li> <li>Pipeline danych z MongoDB</li> <li>Analiza danych</li> <li>Projekty do portfolio</li> </ul>"},{"location":"MongoDB/PL/08-projets/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Projekt 1 : Aplikacja Python</li> <li>Projekt 2 : Pipeline danych</li> <li>Projekt 3 : Analiza danych</li> <li>Projekt 4 : API REST</li> </ol>"},{"location":"MongoDB/PL/08-projets/#projekt-1-aplikacja-python","title":"Projekt 1 : Aplikacja Python","text":""},{"location":"MongoDB/PL/08-projets/#cel","title":"Cel","text":"<p>Utworzy\u0107 aplikacj\u0119 Python u\u017cywaj\u0105c\u0105 MongoDB.</p>"},{"location":"MongoDB/PL/08-projets/#kod-python","title":"Kod Python","text":"<pre><code>from pymongo import MongoClient\nfrom datetime import datetime\n\n# Po\u0142\u0105czenie\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['mydb']\ncollection = db['users']\n\n# Wstawi\u0107 u\u017cytkownika\ndef create_user(name, email):\n    user = {\n        'name': name,\n        'email': email,\n        'created_at': datetime.now()\n    }\n    result = collection.insert_one(user)\n    return result.inserted_id\n\n# Znale\u017a\u0107 u\u017cytkownika\ndef find_user(email):\n    return collection.find_one({'email': email})\n\n# U\u017cycie\ncreate_user('John', 'john@example.com')\nuser = find_user('john@example.com')\nprint(user)\n</code></pre>"},{"location":"MongoDB/PL/08-projets/#projekt-2-pipeline-danych","title":"Projekt 2 : Pipeline danych","text":""},{"location":"MongoDB/PL/08-projets/#cel_1","title":"Cel","text":"<p>Utworzy\u0107 pipeline ETL z MongoDB.</p>"},{"location":"MongoDB/PL/08-projets/#kod","title":"Kod","text":"<pre><code>from pymongo import MongoClient\nimport pandas as pd\n\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['etl_db']\n\n# Extract\ndef extract():\n    df = pd.read_csv('data.csv')\n    return df\n\n# Transform\ndef transform(df):\n    df['processed_at'] = pd.Timestamp.now()\n    df = df.dropna()\n    return df\n\n# Load\ndef load(df):\n    records = df.to_dict('records')\n    db.data.insert_many(records)\n\n# Pipeline\ndf = extract()\ndf = transform(df)\nload(df)\n</code></pre>"},{"location":"MongoDB/PL/08-projets/#projekt-3-analiza-danych","title":"Projekt 3 : Analiza danych","text":""},{"location":"MongoDB/PL/08-projets/#cel_2","title":"Cel","text":"<p>Analizowa\u0107 dane z MongoDB Aggregation.</p>"},{"location":"MongoDB/PL/08-projets/#kod_1","title":"Kod","text":"<pre><code>from pymongo import MongoClient\n\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['analytics_db']\n\n# Pipeline agregacji\npipeline = [\n    {'$match': {'date': {'$gte': datetime(2024, 1, 1)}}},\n    {'$group': {\n        '_id': '$product',\n        'total_sales': {'$sum': '$amount'},\n        'count': {'$sum': 1}\n    }},\n    {'$sort': {'total_sales': -1}},\n    {'$limit': 10}\n]\n\n# Wykona\u0107\nresults = db.sales.aggregate(pipeline)\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"MongoDB/PL/08-projets/#projekt-4-api-rest","title":"Projekt 4 : API REST","text":""},{"location":"MongoDB/PL/08-projets/#cel_3","title":"Cel","text":"<p>Utworzy\u0107 API REST z Flask i MongoDB.</p>"},{"location":"MongoDB/PL/08-projets/#kod_2","title":"Kod","text":"<pre><code>from flask import Flask, request, jsonify\nfrom pymongo import MongoClient\n\napp = Flask(__name__)\nclient = MongoClient('mongodb://localhost:27017/')\ndb = client['api_db']\ncollection = db['items']\n\n@app.route('/items', methods=['GET'])\ndef get_items():\n    items = list(collection.find({}, {'_id': 0}))\n    return jsonify(items)\n\n@app.route('/items', methods=['POST'])\ndef create_item():\n    data = request.json\n    result = collection.insert_one(data)\n    return jsonify({'id': str(result.inserted_id)})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"MongoDB/PL/08-projets/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Python : Integracja z pymongo</li> <li>Pipeline : ETL z MongoDB</li> <li>Agregacja : Analiza danych</li> <li>API : REST z MongoDB</li> <li>Portfolio : Projekty demonstrowalne</li> </ol>"},{"location":"MongoDB/PL/08-projets/#zasoby","title":"\ud83d\udd17 Zasoby","text":"<ul> <li>Dokumentacja PyMongo</li> <li>Przyk\u0142ady MongoDB</li> </ul> <p>Gratulacje ! Uko\u0144czy\u0142e\u015b szkolenie MongoDB. Mo\u017cesz teraz u\u017cywa\u0107 MongoDB w projektach Data Analyst.</p>"},{"location":"Qdrant/Data/EN/","title":"Qdrant Training for Data Analyst","text":""},{"location":"Qdrant/Data/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through learning Qdrant as a Data Analyst. Qdrant is a vector database optimized for similarity search, ideal for AI, machine learning and semantic search.</p>"},{"location":"Qdrant/Data/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand vector databases</li> <li>Install and configure Qdrant</li> <li>Create and manage collections</li> <li>Index and search vectors</li> <li>Use similarity search</li> <li>Integrate with Python and AI</li> <li>Create practical projects</li> </ul>"},{"location":"Qdrant/Data/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 Qdrant Community Edition : Free and open-source - \u2705 Qdrant Python Client : Free library - \u2705 Official documentation : Complete free guides</p> <p>Total budget: $0</p>"},{"location":"Qdrant/Data/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"Qdrant/Data/EN/#1-getting-started-with-qdrant","title":"1. Getting Started with Qdrant","text":""},{"location":"Qdrant/Data/EN/#2-collections-and-vectors","title":"2. Collections and Vectors","text":""},{"location":"Qdrant/Data/EN/#3-similarity-search","title":"3. Similarity Search","text":""},{"location":"Qdrant/Data/EN/#4-filters-and-metadata","title":"4. Filters and Metadata","text":""},{"location":"Qdrant/Data/EN/#5-python-integration","title":"5. Python Integration","text":""},{"location":"Qdrant/Data/EN/#6-aiml-use-cases","title":"6. AI/ML Use Cases","text":""},{"location":"Qdrant/Data/EN/#7-best-practices","title":"7. Best Practices","text":""},{"location":"Qdrant/Data/EN/#8-practical-projects","title":"8. Practical Projects","text":""},{"location":"Qdrant/Data/EN/#prerequisites","title":"\ud83d\ude80 Prerequisites","text":"<ul> <li>Python basics</li> <li>AI/ML concepts (optional)</li> <li>Embeddings concepts (optional)</li> </ul>"},{"location":"Qdrant/Data/EN/#estimated-duration","title":"\u23f1\ufe0f Estimated Duration","text":"<ul> <li>Total : 30-40 hours</li> <li>Per module : 4-5 hours</li> </ul> <p>Happy learning! \ud83d\ude80</p>"},{"location":"Qdrant/Data/EN/01-getting-started/","title":"1. Getting Started with Qdrant","text":""},{"location":"Qdrant/Data/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand vector databases</li> <li>Install Qdrant</li> <li>Understand basic concepts</li> <li>First operations</li> </ul>"},{"location":"Qdrant/Data/EN/01-getting-started/#introduction-to-qdrant","title":"Introduction to Qdrant","text":"<p>Qdrant = Vector database</p> <ul> <li>Vectors : Numerical representations (embeddings)</li> <li>Similarity : Similarity search</li> <li>AI/ML : Optimized for artificial intelligence</li> <li>Open-source : Free and open-source</li> </ul>"},{"location":"Qdrant/Data/EN/01-getting-started/#installation","title":"Installation","text":""},{"location":"Qdrant/Data/EN/01-getting-started/#docker-recommended","title":"Docker (recommended)","text":"<pre><code>docker run -p 6333:6333 qdrant/qdrant\n</code></pre>"},{"location":"Qdrant/Data/EN/01-getting-started/#python-client","title":"Python Client","text":"<pre><code>pip install qdrant-client\n</code></pre>"},{"location":"Qdrant/Data/EN/01-getting-started/#first-example","title":"First Example","text":"<pre><code>from qdrant_client import QdrantClient\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\nclient.create_collection(\n    collection_name=\"test_collection\",\n    vectors_config={\n        \"size\": 128,\n        \"distance\": \"Cosine\"\n    }\n)\n</code></pre> <p>Next step : Collections and Vectors</p>"},{"location":"Qdrant/Data/EN/02-collections-vectors/","title":"2. Collections and Vectors","text":""},{"location":"Qdrant/Data/EN/02-collections-vectors/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create and manage collections</li> <li>Insert vectors</li> <li>Understand distances</li> <li>Manage metadata</li> </ul>"},{"location":"Qdrant/Data/EN/02-collections-vectors/#create-collection","title":"Create Collection","text":"<pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\nclient.create_collection(\n    collection_name=\"products\",\n    vectors_config=VectorParams(\n        size=128,\n        distance=Distance.COSINE\n    )\n)\n</code></pre>"},{"location":"Qdrant/Data/EN/02-collections-vectors/#insert-vectors","title":"Insert Vectors","text":"<pre><code>from qdrant_client.models import PointStruct\n\npoints = [\n    PointStruct(\n        id=1,\n        vector=[0.1, 0.2, 0.3, ...],\n        payload={\"name\": \"Product A\", \"category\": \"electronics\"}\n    )\n]\n\nclient.upsert(collection_name=\"products\", points=points)\n</code></pre> <p>Next step : Similarity Search</p>"},{"location":"Qdrant/Data/EN/03-similarity-search/","title":"3. Similarity Search","text":""},{"location":"Qdrant/Data/EN/03-similarity-search/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Perform similarity searches</li> <li>Use different algorithms</li> <li>Optimize performance</li> <li>Understand scores</li> </ul>"},{"location":"Qdrant/Data/EN/03-similarity-search/#simple-search","title":"Simple Search","text":"<pre><code>results = client.search(\n    collection_name=\"products\",\n    query_vector=[0.1, 0.2, 0.3, ...],\n    limit=10\n)\n\nfor result in results:\n    print(f\"ID: {result.id}, Score: {result.score}\")\n</code></pre>"},{"location":"Qdrant/Data/EN/03-similarity-search/#search-with-filters","title":"Search with Filters","text":"<pre><code>from qdrant_client.models import Filter, FieldCondition, MatchValue\n\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=[0.1, 0.2, 0.3, ...],\n    query_filter=Filter(\n        must=[\n            FieldCondition(\n                key=\"category\",\n                match=MatchValue(value=\"electronics\")\n            )\n        ]\n    ),\n    limit=10\n)\n</code></pre> <p>Next step : Filters and Metadata</p>"},{"location":"Qdrant/Data/EN/04-filters-metadata/","title":"4. Filters and Metadata","text":""},{"location":"Qdrant/Data/EN/04-filters-metadata/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Use advanced filters</li> <li>Manage metadata (payload)</li> <li>Combine multiple conditions</li> <li>Optimize filtered queries</li> </ul>"},{"location":"Qdrant/Data/EN/04-filters-metadata/#simple-filters","title":"Simple Filters","text":"<pre><code>from qdrant_client.models import Filter, FieldCondition, MatchValue\n\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"category\",\n            match=MatchValue(value=\"electronics\")\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/EN/04-filters-metadata/#range-filters","title":"Range Filters","text":"<pre><code>from qdrant_client.models import Range\n\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"price\",\n            range=Range(gte=100, lte=500)\n        )\n    ]\n)\n</code></pre> <p>Next step : Python Integration</p>"},{"location":"Qdrant/Data/EN/05-python-integration/","title":"5. Python Integration","text":""},{"location":"Qdrant/Data/EN/05-python-integration/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Integrate with embedding models</li> <li>Use with AI frameworks</li> <li>Create data pipelines</li> <li>Optimize performance</li> </ul>"},{"location":"Qdrant/Data/EN/05-python-integration/#with-sentence-transformers","title":"With sentence-transformers","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ntexts = [\"Product description 1\", \"Product description 2\"]\nembeddings = model.encode(texts)\n\nclient = QdrantClient(host=\"localhost\", port=6333)\nclient.upsert(\n    collection_name=\"products\",\n    points=[\n        PointStruct(id=i, vector=emb.tolist(), payload={\"text\": text})\n        for i, (emb, text) in enumerate(zip(embeddings, texts))\n    ]\n)\n</code></pre> <p>Next step : AI/ML Use Cases</p>"},{"location":"Qdrant/Data/EN/06-ai-ml-use-cases/","title":"6. AI/ML Use Cases","text":""},{"location":"Qdrant/Data/EN/06-ai-ml-use-cases/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand typical use cases</li> <li>Implement semantic search</li> <li>Create recommendation systems</li> <li>Use with RAG</li> </ul>"},{"location":"Qdrant/Data/EN/06-ai-ml-use-cases/#semantic-search","title":"Semantic Search","text":"<pre><code>def semantic_search(query_text, limit=10):\n    query_embedding = model.encode([query_text])[0]\n    results = client.search(\n        collection_name=\"documents\",\n        query_vector=query_embedding.tolist(),\n        limit=limit\n    )\n    return results\n</code></pre>"},{"location":"Qdrant/Data/EN/06-ai-ml-use-cases/#recommendation-system","title":"Recommendation System","text":"<pre><code>def recommend_similar_products(product_id, limit=5):\n    product = client.retrieve(\n        collection_name=\"products\",\n        ids=[product_id]\n    )[0]\n\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=product.vector,\n        limit=limit\n    )\n    return results\n</code></pre> <p>Next step : Best Practices</p>"},{"location":"Qdrant/Data/EN/07-best-practices/","title":"7. Best Practices","text":""},{"location":"Qdrant/Data/EN/07-best-practices/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Optimize performance</li> <li>Manage vector dimensions</li> <li>Choose the right distance</li> <li>Structure metadata</li> </ul>"},{"location":"Qdrant/Data/EN/07-best-practices/#vector-dimensions","title":"Vector Dimensions","text":"<ul> <li>128-256 : For small datasets</li> <li>384-512 : For text embeddings (sentence-transformers)</li> <li>768-1536 : For advanced models (BERT, etc.)</li> </ul>"},{"location":"Qdrant/Data/EN/07-best-practices/#distance","title":"Distance","text":"<ul> <li>COSINE : Recommended for texts, normalized embeddings</li> <li>EUCLID : For numerical data</li> <li>DOT : For non-normalized vectors</li> </ul> <p>Next step : Practical Projects</p>"},{"location":"Qdrant/Data/EN/08-projets/","title":"8. Practical Projects","text":""},{"location":"Qdrant/Data/EN/08-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create complete projects</li> <li>Apply knowledge</li> <li>Integrate with AI</li> <li>Optimize performance</li> </ul>"},{"location":"Qdrant/Data/EN/08-projets/#project-1-semantic-search-engine","title":"Project 1 : Semantic Search Engine","text":"<pre><code>class SemanticSearchEngine:\n    def __init__(self):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n\n    def search(self, query, limit=10):\n        query_embedding = self.model.encode([query])[0]\n        results = self.client.search(\n            collection_name=\"documents\",\n            query_vector=query_embedding.tolist(),\n            limit=limit\n        )\n        return results\n</code></pre>"},{"location":"Qdrant/Data/EN/08-projets/#project-2-recommendation-system","title":"Project 2 : Recommendation System","text":"<pre><code>def recommend_products(user_id, limit=10):\n    user_history = get_user_history(user_id)\n    user_embedding = model.encode(user_history).mean(axis=0)\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=user_embedding.tolist(),\n        limit=limit\n    )\n    return results\n</code></pre> <p>Congratulations! You have completed the Qdrant training for Data Analyst! \ud83c\udf89</p>"},{"location":"Qdrant/Data/FR/","title":"Formation Qdrant pour Data Analyst","text":""},{"location":"Qdrant/Data/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'apprentissage de Qdrant en tant que Data Analyst. Qdrant est une base de donn\u00e9es vectorielle optimis\u00e9e pour la recherche par similarit\u00e9, id\u00e9ale pour l'IA, le machine learning et la recherche s\u00e9mantique.</p>"},{"location":"Qdrant/Data/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre les bases de donn\u00e9es vectorielles</li> <li>Installer et configurer Qdrant</li> <li>Cr\u00e9er et g\u00e9rer des collections</li> <li>Indexer et rechercher des vecteurs</li> <li>Utiliser la recherche par similarit\u00e9</li> <li>Int\u00e9grer avec Python et l'IA</li> <li>Cr\u00e9er des projets pratiques</li> </ul>"},{"location":"Qdrant/Data/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 Qdrant Community Edition : Gratuit et open-source - \u2705 Qdrant Python Client : Biblioth\u00e8que gratuite - \u2705 Documentation officielle : Guides complets gratuits</p> <p>Budget total : 0\u20ac</p>"},{"location":"Qdrant/Data/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"Qdrant/Data/FR/#1-prise-en-main-qdrant","title":"1. Prise en main Qdrant","text":""},{"location":"Qdrant/Data/FR/#2-collections-et-vecteurs","title":"2. Collections et Vecteurs","text":""},{"location":"Qdrant/Data/FR/#3-recherche-par-similarite","title":"3. Recherche par Similarit\u00e9","text":""},{"location":"Qdrant/Data/FR/#4-filtres-et-metadonnees","title":"4. Filtres et M\u00e9tadonn\u00e9es","text":""},{"location":"Qdrant/Data/FR/#5-integration-python","title":"5. Int\u00e9gration Python","text":""},{"location":"Qdrant/Data/FR/#6-cas-dusage-iaml","title":"6. Cas d'usage IA/ML","text":""},{"location":"Qdrant/Data/FR/#7-bonnes-pratiques","title":"7. Bonnes Pratiques","text":""},{"location":"Qdrant/Data/FR/#8-projets-pratiques","title":"8. Projets Pratiques","text":""},{"location":"Qdrant/Data/FR/#prerequis","title":"\ud83d\ude80 Pr\u00e9requis","text":"<ul> <li>Notions de Python</li> <li>Concepts de base en IA/ML (optionnel)</li> <li>Notions d'embeddings (optionnel)</li> </ul>"},{"location":"Qdrant/Data/FR/#duree-estimee","title":"\u23f1\ufe0f Dur\u00e9e estim\u00e9e","text":"<ul> <li>Total : 30-40 heures</li> <li>Par module : 4-5 heures</li> </ul> <p>Bon apprentissage ! \ud83d\ude80</p>"},{"location":"Qdrant/Data/FR/01-getting-started/","title":"1. Prise en main Qdrant","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les bases de donn\u00e9es vectorielles</li> <li>Installer Qdrant</li> <li>Comprendre les concepts de base</li> <li>Premi\u00e8res op\u00e9rations</li> <li>V\u00e9rifier l'installation</li> </ul>"},{"location":"Qdrant/Data/FR/01-getting-started/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction \u00e0 Qdrant</li> <li>Bases de donn\u00e9es vectorielles</li> <li>Installation</li> <li>Premier exemple</li> <li>Interface Web</li> <li>V\u00e9rification</li> </ol>"},{"location":"Qdrant/Data/FR/01-getting-started/#introduction-a-qdrant","title":"Introduction \u00e0 Qdrant","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#quest-ce-que-qdrant","title":"Qu'est-ce que Qdrant ?","text":"<p>Qdrant = Base de donn\u00e9es vectorielle open-source</p> <ul> <li>Vecteurs : Repr\u00e9sentations num\u00e9riques (embeddings)</li> <li>Similarit\u00e9 : Recherche par similarit\u00e9 ultra-rapide</li> <li>IA/ML : Optimis\u00e9 pour l'intelligence artificielle</li> <li>Open-source : Gratuit et open-source</li> <li>Scalable : G\u00e8re des millions de vecteurs</li> <li>Production-ready : Utilis\u00e9 en production</li> </ul>"},{"location":"Qdrant/Data/FR/01-getting-started/#caracteristiques-principales","title":"Caract\u00e9ristiques principales","text":"<ul> <li>Recherche rapide : Algorithmes HNSW optimis\u00e9s</li> <li>Filtres avanc\u00e9s : Filtrage par m\u00e9tadonn\u00e9es</li> <li>API REST : Interface HTTP simple</li> <li>Python/Java/Go : Clients officiels</li> <li>Docker : D\u00e9ploiement facile</li> </ul>"},{"location":"Qdrant/Data/FR/01-getting-started/#pourquoi-qdrant-pour-data-analyst","title":"Pourquoi Qdrant pour Data Analyst ?","text":"<ul> <li>Recherche s\u00e9mantique : Recherche par sens, pas par mots-cl\u00e9s</li> <li>Recommandations : Syst\u00e8mes de recommandation performants</li> <li>IA/ML : Int\u00e9gration native avec mod\u00e8les d'embeddings</li> <li>Performance : Recherche rapide sur millions de vecteurs</li> <li>Flexibilit\u00e9 : Filtres complexes sur m\u00e9tadonn\u00e9es</li> <li>Cas d'usage modernes : RAG, chatbots, recherche intelligente</li> </ul>"},{"location":"Qdrant/Data/FR/01-getting-started/#cas-dusage-typiques","title":"Cas d'usage typiques","text":"<ul> <li>Recherche s\u00e9mantique : Documents, produits, contenu</li> <li>Recommandations : Produits similaires, contenu personnalis\u00e9</li> <li>D\u00e9duplication : D\u00e9tecter les doublons</li> <li>Clustering : Grouper des \u00e9l\u00e9ments similaires</li> <li>RAG : Retrieval-Augmented Generation pour LLMs</li> <li>Anomaly detection : D\u00e9tecter les anomalies</li> </ul>"},{"location":"Qdrant/Data/FR/01-getting-started/#bases-de-donnees-vectorielles","title":"Bases de donn\u00e9es vectorielles","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#quest-ce-quun-vecteur","title":"Qu'est-ce qu'un vecteur ?","text":"<p>Un vecteur est une liste de nombres qui repr\u00e9sente un objet :</p> <pre><code># Exemple de vecteur (embedding) de dimension 128\nvector = [0.1, -0.3, 0.7, 0.2, ..., 0.5]  # 128 nombres\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#quest-ce-quun-embedding","title":"Qu'est-ce qu'un embedding ?","text":"<p>Un embedding est une repr\u00e9sentation vectorielle d'un objet : - Texte \u2192 Vecteur de nombres - Image \u2192 Vecteur de nombres - Audio \u2192 Vecteur de nombres</p>"},{"location":"Qdrant/Data/FR/01-getting-started/#exemple-embedding-de-texte","title":"Exemple : Embedding de texte","text":"<pre><code># Texte original\ntext = \"Laptop computer for work\"\n\n# Embedding (simplifi\u00e9, dimension 4 pour l'exemple)\nembedding = [0.2, -0.1, 0.8, 0.3]\n\n# Texte similaire\nsimilar_text = \"Computer laptop professional\"\n\n# Embedding similaire (proche dans l'espace vectoriel)\nsimilar_embedding = [0.25, -0.08, 0.75, 0.28]\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#similarite-vectorielle","title":"Similarit\u00e9 vectorielle","text":"<p>La similarit\u00e9 mesure \u00e0 quel point deux vecteurs sont proches :</p> <ul> <li>Distance cosinus : Angle entre vecteurs (0-1, 1 = identique)</li> <li>Distance euclidienne : Distance dans l'espace (0-\u221e, 0 = identique)</li> <li>Produit scalaire : Projection d'un vecteur sur l'autre</li> </ul>"},{"location":"Qdrant/Data/FR/01-getting-started/#pourquoi-une-base-de-donnees-vectorielle","title":"Pourquoi une base de donn\u00e9es vectorielle ?","text":"<p>Probl\u00e8me avec les bases de donn\u00e9es classiques : - Recherche par mots-cl\u00e9s exacts - Ne comprend pas le sens - Pas optimis\u00e9e pour la similarit\u00e9</p> <p>Solution avec Qdrant : - Recherche par sens (s\u00e9mantique) - Comprend les relations - Optimis\u00e9e pour la similarit\u00e9</p>"},{"location":"Qdrant/Data/FR/01-getting-started/#installation","title":"Installation","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#option-1-docker-recommande","title":"Option 1 : Docker (recommand\u00e9)","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#installation-docker","title":"Installation Docker","text":"<p>Si Docker n'est pas install\u00e9 : - Windows : Docker Desktop - Linux : <code>sudo apt-get install docker.io</code> - Mac : Docker Desktop</p>"},{"location":"Qdrant/Data/FR/01-getting-started/#lancer-qdrant","title":"Lancer Qdrant","text":"<pre><code># Lancer Qdrant\ndocker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n\n# Ou avec volume persistant\ndocker run -p 6333:6333 -p 6334:6334 \\\n  -v $(pwd)/qdrant_storage:/qdrant/storage \\\n  qdrant/qdrant\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#verifier-que-qdrant-tourne","title":"V\u00e9rifier que Qdrant tourne","text":"<pre><code># V\u00e9rifier les conteneurs\ndocker ps\n\n# V\u00e9rifier les logs\ndocker logs &lt;container_id&gt;\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#option-2-installation-native-linux","title":"Option 2 : Installation native (Linux)","text":"<pre><code># T\u00e9l\u00e9charger Qdrant\nwget https://github.com/qdrant/qdrant/releases/download/v1.7.0/qdrant-x86_64-unknown-linux-gnu\n\n# Rendre ex\u00e9cutable\nchmod +x qdrant-x86_64-unknown-linux-gnu\n\n# Lancer\n./qdrant-x86_64-unknown-linux-gnu\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#option-3-qdrant-cloud-gratuit","title":"Option 3 : Qdrant Cloud (gratuit)","text":"<ol> <li>Cr\u00e9er un compte sur Qdrant Cloud</li> <li>Cr\u00e9er un cluster gratuit</li> <li>Obtenir l'URL et la cl\u00e9 API</li> </ol>"},{"location":"Qdrant/Data/FR/01-getting-started/#installation-du-client-python","title":"Installation du client Python","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#avec-pip","title":"Avec pip","text":"<pre><code># Installation de base\npip install qdrant-client\n\n# Avec d\u00e9pendances optionnelles\npip install qdrant-client[fastembed]\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#avec-conda","title":"Avec conda","text":"<pre><code>conda install -c conda-forge qdrant-client\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#verifier-linstallation","title":"V\u00e9rifier l'installation","text":"<pre><code>import qdrant_client\nprint(qdrant_client.__version__)\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#premier-exemple","title":"Premier exemple","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#connexion-a-qdrant","title":"Connexion \u00e0 Qdrant","text":"<pre><code>from qdrant_client import QdrantClient\n\n# Connexion locale\nclient = QdrantClient(host=\"localhost\", port=6333)\n\n# Ou avec URL compl\u00e8te\nclient = QdrantClient(url=\"http://localhost:6333\")\n\n# Connexion distante\nclient = QdrantClient(\n    url=\"https://your-cluster.qdrant.io\",\n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#creer-une-collection","title":"Cr\u00e9er une collection","text":"<pre><code>from qdrant_client.models import Distance, VectorParams\n\n# Cr\u00e9er une collection simple\nclient.create_collection(\n    collection_name=\"test_collection\",\n    vectors_config=VectorParams(\n        size=128,  # Dimension des vecteurs\n        distance=Distance.COSINE  # Type de distance\n    )\n)\n\nprint(\"Collection cr\u00e9\u00e9e avec succ\u00e8s!\")\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#verifier-la-collection","title":"V\u00e9rifier la collection","text":"<pre><code># Lister toutes les collections\ncollections = client.get_collections()\nprint(f\"Collections: {collections.collections}\")\n\n# Obtenir les informations d'une collection\ncollection_info = client.get_collection(\"test_collection\")\nprint(f\"Collection info: {collection_info}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#inserer-un-premier-vecteur","title":"Ins\u00e9rer un premier vecteur","text":"<pre><code>from qdrant_client.models import PointStruct\nimport random\n\n# G\u00e9n\u00e9rer un vecteur al\u00e9atoire (exemple)\nvector = [random.random() for _ in range(128)]\n\n# Cr\u00e9er un point\npoint = PointStruct(\n    id=1,\n    vector=vector,\n    payload={\n        \"name\": \"Premier document\",\n        \"category\": \"test\"\n    }\n)\n\n# Ins\u00e9rer le point\nclient.upsert(\n    collection_name=\"test_collection\",\n    points=[point]\n)\n\nprint(\"Point ins\u00e9r\u00e9 avec succ\u00e8s!\")\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#premiere-recherche","title":"Premi\u00e8re recherche","text":"<pre><code># Rechercher des vecteurs similaires\nresults = client.search(\n    collection_name=\"test_collection\",\n    query_vector=vector,  # Vecteur de requ\u00eate\n    limit=5  # Nombre de r\u00e9sultats\n)\n\n# Afficher les r\u00e9sultats\nfor result in results:\n    print(f\"ID: {result.id}, Score: {result.score:.4f}\")\n    print(f\"Payload: {result.payload}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#interface-web","title":"Interface Web","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#acceder-a-linterface","title":"Acc\u00e9der \u00e0 l'interface","text":"<p>Une fois Qdrant lanc\u00e9, acc\u00e9dez \u00e0 : - Interface Web : http://localhost:6333/dashboard - API REST : http://localhost:6333/docs</p>"},{"location":"Qdrant/Data/FR/01-getting-started/#fonctionnalites-de-linterface","title":"Fonctionnalit\u00e9s de l'interface","text":"<ul> <li>Collections : Voir et g\u00e9rer les collections</li> <li>Points : Visualiser les points et vecteurs</li> <li>Recherche : Tester des recherches</li> <li>M\u00e9triques : Statistiques et performances</li> </ul>"},{"location":"Qdrant/Data/FR/01-getting-started/#verification","title":"V\u00e9rification","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#script-de-verification-complet","title":"Script de v\u00e9rification complet","text":"<pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\nimport random\n\ndef test_qdrant_installation():\n    \"\"\"Test complet de l'installation Qdrant\"\"\"\n\n    # 1. Connexion\n    try:\n        client = QdrantClient(host=\"localhost\", port=6333)\n        print(\"\u2705 Connexion r\u00e9ussie\")\n    except Exception as e:\n        print(f\"\u274c Erreur de connexion: {e}\")\n        return False\n\n    # 2. Cr\u00e9er une collection de test\n    try:\n        client.create_collection(\n            collection_name=\"test_installation\",\n            vectors_config=VectorParams(size=128, distance=Distance.COSINE)\n        )\n        print(\"\u2705 Collection cr\u00e9\u00e9e\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Collection existe peut-\u00eatre d\u00e9j\u00e0: {e}\")\n\n    # 3. Ins\u00e9rer des points de test\n    try:\n        points = [\n            PointStruct(\n                id=i,\n                vector=[random.random() for _ in range(128)],\n                payload={\"test\": f\"point_{i}\"}\n            )\n            for i in range(10)\n        ]\n        client.upsert(collection_name=\"test_installation\", points=points)\n        print(\"\u2705 Points ins\u00e9r\u00e9s\")\n    except Exception as e:\n        print(f\"\u274c Erreur insertion: {e}\")\n        return False\n\n    # 4. Recherche\n    try:\n        query_vector = [random.random() for _ in range(128)]\n        results = client.search(\n            collection_name=\"test_installation\",\n            query_vector=query_vector,\n            limit=5\n        )\n        print(f\"\u2705 Recherche r\u00e9ussie ({len(results)} r\u00e9sultats)\")\n    except Exception as e:\n        print(f\"\u274c Erreur recherche: {e}\")\n        return False\n\n    # 5. Nettoyage\n    try:\n        client.delete_collection(\"test_installation\")\n        print(\"\u2705 Collection de test supprim\u00e9e\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Erreur suppression: {e}\")\n\n    print(\"\\n\ud83c\udf89 Installation Qdrant v\u00e9rifi\u00e9e avec succ\u00e8s!\")\n    return True\n\nif __name__ == \"__main__\":\n    test_qdrant_installation()\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#executer-le-test","title":"Ex\u00e9cuter le test","text":"<pre><code>python test_qdrant.py\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#exercices-pratiques","title":"Exercices pratiques","text":""},{"location":"Qdrant/Data/FR/01-getting-started/#exercice-1-installation-et-premiere-collection","title":"Exercice 1 : Installation et premi\u00e8re collection","text":"<ol> <li>Installer Qdrant avec Docker</li> <li>Cr\u00e9er une collection nomm\u00e9e \"exercice1\" avec des vecteurs de dimension 64</li> <li>V\u00e9rifier que la collection existe</li> </ol> <p>Solution :</p> <pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\nclient.create_collection(\n    collection_name=\"exercice1\",\n    vectors_config=VectorParams(size=64, distance=Distance.COSINE)\n)\n\n# V\u00e9rifier\ncollections = client.get_collections()\nprint(\"exercice1\" in [c.name for c in collections.collections])\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#exercice-2-inserer-et-rechercher","title":"Exercice 2 : Ins\u00e9rer et rechercher","text":"<ol> <li>Ins\u00e9rer 5 points avec des vecteurs al\u00e9atoires</li> <li>Effectuer une recherche avec un nouveau vecteur</li> <li>Afficher les 3 meilleurs r\u00e9sultats</li> </ol> <p>Solution :</p> <pre><code>from qdrant_client.models import PointStruct\nimport random\n\n# Ins\u00e9rer 5 points\npoints = [\n    PointStruct(\n        id=i,\n        vector=[random.random() for _ in range(64)],\n        payload={\"name\": f\"Item {i}\"}\n    )\n    for i in range(5)\n]\nclient.upsert(collection_name=\"exercice1\", points=points)\n\n# Rechercher\nquery_vector = [random.random() for _ in range(64)]\nresults = client.search(\n    collection_name=\"exercice1\",\n    query_vector=query_vector,\n    limit=3\n)\n\nfor result in results:\n    print(f\"ID: {result.id}, Score: {result.score:.4f}, Name: {result.payload['name']}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/01-getting-started/#points-cles-a-retenir","title":"\ud83c\udfaf Points cl\u00e9s \u00e0 retenir","text":"<p>\u2705 Qdrant est une base de donn\u00e9es vectorielle optimis\u00e9e pour la recherche par similarit\u00e9 \u2705 Les vecteurs (embeddings) repr\u00e9sentent des objets sous forme num\u00e9rique \u2705 La similarit\u00e9 mesure la proximit\u00e9 entre vecteurs \u2705 Qdrant peut \u00eatre install\u00e9 avec Docker, nativement ou via le cloud \u2705 L'interface web permet de visualiser et tester les collections  </p>"},{"location":"Qdrant/Data/FR/01-getting-started/#ressources-complementaires","title":"\ud83d\udcda Ressources compl\u00e9mentaires","text":"<ul> <li>Documentation officielle Qdrant</li> <li>Guide de d\u00e9marrage rapide</li> <li>API REST</li> <li>Exemples Python</li> </ul> <p>Prochaine \u00e9tape : Collections et Vecteurs</p>"},{"location":"Qdrant/Data/FR/02-collections-vectors/","title":"2. Collections et Vecteurs","text":""},{"location":"Qdrant/Data/FR/02-collections-vectors/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er et g\u00e9rer des collections</li> <li>Comprendre les configurations de collections</li> <li>Ins\u00e9rer des vecteurs efficacement</li> <li>Comprendre les distances et m\u00e9triques</li> <li>G\u00e9rer les m\u00e9tadonn\u00e9es (payload)</li> <li>Optimiser les collections</li> </ul>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Collections</li> <li>Configuration des collections</li> <li>Types de distances</li> <li>Insertion de vecteurs</li> <li>Gestion des collections</li> <li>M\u00e9tadonn\u00e9es (Payload)</li> <li>Optimisation</li> </ol>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#collections","title":"Collections","text":""},{"location":"Qdrant/Data/FR/02-collections-vectors/#quest-ce-quune-collection","title":"Qu'est-ce qu'une collection ?","text":"<p>Une collection est un conteneur pour des points (vecteurs) similaires : - M\u00eame dimension de vecteurs - M\u00eame type de distance - Partage des m\u00eames index</p>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#structure-dune-collection","title":"Structure d'une collection","text":"<pre><code>Collection \"products\"\n\u251c\u2500\u2500 Configuration\n\u2502   \u251c\u2500\u2500 Dimension: 128\n\u2502   \u251c\u2500\u2500 Distance: COSINE\n\u2502   \u2514\u2500\u2500 Index: HNSW\n\u2514\u2500\u2500 Points\n    \u251c\u2500\u2500 Point 1: [vector, payload]\n    \u251c\u2500\u2500 Point 2: [vector, payload]\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#configuration-des-collections","title":"Configuration des collections","text":""},{"location":"Qdrant/Data/FR/02-collections-vectors/#creation-basique","title":"Cr\u00e9ation basique","text":"<pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\n# Collection simple\nclient.create_collection(\n    collection_name=\"products\",\n    vectors_config=VectorParams(\n        size=128,  # Dimension des vecteurs\n        distance=Distance.COSINE  # Type de distance\n    )\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#configuration-avancee","title":"Configuration avanc\u00e9e","text":"<pre><code>from qdrant_client.models import (\n    Distance, VectorParams, HnswConfigDiff, \n    OptimizersConfigDiff, QuantizationConfig\n)\n\nclient.create_collection(\n    collection_name=\"products_advanced\",\n    vectors_config=VectorParams(\n        size=384,  # Dimension (ex: sentence-transformers)\n        distance=Distance.COSINE\n    ),\n    # Configuration HNSW (index)\n    hnsw_config=HnswConfigDiff(\n        m=16,  # Nombre de connexions (plus = plus pr\u00e9cis, plus lent)\n        ef_construct=100,  # Pr\u00e9cision de construction\n        full_scan_threshold=10000  # Seuil pour scan complet\n    ),\n    # Configuration des optimiseurs\n    optimizers_config=OptimizersConfigDiff(\n        indexing_threshold=20000,  # Seuil pour indexation\n        memmap_threshold=50000  # Seuil pour memmap\n    ),\n    # Quantisation (compression)\n    quantization_config=QuantizationConfig(\n        scalar=ScalarQuantization(\n            type=ScalarType.INT8,\n            quantile=0.99,\n            always_ram=True\n        )\n    )\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#collections-avec-plusieurs-vecteurs","title":"Collections avec plusieurs vecteurs","text":"<pre><code># Collection avec plusieurs vecteurs par point\nclient.create_collection(\n    collection_name=\"multivector\",\n    vectors_config={\n        \"image\": VectorParams(size=512, distance=Distance.COSINE),\n        \"text\": VectorParams(size=384, distance=Distance.COSINE)\n    }\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#types-de-distances","title":"Types de distances","text":""},{"location":"Qdrant/Data/FR/02-collections-vectors/#distance-cosine-recommande-pour-textes","title":"Distance COSINE (recommand\u00e9 pour textes)","text":"<p>Utilisation : Embeddings normalis\u00e9s, textes, recherche s\u00e9mantique</p> <pre><code>VectorParams(size=128, distance=Distance.COSINE)\n</code></pre> <p>Caract\u00e9ristiques : - Mesure l'angle entre vecteurs - Score entre 0 et 1 (1 = identique) - Insensible \u00e0 la magnitude - Id\u00e9al pour embeddings normalis\u00e9s</p> <p>Exemple : <pre><code># Vecteurs normalis\u00e9s\nv1 = [0.5, 0.5, 0.5, 0.5]  # Norme = 1\nv2 = [1.0, 1.0, 1.0, 1.0]  # Norme = 2\n# Similarit\u00e9 cosinus = 1.0 (m\u00eame direction)\n</code></pre></p>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#distance-euclid","title":"Distance EUCLID","text":"<p>Utilisation : Donn\u00e9es num\u00e9riques, coordonn\u00e9es, features brutes</p> <pre><code>VectorParams(size=128, distance=Distance.EUCLID)\n</code></pre> <p>Caract\u00e9ristiques : - Distance g\u00e9om\u00e9trique dans l'espace - Score entre 0 et \u221e (0 = identique) - Sensible \u00e0 la magnitude - Id\u00e9al pour donn\u00e9es num\u00e9riques</p> <p>Exemple : <pre><code># Coordonn\u00e9es 2D\npoint1 = [0, 0]\npoint2 = [3, 4]\n# Distance euclidienne = 5.0\n</code></pre></p>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#distance-dot-produit-scalaire","title":"Distance DOT (produit scalaire)","text":"<p>Utilisation : Vecteurs non normalis\u00e9s, scores pond\u00e9r\u00e9s</p> <pre><code>VectorParams(size=128, distance=Distance.DOT)\n</code></pre> <p>Caract\u00e9ristiques : - Produit scalaire entre vecteurs - Score entre -\u221e et +\u221e - Sensible \u00e0 la magnitude - Id\u00e9al pour scores pond\u00e9r\u00e9s</p> <p>Quand utiliser chaque distance ?</p> Distance Cas d'usage Exemple COSINE Textes, embeddings normalis\u00e9s Recherche s\u00e9mantique EUCLID Coordonn\u00e9es, features num\u00e9riques Classification d'images DOT Scores pond\u00e9r\u00e9s, recommandations Syst\u00e8me de scoring"},{"location":"Qdrant/Data/FR/02-collections-vectors/#insertion-de-vecteurs","title":"Insertion de vecteurs","text":""},{"location":"Qdrant/Data/FR/02-collections-vectors/#insertion-simple","title":"Insertion simple","text":"<pre><code>from qdrant_client.models import PointStruct\n\n# Un seul point\npoint = PointStruct(\n    id=1,\n    vector=[0.1, 0.2, 0.3, ...],  # Vecteur de dimension 128\n    payload={\n        \"name\": \"Product A\",\n        \"category\": \"electronics\",\n        \"price\": 99.99\n    }\n)\n\nclient.upsert(\n    collection_name=\"products\",\n    points=[point]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#insertion-multiple-batch","title":"Insertion multiple (batch)","text":"<pre><code>import numpy as np\nfrom qdrant_client.models import PointStruct\n\n# G\u00e9n\u00e9rer des vecteurs (exemple)\nvectors = np.random.rand(1000, 128).tolist()\n\n# Cr\u00e9er les points\npoints = [\n    PointStruct(\n        id=i,\n        vector=vectors[i],\n        payload={\n            \"name\": f\"Product {i}\",\n            \"category\": [\"electronics\", \"books\", \"clothing\"][i % 3],\n            \"price\": round(10 + i * 0.5, 2)\n        }\n    )\n    for i in range(1000)\n]\n\n# Insertion batch\nclient.upsert(\n    collection_name=\"products\",\n    points=points\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#insertion-avec-wait","title":"Insertion avec wait","text":"<pre><code># Attendre que l'insertion soit confirm\u00e9e\nclient.upsert(\n    collection_name=\"products\",\n    points=points,\n    wait=True  # Attendre la confirmation\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#insertion-avec-ordering","title":"Insertion avec ordering","text":"<pre><code># Garantir l'ordre d'insertion\nclient.upsert(\n    collection_name=\"products\",\n    points=points,\n    ordering=WriteOrdering.STRONG  # Ordre garanti\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#gestion-des-ids","title":"Gestion des IDs","text":"<pre><code># IDs num\u00e9riques\nPointStruct(id=1, vector=..., payload=...)\n\n# IDs UUID\nimport uuid\nPointStruct(id=str(uuid.uuid4()), vector=..., payload=...)\n\n# IDs personnalis\u00e9s\nPointStruct(id=\"product_123\", vector=..., payload=...)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#gestion-des-collections","title":"Gestion des collections","text":""},{"location":"Qdrant/Data/FR/02-collections-vectors/#lister-les-collections","title":"Lister les collections","text":"<pre><code># Obtenir toutes les collections\ncollections = client.get_collections()\nfor collection in collections.collections:\n    print(f\"Collection: {collection.name}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#informations-dune-collection","title":"Informations d'une collection","text":"<pre><code># Obtenir les informations d\u00e9taill\u00e9es\ncollection_info = client.get_collection(\"products\")\n\nprint(f\"Points count: {collection_info.points_count}\")\nprint(f\"Vectors count: {collection_info.vectors_count}\")\nprint(f\"Indexed vectors: {collection_info.indexed_vectors_count}\")\nprint(f\"Status: {collection_info.status}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#verifier-lexistence","title":"V\u00e9rifier l'existence","text":"<pre><code># V\u00e9rifier si une collection existe\ncollections = client.get_collections()\ncollection_names = [c.name for c in collections.collections]\n\nif \"products\" in collection_names:\n    print(\"Collection existe\")\nelse:\n    print(\"Collection n'existe pas\")\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#supprimer-une-collection","title":"Supprimer une collection","text":"<pre><code># Supprimer une collection\nclient.delete_collection(\"products\")\n\n# V\u00e9rifier la suppression\ncollections = client.get_collections()\nprint(\"products\" not in [c.name for c in collections.collections])\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#renommer-une-collection","title":"Renommer une collection","text":"<pre><code># Qdrant ne supporte pas directement le renommage\n# Solution : cr\u00e9er une nouvelle collection et copier les donn\u00e9es\n\n# 1. Cr\u00e9er nouvelle collection\nclient.create_collection(\n    collection_name=\"products_new\",\n    vectors_config=VectorParams(size=128, distance=Distance.COSINE)\n)\n\n# 2. R\u00e9cup\u00e9rer tous les points (par batches)\n# 3. Ins\u00e9rer dans la nouvelle collection\n# 4. Supprimer l'ancienne\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#metadonnees-payload","title":"M\u00e9tadonn\u00e9es (Payload)","text":""},{"location":"Qdrant/Data/FR/02-collections-vectors/#quest-ce-quun-payload","title":"Qu'est-ce qu'un payload ?","text":"<p>Le payload contient les m\u00e9tadonn\u00e9es associ\u00e9es \u00e0 un vecteur : - Informations structur\u00e9es - Filtres possibles - Pas utilis\u00e9 pour la recherche vectorielle</p>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#types-de-payload","title":"Types de payload","text":"<pre><code>PointStruct(\n    id=1,\n    vector=[...],\n    payload={\n        # String\n        \"name\": \"Product A\",\n\n        # Number\n        \"price\": 99.99,\n        \"quantity\": 10,\n\n        # Boolean\n        \"in_stock\": True,\n\n        # Array\n        \"tags\": [\"electronics\", \"laptop\", \"gaming\"],\n\n        # Nested object\n        \"metadata\": {\n            \"brand\": \"BrandX\",\n            \"model\": \"Model123\"\n        },\n\n        # Date (comme string ISO)\n        \"created_at\": \"2024-01-15T10:00:00Z\"\n    }\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#mettre-a-jour-le-payload","title":"Mettre \u00e0 jour le payload","text":"<pre><code># Mettre \u00e0 jour le payload d'un point\nclient.set_payload(\n    collection_name=\"products\",\n    payload={\n        \"discount\": 0.1,\n        \"on_sale\": True\n    },\n    points=[1, 2, 3]  # IDs des points\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#supprimer-des-champs-du-payload","title":"Supprimer des champs du payload","text":"<pre><code># Supprimer des cl\u00e9s du payload\nclient.delete_payload(\n    collection_name=\"products\",\n    keys=[\"discount\", \"on_sale\"],\n    points=[1, 2, 3]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#recuperer-le-payload","title":"R\u00e9cup\u00e9rer le payload","text":"<pre><code># R\u00e9cup\u00e9rer des points avec leur payload\npoints = client.retrieve(\n    collection_name=\"products\",\n    ids=[1, 2, 3],\n    with_payload=True,\n    with_vectors=False\n)\n\nfor point in points:\n    print(f\"ID: {point.id}\")\n    print(f\"Payload: {point.payload}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#optimisation","title":"Optimisation","text":""},{"location":"Qdrant/Data/FR/02-collections-vectors/#index-hnsw","title":"Index HNSW","text":"<p>HNSW (Hierarchical Navigable Small World) est l'algorithme d'indexation :</p> <pre><code>from qdrant_client.models import HnswConfigDiff\n\nclient.create_collection(\n    collection_name=\"optimized\",\n    vectors_config=VectorParams(size=128, distance=Distance.COSINE),\n    hnsw_config=HnswConfigDiff(\n        m=16,  # Plus de connexions = plus pr\u00e9cis mais plus lent\n        ef_construct=100,  # Pr\u00e9cision lors de la construction\n        full_scan_threshold=10000  # Scan complet si &lt; 10000 points\n    )\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#parametres-hnsw","title":"Param\u00e8tres HNSW","text":"<ul> <li>m : Nombre de connexions (d\u00e9faut: 16)</li> <li>Plus \u00e9lev\u00e9 = plus pr\u00e9cis, plus lent</li> <li> <p>Recommand\u00e9: 16-64</p> </li> <li> <p>ef_construct : Pr\u00e9cision de construction (d\u00e9faut: 100)</p> </li> <li>Plus \u00e9lev\u00e9 = meilleure qualit\u00e9, plus lent</li> <li>Recommand\u00e9: 100-200</li> </ul>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#quantisation","title":"Quantisation","text":"<p>La quantisation r\u00e9duit la taille m\u00e9moire :</p> <pre><code>from qdrant_client.models import QuantizationConfig, ScalarQuantization, ScalarType\n\nclient.create_collection(\n    collection_name=\"quantized\",\n    vectors_config=VectorParams(size=128, distance=Distance.COSINE),\n    quantization_config=QuantizationConfig(\n        scalar=ScalarQuantization(\n            type=ScalarType.INT8,  # R\u00e9duction de 4x (float32 \u2192 int8)\n            quantile=0.99,\n            always_ram=True\n        )\n    )\n)\n</code></pre> <p>Avantages : - R\u00e9duction m\u00e9moire (4x) - Recherche plus rapide - L\u00e9g\u00e8re perte de pr\u00e9cision</p>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#exercices-pratiques","title":"Exercices pratiques","text":""},{"location":"Qdrant/Data/FR/02-collections-vectors/#exercice-1-creer-une-collection-optimisee","title":"Exercice 1 : Cr\u00e9er une collection optimis\u00e9e","text":"<p>Cr\u00e9er une collection pour des embeddings de texte (384 dimensions) avec : - Distance COSINE - HNSW optimis\u00e9 (m=32, ef_construct=200) - Quantisation INT8</p> <p>Solution :</p> <pre><code>from qdrant_client.models import (\n    Distance, VectorParams, HnswConfigDiff,\n    QuantizationConfig, ScalarQuantization, ScalarType\n)\n\nclient.create_collection(\n    collection_name=\"text_embeddings\",\n    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n    hnsw_config=HnswConfigDiff(m=32, ef_construct=200),\n    quantization_config=QuantizationConfig(\n        scalar=ScalarQuantization(\n            type=ScalarType.INT8,\n            quantile=0.99,\n            always_ram=True\n        )\n    )\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#exercice-2-inserer-1000-produits","title":"Exercice 2 : Ins\u00e9rer 1000 produits","text":"<p>Cr\u00e9er 1000 points avec : - IDs s\u00e9quentiels - Vecteurs al\u00e9atoires de dimension 128 - Payload avec name, category, price</p> <p>Solution :</p> <pre><code>import random\nfrom qdrant_client.models import PointStruct\n\ncategories = [\"electronics\", \"books\", \"clothing\", \"food\", \"sports\"]\npoints = []\n\nfor i in range(1000):\n    points.append(\n        PointStruct(\n            id=i,\n            vector=[random.random() for _ in range(128)],\n            payload={\n                \"name\": f\"Product {i}\",\n                \"category\": random.choice(categories),\n                \"price\": round(random.uniform(10, 1000), 2)\n            }\n        )\n    )\n\n# Insertion par batches de 100\nbatch_size = 100\nfor i in range(0, len(points), batch_size):\n    batch = points[i:i+batch_size]\n    client.upsert(collection_name=\"products\", points=batch)\n    print(f\"Inserted batch {i//batch_size + 1}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/02-collections-vectors/#points-cles-a-retenir","title":"\ud83c\udfaf Points cl\u00e9s \u00e0 retenir","text":"<p>\u2705 Une collection regroupe des vecteurs de m\u00eame dimension et distance \u2705 COSINE pour textes, EUCLID pour coordonn\u00e9es, DOT pour scores \u2705 L'insertion batch est plus efficace que point par point \u2705 Le payload contient les m\u00e9tadonn\u00e9es filtrables \u2705 HNSW et quantisation optimisent performances et m\u00e9moire  </p> <p>Prochaine \u00e9tape : Recherche par Similarit\u00e9</p>"},{"location":"Qdrant/Data/FR/03-similarity-search/","title":"3. Recherche par Similarit\u00e9","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Effectuer des recherches par similarit\u00e9</li> <li>Comprendre les algorithmes de recherche</li> <li>Utiliser les param\u00e8tres de recherche</li> <li>Optimiser les performances</li> <li>Comprendre et interpr\u00e9ter les scores</li> <li>Utiliser la recherche batch</li> </ul>"},{"location":"Qdrant/Data/FR/03-similarity-search/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Recherche simple</li> <li>Comprendre les scores</li> <li>Param\u00e8tres de recherche</li> <li>Recherche avec filtres</li> <li>Recherche batch</li> <li>Recherche par ID</li> <li>Recherche recommand\u00e9e</li> <li>Optimisation</li> </ol>"},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-simple","title":"Recherche simple","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-basique","title":"Recherche basique","text":"<pre><code>from qdrant_client import QdrantClient\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\n# Vecteur de requ\u00eate (m\u00eame dimension que les vecteurs de la collection)\nquery_vector = [0.1, 0.2, 0.3, ...]  # Dimension 128\n\n# Recherche\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    limit=10  # Nombre de r\u00e9sultats\n)\n\n# Afficher les r\u00e9sultats\nfor result in results:\n    print(f\"ID: {result.id}\")\n    print(f\"Score: {result.score:.4f}\")\n    print(f\"Payload: {result.payload}\")\n    print(\"---\")\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#structure-des-resultats","title":"Structure des r\u00e9sultats","text":"<pre><code>results = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    limit=5\n)\n\n# Chaque r\u00e9sultat est un ScoredPoint\nfor result in results:\n    # ID du point\n    point_id = result.id\n\n    # Score de similarit\u00e9 (0-1 pour COSINE, 0-\u221e pour EUCLID)\n    similarity_score = result.score\n\n    # Payload (m\u00e9tadonn\u00e9es)\n    metadata = result.payload\n\n    # Vecteur (si demand\u00e9)\n    vector = result.vector  # None par d\u00e9faut\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#comprendre-les-scores","title":"Comprendre les scores","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#score-cosine","title":"Score COSINE","text":"<p>Pour la distance COSINE, le score est entre 0 et 1 : - 1.0 : Vecteurs identiques (m\u00eame direction) - 0.9-1.0 : Tr\u00e8s similaires - 0.7-0.9 : Similaires - 0.5-0.7 : Mod\u00e9r\u00e9ment similaires - 0.0-0.5 : Peu similaires - 0.0 : Orthogonaux (perpendiculaires)</p> <pre><code># Exemple d'interpr\u00e9tation\nfor result in results:\n    score = result.score\n\n    if score &gt; 0.9:\n        print(f\"Tr\u00e8s similaire: {result.id} (score: {score:.4f})\")\n    elif score &gt; 0.7:\n        print(f\"Similaire: {result.id} (score: {score:.4f})\")\n    elif score &gt; 0.5:\n        print(f\"Mod\u00e9r\u00e9ment similaire: {result.id} (score: {score:.4f})\")\n    else:\n        print(f\"Peu similaire: {result.id} (score: {score:.4f})\")\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#score-euclid","title":"Score EUCLID","text":"<p>Pour la distance EUCLID, le score est la distance (0 \u00e0 \u221e) : - 0.0 : Vecteurs identiques - Plus petit = plus similaire - Pas de limite sup\u00e9rieure</p> <pre><code># Pour EUCLID, trier par score croissant\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    limit=10\n)\n\n# Le premier r\u00e9sultat a la plus petite distance (plus similaire)\nfor i, result in enumerate(results):\n    print(f\"Rang {i+1}: ID={result.id}, Distance={result.score:.4f}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#score-dot","title":"Score DOT","text":"<p>Pour le produit scalaire, le score peut \u00eatre n\u00e9gatif ou positif : - Plus grand = plus similaire - Peut \u00eatre n\u00e9gatif si vecteurs oppos\u00e9s</p>"},{"location":"Qdrant/Data/FR/03-similarity-search/#parametres-de-recherche","title":"Param\u00e8tres de recherche","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#avec-vecteur-de-requete","title":"Avec vecteur de requ\u00eate","text":"<pre><code>results = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,  # Vecteur de requ\u00eate\n    limit=10,  # Nombre de r\u00e9sultats\n    score_threshold=0.7,  # Score minimum (optionnel)\n    with_payload=True,  # Inclure le payload (d\u00e9faut: True)\n    with_vectors=False  # Inclure les vecteurs (d\u00e9faut: False)\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-par-id-trouver-des-similaires","title":"Recherche par ID (trouver des similaires)","text":"<pre><code># Trouver des points similaires \u00e0 un point existant\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=None,  # Pas de vecteur\n    query_filter=Filter(\n        must=[FieldCondition(key=\"id\", match=MatchValue(value=123))]\n    ),\n    limit=10,\n    using=\"default\"  # Nom du vecteur (si multivector)\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#score-threshold","title":"Score threshold","text":"<pre><code># Filtrer par score minimum\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    limit=100,  # Chercher jusqu'\u00e0 100\n    score_threshold=0.8  # Ne retourner que score &gt;= 0.8\n)\n\n# R\u00e9sultats filtr\u00e9s\nprint(f\"R\u00e9sultats avec score &gt;= 0.8: {len(results)}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#avec-vecteurs","title":"Avec vecteurs","text":"<pre><code># Inclure les vecteurs dans les r\u00e9sultats\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    limit=5,\n    with_vectors=True  # Inclure les vecteurs\n)\n\nfor result in results:\n    print(f\"ID: {result.id}\")\n    print(f\"Vector: {result.vector[:5]}...\")  # Afficher les 5 premiers\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-avec-filtres","title":"Recherche avec filtres","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#filtre-simple","title":"Filtre simple","text":"<pre><code>from qdrant_client.models import Filter, FieldCondition, MatchValue\n\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    query_filter=Filter(\n        must=[\n            FieldCondition(\n                key=\"category\",\n                match=MatchValue(value=\"electronics\")\n            )\n        ]\n    ),\n    limit=10\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#filtres-multiples-et","title":"Filtres multiples (ET)","text":"<pre><code>from qdrant_client.models import Range\n\n# Tous les crit\u00e8res doivent \u00eatre satisfaits (ET)\nfilter = Filter(\n    must=[\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        FieldCondition(\n            key=\"price\",\n            range=Range(gte=100, lte=500)\n        ),\n        FieldCondition(key=\"in_stock\", match=MatchValue(value=True))\n    ]\n)\n\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    query_filter=filter,\n    limit=10\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#filtres-ou","title":"Filtres OU","text":"<pre><code># Au moins un crit\u00e8re doit \u00eatre satisfait (OU)\nfilter = Filter(\n    should=[\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        FieldCondition(key=\"category\", match=MatchValue(value=\"books\")),\n        FieldCondition(key=\"category\", match=MatchValue(value=\"clothing\"))\n    ],\n    min_should_match=1  # Au moins 1 doit correspondre\n)\n\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    query_filter=filter,\n    limit=10\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#filtres-not","title":"Filtres NOT","text":"<pre><code># Exclure certains points\nfilter = Filter(\n    must_not=[\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        FieldCondition(\n            key=\"price\",\n            range=Range(lt=50)  # Exclure les produits &lt; 50\u20ac\n        )\n    ]\n)\n\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    query_filter=filter,\n    limit=10\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#filtres-complexes","title":"Filtres complexes","text":"<pre><code># Combinaison complexe\nfilter = Filter(\n    must=[\n        # Doit \u00eatre en stock\n        FieldCondition(key=\"in_stock\", match=MatchValue(value=True))\n    ],\n    should=[\n        # OU \u00e9lectronique OU livres\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        FieldCondition(key=\"category\", match=MatchValue(value=\"books\"))\n    ],\n    must_not=[\n        # Mais pas les produits &lt; 20\u20ac\n        FieldCondition(key=\"price\", range=Range(lt=20))\n    ],\n    min_should_match=1\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-batch","title":"Recherche batch","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-multiple","title":"Recherche multiple","text":"<pre><code># Rechercher avec plusieurs vecteurs de requ\u00eate\nquery_vectors = [\n    [0.1, 0.2, 0.3, ...],  # Requ\u00eate 1\n    [0.4, 0.5, 0.6, ...],  # Requ\u00eate 2\n    [0.7, 0.8, 0.9, ...]   # Requ\u00eate 3\n]\n\n# Recherche batch\nbatch_results = client.search_batch(\n    collection_name=\"products\",\n    requests=[\n        {\n            \"vector\": query_vector,\n            \"limit\": 10,\n            \"filter\": None  # Optionnel\n        }\n        for query_vector in query_vectors\n    ]\n)\n\n# R\u00e9sultats pour chaque requ\u00eate\nfor i, results in enumerate(batch_results):\n    print(f\"R\u00e9sultats pour requ\u00eate {i+1}:\")\n    for result in results:\n        print(f\"  ID: {result.id}, Score: {result.score:.4f}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-batch-avec-filtres-differents","title":"Recherche batch avec filtres diff\u00e9rents","text":"<pre><code>from qdrant_client.models import Filter, FieldCondition, MatchValue\n\nbatch_results = client.search_batch(\n    collection_name=\"products\",\n    requests=[\n        {\n            \"vector\": query_vector,\n            \"limit\": 10,\n            \"filter\": Filter(\n                must=[FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\"))]\n            )\n        },\n        {\n            \"vector\": query_vector,\n            \"limit\": 10,\n            \"filter\": Filter(\n                must=[FieldCondition(key=\"category\", match=MatchValue(value=\"books\"))]\n            )\n        }\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-recommandee","title":"Recherche recommand\u00e9e","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#recommandation-basique","title":"Recommandation basique","text":"<pre><code>def recommend_similar_items(item_id, limit=10):\n    \"\"\"Recommander des items similaires \u00e0 un item donn\u00e9\"\"\"\n\n    # 1. R\u00e9cup\u00e9rer le vecteur de l'item\n    points = client.retrieve(\n        collection_name=\"products\",\n        ids=[item_id],\n        with_vectors=True\n    )\n\n    if not points:\n        return []\n\n    item_vector = points[0].vector\n\n    # 2. Rechercher des items similaires (exclure l'item original)\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=item_vector,\n        query_filter=Filter(\n            must_not=[\n                FieldCondition(key=\"id\", match=MatchValue(value=item_id))\n            ]\n        ),\n        limit=limit\n    )\n\n    return results\n\n# Utilisation\nrecommendations = recommend_similar_items(item_id=123, limit=5)\nfor rec in recommendations:\n    print(f\"Recommand\u00e9: {rec.payload['name']} (score: {rec.score:.4f})\")\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#recommandation-hybride","title":"Recommandation hybride","text":"<pre><code>def hybrid_recommendation(item_id, user_preferences, limit=10):\n    \"\"\"Recommandation combinant similarit\u00e9 et pr\u00e9f\u00e9rences utilisateur\"\"\"\n\n    # 1. Vecteur de l'item\n    points = client.retrieve(\n        collection_name=\"products\",\n        ids=[item_id],\n        with_vectors=True\n    )\n    item_vector = points[0].vector\n\n    # 2. Recherche avec filtres de pr\u00e9f\u00e9rences\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=item_vector,\n        query_filter=Filter(\n            must=[\n                # Pr\u00e9f\u00e9rences utilisateur\n                FieldCondition(\n                    key=\"category\",\n                    match=MatchValue(value=user_preferences[\"preferred_category\"])\n                ),\n                FieldCondition(\n                    key=\"price\",\n                    range=Range(\n                        gte=user_preferences[\"min_price\"],\n                        lte=user_preferences[\"max_price\"]\n                    )\n                )\n            ],\n            must_not=[\n                FieldCondition(key=\"id\", match=MatchValue(value=item_id))\n            ]\n        ),\n        limit=limit,\n        score_threshold=0.7  # Score minimum\n    )\n\n    return results\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-par-id","title":"Recherche par ID","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#recuperer-des-points-par-id","title":"R\u00e9cup\u00e9rer des points par ID","text":"<pre><code># R\u00e9cup\u00e9rer des points sp\u00e9cifiques\npoints = client.retrieve(\n    collection_name=\"products\",\n    ids=[1, 2, 3, 4, 5],\n    with_payload=True,\n    with_vectors=False\n)\n\nfor point in points:\n    print(f\"ID: {point.id}\")\n    print(f\"Payload: {point.payload}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#scroll-parcourir-tous-les-points","title":"Scroll (parcourir tous les points)","text":"<pre><code># Parcourir tous les points (par batches)\nscroll_result = client.scroll(\n    collection_name=\"products\",\n    limit=100,  # Nombre de points par batch\n    with_payload=True,\n    with_vectors=False\n)\n\npoints, next_page_offset = scroll_result\n\n# Continuer avec le prochain batch\nwhile next_page_offset is not None:\n    scroll_result = client.scroll(\n        collection_name=\"products\",\n        limit=100,\n        offset=next_page_offset,\n        with_payload=True,\n        with_vectors=False\n    )\n    points, next_page_offset = scroll_result\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#optimisation","title":"Optimisation","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#parametre-ef-exactness-factor","title":"Param\u00e8tre ef (exactness factor)","text":"<pre><code># Augmenter la pr\u00e9cision de recherche (plus lent)\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    limit=10,\n    ef=128  # Plus \u00e9lev\u00e9 = plus pr\u00e9cis mais plus lent (d\u00e9faut: auto)\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#recherche-avec-index","title":"Recherche avec index","text":"<pre><code># Utiliser un index sp\u00e9cifique\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    limit=10,\n    using=\"default\"  # Nom du vecteur (pour collections multivector)\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#pre-filtrage-vs-post-filtrage","title":"Pr\u00e9-filtrage vs Post-filtrage","text":"<pre><code># Pr\u00e9-filtrage (recommand\u00e9 si peu de points apr\u00e8s filtrage)\n# Filtre d'abord, puis recherche vectorielle\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    query_filter=filter,  # Filtre appliqu\u00e9 AVANT la recherche\n    limit=10\n)\n\n# Post-filtrage (si beaucoup de points apr\u00e8s filtrage)\n# Recherche vectorielle d'abord, puis filtre\n# Utiliser un limit plus \u00e9lev\u00e9 puis filtrer manuellement\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#exercices-pratiques","title":"Exercices pratiques","text":""},{"location":"Qdrant/Data/FR/03-similarity-search/#exercice-1-recherche-avec-seuil","title":"Exercice 1 : Recherche avec seuil","text":"<p>Cr\u00e9er une fonction qui recherche des produits similaires avec un score minimum de 0.8.</p> <p>Solution :</p> <pre><code>def search_high_similarity(query_vector, min_score=0.8, limit=10):\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=query_vector,\n        limit=limit,\n        score_threshold=min_score\n    )\n    return results\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#exercice-2-top-k-par-categorie","title":"Exercice 2 : Top-K par cat\u00e9gorie","text":"<p>Pour chaque cat\u00e9gorie, trouver les 5 produits les plus similaires \u00e0 un vecteur de requ\u00eate.</p> <p>Solution :</p> <pre><code>categories = [\"electronics\", \"books\", \"clothing\"]\n\nfor category in categories:\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=query_vector,\n        query_filter=Filter(\n            must=[\n                FieldCondition(key=\"category\", match=MatchValue(value=category))\n            ]\n        ),\n        limit=5\n    )\n\n    print(f\"\\nTop 5 {category}:\")\n    for result in results:\n        print(f\"  {result.payload['name']} (score: {result.score:.4f})\")\n</code></pre>"},{"location":"Qdrant/Data/FR/03-similarity-search/#points-cles-a-retenir","title":"\ud83c\udfaf Points cl\u00e9s \u00e0 retenir","text":"<p>\u2705 Le score COSINE est entre 0 et 1 (1 = identique) \u2705 Utiliser score_threshold pour filtrer par qualit\u00e9 \u2705 Les filtres peuvent \u00eatre combin\u00e9s (must, should, must_not) \u2705 La recherche batch est efficace pour plusieurs requ\u00eates \u2705 Le param\u00e8tre ef contr\u00f4le le compromis pr\u00e9cision/vitesse  </p> <p>Prochaine \u00e9tape : Filtres et M\u00e9tadonn\u00e9es</p>"},{"location":"Qdrant/Data/FR/04-filters-metadata/","title":"4. Filtres et M\u00e9tadonn\u00e9es","text":""},{"location":"Qdrant/Data/FR/04-filters-metadata/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Utiliser les filtres avanc\u00e9s</li> <li>G\u00e9rer les m\u00e9tadonn\u00e9es (payload)</li> <li>Combiner plusieurs conditions</li> <li>Optimiser les requ\u00eates filtr\u00e9es</li> <li>Comprendre les types de filtres</li> <li>Indexer les champs pour les filtres</li> </ul>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Introduction aux filtres</li> <li>Types de filtres</li> <li>Op\u00e9rateurs de filtrage</li> <li>Filtres complexes</li> <li>Gestion du payload</li> <li>Index de payload</li> <li>Optimisation des filtres</li> </ol>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#introduction-aux-filtres","title":"Introduction aux filtres","text":""},{"location":"Qdrant/Data/FR/04-filters-metadata/#quest-ce-quun-filtre","title":"Qu'est-ce qu'un filtre ?","text":"<p>Un filtre permet de restreindre la recherche vectorielle \u00e0 un sous-ensemble de points bas\u00e9 sur leurs m\u00e9tadonn\u00e9es (payload) :</p> <ul> <li>Avant filtrage : Recherche sur tous les points</li> <li>Apr\u00e8s filtrage : Recherche uniquement sur les points correspondants</li> </ul>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#pourquoi-utiliser-des-filtres","title":"Pourquoi utiliser des filtres ?","text":"<ul> <li>Performance : R\u00e9duire l'espace de recherche</li> <li>Pertinence : Retourner uniquement les r\u00e9sultats pertinents</li> <li>Flexibilit\u00e9 : Combiner recherche vectorielle et filtres m\u00e9tier</li> </ul>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#exemple-simple","title":"Exemple simple","text":"<pre><code>from qdrant_client.models import Filter, FieldCondition, MatchValue\n\n# Rechercher des produits \u00e9lectroniques similaires\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    query_filter=Filter(\n        must=[\n            FieldCondition(\n                key=\"category\",\n                match=MatchValue(value=\"electronics\")\n            )\n        ]\n    ),\n    limit=10\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#types-de-filtres","title":"Types de filtres","text":""},{"location":"Qdrant/Data/FR/04-filters-metadata/#match-valeur-exacte","title":"Match (valeur exacte)","text":"<pre><code>from qdrant_client.models import Filter, FieldCondition, MatchValue\n\n# String exact\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"category\",\n            match=MatchValue(value=\"electronics\")\n        )\n    ]\n)\n\n# Number exact\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"product_id\",\n            match=MatchValue(value=12345)\n        )\n    ]\n)\n\n# Boolean\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"in_stock\",\n            match=MatchValue(value=True)\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#match-any-plusieurs-valeurs","title":"Match Any (plusieurs valeurs)","text":"<pre><code>from qdrant_client.models import MatchAny\n\n# Correspond \u00e0 n'importe quelle valeur de la liste\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"category\",\n            match=MatchAny(any=[\"electronics\", \"books\", \"clothing\"])\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#match-text-recherche-de-texte","title":"Match Text (recherche de texte)","text":"<pre><code>from qdrant_client.models import MatchText\n\n# Recherche de texte (contient)\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"description\",\n            match=MatchText(text=\"laptop\")\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#range-plage-de-valeurs","title":"Range (plage de valeurs)","text":"<pre><code>from qdrant_client.models import Range\n\n# Plage num\u00e9rique\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"price\",\n            range=Range(\n                gte=100,  # Greater than or equal\n                lte=500   # Less than or equal\n            )\n        )\n    ]\n)\n\n# Seulement minimum\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"price\",\n            range=Range(gte=100)\n        )\n    ]\n)\n\n# Seulement maximum\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"price\",\n            range=Range(lte=500)\n        )\n    ]\n)\n\n# Entre deux valeurs\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"price\",\n            range=Range(gt=100, lt=500)  # Strictement entre\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#geo-geolocalisation","title":"Geo (g\u00e9olocalisation)","text":"<pre><code>from qdrant_client.models import GeoRadius\n\n# Points dans un rayon (coordonn\u00e9es GPS)\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"location\",\n            geo_radius=GeoRadius(\n                center={\"lat\": 48.8566, \"lon\": 2.3522},  # Paris\n                radius=1000  # 1km en m\u00e8tres\n            )\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#is-null-is-empty","title":"Is Null / Is Empty","text":"<pre><code>from qdrant_client.models import IsNullCondition\n\n# V\u00e9rifier si un champ est null ou absent\nfilter = Filter(\n    must=[\n        IsNullCondition(\n            is_null=FieldCondition(key=\"discount\")\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#operateurs-de-filtrage","title":"Op\u00e9rateurs de filtrage","text":""},{"location":"Qdrant/Data/FR/04-filters-metadata/#must-et-logique","title":"MUST (ET logique)","text":"<p>Tous les crit\u00e8res doivent \u00eatre satisfaits :</p> <pre><code>filter = Filter(\n    must=[\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        FieldCondition(key=\"in_stock\", match=MatchValue(value=True)),\n        FieldCondition(key=\"price\", range=Range(gte=100, lte=500))\n    ]\n)\n\n# \u00c9quivalent \u00e0: category=\"electronics\" AND in_stock=True AND price BETWEEN 100 AND 500\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#should-ou-logique","title":"SHOULD (OU logique)","text":"<p>Au moins un crit\u00e8re doit \u00eatre satisfait :</p> <pre><code>filter = Filter(\n    should=[\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        FieldCondition(key=\"category\", match=MatchValue(value=\"books\")),\n        FieldCondition(key=\"category\", match=MatchValue(value=\"clothing\"))\n    ],\n    min_should_match=1  # Au moins 1 doit correspondre\n)\n\n# \u00c9quivalent \u00e0: category=\"electronics\" OR category=\"books\" OR category=\"clothing\"\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#must_not-non-logique","title":"MUST_NOT (NON logique)","text":"<p>Exclure les points correspondants :</p> <pre><code>filter = Filter(\n    must_not=[\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        FieldCondition(key=\"price\", range=Range(lt=50))\n    ]\n)\n\n# \u00c9quivalent \u00e0: NOT (category=\"electronics\" OR price &lt; 50)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#combinaison-complexe","title":"Combinaison complexe","text":"<pre><code># (category=\"electronics\" OR category=\"books\") \n# AND price BETWEEN 100 AND 500 \n# AND NOT (in_stock=False)\n\nfilter = Filter(\n    must=[\n        # ET: prix entre 100 et 500\n        FieldCondition(key=\"price\", range=Range(gte=100, lte=500))\n    ],\n    should=[\n        # OU: \u00e9lectronique OU livres\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        FieldCondition(key=\"category\", match=MatchValue(value=\"books\"))\n    ],\n    must_not=[\n        # NON: pas en rupture de stock\n        FieldCondition(key=\"in_stock\", match=MatchValue(value=False))\n    ],\n    min_should_match=1  # Au moins une cat\u00e9gorie doit correspondre\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#filtres-complexes","title":"Filtres complexes","text":""},{"location":"Qdrant/Data/FR/04-filters-metadata/#filtres-imbriques","title":"Filtres imbriqu\u00e9s","text":"<pre><code># (category=\"electronics\" AND price&gt;100) OR (category=\"books\" AND price&lt;50)\nfilter = Filter(\n    should=[\n        Filter(\n            must=[\n                FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n                FieldCondition(key=\"price\", range=Range(gt=100))\n            ]\n        ),\n        Filter(\n            must=[\n                FieldCondition(key=\"category\", match=MatchValue(value=\"books\")),\n                FieldCondition(key=\"price\", range=Range(lt=50))\n            ]\n        )\n    ],\n    min_should_match=1\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#filtres-sur-tableaux","title":"Filtres sur tableaux","text":"<pre><code># V\u00e9rifier si un \u00e9l\u00e9ment est dans un tableau\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"tags\",\n            match=MatchValue(value=\"gaming\")  # \"gaming\" dans le tableau tags\n        )\n    ]\n)\n\n# Plusieurs tags\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"tags\",\n            match=MatchAny(any=[\"gaming\", \"laptop\", \"professional\"])\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#filtres-sur-objets-imbriques","title":"Filtres sur objets imbriqu\u00e9s","text":"<pre><code># Payload: {\"metadata\": {\"brand\": \"Apple\", \"model\": \"MacBook\"}}\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"metadata.brand\",  # Acc\u00e8s aux champs imbriqu\u00e9s\n            match=MatchValue(value=\"Apple\")\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#gestion-du-payload","title":"Gestion du payload","text":""},{"location":"Qdrant/Data/FR/04-filters-metadata/#structure-du-payload","title":"Structure du payload","text":"<pre><code>payload = {\n    # Types simples\n    \"name\": \"Product A\",           # String\n    \"price\": 99.99,                # Number (float/int)\n    \"quantity\": 10,                # Integer\n    \"in_stock\": True,              # Boolean\n\n    # Tableaux\n    \"tags\": [\"electronics\", \"laptop\", \"gaming\"],\n    \"sizes\": [10, 12, 14],\n\n    # Objets\n    \"metadata\": {\n        \"brand\": \"BrandX\",\n        \"model\": \"Model123\",\n        \"specs\": {\n            \"cpu\": \"Intel i7\",\n            \"ram\": \"16GB\"\n        }\n    },\n\n    # Dates (comme string ISO)\n    \"created_at\": \"2024-01-15T10:00:00Z\",\n    \"updated_at\": \"2024-01-20T15:30:00Z\"\n}\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#inserer-avec-payload","title":"Ins\u00e9rer avec payload","text":"<pre><code>from qdrant_client.models import PointStruct\n\npoint = PointStruct(\n    id=1,\n    vector=[0.1, 0.2, 0.3, ...],\n    payload={\n        \"name\": \"Laptop Gaming\",\n        \"category\": \"electronics\",\n        \"price\": 1299.99,\n        \"in_stock\": True,\n        \"tags\": [\"gaming\", \"laptop\", \"high-performance\"],\n        \"metadata\": {\n            \"brand\": \"BrandX\",\n            \"model\": \"GamingPro-2024\"\n        }\n    }\n)\n\nclient.upsert(collection_name=\"products\", points=[point])\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#mettre-a-jour-le-payload","title":"Mettre \u00e0 jour le payload","text":"<pre><code># Ajouter/modifier des champs\nclient.set_payload(\n    collection_name=\"products\",\n    payload={\n        \"discount\": 0.15,  # Nouveau champ\n        \"on_sale\": True,   # Nouveau champ\n        \"price\": 1099.99   # Mise \u00e0 jour\n    },\n    points=[1, 2, 3]  # IDs des points \u00e0 mettre \u00e0 jour\n)\n\n# Mettre \u00e0 jour tous les points d'une collection (avec filtre)\nclient.set_payload(\n    collection_name=\"products\",\n    payload={\"last_updated\": \"2024-01-20\"},\n    points=None,  # Tous les points\n    key=\"category\",\n    match=MatchValue(value=\"electronics\")  # Seulement \u00e9lectronique\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#supprimer-des-champs-du-payload","title":"Supprimer des champs du payload","text":"<pre><code># Supprimer des cl\u00e9s sp\u00e9cifiques\nclient.delete_payload(\n    collection_name=\"products\",\n    keys=[\"discount\", \"on_sale\"],  # Cl\u00e9s \u00e0 supprimer\n    points=[1, 2, 3]\n)\n\n# Supprimer pour tous les points\nclient.delete_payload(\n    collection_name=\"products\",\n    keys=[\"temporary_field\"],\n    points=None  # Tous les points\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#recuperer-le-payload","title":"R\u00e9cup\u00e9rer le payload","text":"<pre><code># R\u00e9cup\u00e9rer des points avec leur payload\npoints = client.retrieve(\n    collection_name=\"products\",\n    ids=[1, 2, 3],\n    with_payload=True,  # Inclure le payload\n    with_vectors=False\n)\n\nfor point in points:\n    print(f\"ID: {point.id}\")\n    print(f\"Payload: {point.payload}\")\n    print(f\"Name: {point.payload.get('name')}\")\n    print(f\"Price: {point.payload.get('price')}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#overwrite-payload-remplacer-completement","title":"Overwrite payload (remplacer compl\u00e8tement)","text":"<pre><code># Remplacer compl\u00e8tement le payload\nclient.overwrite_payload(\n    collection_name=\"products\",\n    payload={\n        \"name\": \"New Name\",\n        \"price\": 999.99\n        # Tous les autres champs sont supprim\u00e9s\n    },\n    points=[1]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#index-de-payload","title":"Index de payload","text":""},{"location":"Qdrant/Data/FR/04-filters-metadata/#pourquoi-indexer-le-payload","title":"Pourquoi indexer le payload ?","text":"<p>Les index de payload acc\u00e9l\u00e8rent les filtres : - Sans index : Scan complet de tous les points - Avec index : Recherche rapide dans l'index</p>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#creer-un-index","title":"Cr\u00e9er un index","text":"<pre><code>from qdrant_client.models import PayloadSchemaType\n\n# Index sur un champ string\nclient.create_payload_index(\n    collection_name=\"products\",\n    field_name=\"category\",\n    field_schema=PayloadSchemaType.KEYWORD  # Pour valeurs exactes\n)\n\n# Index sur un champ num\u00e9rique\nclient.create_payload_index(\n    collection_name=\"products\",\n    field_name=\"price\",\n    field_schema=PayloadSchemaType.FLOAT  # Pour ranges\n)\n\n# Index sur un champ integer\nclient.create_payload_index(\n    collection_name=\"products\",\n    field_name=\"product_id\",\n    field_schema=PayloadSchemaType.INTEGER\n)\n\n# Index g\u00e9ospatial\nclient.create_payload_index(\n    collection_name=\"products\",\n    field_name=\"location\",\n    field_schema=PayloadSchemaType.GEO\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#types-dindex","title":"Types d'index","text":"<ul> <li>KEYWORD : Pour valeurs exactes (strings)</li> <li>INTEGER : Pour nombres entiers</li> <li>FLOAT : Pour nombres d\u00e9cimaux</li> <li>GEO : Pour coordonn\u00e9es g\u00e9ographiques</li> <li>BOOL : Pour valeurs bool\u00e9ennes</li> </ul>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#verifier-les-index","title":"V\u00e9rifier les index","text":"<pre><code># Obtenir les informations de la collection\ncollection_info = client.get_collection(\"products\")\n\n# Voir les index de payload\nprint(collection_info.payload_schema)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#supprimer-un-index","title":"Supprimer un index","text":"<pre><code>client.delete_payload_index(\n    collection_name=\"products\",\n    field_name=\"category\"\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#optimisation-des-filtres","title":"Optimisation des filtres","text":""},{"location":"Qdrant/Data/FR/04-filters-metadata/#pre-filtrage-vs-post-filtrage","title":"Pr\u00e9-filtrage vs Post-filtrage","text":"<p>Pr\u00e9-filtrage (recommand\u00e9 si peu de r\u00e9sultats apr\u00e8s filtrage) : <pre><code># Filtre appliqu\u00e9 AVANT la recherche vectorielle\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    query_filter=filter,  # Filtre d'abord\n    limit=10\n)\n</code></pre></p> <p>Post-filtrage (si beaucoup de r\u00e9sultats apr\u00e8s filtrage) : <pre><code># Recherche vectorielle d'abord, puis filtre\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=query_vector,\n    limit=100  # Plus de r\u00e9sultats\n)\n\n# Filtrer manuellement\nfiltered_results = [\n    r for r in results \n    if r.payload.get(\"category\") == \"electronics\" \n    and 100 &lt;= r.payload.get(\"price\", 0) &lt;= 500\n]\n</code></pre></p>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#indexer-les-champs-frequemment-filtres","title":"Indexer les champs fr\u00e9quemment filtr\u00e9s","text":"<pre><code># Indexer les champs utilis\u00e9s dans les filtres\nclient.create_payload_index(\n    collection_name=\"products\",\n    field_name=\"category\",\n    field_schema=PayloadSchemaType.KEYWORD\n)\n\nclient.create_payload_index(\n    collection_name=\"products\",\n    field_name=\"price\",\n    field_schema=PayloadSchemaType.FLOAT\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#ordre-des-conditions-dans-must","title":"Ordre des conditions dans MUST","text":"<pre><code># Mettre les conditions les plus s\u00e9lectives en premier\nfilter = Filter(\n    must=[\n        # 1. Condition la plus s\u00e9lective (peu de r\u00e9sultats)\n        FieldCondition(key=\"product_id\", match=MatchValue(value=12345)),\n        # 2. Condition moins s\u00e9lective\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        # 3. Condition la moins s\u00e9lective\n        FieldCondition(key=\"in_stock\", match=MatchValue(value=True))\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#exercices-pratiques","title":"Exercices pratiques","text":""},{"location":"Qdrant/Data/FR/04-filters-metadata/#exercice-1-filtre-complexe","title":"Exercice 1 : Filtre complexe","text":"<p>Cr\u00e9er un filtre pour trouver des produits : - Cat\u00e9gorie \"electronics\" OU \"books\" - Prix entre 50 et 200 - En stock - Pas de produits avec tag \"discontinued\"</p> <p>Solution :</p> <pre><code>filter = Filter(\n    must=[\n        FieldCondition(key=\"price\", range=Range(gte=50, lte=200)),\n        FieldCondition(key=\"in_stock\", match=MatchValue(value=True))\n    ],\n    should=[\n        FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\")),\n        FieldCondition(key=\"category\", match=MatchValue(value=\"books\"))\n    ],\n    must_not=[\n        FieldCondition(key=\"tags\", match=MatchValue(value=\"discontinued\"))\n    ],\n    min_should_match=1\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#exercice-2-mise-a-jour-conditionnelle","title":"Exercice 2 : Mise \u00e0 jour conditionnelle","text":"<p>Mettre \u00e0 jour le prix de tous les produits \u00e9lectroniques avec une r\u00e9duction de 10%.</p> <p>Solution :</p> <pre><code># 1. R\u00e9cup\u00e9rer tous les produits \u00e9lectroniques\nscroll_result = client.scroll(\n    collection_name=\"products\",\n    scroll_filter=Filter(\n        must=[\n            FieldCondition(key=\"category\", match=MatchValue(value=\"electronics\"))\n        ]\n    ),\n    limit=100,\n    with_payload=True,\n    with_vectors=False\n)\n\npoints, next_offset = scroll_result\nupdated_points = []\n\nfor point in points:\n    old_price = point.payload.get(\"price\", 0)\n    new_price = old_price * 0.9  # R\u00e9duction de 10%\n\n    # Mettre \u00e0 jour le payload\n    client.set_payload(\n        collection_name=\"products\",\n        payload={\"price\": round(new_price, 2)},\n        points=[point.id]\n    )\n</code></pre>"},{"location":"Qdrant/Data/FR/04-filters-metadata/#points-cles-a-retenir","title":"\ud83c\udfaf Points cl\u00e9s \u00e0 retenir","text":"<p>\u2705 Les filtres restreignent la recherche aux points correspondants \u2705 MUST = ET, SHOULD = OU, MUST_NOT = NON \u2705 Indexer les champs fr\u00e9quemment filtr\u00e9s pour de meilleures performances \u2705 Pr\u00e9-filtrage si peu de r\u00e9sultats, post-filtrage si beaucoup \u2705 Le payload peut contenir des types simples, tableaux et objets imbriqu\u00e9s  </p> <p>Prochaine \u00e9tape : Int\u00e9gration Python</p>"},{"location":"Qdrant/Data/FR/05-python-integration/","title":"5. Int\u00e9gration Python","text":""},{"location":"Qdrant/Data/FR/05-python-integration/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Int\u00e9grer avec des mod\u00e8les d'embeddings</li> <li>Utiliser sentence-transformers</li> <li>Int\u00e9grer avec OpenAI, Cohere, etc.</li> <li>Cr\u00e9er des pipelines de donn\u00e9es</li> <li>Optimiser les performances</li> <li>G\u00e9rer les batchs d'embeddings</li> </ul>"},{"location":"Qdrant/Data/FR/05-python-integration/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Mod\u00e8les d'embeddings</li> <li>sentence-transformers</li> <li>OpenAI Embeddings</li> <li>Cohere Embeddings</li> <li>Pipelines de donn\u00e9es</li> <li>Optimisation batch</li> <li>Gestion des erreurs</li> </ol>"},{"location":"Qdrant/Data/FR/05-python-integration/#modeles-dembeddings","title":"Mod\u00e8les d'embeddings","text":""},{"location":"Qdrant/Data/FR/05-python-integration/#quest-ce-quun-embedding","title":"Qu'est-ce qu'un embedding ?","text":"<p>Un embedding est une repr\u00e9sentation vectorielle d'un objet : - Texte \u2192 Vecteur de nombres - Image \u2192 Vecteur de nombres - Audio \u2192 Vecteur de nombres</p>"},{"location":"Qdrant/Data/FR/05-python-integration/#types-de-modeles","title":"Types de mod\u00e8les","text":"<ul> <li>Text embeddings : Pour textes (sentence-transformers, OpenAI)</li> <li>Image embeddings : Pour images (CLIP, ResNet)</li> <li>Multimodal : Pour texte + image (CLIP)</li> </ul>"},{"location":"Qdrant/Data/FR/05-python-integration/#sentence-transformers","title":"sentence-transformers","text":""},{"location":"Qdrant/Data/FR/05-python-integration/#installation","title":"Installation","text":"<pre><code>pip install sentence-transformers\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#modele-de-base","title":"Mod\u00e8le de base","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import PointStruct\n\n# Charger un mod\u00e8le pr\u00e9-entra\u00een\u00e9\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # Dimension 384\n\n# G\u00e9n\u00e9rer des embeddings\ntexts = [\n    \"Laptop computer for work\",\n    \"Gaming laptop with high performance\",\n    \"Professional workstation\"\n]\n\nembeddings = model.encode(texts)\nprint(f\"Shape: {embeddings.shape}\")  # (3, 384)\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#modeles-recommandes","title":"Mod\u00e8les recommand\u00e9s","text":"<pre><code># Mod\u00e8les populaires\nmodels = {\n    # L\u00e9ger et rapide\n    \"all-MiniLM-L6-v2\": 384,  # 22MB, rapide\n\n    # \u00c9quilibr\u00e9\n    \"all-mpnet-base-v2\": 768,  # 420MB, meilleure qualit\u00e9\n\n    # Multilingue\n    \"paraphrase-multilingual-MiniLM-L12-v2\": 384,  # Support multilingue\n\n    # Sp\u00e9cialis\u00e9\n    \"ms-marco-MiniLM-L-6-v2\": 384,  # Optimis\u00e9 pour recherche\n}\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#indexer-des-documents","title":"Indexer des documents","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\n\n# Initialisation\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nclient = QdrantClient(host=\"localhost\", port=6333)\n\n# Cr\u00e9er la collection\nclient.create_collection(\n    collection_name=\"documents\",\n    vectors_config=VectorParams(\n        size=384,  # Dimension du mod\u00e8le\n        distance=Distance.COSINE\n    )\n)\n\n# Documents \u00e0 indexer\ndocuments = [\n    {\"id\": 1, \"text\": \"Laptop computer for professional work\"},\n    {\"id\": 2, \"text\": \"Gaming laptop with high-end graphics\"},\n    {\"id\": 3, \"text\": \"Business workstation for office use\"}\n]\n\n# G\u00e9n\u00e9rer les embeddings\ntexts = [doc[\"text\"] for doc in documents]\nembeddings = model.encode(texts)\n\n# Cr\u00e9er les points\npoints = [\n    PointStruct(\n        id=doc[\"id\"],\n        vector=embedding.tolist(),\n        payload={\"text\": doc[\"text\"]}\n    )\n    for doc, embedding in zip(documents, embeddings)\n]\n\n# Ins\u00e9rer dans Qdrant\nclient.upsert(collection_name=\"documents\", points=points)\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#recherche-semantique","title":"Recherche s\u00e9mantique","text":"<pre><code># Requ\u00eate utilisateur\nquery = \"computer for work\"\n\n# G\u00e9n\u00e9rer l'embedding de la requ\u00eate\nquery_embedding = model.encode([query])[0]\n\n# Rechercher dans Qdrant\nresults = client.search(\n    collection_name=\"documents\",\n    query_vector=query_embedding.tolist(),\n    limit=5\n)\n\n# Afficher les r\u00e9sultats\nfor result in results:\n    print(f\"Score: {result.score:.4f}\")\n    print(f\"Text: {result.payload['text']}\")\n    print(\"---\")\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#batch-processing","title":"Batch processing","text":"<pre><code>def index_documents_batch(documents, batch_size=100):\n    \"\"\"Indexer des documents par batches\"\"\"\n\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    for i in range(0, len(documents), batch_size):\n        batch = documents[i:i+batch_size]\n\n        # G\u00e9n\u00e9rer embeddings\n        texts = [doc[\"text\"] for doc in batch]\n        embeddings = model.encode(texts, show_progress_bar=True)\n\n        # Cr\u00e9er points\n        points = [\n            PointStruct(\n                id=doc[\"id\"],\n                vector=emb.tolist(),\n                payload={\"text\": doc[\"text\"]}\n            )\n            for doc, emb in zip(batch, embeddings)\n        ]\n\n        # Ins\u00e9rer\n        client.upsert(collection_name=\"documents\", points=points)\n        print(f\"Indexed batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#openai-embeddings","title":"OpenAI Embeddings","text":""},{"location":"Qdrant/Data/FR/05-python-integration/#installation_1","title":"Installation","text":"<pre><code>pip install openai\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#configuration","title":"Configuration","text":"<pre><code>import os\nfrom openai import OpenAI\n\n# Configuration\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\nclient_openai = OpenAI()\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#generer-des-embeddings","title":"G\u00e9n\u00e9rer des embeddings","text":"<pre><code>from openai import OpenAI\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import PointStruct\n\nclient_openai = OpenAI()\nclient_qdrant = QdrantClient(host=\"localhost\", port=6333)\n\n# Documents\ntexts = [\n    \"Laptop computer for work\",\n    \"Gaming laptop with high performance\"\n]\n\n# G\u00e9n\u00e9rer embeddings avec OpenAI\nresponse = client_openai.embeddings.create(\n    model=\"text-embedding-ada-002\",  # Dimension 1536\n    input=texts\n)\n\n# Extraire les embeddings\nembeddings = [item.embedding for item in response.data]\n\n# Cr\u00e9er les points\npoints = [\n    PointStruct(\n        id=i,\n        vector=embedding,\n        payload={\"text\": text}\n    )\n    for i, (text, embedding) in enumerate(zip(texts, embeddings))\n]\n\n# Ins\u00e9rer dans Qdrant\nclient_qdrant.upsert(\n    collection_name=\"documents_openai\",\n    points=points\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#recherche-avec-openai","title":"Recherche avec OpenAI","text":"<pre><code># Requ\u00eate\nquery = \"computer for professional use\"\n\n# Embedding de la requ\u00eate\nresponse = client_openai.embeddings.create(\n    model=\"text-embedding-ada-002\",\n    input=[query]\n)\nquery_embedding = response.data[0].embedding\n\n# Recherche\nresults = client_qdrant.search(\n    collection_name=\"documents_openai\",\n    query_vector=query_embedding,\n    limit=5\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#cohere-embeddings","title":"Cohere Embeddings","text":""},{"location":"Qdrant/Data/FR/05-python-integration/#installation_2","title":"Installation","text":"<pre><code>pip install cohere\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#utilisation","title":"Utilisation","text":"<pre><code>import cohere\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import PointStruct\n\n# Initialisation\nco = cohere.Client(\"your-api-key\")\nclient = QdrantClient(host=\"localhost\", port=6333)\n\n# G\u00e9n\u00e9rer embeddings\ntexts = [\"Laptop computer\", \"Gaming laptop\"]\nresponse = co.embed(\n    texts=texts,\n    model=\"embed-english-v2.0\"  # Dimension 4096\n)\n\nembeddings = response.embeddings\n\n# Ins\u00e9rer dans Qdrant\npoints = [\n    PointStruct(\n        id=i,\n        vector=embedding,\n        payload={\"text\": text}\n    )\n    for i, (text, embedding) in enumerate(zip(texts, embeddings))\n]\n\nclient.upsert(collection_name=\"documents_cohere\", points=points)\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#pipelines-de-donnees","title":"Pipelines de donn\u00e9es","text":""},{"location":"Qdrant/Data/FR/05-python-integration/#pipeline-complet","title":"Pipeline complet","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\nimport pandas as pd\n\nclass DocumentIndexer:\n    def __init__(self, collection_name=\"documents\"):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n        self.collection_name = collection_name\n        self._ensure_collection()\n\n    def _ensure_collection(self):\n        \"\"\"Cr\u00e9er la collection si elle n'existe pas\"\"\"\n        collections = self.client.get_collections()\n        if self.collection_name not in [c.name for c in collections.collections]:\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(\n                    size=384,\n                    distance=Distance.COSINE\n                )\n            )\n\n    def index_dataframe(self, df, text_column, id_column=None, batch_size=100):\n        \"\"\"Indexer un DataFrame\"\"\"\n\n        if id_column is None:\n            df['_id'] = range(len(df))\n            id_column = '_id'\n\n        for i in range(0, len(df), batch_size):\n            batch = df.iloc[i:i+batch_size]\n\n            # G\u00e9n\u00e9rer embeddings\n            texts = batch[text_column].tolist()\n            embeddings = self.model.encode(texts)\n\n            # Cr\u00e9er points\n            points = [\n                PointStruct(\n                    id=int(row[id_column]),\n                    vector=emb.tolist(),\n                    payload=row.to_dict()\n                )\n                for row, emb in zip(batch.itertuples(), embeddings)\n            ]\n\n            # Ins\u00e9rer\n            self.client.upsert(\n                collection_name=self.collection_name,\n                points=points\n            )\n\n            print(f\"Indexed {min(i+batch_size, len(df))}/{len(df)}\")\n\n    def search(self, query, limit=10, filter=None):\n        \"\"\"Rechercher dans la collection\"\"\"\n\n        query_embedding = self.model.encode([query])[0]\n\n        results = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=query_embedding.tolist(),\n            query_filter=filter,\n            limit=limit\n        )\n\n        return results\n\n# Utilisation\nindexer = DocumentIndexer()\n\n# Charger des donn\u00e9es\ndf = pd.read_csv(\"products.csv\")\n\n# Indexer\nindexer.index_dataframe(df, text_column=\"description\", id_column=\"product_id\")\n\n# Rechercher\nresults = indexer.search(\"laptop computer\", limit=5)\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#optimisation-batch","title":"Optimisation batch","text":""},{"location":"Qdrant/Data/FR/05-python-integration/#batch-encoding","title":"Batch encoding","text":"<pre><code># Plus efficace : encoder par batches\ntexts = [\"text1\", \"text2\", ..., \"text1000\"]\n\n# \u2705 Bon : batch encoding\nembeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n\n# \u274c Moins bon : un par un\nembeddings = [model.encode([text])[0] for text in texts]\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#batch-insertion","title":"Batch insertion","text":"<pre><code>def efficient_indexing(documents, batch_size=100):\n    \"\"\"Indexation efficace par batches\"\"\"\n\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    # Encoder tous les documents\n    texts = [doc[\"text\"] for doc in documents]\n    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n\n    # Ins\u00e9rer par batches dans Qdrant\n    for i in range(0, len(documents), batch_size):\n        batch_docs = documents[i:i+batch_size]\n        batch_embeddings = embeddings[i:i+batch_size]\n\n        points = [\n            PointStruct(\n                id=doc[\"id\"],\n                vector=emb.tolist(),\n                payload={\"text\": doc[\"text\"]}\n            )\n            for doc, emb in zip(batch_docs, batch_embeddings)\n        ]\n\n        client.upsert(collection_name=\"documents\", points=points)\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#utilisation-du-gpu","title":"Utilisation du GPU","text":"<pre><code>import torch\n\n# V\u00e9rifier si GPU disponible\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Charger le mod\u00e8le sur GPU\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n\n# Encoder (plus rapide sur GPU)\nembeddings = model.encode(texts, batch_size=64)  # Batch plus grand sur GPU\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#gestion-des-erreurs","title":"Gestion des erreurs","text":""},{"location":"Qdrant/Data/FR/05-python-integration/#gestion-robuste","title":"Gestion robuste","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nimport time\n\ndef robust_indexing(documents, max_retries=3):\n    \"\"\"Indexation avec gestion d'erreurs\"\"\"\n\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    for doc in documents:\n        retries = 0\n        while retries &lt; max_retries:\n            try:\n                # G\u00e9n\u00e9rer embedding\n                embedding = model.encode([doc[\"text\"]])[0]\n\n                # Ins\u00e9rer\n                point = PointStruct(\n                    id=doc[\"id\"],\n                    vector=embedding.tolist(),\n                    payload={\"text\": doc[\"text\"]}\n                )\n\n                client.upsert(collection_name=\"documents\", points=[point])\n                break  # Succ\u00e8s\n\n            except Exception as e:\n                retries += 1\n                print(f\"Error indexing doc {doc['id']}: {e}\")\n                if retries &lt; max_retries:\n                    time.sleep(2 ** retries)  # Backoff exponentiel\n                else:\n                    print(f\"Failed to index doc {doc['id']} after {max_retries} retries\")\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#exercices-pratiques","title":"Exercices pratiques","text":""},{"location":"Qdrant/Data/FR/05-python-integration/#exercice-1-pipeline-dindexation","title":"Exercice 1 : Pipeline d'indexation","text":"<p>Cr\u00e9er un pipeline qui : 1. Lit un fichier CSV 2. G\u00e9n\u00e8re des embeddings 3. Indexe dans Qdrant 4. Affiche la progression</p> <p>Solution :</p> <pre><code>import pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import PointStruct\n\ndef index_csv(csv_path, text_column, id_column):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    df = pd.read_csv(csv_path)\n\n    # Encoder tous les textes\n    texts = df[text_column].tolist()\n    embeddings = model.encode(texts, show_progress_bar=True)\n\n    # Cr\u00e9er et ins\u00e9rer les points\n    points = [\n        PointStruct(\n            id=int(row[id_column]),\n            vector=emb.tolist(),\n            payload=row.to_dict()\n        )\n        for row, emb in zip(df.itertuples(), embeddings)\n    ]\n\n    client.upsert(collection_name=\"documents\", points=points)\n    print(f\"Indexed {len(points)} documents\")\n</code></pre>"},{"location":"Qdrant/Data/FR/05-python-integration/#points-cles-a-retenir","title":"\ud83c\udfaf Points cl\u00e9s \u00e0 retenir","text":"<p>\u2705 sentence-transformers est id\u00e9al pour les embeddings de texte \u2705 Utiliser batch encoding pour de meilleures performances \u2705 GPU acc\u00e9l\u00e8re significativement l'encodage \u2705 G\u00e9rer les erreurs avec retry logic \u2705 Cr\u00e9er des pipelines r\u00e9utilisables pour l'indexation  </p> <p>Prochaine \u00e9tape : Cas d'usage IA/ML</p>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/","title":"6. Cas d'usage IA/ML","text":""},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre les cas d'usage typiques</li> <li>Impl\u00e9menter la recherche s\u00e9mantique</li> <li>Cr\u00e9er des syst\u00e8mes de recommandation</li> <li>Utiliser avec RAG (Retrieval-Augmented Generation)</li> <li>D\u00e9tecter les doublons</li> <li>Clustering et classification</li> </ul>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Recherche s\u00e9mantique</li> <li>Syst\u00e8mes de recommandation</li> <li>RAG (Retrieval-Augmented Generation)</li> <li>D\u00e9duplication</li> <li>Clustering</li> <li>Classification</li> <li>Anomaly Detection</li> </ol>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#recherche-semantique","title":"Recherche s\u00e9mantique","text":""},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#quest-ce-que-la-recherche-semantique","title":"Qu'est-ce que la recherche s\u00e9mantique ?","text":"<p>La recherche s\u00e9mantique trouve des r\u00e9sultats bas\u00e9s sur le sens plut\u00f4t que sur des mots-cl\u00e9s exacts.</p> <p>Recherche classique (mots-cl\u00e9s) : - Requ\u00eate : \"laptop\" - R\u00e9sultats : Documents contenant le mot \"laptop\"</p> <p>Recherche s\u00e9mantique : - Requ\u00eate : \"computer for work\" - R\u00e9sultats : Documents sur \"laptop\", \"workstation\", \"desktop\", etc.</p>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#implementation-complete","title":"Impl\u00e9mentation compl\u00e8te","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\n\nclass SemanticSearchEngine:\n    def __init__(self, collection_name=\"documents\"):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n        self.collection_name = collection_name\n        self._ensure_collection()\n\n    def _ensure_collection(self):\n        \"\"\"Cr\u00e9er la collection si n\u00e9cessaire\"\"\"\n        collections = self.client.get_collections()\n        if self.collection_name not in [c.name for c in collections.collections]:\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n            )\n\n    def index_documents(self, documents):\n        \"\"\"Indexer des documents\"\"\"\n        texts = [doc[\"text\"] for doc in documents]\n        embeddings = self.model.encode(texts)\n\n        points = [\n            PointStruct(\n                id=doc[\"id\"],\n                vector=emb.tolist(),\n                payload={\"text\": doc[\"text\"], **doc.get(\"metadata\", {})}\n            )\n            for doc, emb in zip(documents, embeddings)\n        ]\n\n        self.client.upsert(collection_name=self.collection_name, points=points)\n\n    def search(self, query, limit=10, filter=None):\n        \"\"\"Rechercher des documents similaires\"\"\"\n        query_embedding = self.model.encode([query])[0]\n\n        results = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=query_embedding.tolist(),\n            query_filter=filter,\n            limit=limit\n        )\n\n        return [\n            {\n                \"id\": r.id,\n                \"score\": r.score,\n                \"text\": r.payload[\"text\"],\n                \"metadata\": {k: v for k, v in r.payload.items() if k != \"text\"}\n            }\n            for r in results\n        ]\n\n# Utilisation\nengine = SemanticSearchEngine()\n\n# Indexer\ndocuments = [\n    {\"id\": 1, \"text\": \"Laptop computer for professional work\"},\n    {\"id\": 2, \"text\": \"Gaming laptop with high-end graphics\"},\n    {\"id\": 3, \"text\": \"Business workstation for office use\"}\n]\nengine.index_documents(documents)\n\n# Rechercher\nresults = engine.search(\"computer for work\", limit=5)\nfor result in results:\n    print(f\"Score: {result['score']:.4f} - {result['text']}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#systemes-de-recommandation","title":"Syst\u00e8mes de recommandation","text":""},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#recommandation-basique-similarite","title":"Recommandation basique (similarit\u00e9)","text":"<pre><code>def recommend_similar_products(product_id, limit=10):\n    \"\"\"Recommander des produits similaires\"\"\"\n\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    # 1. R\u00e9cup\u00e9rer le vecteur du produit\n    points = client.retrieve(\n        collection_name=\"products\",\n        ids=[product_id],\n        with_vectors=True\n    )\n\n    if not points:\n        return []\n\n    product_vector = points[0].vector\n\n    # 2. Rechercher des produits similaires (exclure l'original)\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=product_vector,\n        query_filter=Filter(\n            must_not=[\n                FieldCondition(key=\"id\", match=MatchValue(value=product_id))\n            ]\n        ),\n        limit=limit\n    )\n\n    return results\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#recommandation-hybride-similarite-preferences","title":"Recommandation hybride (similarit\u00e9 + pr\u00e9f\u00e9rences)","text":"<pre><code>def hybrid_recommendation(product_id, user_preferences, limit=10):\n    \"\"\"Recommandation combinant similarit\u00e9 et pr\u00e9f\u00e9rences\"\"\"\n\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    # Vecteur du produit\n    points = client.retrieve(\n        collection_name=\"products\",\n        ids=[product_id],\n        with_vectors=True\n    )\n    product_vector = points[0].vector\n\n    # Recherche avec filtres de pr\u00e9f\u00e9rences\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=product_vector,\n        query_filter=Filter(\n            must=[\n                FieldCondition(\n                    key=\"category\",\n                    match=MatchValue(value=user_preferences[\"preferred_category\"])\n                ),\n                FieldCondition(\n                    key=\"price\",\n                    range=Range(\n                        gte=user_preferences[\"min_price\"],\n                        lte=user_preferences[\"max_price\"]\n                    )\n                )\n            ],\n            must_not=[\n                FieldCondition(key=\"id\", match=MatchValue(value=product_id))\n            ]\n        ),\n        limit=limit,\n        score_threshold=0.7\n    )\n\n    return results\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#recommandation-basee-sur-lhistorique-utilisateur","title":"Recommandation bas\u00e9e sur l'historique utilisateur","text":"<pre><code>def user_based_recommendation(user_id, limit=10):\n    \"\"\"Recommandation bas\u00e9e sur l'historique utilisateur\"\"\"\n\n    client = QdrantClient(host=\"localhost\", port=6333)\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n\n    # 1. R\u00e9cup\u00e9rer l'historique utilisateur\n    user_history = get_user_purchase_history(user_id)  # Fonction externe\n\n    # 2. G\u00e9n\u00e9rer embeddings des produits achet\u00e9s\n    product_texts = [item[\"description\"] for item in user_history]\n    embeddings = model.encode(product_texts)\n\n    # 3. Calculer le vecteur moyen (profil utilisateur)\n    user_profile_vector = embeddings.mean(axis=0).tolist()\n\n    # 4. Recommander des produits similaires\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=user_profile_vector,\n        query_filter=Filter(\n            must_not=[\n                FieldCondition(\n                    key=\"id\",\n                    match=MatchAny(any=[item[\"product_id\"] for item in user_history])\n                )\n            ]\n        ),\n        limit=limit\n    )\n\n    return results\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":""},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#quest-ce-que-rag","title":"Qu'est-ce que RAG ?","text":"<p>RAG combine : - Retrieval : Recherche de documents pertinents (Qdrant) - Augmented Generation : G\u00e9n\u00e9ration de texte avec LLM (GPT, etc.)</p>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#pipeline-rag-simple","title":"Pipeline RAG simple","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom openai import OpenAI\n\nclass RAGSystem:\n    def __init__(self):\n        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n        self.llm = OpenAI()  # Ou autre LLM\n\n    def retrieve_context(self, query, limit=5):\n        \"\"\"R\u00e9cup\u00e9rer le contexte pertinent\"\"\"\n        query_embedding = self.embedding_model.encode([query])[0]\n\n        results = self.client.search(\n            collection_name=\"knowledge_base\",\n            query_vector=query_embedding.tolist(),\n            limit=limit\n        )\n\n        # Extraire le texte\n        context = \"\\n\\n\".join([\n            result.payload[\"text\"] for result in results\n        ])\n\n        return context\n\n    def generate_answer(self, query, context):\n        \"\"\"G\u00e9n\u00e9rer une r\u00e9ponse avec le contexte\"\"\"\n\n        prompt = f\"\"\"Contexte:\n{context}\n\nQuestion: {query}\n\nR\u00e9ponds \u00e0 la question en utilisant le contexte fourni.\"\"\"\n\n        response = self.llm.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Tu es un assistant utile.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n\n        return response.choices[0].message.content\n\n    def answer(self, query, limit=5):\n        \"\"\"Pipeline RAG complet\"\"\"\n        # 1. Retrieval\n        context = self.retrieve_context(query, limit)\n\n        # 2. Generation\n        answer = self.generate_answer(query, context)\n\n        return {\n            \"answer\": answer,\n            \"context\": context\n        }\n\n# Utilisation\nrag = RAGSystem()\nresult = rag.answer(\"Quels sont les avantages des laptops gaming?\")\nprint(result[\"answer\"])\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#rag-avec-sources","title":"RAG avec sources","text":"<pre><code>def rag_with_sources(query, limit=5):\n    \"\"\"RAG qui retourne aussi les sources\"\"\"\n\n    query_embedding = model.encode([query])[0]\n\n    results = client.search(\n        collection_name=\"knowledge_base\",\n        query_vector=query_embedding.tolist(),\n        limit=limit\n    )\n\n    # Construire le contexte avec sources\n    context_parts = []\n    sources = []\n\n    for i, result in enumerate(results):\n        text = result.payload[\"text\"]\n        source = result.payload.get(\"source\", f\"Document {result.id}\")\n\n        context_parts.append(f\"[Source {i+1}]: {text}\")\n        sources.append({\n            \"id\": result.id,\n            \"source\": source,\n            \"score\": result.score\n        })\n\n    context = \"\\n\\n\".join(context_parts)\n\n    # G\u00e9n\u00e9rer r\u00e9ponse\n    answer = generate_answer(query, context)\n\n    return {\n        \"answer\": answer,\n        \"sources\": sources\n    }\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#deduplication","title":"D\u00e9duplication","text":""},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#detecter-les-doublons","title":"D\u00e9tecter les doublons","text":"<pre><code>def find_duplicates(threshold=0.95):\n    \"\"\"Trouver les documents dupliqu\u00e9s\"\"\"\n\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    # Parcourir tous les points\n    all_points = []\n    scroll_result = client.scroll(\n        collection_name=\"documents\",\n        limit=100,\n        with_vectors=True,\n        with_payload=True\n    )\n\n    points, next_offset = scroll_result\n    all_points.extend(points)\n\n    # Continuer le scroll\n    while next_offset is not None:\n        scroll_result = client.scroll(\n            collection_name=\"documents\",\n            limit=100,\n            offset=next_offset,\n            with_vectors=True,\n            with_payload=True\n        )\n        points, next_offset = scroll_result\n        all_points.extend(points)\n\n    # Trouver les doublons\n    duplicates = []\n    processed = set()\n\n    for i, point1 in enumerate(all_points):\n        if point1.id in processed:\n            continue\n\n        similar = client.search(\n            collection_name=\"documents\",\n            query_vector=point1.vector,\n            limit=10,\n            score_threshold=threshold\n        )\n\n        # Filtrer l'\u00e9l\u00e9ment lui-m\u00eame\n        similar = [s for s in similar if s.id != point1.id]\n\n        if similar:\n            duplicates.append({\n                \"original\": point1.id,\n                \"duplicates\": [s.id for s in similar]\n            })\n            processed.add(point1.id)\n            processed.update([s.id for s in similar])\n\n    return duplicates\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#clustering","title":"Clustering","text":""},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#clustering-simple","title":"Clustering simple","text":"<pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\ndef cluster_documents(n_clusters=5):\n    \"\"\"Grouper des documents similaires\"\"\"\n\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    # R\u00e9cup\u00e9rer tous les vecteurs\n    all_points = []\n    scroll_result = client.scroll(\n        collection_name=\"documents\",\n        limit=1000,\n        with_vectors=True,\n        with_payload=True\n    )\n    points, _ = scroll_result\n    all_points.extend(points)\n\n    # Extraire les vecteurs\n    vectors = np.array([point.vector for point in all_points])\n\n    # Clustering K-means\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    clusters = kmeans.fit_predict(vectors)\n\n    # Mettre \u00e0 jour le payload avec les clusters\n    for point, cluster_id in zip(all_points, clusters):\n        client.set_payload(\n            collection_name=\"documents\",\n            payload={\"cluster\": int(cluster_id)},\n            points=[point.id]\n        )\n\n    return clusters\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#classification","title":"Classification","text":""},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#classification-par-similarite","title":"Classification par similarit\u00e9","text":"<pre><code>def classify_document(query_vector, categories):\n    \"\"\"Classer un document dans une cat\u00e9gorie\"\"\"\n\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    # Pour chaque cat\u00e9gorie, trouver le document le plus similaire\n    category_scores = {}\n\n    for category in categories:\n        results = client.search(\n            collection_name=\"documents\",\n            query_vector=query_vector,\n            query_filter=Filter(\n                must=[\n                    FieldCondition(key=\"category\", match=MatchValue(value=category))\n                ]\n            ),\n            limit=1\n        )\n\n        if results:\n            category_scores[category] = results[0].score\n\n    # Retourner la cat\u00e9gorie avec le meilleur score\n    if category_scores:\n        best_category = max(category_scores, key=category_scores.get)\n        confidence = category_scores[best_category]\n        return best_category, confidence\n\n    return None, 0.0\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#anomaly-detection","title":"Anomaly Detection","text":""},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#detecter-les-anomalies","title":"D\u00e9tecter les anomalies","text":"<pre><code>def detect_anomalies(threshold=0.3):\n    \"\"\"D\u00e9tecter des documents anormaux (peu similaires aux autres)\"\"\"\n\n    client = QdrantClient(host=\"localhost\", port=6333)\n\n    # Parcourir tous les points\n    all_points = []\n    scroll_result = client.scroll(\n        collection_name=\"documents\",\n        limit=1000,\n        with_vectors=True\n    )\n    points, _ = scroll_result\n    all_points.extend(points)\n\n    anomalies = []\n\n    for point in all_points:\n        # Chercher les documents similaires\n        results = client.search(\n            collection_name=\"documents\",\n            query_vector=point.vector,\n            limit=10,\n            score_threshold=threshold\n        )\n\n        # Si peu de documents similaires, c'est une anomalie\n        if len(results) &lt; 3:  # Moins de 3 documents similaires\n            anomalies.append(point.id)\n\n    return anomalies\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#exercices-pratiques","title":"Exercices pratiques","text":""},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#exercice-1-systeme-de-recommandation-complet","title":"Exercice 1 : Syst\u00e8me de recommandation complet","text":"<p>Cr\u00e9er un syst\u00e8me qui recommande des produits bas\u00e9 sur : - Similarit\u00e9 vectorielle - Cat\u00e9gorie pr\u00e9f\u00e9r\u00e9e de l'utilisateur - Budget de l'utilisateur</p> <p>Solution :</p> <pre><code>def recommend_products(user_id, limit=10):\n    # R\u00e9cup\u00e9rer pr\u00e9f\u00e9rences utilisateur\n    user_prefs = get_user_preferences(user_id)\n\n    # Historique utilisateur\n    history = get_user_history(user_id)\n\n    # Profil utilisateur (vecteur moyen)\n    if history:\n        vectors = [get_product_vector(pid) for pid in history]\n        user_vector = np.mean(vectors, axis=0).tolist()\n    else:\n        # Pas d'historique : utiliser cat\u00e9gorie pr\u00e9f\u00e9r\u00e9e\n        category_products = get_category_products(user_prefs[\"category\"])\n        vectors = [p.vector for p in category_products]\n        user_vector = np.mean(vectors, axis=0).tolist()\n\n    # Recherche avec filtres\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=user_vector,\n        query_filter=Filter(\n            must=[\n                FieldCondition(\n                    key=\"price\",\n                    range=Range(\n                        gte=user_prefs[\"min_price\"],\n                        lte=user_prefs[\"max_price\"]\n                    )\n                )\n            ],\n            must_not=[\n                FieldCondition(\n                    key=\"id\",\n                    match=MatchAny(any=history)\n                )\n            ]\n        ),\n        limit=limit\n    )\n\n    return results\n</code></pre>"},{"location":"Qdrant/Data/FR/06-ai-ml-use-cases/#points-cles-a-retenir","title":"\ud83c\udfaf Points cl\u00e9s \u00e0 retenir","text":"<p>\u2705 La recherche s\u00e9mantique trouve par sens, pas par mots-cl\u00e9s \u2705 Les recommandations combinent similarit\u00e9 et pr\u00e9f\u00e9rences \u2705 RAG combine retrieval (Qdrant) et generation (LLM) \u2705 La d\u00e9duplication utilise des seuils de similarit\u00e9 \u00e9lev\u00e9s \u2705 Le clustering groupe des documents similaires  </p> <p>Prochaine \u00e9tape : Bonnes Pratiques</p>"},{"location":"Qdrant/Data/FR/07-best-practices/","title":"7. Bonnes Pratiques","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Optimiser les performances</li> <li>G\u00e9rer la dimension des vecteurs</li> <li>Choisir la bonne distance</li> <li>Structurer les m\u00e9tadonn\u00e9es</li> <li>G\u00e9rer la m\u00e9moire</li> <li>Monitoring et maintenance</li> </ul>"},{"location":"Qdrant/Data/FR/07-best-practices/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Dimension des vecteurs</li> <li>Choix de la distance</li> <li>Configuration HNSW</li> <li>Structure des m\u00e9tadonn\u00e9es</li> <li>Gestion de la m\u00e9moire</li> <li>Performance</li> <li>S\u00e9curit\u00e9</li> <li>Monitoring</li> </ol>"},{"location":"Qdrant/Data/FR/07-best-practices/#dimension-des-vecteurs","title":"Dimension des vecteurs","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#choisir-la-bonne-dimension","title":"Choisir la bonne dimension","text":"<p>La dimension affecte : - Performance : Plus grande = plus lent - Qualit\u00e9 : Plus grande = g\u00e9n\u00e9ralement meilleure - M\u00e9moire : Plus grande = plus de m\u00e9moire</p>"},{"location":"Qdrant/Data/FR/07-best-practices/#recommandations-par-cas-dusage","title":"Recommandations par cas d'usage","text":"Cas d'usage Dimension Mod\u00e8le exemple Textes courts 128-256 MiniLM Textes moyens 384-512 sentence-transformers Textes longs 768 BERT base Textes avanc\u00e9s 1536 OpenAI ada-002 Images 512-768 CLIP, ResNet Multimodal 512-768 CLIP"},{"location":"Qdrant/Data/FR/07-best-practices/#impact-sur-les-performances","title":"Impact sur les performances","text":"<pre><code># Dimension 128 : Rapide, moins pr\u00e9cis\nVectorParams(size=128, distance=Distance.COSINE)\n\n# Dimension 384 : \u00c9quilibr\u00e9 (recommand\u00e9)\nVectorParams(size=384, distance=Distance.COSINE)\n\n# Dimension 1536 : Lent, tr\u00e8s pr\u00e9cis\nVectorParams(size=1536, distance=Distance.COSINE)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#regle-generale","title":"R\u00e8gle g\u00e9n\u00e9rale","text":"<ul> <li>&lt; 1M points : 384-512 dimensions</li> <li>1M - 10M points : 256-384 dimensions</li> <li>&gt; 10M points : 128-256 dimensions</li> </ul>"},{"location":"Qdrant/Data/FR/07-best-practices/#choix-de-la-distance","title":"Choix de la distance","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#distance-cosine-recommande-pour-textes","title":"Distance COSINE (recommand\u00e9 pour textes)","text":"<p>Utilisation : - Embeddings normalis\u00e9s - Textes et recherche s\u00e9mantique - Recommandations</p> <p>Avantages : - Insensible \u00e0 la magnitude - Score entre 0 et 1 (facile \u00e0 interpr\u00e9ter) - Id\u00e9al pour embeddings normalis\u00e9s</p> <pre><code>VectorParams(size=384, distance=Distance.COSINE)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#distance-euclid","title":"Distance EUCLID","text":"<p>Utilisation : - Coordonn\u00e9es g\u00e9ographiques - Features num\u00e9riques brutes - Classification d'images</p> <p>Avantages : - Distance g\u00e9om\u00e9trique intuitive - Bon pour donn\u00e9es num\u00e9riques</p> <pre><code>VectorParams(size=128, distance=Distance.EUCLID)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#distance-dot","title":"Distance DOT","text":"<p>Utilisation : - Vecteurs non normalis\u00e9s - Scores pond\u00e9r\u00e9s - Recommandations avec poids</p> <p>Avantages : - Prend en compte la magnitude - Bon pour scores pond\u00e9r\u00e9s</p> <pre><code>VectorParams(size=128, distance=Distance.DOT)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#tableau-de-decision","title":"Tableau de d\u00e9cision","text":"Type de donn\u00e9es Distance recommand\u00e9e Textes (embeddings normalis\u00e9s) COSINE Textes (embeddings non normalis\u00e9s) DOT Coordonn\u00e9es GPS EUCLID Features num\u00e9riques EUCLID Images COSINE ou EUCLID Recommandations COSINE"},{"location":"Qdrant/Data/FR/07-best-practices/#configuration-hnsw","title":"Configuration HNSW","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#parametres-hnsw","title":"Param\u00e8tres HNSW","text":"<pre><code>from qdrant_client.models import HnswConfigDiff\n\nhnsw_config = HnswConfigDiff(\n    m=16,  # Nombre de connexions (d\u00e9faut: 16)\n    ef_construct=100,  # Pr\u00e9cision de construction (d\u00e9faut: 100)\n    full_scan_threshold=10000  # Seuil pour scan complet\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#parametre-m-connexions","title":"Param\u00e8tre m (connexions)","text":"<ul> <li>m=8-16 : Rapide, moins pr\u00e9cis (petites collections)</li> <li>m=16-32 : \u00c9quilibr\u00e9 (recommand\u00e9)</li> <li>m=32-64 : Lent, tr\u00e8s pr\u00e9cis (grandes collections)</li> </ul> <pre><code># Collection rapide (petite)\nHnswConfigDiff(m=8, ef_construct=50)\n\n# Collection \u00e9quilibr\u00e9e (moyenne)\nHnswConfigDiff(m=16, ef_construct=100)\n\n# Collection pr\u00e9cise (grande)\nHnswConfigDiff(m=32, ef_construct=200)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#parametre-ef_construct","title":"Param\u00e8tre ef_construct","text":"<ul> <li>ef_construct=50-100 : Construction rapide</li> <li>ef_construct=100-200 : \u00c9quilibr\u00e9</li> <li>ef_construct=200+ : Construction lente, meilleure qualit\u00e9</li> </ul>"},{"location":"Qdrant/Data/FR/07-best-practices/#parametre-ef-recherche","title":"Param\u00e8tre ef (recherche)","text":"<pre><code># Recherche rapide (moins pr\u00e9cis)\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=vector,\n    limit=10,\n    ef=32  # Plus petit = plus rapide\n)\n\n# Recherche pr\u00e9cise (plus lent)\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=vector,\n    limit=10,\n    ef=128  # Plus grand = plus pr\u00e9cis\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#structure-des-metadonnees","title":"Structure des m\u00e9tadonn\u00e9es","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#bonnes-pratiques-pour-le-payload","title":"Bonnes pratiques pour le payload","text":"<pre><code># \u2705 Bon : Structure claire et typ\u00e9e\npayload = {\n    \"title\": \"Product Name\",  # String\n    \"category\": \"electronics\",  # String (indexable)\n    \"price\": 99.99,  # Float (indexable)\n    \"quantity\": 10,  # Integer\n    \"in_stock\": True,  # Boolean\n    \"tags\": [\"laptop\", \"gaming\"],  # Array\n    \"created_at\": \"2024-01-15T10:00:00Z\",  # ISO date string\n    \"metadata\": {  # Objet imbriqu\u00e9\n        \"brand\": \"BrandX\",\n        \"model\": \"Model123\"\n    }\n}\n\n# \u274c Moins bon : Structure incoh\u00e9rente\npayload = {\n    \"title\": \"Product Name\",\n    \"Category\": \"electronics\",  # Incoh\u00e9rence de casse\n    \"price\": \"99.99\",  # String au lieu de number\n    \"tags\": \"laptop,gaming\",  # String au lieu d'array\n    \"created\": \"15/01/2024\"  # Format date non standard\n}\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#indexer-les-champs-frequemment-filtres","title":"Indexer les champs fr\u00e9quemment filtr\u00e9s","text":"<pre><code># Indexer les champs utilis\u00e9s dans les filtres\nclient.create_payload_index(\n    collection_name=\"products\",\n    field_name=\"category\",\n    field_schema=PayloadSchemaType.KEYWORD\n)\n\nclient.create_payload_index(\n    collection_name=\"products\",\n    field_name=\"price\",\n    field_schema=PayloadSchemaType.FLOAT\n)\n\nclient.create_payload_index(\n    collection_name=\"products\",\n    field_name=\"created_at\",\n    field_schema=PayloadSchemaType.KEYWORD  # Pour dates\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#eviter-les-payloads-trop-volumineux","title":"\u00c9viter les payloads trop volumineux","text":"<pre><code># \u2705 Bon : Payload concis\npayload = {\n    \"id\": 123,\n    \"title\": \"Product\",\n    \"category\": \"electronics\"\n}\n\n# \u274c Moins bon : Payload trop volumineux\npayload = {\n    \"id\": 123,\n    \"title\": \"Product\",\n    \"full_description\": \"...\" * 1000,  # Texte tr\u00e8s long\n    \"high_res_image\": base64_image,  # Image encod\u00e9e\n    \"full_specs\": {...}  # Objet tr\u00e8s volumineux\n}\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#gestion-de-la-memoire","title":"Gestion de la m\u00e9moire","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#quantisation","title":"Quantisation","text":"<p>La quantisation r\u00e9duit la m\u00e9moire utilis\u00e9e :</p> <pre><code>from qdrant_client.models import QuantizationConfig, ScalarQuantization, ScalarType\n\n# Quantisation INT8 (r\u00e9duction 4x)\nquantization_config = QuantizationConfig(\n    scalar=ScalarQuantization(\n        type=ScalarType.INT8,\n        quantile=0.99,\n        always_ram=True\n    )\n)\n\nclient.create_collection(\n    collection_name=\"products\",\n    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n    quantization_config=quantization_config\n)\n</code></pre> <p>Avantages : - R\u00e9duction m\u00e9moire : 4x (float32 \u2192 int8) - Recherche plus rapide - L\u00e9g\u00e8re perte de pr\u00e9cision</p>"},{"location":"Qdrant/Data/FR/07-best-practices/#memmap-memoire-mappee","title":"Memmap (m\u00e9moire mapp\u00e9e)","text":"<p>Pour tr\u00e8s grandes collections :</p> <pre><code>from qdrant_client.models import OptimizersConfigDiff\n\noptimizers_config = OptimizersConfigDiff(\n    memmap_threshold=50000  # Utiliser memmap si &gt; 50000 points\n)\n\nclient.create_collection(\n    collection_name=\"large_collection\",\n    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n    optimizers_config=optimizers_config\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#performance","title":"Performance","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#batch-operations","title":"Batch operations","text":"<pre><code># \u2705 Bon : Insertion par batches\npoints = [PointStruct(...) for _ in range(1000)]\nclient.upsert(collection_name=\"products\", points=points)\n\n# \u274c Moins bon : Insertion point par point\nfor point in points:\n    client.upsert(collection_name=\"products\", points=[point])\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#pre-filtrage-vs-post-filtrage","title":"Pr\u00e9-filtrage vs Post-filtrage","text":"<pre><code># Pr\u00e9-filtrage (recommand\u00e9 si peu de r\u00e9sultats apr\u00e8s filtrage)\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=vector,\n    query_filter=filter,  # Filtre d'abord\n    limit=10\n)\n\n# Post-filtrage (si beaucoup de r\u00e9sultats apr\u00e8s filtrage)\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=vector,\n    limit=100  # Plus de r\u00e9sultats\n)\n# Filtrer manuellement ensuite\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#optimiser-les-embeddings","title":"Optimiser les embeddings","text":"<pre><code># \u2705 Bon : Batch encoding\nembeddings = model.encode(texts, batch_size=32)\n\n# \u274c Moins bon : Un par un\nembeddings = [model.encode([text])[0] for text in texts]\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#utiliser-le-gpu","title":"Utiliser le GPU","text":"<pre><code>import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n\n# Batch plus grand sur GPU\nembeddings = model.encode(texts, batch_size=64)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#securite","title":"S\u00e9curit\u00e9","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#authentification","title":"Authentification","text":"<pre><code># Connexion avec API key\nclient = QdrantClient(\n    url=\"https://your-cluster.qdrant.io\",\n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#permissions","title":"Permissions","text":"<ul> <li>Utiliser des API keys diff\u00e9rentes par environnement</li> <li>Limiter les permissions par collection</li> <li>Ne pas exposer les API keys dans le code</li> </ul>"},{"location":"Qdrant/Data/FR/07-best-practices/#donnees-sensibles","title":"Donn\u00e9es sensibles","text":"<pre><code># \u274c Ne pas stocker de donn\u00e9es sensibles dans le payload\npayload = {\n    \"name\": \"Product\",\n    \"password\": \"secret123\"  # \u274c Ne jamais faire \u00e7a\n}\n\n# \u2705 Stocker seulement les donn\u00e9es n\u00e9cessaires\npayload = {\n    \"name\": \"Product\",\n    \"category\": \"electronics\"\n}\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#monitoring","title":"Monitoring","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#verifier-la-sante","title":"V\u00e9rifier la sant\u00e9","text":"<pre><code># Informations de la collection\ncollection_info = client.get_collection(\"products\")\n\nprint(f\"Points: {collection_info.points_count}\")\nprint(f\"Indexed: {collection_info.indexed_vectors_count}\")\nprint(f\"Status: {collection_info.status}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#statistiques","title":"Statistiques","text":"<pre><code># Statistiques de la collection\nstats = client.get_collection(\"products\")\n\nprint(f\"Vectors count: {stats.vectors_count}\")\nprint(f\"Indexed vectors: {stats.indexed_vectors_count}\")\nprint(f\"Points count: {stats.points_count}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#monitoring-des-performances","title":"Monitoring des performances","text":"<pre><code>import time\n\n# Mesurer le temps de recherche\nstart = time.time()\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=vector,\n    limit=10\n)\nduration = time.time() - start\n\nprint(f\"Search took {duration:.3f}s\")\nprint(f\"Results: {len(results)}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/07-best-practices/#checklist-de-bonnes-pratiques","title":"Checklist de bonnes pratiques","text":""},{"location":"Qdrant/Data/FR/07-best-practices/#configuration","title":"Configuration","text":"<ul> <li>[ ] Dimension appropri\u00e9e pour le cas d'usage</li> <li>[ ] Distance correcte (COSINE pour textes)</li> <li>[ ] HNSW configur\u00e9 selon la taille de la collection</li> <li>[ ] Quantisation activ\u00e9e si n\u00e9cessaire</li> </ul>"},{"location":"Qdrant/Data/FR/07-best-practices/#donnees","title":"Donn\u00e9es","text":"<ul> <li>[ ] Payload structur\u00e9 et coh\u00e9rent</li> <li>[ ] Champs fr\u00e9quemment filtr\u00e9s index\u00e9s</li> <li>[ ] Pas de donn\u00e9es sensibles dans le payload</li> <li>[ ] Payload pas trop volumineux</li> </ul>"},{"location":"Qdrant/Data/FR/07-best-practices/#performance_1","title":"Performance","text":"<ul> <li>[ ] Insertion par batches</li> <li>[ ] Pr\u00e9-filtrage utilis\u00e9 quand appropri\u00e9</li> <li>[ ] Batch encoding pour les embeddings</li> <li>[ ] GPU utilis\u00e9 si disponible</li> </ul>"},{"location":"Qdrant/Data/FR/07-best-practices/#maintenance","title":"Maintenance","text":"<ul> <li>[ ] Monitoring r\u00e9gulier</li> <li>[ ] V\u00e9rification de la sant\u00e9 des collections</li> <li>[ ] Backup r\u00e9gulier</li> <li>[ ] Documentation \u00e0 jour</li> </ul>"},{"location":"Qdrant/Data/FR/07-best-practices/#points-cles-a-retenir","title":"\ud83c\udfaf Points cl\u00e9s \u00e0 retenir","text":"<p>\u2705 Dimension 384-512 pour textes, 128-256 pour grandes collections \u2705 COSINE pour textes, EUCLID pour coordonn\u00e9es \u2705 HNSW m=16-32 pour \u00e9quilibre performance/pr\u00e9cision \u2705 Indexer les champs fr\u00e9quemment filtr\u00e9s \u2705 Utiliser batch operations pour meilleures performances \u2705 Quantisation pour r\u00e9duire la m\u00e9moire \u2705 Monitoring r\u00e9gulier pour d\u00e9tecter les probl\u00e8mes  </p> <p>Prochaine \u00e9tape : Projets Pratiques</p>"},{"location":"Qdrant/Data/FR/08-projets/","title":"8. Projets Pratiques","text":""},{"location":"Qdrant/Data/FR/08-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er des projets complets</li> <li>Appliquer les connaissances acquises</li> <li>Int\u00e9grer avec l'IA</li> <li>Optimiser les performances</li> <li>Construire un portfolio</li> </ul>"},{"location":"Qdrant/Data/FR/08-projets/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Projet 1 : Moteur de recherche s\u00e9mantique</li> <li>Projet 2 : Syst\u00e8me de recommandation</li> <li>Projet 3 : Chatbot avec RAG</li> <li>Projet 4 : D\u00e9duplication de documents</li> <li>Projet 5 : Classification de textes</li> </ol>"},{"location":"Qdrant/Data/FR/08-projets/#projet-1-moteur-de-recherche-semantique","title":"Projet 1 : Moteur de recherche s\u00e9mantique","text":""},{"location":"Qdrant/Data/FR/08-projets/#objectif","title":"Objectif","text":"<p>Cr\u00e9er un moteur de recherche s\u00e9mantique pour des documents, permettant de trouver des r\u00e9sultats par sens plut\u00f4t que par mots-cl\u00e9s.</p>"},{"location":"Qdrant/Data/FR/08-projets/#fonctionnalites","title":"Fonctionnalit\u00e9s","text":"<ul> <li>Indexation de documents</li> <li>Recherche s\u00e9mantique</li> <li>Filtres avanc\u00e9s</li> <li>Interface simple</li> </ul>"},{"location":"Qdrant/Data/FR/08-projets/#implementation-complete","title":"Impl\u00e9mentation compl\u00e8te","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\nimport pandas as pd\n\nclass SemanticSearchEngine:\n    def __init__(self, collection_name=\"documents\"):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n        self.collection_name = collection_name\n        self._ensure_collection()\n\n    def _ensure_collection(self):\n        \"\"\"Cr\u00e9er la collection si n\u00e9cessaire\"\"\"\n        collections = self.client.get_collections()\n        if self.collection_name not in [c.name for c in collections.collections]:\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n            )\n\n    def index_documents(self, documents, batch_size=100):\n        \"\"\"Indexer des documents\"\"\"\n        for i in range(0, len(documents), batch_size):\n            batch = documents[i:i+batch_size]\n\n            texts = [doc[\"text\"] for doc in batch]\n            embeddings = self.model.encode(texts, show_progress_bar=True)\n\n            points = [\n                PointStruct(\n                    id=doc[\"id\"],\n                    vector=emb.tolist(),\n                    payload={\n                        \"text\": doc[\"text\"],\n                        \"title\": doc.get(\"title\", \"\"),\n                        \"category\": doc.get(\"category\", \"\"),\n                        \"author\": doc.get(\"author\", \"\"),\n                        \"date\": doc.get(\"date\", \"\")\n                    }\n                )\n                for doc, emb in zip(batch, embeddings)\n            ]\n\n            self.client.upsert(\n                collection_name=self.collection_name,\n                points=points\n            )\n            print(f\"Indexed {min(i+batch_size, len(documents))}/{len(documents)}\")\n\n    def search(self, query, limit=10, category=None, min_score=0.5):\n        \"\"\"Rechercher des documents\"\"\"\n        query_embedding = self.model.encode([query])[0]\n\n        # Construire le filtre\n        filter_conditions = []\n        if category:\n            filter_conditions.append(\n                FieldCondition(key=\"category\", match=MatchValue(value=category))\n            )\n\n        query_filter = Filter(must=filter_conditions) if filter_conditions else None\n\n        # Recherche\n        results = self.client.search(\n            collection_name=self.collection_name,\n            query_vector=query_embedding.tolist(),\n            query_filter=query_filter,\n            limit=limit,\n            score_threshold=min_score\n        )\n\n        return [\n            {\n                \"id\": r.id,\n                \"score\": r.score,\n                \"title\": r.payload.get(\"title\", \"\"),\n                \"text\": r.payload.get(\"text\", \"\"),\n                \"category\": r.payload.get(\"category\", \"\")\n            }\n            for r in results\n        ]\n\n    def index_from_csv(self, csv_path, text_column, id_column=None):\n        \"\"\"Indexer depuis un CSV\"\"\"\n        df = pd.read_csv(csv_path)\n\n        if id_column is None:\n            df['_id'] = range(len(df))\n            id_column = '_id'\n\n        documents = [\n            {\n                \"id\": int(row[id_column]),\n                \"text\": str(row[text_column]),\n                \"title\": str(row.get(\"title\", \"\")),\n                \"category\": str(row.get(\"category\", \"\")),\n                \"author\": str(row.get(\"author\", \"\"))\n            }\n            for _, row in df.iterrows()\n        ]\n\n        self.index_documents(documents)\n\n# Utilisation\nengine = SemanticSearchEngine()\n\n# Indexer depuis CSV\nengine.index_from_csv(\"documents.csv\", text_column=\"content\", id_column=\"doc_id\")\n\n# Rechercher\nresults = engine.search(\"computer for work\", limit=5, category=\"technology\")\n\nfor result in results:\n    print(f\"Score: {result['score']:.4f}\")\n    print(f\"Title: {result['title']}\")\n    print(f\"Text: {result['text'][:100]}...\")\n    print(\"---\")\n</code></pre>"},{"location":"Qdrant/Data/FR/08-projets/#ameliorations-possibles","title":"Am\u00e9liorations possibles","text":"<ul> <li>Interface web (Flask/FastAPI)</li> <li>Pagination des r\u00e9sultats</li> <li>Highlighting des termes pertinents</li> <li>Historique de recherche</li> <li>Analytics des recherches</li> </ul>"},{"location":"Qdrant/Data/FR/08-projets/#projet-2-systeme-de-recommandation","title":"Projet 2 : Syst\u00e8me de recommandation","text":""},{"location":"Qdrant/Data/FR/08-projets/#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er un syst\u00e8me de recommandation de produits combinant similarit\u00e9 vectorielle et pr\u00e9f\u00e9rences utilisateur.</p>"},{"location":"Qdrant/Data/FR/08-projets/#fonctionnalites_1","title":"Fonctionnalit\u00e9s","text":"<ul> <li>Recommandation par similarit\u00e9</li> <li>Filtres par pr\u00e9f\u00e9rences</li> <li>Historique utilisateur</li> <li>Scoring hybride</li> </ul>"},{"location":"Qdrant/Data/FR/08-projets/#implementation","title":"Impl\u00e9mentation","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance, VectorParams, PointStruct, Filter, \n    FieldCondition, MatchValue, Range, MatchAny\n)\nimport numpy as np\n\nclass RecommendationSystem:\n    def __init__(self):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n        self._ensure_collection()\n\n    def _ensure_collection(self):\n        collections = self.client.get_collections()\n        if \"products\" not in [c.name for c in collections.collections]:\n            self.client.create_collection(\n                collection_name=\"products\",\n                vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n            )\n\n    def index_products(self, products):\n        \"\"\"Indexer des produits\"\"\"\n        texts = [p[\"description\"] for p in products]\n        embeddings = self.model.encode(texts)\n\n        points = [\n            PointStruct(\n                id=p[\"id\"],\n                vector=emb.tolist(),\n                payload={\n                    \"name\": p[\"name\"],\n                    \"description\": p[\"description\"],\n                    \"category\": p[\"category\"],\n                    \"price\": p[\"price\"],\n                    \"brand\": p.get(\"brand\", \"\")\n                }\n            )\n            for p, emb in zip(products, embeddings)\n        ]\n\n        self.client.upsert(collection_name=\"products\", points=points)\n\n    def recommend_similar(self, product_id, limit=10):\n        \"\"\"Recommandation par similarit\u00e9\"\"\"\n        # R\u00e9cup\u00e9rer le produit\n        points = self.client.retrieve(\n            collection_name=\"products\",\n            ids=[product_id],\n            with_vectors=True\n        )\n\n        if not points:\n            return []\n\n        product_vector = points[0].vector\n\n        # Rechercher des produits similaires\n        results = self.client.search(\n            collection_name=\"products\",\n            query_vector=product_vector,\n            query_filter=Filter(\n                must_not=[\n                    FieldCondition(key=\"id\", match=MatchValue(value=product_id))\n                ]\n            ),\n            limit=limit\n        )\n\n        return results\n\n    def recommend_for_user(self, user_id, user_preferences, limit=10):\n        \"\"\"Recommandation personnalis\u00e9e\"\"\"\n        # R\u00e9cup\u00e9rer l'historique utilisateur\n        user_history = self._get_user_history(user_id)\n\n        if user_history:\n            # Profil utilisateur (vecteur moyen)\n            product_ids = [item[\"product_id\"] for item in user_history]\n            points = self.client.retrieve(\n                collection_name=\"products\",\n                ids=product_ids,\n                with_vectors=True\n            )\n\n            vectors = np.array([p.vector for p in points])\n            user_vector = vectors.mean(axis=0).tolist()\n        else:\n            # Pas d'historique : utiliser cat\u00e9gorie pr\u00e9f\u00e9r\u00e9e\n            category_products = self._get_category_products(user_preferences[\"category\"])\n            if category_products:\n                vectors = np.array([p.vector for p in category_products])\n                user_vector = vectors.mean(axis=0).tolist()\n            else:\n                return []\n\n        # Recherche avec filtres\n        filter_conditions = []\n\n        if user_preferences.get(\"min_price\"):\n            filter_conditions.append(\n                FieldCondition(\n                    key=\"price\",\n                    range=Range(gte=user_preferences[\"min_price\"])\n                )\n            )\n\n        if user_preferences.get(\"max_price\"):\n            if filter_conditions:\n                # Mettre \u00e0 jour le range existant\n                for cond in filter_conditions:\n                    if hasattr(cond, 'key') and cond.key == \"price\":\n                        cond.range.lte = user_preferences[\"max_price\"]\n                        break\n            else:\n                filter_conditions.append(\n                    FieldCondition(\n                        key=\"price\",\n                        range=Range(lte=user_preferences[\"max_price\"])\n                    )\n                )\n\n        if user_preferences.get(\"category\"):\n            filter_conditions.append(\n                FieldCondition(\n                    key=\"category\",\n                    match=MatchValue(value=user_preferences[\"category\"])\n                )\n            )\n\n        # Exclure les produits d\u00e9j\u00e0 achet\u00e9s\n        if user_history:\n            purchased_ids = [item[\"product_id\"] for item in user_history]\n            filter_conditions.append(\n                FieldCondition(\n                    key=\"id\",\n                    match=MatchAny(any=purchased_ids)\n                )\n            )\n\n        query_filter = Filter(must=filter_conditions) if filter_conditions else None\n\n        results = self.client.search(\n            collection_name=\"products\",\n            query_vector=user_vector,\n            query_filter=query_filter,\n            limit=limit,\n            score_threshold=0.7\n        )\n\n        return results\n\n    def _get_user_history(self, user_id):\n        # Impl\u00e9mentation : r\u00e9cup\u00e9rer depuis une base de donn\u00e9es\n        # Exemple simplifi\u00e9\n        return []\n\n    def _get_category_products(self, category):\n        # R\u00e9cup\u00e9rer quelques produits de la cat\u00e9gorie pour cr\u00e9er le profil\n        results = self.client.scroll(\n            collection_name=\"products\",\n            scroll_filter=Filter(\n                must=[\n                    FieldCondition(key=\"category\", match=MatchValue(value=category))\n                ]\n            ),\n            limit=10,\n            with_vectors=True\n        )\n        return results[0] if results[0] else []\n\n# Utilisation\nrecommender = RecommendationSystem()\n\n# Recommandation par similarit\u00e9\nsimilar = recommender.recommend_similar(product_id=123, limit=5)\n\n# Recommandation personnalis\u00e9e\nuser_prefs = {\n    \"category\": \"electronics\",\n    \"min_price\": 100,\n    \"max_price\": 500\n}\nrecommendations = recommender.recommend_for_user(\n    user_id=456,\n    user_preferences=user_prefs,\n    limit=10\n)\n</code></pre>"},{"location":"Qdrant/Data/FR/08-projets/#projet-3-chatbot-avec-rag","title":"Projet 3 : Chatbot avec RAG","text":""},{"location":"Qdrant/Data/FR/08-projets/#objectif_2","title":"Objectif","text":"<p>Cr\u00e9er un chatbot qui utilise RAG (Retrieval-Augmented Generation) pour r\u00e9pondre aux questions en utilisant une base de connaissances.</p>"},{"location":"Qdrant/Data/FR/08-projets/#implementation_1","title":"Impl\u00e9mentation","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\nfrom openai import OpenAI  # Ou autre LLM\n\nclass RAGChatbot:\n    def __init__(self):\n        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n        self.llm = OpenAI()  # Configuration n\u00e9cessaire\n        self._ensure_collection()\n\n    def _ensure_collection(self):\n        collections = self.client.get_collections()\n        if \"knowledge_base\" not in [c.name for c in collections.collections]:\n            self.client.create_collection(\n                collection_name=\"knowledge_base\",\n                vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n            )\n\n    def index_knowledge(self, documents):\n        \"\"\"Indexer la base de connaissances\"\"\"\n        texts = [doc[\"text\"] for doc in documents]\n        embeddings = self.embedding_model.encode(texts)\n\n        points = [\n            PointStruct(\n                id=doc[\"id\"],\n                vector=emb.tolist(),\n                payload={\n                    \"text\": doc[\"text\"],\n                    \"title\": doc.get(\"title\", \"\"),\n                    \"source\": doc.get(\"source\", \"\"),\n                    \"section\": doc.get(\"section\", \"\")\n                }\n            )\n            for doc, emb in zip(documents, embeddings)\n        ]\n\n        self.client.upsert(collection_name=\"knowledge_base\", points=points)\n\n    def retrieve_context(self, query, limit=5):\n        \"\"\"R\u00e9cup\u00e9rer le contexte pertinent\"\"\"\n        query_embedding = self.embedding_model.encode([query])[0]\n\n        results = self.client.search(\n            collection_name=\"knowledge_base\",\n            query_vector=query_embedding.tolist(),\n            limit=limit,\n            score_threshold=0.7\n        )\n\n        context_parts = []\n        sources = []\n\n        for i, result in enumerate(results):\n            context_parts.append(f\"[Source {i+1}]: {result.payload['text']}\")\n            sources.append({\n                \"id\": result.id,\n                \"title\": result.payload.get(\"title\", \"\"),\n                \"source\": result.payload.get(\"source\", \"\"),\n                \"score\": result.score\n            })\n\n        context = \"\\n\\n\".join(context_parts)\n        return context, sources\n\n    def generate_answer(self, query, context):\n        \"\"\"G\u00e9n\u00e9rer une r\u00e9ponse avec le contexte\"\"\"\n        prompt = f\"\"\"Tu es un assistant utile. R\u00e9ponds \u00e0 la question en utilisant uniquement le contexte fourni.\n\nContexte:\n{context}\n\nQuestion: {query}\n\nR\u00e9ponds de mani\u00e8re claire et concise. Si la r\u00e9ponse n'est pas dans le contexte, dis-le.\"\"\"\n\n        response = self.llm.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Tu es un assistant utile et pr\u00e9cis.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.7,\n            max_tokens=500\n        )\n\n        return response.choices[0].message.content\n\n    def answer(self, query, limit=5):\n        \"\"\"Pipeline RAG complet\"\"\"\n        # 1. Retrieval\n        context, sources = self.retrieve_context(query, limit)\n\n        if not context:\n            return {\n                \"answer\": \"D\u00e9sol\u00e9, je n'ai pas trouv\u00e9 d'information pertinente dans la base de connaissances.\",\n                \"sources\": []\n            }\n\n        # 2. Generation\n        answer = self.generate_answer(query, context)\n\n        return {\n            \"answer\": answer,\n            \"sources\": sources\n        }\n\n# Utilisation\nchatbot = RAGChatbot()\n\n# Indexer la base de connaissances\nknowledge = [\n    {\"id\": 1, \"text\": \"Les laptops gaming sont optimis\u00e9s pour les jeux vid\u00e9o...\", \"title\": \"Laptops Gaming\"},\n    {\"id\": 2, \"text\": \"Les workstations professionnelles sont con\u00e7ues pour...\", \"title\": \"Workstations\"}\n]\nchatbot.index_knowledge(knowledge)\n\n# Poser une question\nresult = chatbot.answer(\"Quels sont les avantages des laptops gaming?\")\nprint(f\"R\u00e9ponse: {result['answer']}\")\nprint(f\"Sources: {result['sources']}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/08-projets/#projet-4-deduplication-de-documents","title":"Projet 4 : D\u00e9duplication de documents","text":""},{"location":"Qdrant/Data/FR/08-projets/#objectif_3","title":"Objectif","text":"<p>D\u00e9tecter et supprimer les documents dupliqu\u00e9s dans une collection.</p>"},{"location":"Qdrant/Data/FR/08-projets/#implementation_2","title":"Impl\u00e9mentation","text":"<pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.models import Filter, FieldCondition, MatchValue\n\nclass DeduplicationSystem:\n    def __init__(self, collection_name=\"documents\"):\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n        self.collection_name = collection_name\n\n    def find_duplicates(self, threshold=0.95):\n        \"\"\"Trouver les documents dupliqu\u00e9s\"\"\"\n        # R\u00e9cup\u00e9rer tous les points\n        all_points = []\n        scroll_result = self.client.scroll(\n            collection_name=self.collection_name,\n            limit=1000,\n            with_vectors=True,\n            with_payload=True\n        )\n        points, next_offset = scroll_result\n        all_points.extend(points)\n\n        while next_offset is not None:\n            scroll_result = self.client.scroll(\n                collection_name=self.collection_name,\n                limit=1000,\n                offset=next_offset,\n                with_vectors=True,\n                with_payload=True\n            )\n            points, next_offset = scroll_result\n            all_points.extend(points)\n\n        # Trouver les doublons\n        duplicates = []\n        processed = set()\n\n        for point in all_points:\n            if point.id in processed:\n                continue\n\n            # Chercher des documents similaires\n            similar = self.client.search(\n                collection_name=self.collection_name,\n                query_vector=point.vector,\n                limit=10,\n                score_threshold=threshold\n            )\n\n            # Filtrer l'\u00e9l\u00e9ment lui-m\u00eame\n            similar = [s for s in similar if s.id != point.id]\n\n            if similar:\n                duplicates.append({\n                    \"original\": point.id,\n                    \"duplicates\": [s.id for s in similar],\n                    \"scores\": [s.score for s in similar]\n                })\n                processed.add(point.id)\n                processed.update([s.id for s in similar])\n\n        return duplicates\n\n    def remove_duplicates(self, threshold=0.95, keep=\"first\"):\n        \"\"\"Supprimer les doublons\"\"\"\n        duplicates = self.find_duplicates(threshold)\n\n        ids_to_delete = []\n\n        for group in duplicates:\n            if keep == \"first\":\n                # Garder le premier (original), supprimer les autres\n                ids_to_delete.extend(group[\"duplicates\"])\n            elif keep == \"best\":\n                # Garder celui avec le meilleur score, supprimer les autres\n                # Impl\u00e9mentation simplifi\u00e9e\n                ids_to_delete.extend(group[\"duplicates\"])\n\n        # Supprimer les points\n        if ids_to_delete:\n            self.client.delete(\n                collection_name=self.collection_name,\n                points_selector=ids_to_delete\n            )\n\n        return len(ids_to_delete)\n\n# Utilisation\ndedup = DeduplicationSystem()\nduplicates = dedup.find_duplicates(threshold=0.95)\nprint(f\"Found {len(duplicates)} duplicate groups\")\n\n# Supprimer les doublons\nremoved = dedup.remove_duplicates(threshold=0.95, keep=\"first\")\nprint(f\"Removed {removed} duplicate documents\")\n</code></pre>"},{"location":"Qdrant/Data/FR/08-projets/#projet-5-classification-de-textes","title":"Projet 5 : Classification de textes","text":""},{"location":"Qdrant/Data/FR/08-projets/#objectif_4","title":"Objectif","text":"<p>Classer automatiquement des textes dans des cat\u00e9gories pr\u00e9d\u00e9finies.</p>"},{"location":"Qdrant/Data/FR/08-projets/#implementation_3","title":"Impl\u00e9mentation","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Filter, FieldCondition, MatchValue\n\nclass TextClassifier:\n    def __init__(self):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n\n    def train_classifier(self, training_data):\n        \"\"\"Entra\u00eener le classifieur avec des exemples\"\"\"\n        # training_data = [{\"text\": \"...\", \"category\": \"electronics\"}, ...]\n\n        # Indexer les exemples d'entra\u00eenement\n        texts = [item[\"text\"] for item in training_data]\n        embeddings = self.model.encode(texts)\n\n        points = [\n            PointStruct(\n                id=i,\n                vector=emb.tolist(),\n                payload={\n                    \"text\": item[\"text\"],\n                    \"category\": item[\"category\"]\n                }\n            )\n            for i, (item, emb) in enumerate(zip(training_data, embeddings))\n        ]\n\n        self.client.upsert(collection_name=\"training_data\", points=points)\n\n    def classify(self, text, categories):\n        \"\"\"Classer un texte\"\"\"\n        query_embedding = self.model.encode([text])[0]\n\n        category_scores = {}\n\n        for category in categories:\n            results = self.client.search(\n                collection_name=\"training_data\",\n                query_vector=query_embedding.tolist(),\n                query_filter=Filter(\n                    must=[\n                        FieldCondition(key=\"category\", match=MatchValue(value=category))\n                    ]\n                ),\n                limit=1\n            )\n\n            if results:\n                category_scores[category] = results[0].score\n\n        if category_scores:\n            best_category = max(category_scores, key=category_scores.get)\n            confidence = category_scores[best_category]\n            return best_category, confidence, category_scores\n\n        return None, 0.0, {}\n\n# Utilisation\nclassifier = TextClassifier()\n\n# Entra\u00eener\ntraining = [\n    {\"text\": \"Laptop computer for work\", \"category\": \"electronics\"},\n    {\"text\": \"Gaming laptop with graphics\", \"category\": \"electronics\"},\n    {\"text\": \"Novel about adventure\", \"category\": \"books\"},\n    {\"text\": \"Science fiction book\", \"category\": \"books\"}\n]\nclassifier.train_classifier(training)\n\n# Classer\ncategory, confidence, all_scores = classifier.classify(\n    \"Professional workstation computer\",\n    categories=[\"electronics\", \"books\", \"clothing\"]\n)\n\nprint(f\"Category: {category}\")\nprint(f\"Confidence: {confidence:.4f}\")\nprint(f\"All scores: {all_scores}\")\n</code></pre>"},{"location":"Qdrant/Data/FR/08-projets/#conseils-pour-votre-portfolio","title":"Conseils pour votre portfolio","text":""},{"location":"Qdrant/Data/FR/08-projets/#documentation","title":"Documentation","text":"<ol> <li>README.md : Expliquer le projet, l'installation, l'utilisation</li> <li>Architecture : Diagramme de l'architecture</li> <li>Exemples : Exemples d'utilisation</li> <li>R\u00e9sultats : M\u00e9triques et performances</li> </ol>"},{"location":"Qdrant/Data/FR/08-projets/#github","title":"GitHub","text":"<ol> <li>Code propre : Commentaires, docstrings</li> <li>Structure claire : Organisation des fichiers</li> <li>Requirements.txt : D\u00e9pendances list\u00e9es</li> <li>Exemples : Scripts d'exemple fonctionnels</li> </ol>"},{"location":"Qdrant/Data/FR/08-projets/#demo","title":"D\u00e9mo","text":"<ol> <li>Interface : Web app simple (Flask/FastAPI)</li> <li>Screenshots : Captures d'\u00e9cran</li> <li>Vid\u00e9o : D\u00e9mo vid\u00e9o courte</li> <li>Live demo : D\u00e9mo en ligne si possible</li> </ol>"},{"location":"Qdrant/Data/FR/08-projets/#points-cles-a-retenir","title":"\ud83c\udfaf Points cl\u00e9s \u00e0 retenir","text":"<p>\u2705 Les projets pratiques d\u00e9montrent vos comp\u00e9tences \u2705 Documenter vos projets pour votre portfolio \u2705 Optimiser les performances pour les cas r\u00e9els \u2705 G\u00e9rer les erreurs et cas limites \u2705 Tester avec des donn\u00e9es r\u00e9elles  </p> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation Qdrant pour Data Analyst ! \ud83c\udf89</p> <p>Vous ma\u00eetrisez maintenant : - Les bases de donn\u00e9es vectorielles - Qdrant et ses fonctionnalit\u00e9s - L'int\u00e9gration avec Python et l'IA - Les cas d'usage pratiques - Les bonnes pratiques</p> <p>Continuez \u00e0 pratiquer et \u00e0 construire des projets pour renforcer vos comp\u00e9tences !</p>"},{"location":"Qdrant/Data/PL/","title":"Szkolenie Qdrant dla Data Analyst","text":""},{"location":"Qdrant/Data/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>To szkolenie poprowadzi Ci\u0119 przez nauk\u0119 Qdrant jako Data Analyst. Qdrant to baza danych wektorowa zoptymalizowana pod k\u0105tem wyszukiwania podobie\u0144stwa, idealna do AI, uczenia maszynowego i wyszukiwania semantycznego.</p>"},{"location":"Qdrant/Data/PL/#cele-szkoleniowe","title":"\ud83c\udfaf Cele szkoleniowe","text":"<ul> <li>Zrozumie\u0107 bazy danych wektorowe</li> <li>Zainstalowa\u0107 i skonfigurowa\u0107 Qdrant</li> <li>Tworzy\u0107 i zarz\u0105dza\u0107 kolekcjami</li> <li>Indeksowa\u0107 i wyszukiwa\u0107 wektory</li> <li>U\u017cywa\u0107 wyszukiwania podobie\u0144stwa</li> <li>Integrowa\u0107 z Python i AI</li> <li>Tworzy\u0107 praktyczne projekty</li> </ul>"},{"location":"Qdrant/Data/PL/#wszystko-jest-darmowe","title":"\ud83d\udcb0 Wszystko jest darmowe!","text":"<p>To szkolenie wykorzystuje tylko: - \u2705 Qdrant Community Edition : Darmowe i open-source - \u2705 Klient Python Qdrant : Darmowa biblioteka - \u2705 Oficjalna dokumentacja : Kompletne darmowe przewodniki</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"Qdrant/Data/PL/#struktura-szkolenia","title":"\ud83d\udcd6 Struktura szkolenia","text":""},{"location":"Qdrant/Data/PL/#1-rozpoczecie-pracy-z-qdrant","title":"1. Rozpocz\u0119cie pracy z Qdrant","text":""},{"location":"Qdrant/Data/PL/#2-kolekcje-i-wektory","title":"2. Kolekcje i wektory","text":""},{"location":"Qdrant/Data/PL/#3-wyszukiwanie-podobienstwa","title":"3. Wyszukiwanie podobie\u0144stwa","text":""},{"location":"Qdrant/Data/PL/#4-filtry-i-metadane","title":"4. Filtry i metadane","text":""},{"location":"Qdrant/Data/PL/#5-integracja-python","title":"5. Integracja Python","text":""},{"location":"Qdrant/Data/PL/#6-przypadki-uzycia-aiml","title":"6. Przypadki u\u017cycia AI/ML","text":""},{"location":"Qdrant/Data/PL/#7-najlepsze-praktyki","title":"7. Najlepsze praktyki","text":""},{"location":"Qdrant/Data/PL/#8-projekty-praktyczne","title":"8. Projekty praktyczne","text":""},{"location":"Qdrant/Data/PL/#wymagania-wstepne","title":"\ud83d\ude80 Wymagania wst\u0119pne","text":"<ul> <li>Podstawy Python</li> <li>Poj\u0119cia AI/ML (opcjonalnie)</li> <li>Poj\u0119cia embeddings (opcjonalnie)</li> </ul>"},{"location":"Qdrant/Data/PL/#szacowany-czas-trwania","title":"\u23f1\ufe0f Szacowany czas trwania","text":"<ul> <li>Ca\u0142kowity : 30-40 godzin</li> <li>Na modu\u0142 : 4-5 godzin</li> </ul> <p>Powodzenia w nauce! \ud83d\ude80</p>"},{"location":"Qdrant/Data/PL/01-getting-started/","title":"1. Rozpocz\u0119cie pracy z Qdrant","text":""},{"location":"Qdrant/Data/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 bazy danych wektorowych</li> <li>Zainstalowa\u0107 Qdrant</li> <li>Zrozumie\u0107 podstawowe koncepcje</li> <li>Pierwsze operacje</li> </ul>"},{"location":"Qdrant/Data/PL/01-getting-started/#wprowadzenie-do-qdrant","title":"Wprowadzenie do Qdrant","text":"<p>Qdrant = Baza danych wektorowa</p> <ul> <li>Wektory : Reprezentacje numeryczne (embeddings)</li> <li>Podobie\u0144stwo : Wyszukiwanie podobie\u0144stwa</li> <li>AI/ML : Zoptymalizowana do sztucznej inteligencji</li> <li>Open-source : Darmowa i open-source</li> </ul>"},{"location":"Qdrant/Data/PL/01-getting-started/#instalacja","title":"Instalacja","text":""},{"location":"Qdrant/Data/PL/01-getting-started/#docker-zalecane","title":"Docker (zalecane)","text":"<pre><code>docker run -p 6333:6333 qdrant/qdrant\n</code></pre>"},{"location":"Qdrant/Data/PL/01-getting-started/#klient-python","title":"Klient Python","text":"<pre><code>pip install qdrant-client\n</code></pre>"},{"location":"Qdrant/Data/PL/01-getting-started/#pierwszy-przykad","title":"Pierwszy przyk\u0142ad","text":"<pre><code>from qdrant_client import QdrantClient\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\nclient.create_collection(\n    collection_name=\"test_collection\",\n    vectors_config={\n        \"size\": 128,\n        \"distance\": \"Cosine\"\n    }\n)\n</code></pre> <p>Nast\u0119pny krok : Kolekcje i wektory</p>"},{"location":"Qdrant/Data/PL/02-collections-vectors/","title":"2. Kolekcje i wektory","text":""},{"location":"Qdrant/Data/PL/02-collections-vectors/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 i zarz\u0105dza\u0107 kolekcjami</li> <li>Wstawia\u0107 wektory</li> <li>Zrozumie\u0107 odleg\u0142o\u015bci</li> <li>Zarz\u0105dza\u0107 metadanymi</li> </ul>"},{"location":"Qdrant/Data/PL/02-collections-vectors/#utworzenie-kolekcji","title":"Utworzenie kolekcji","text":"<pre><code>from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\nclient.create_collection(\n    collection_name=\"products\",\n    vectors_config=VectorParams(\n        size=128,\n        distance=Distance.COSINE\n    )\n)\n</code></pre>"},{"location":"Qdrant/Data/PL/02-collections-vectors/#wstawienie-wektorow","title":"Wstawienie wektor\u00f3w","text":"<pre><code>from qdrant_client.models import PointStruct\n\npoints = [\n    PointStruct(\n        id=1,\n        vector=[0.1, 0.2, 0.3, ...],\n        payload={\"name\": \"Product A\", \"category\": \"electronics\"}\n    )\n]\n\nclient.upsert(collection_name=\"products\", points=points)\n</code></pre> <p>Nast\u0119pny krok : Wyszukiwanie podobie\u0144stwa</p>"},{"location":"Qdrant/Data/PL/03-similarity-search/","title":"3. Wyszukiwanie podobie\u0144stwa","text":""},{"location":"Qdrant/Data/PL/03-similarity-search/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Wykonywa\u0107 wyszukiwania podobie\u0144stwa</li> <li>U\u017cywa\u0107 r\u00f3\u017cnych algorytm\u00f3w</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> <li>Zrozumie\u0107 wyniki</li> </ul>"},{"location":"Qdrant/Data/PL/03-similarity-search/#proste-wyszukiwanie","title":"Proste wyszukiwanie","text":"<pre><code>results = client.search(\n    collection_name=\"products\",\n    query_vector=[0.1, 0.2, 0.3, ...],\n    limit=10\n)\n\nfor result in results:\n    print(f\"ID: {result.id}, Wynik: {result.score}\")\n</code></pre>"},{"location":"Qdrant/Data/PL/03-similarity-search/#wyszukiwanie-z-filtrami","title":"Wyszukiwanie z filtrami","text":"<pre><code>from qdrant_client.models import Filter, FieldCondition, MatchValue\n\nresults = client.search(\n    collection_name=\"products\",\n    query_vector=[0.1, 0.2, 0.3, ...],\n    query_filter=Filter(\n        must=[\n            FieldCondition(\n                key=\"category\",\n                match=MatchValue(value=\"electronics\")\n            )\n        ]\n    ),\n    limit=10\n)\n</code></pre> <p>Nast\u0119pny krok : Filtry i metadane</p>"},{"location":"Qdrant/Data/PL/04-filters-metadata/","title":"4. Filtry i metadane","text":""},{"location":"Qdrant/Data/PL/04-filters-metadata/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>U\u017cywa\u0107 zaawansowanych filtr\u00f3w</li> <li>Zarz\u0105dza\u0107 metadanymi (payload)</li> <li>\u0141\u0105czy\u0107 wiele warunk\u00f3w</li> <li>Optymalizowa\u0107 zapytania z filtrami</li> </ul>"},{"location":"Qdrant/Data/PL/04-filters-metadata/#proste-filtry","title":"Proste filtry","text":"<pre><code>from qdrant_client.models import Filter, FieldCondition, MatchValue\n\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"category\",\n            match=MatchValue(value=\"electronics\")\n        )\n    ]\n)\n</code></pre>"},{"location":"Qdrant/Data/PL/04-filters-metadata/#filtry-zakresu","title":"Filtry zakresu","text":"<pre><code>from qdrant_client.models import Range\n\nfilter = Filter(\n    must=[\n        FieldCondition(\n            key=\"price\",\n            range=Range(gte=100, lte=500)\n        )\n    ]\n)\n</code></pre> <p>Nast\u0119pny krok : Integracja Python</p>"},{"location":"Qdrant/Data/PL/05-python-integration/","title":"5. Integracja Python","text":""},{"location":"Qdrant/Data/PL/05-python-integration/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Integrowa\u0107 z modelami embeddings</li> <li>U\u017cywa\u0107 z frameworkami AI</li> <li>Tworzy\u0107 pipeline danych</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> </ul>"},{"location":"Qdrant/Data/PL/05-python-integration/#z-sentence-transformers","title":"Z sentence-transformers","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom qdrant_client import QdrantClient\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ntexts = [\"Opis produktu 1\", \"Opis produktu 2\"]\nembeddings = model.encode(texts)\n\nclient = QdrantClient(host=\"localhost\", port=6333)\nclient.upsert(\n    collection_name=\"products\",\n    points=[\n        PointStruct(id=i, vector=emb.tolist(), payload={\"text\": text})\n        for i, (emb, text) in enumerate(zip(embeddings, texts))\n    ]\n)\n</code></pre> <p>Nast\u0119pny krok : Przypadki u\u017cycia AI/ML</p>"},{"location":"Qdrant/Data/PL/06-ai-ml-use-cases/","title":"6. Przypadki u\u017cycia AI/ML","text":""},{"location":"Qdrant/Data/PL/06-ai-ml-use-cases/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 typowe przypadki u\u017cycia</li> <li>Implementowa\u0107 wyszukiwanie semantyczne</li> <li>Tworzy\u0107 systemy rekomendacji</li> <li>U\u017cywa\u0107 z RAG</li> </ul>"},{"location":"Qdrant/Data/PL/06-ai-ml-use-cases/#wyszukiwanie-semantyczne","title":"Wyszukiwanie semantyczne","text":"<pre><code>def semantic_search(query_text, limit=10):\n    query_embedding = model.encode([query_text])[0]\n    results = client.search(\n        collection_name=\"documents\",\n        query_vector=query_embedding.tolist(),\n        limit=limit\n    )\n    return results\n</code></pre>"},{"location":"Qdrant/Data/PL/06-ai-ml-use-cases/#system-rekomendacji","title":"System rekomendacji","text":"<pre><code>def recommend_similar_products(product_id, limit=5):\n    product = client.retrieve(\n        collection_name=\"products\",\n        ids=[product_id]\n    )[0]\n\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=product.vector,\n        limit=limit\n    )\n    return results\n</code></pre> <p>Nast\u0119pny krok : Najlepsze praktyki</p>"},{"location":"Qdrant/Data/PL/07-best-practices/","title":"7. Najlepsze praktyki","text":""},{"location":"Qdrant/Data/PL/07-best-practices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> <li>Zarz\u0105dza\u0107 wymiarami wektor\u00f3w</li> <li>Wybiera\u0107 odpowiedni\u0105 odleg\u0142o\u015b\u0107</li> <li>Strukturyzowa\u0107 metadane</li> </ul>"},{"location":"Qdrant/Data/PL/07-best-practices/#wymiary-wektorow","title":"Wymiary wektor\u00f3w","text":"<ul> <li>128-256 : Do ma\u0142ych zbior\u00f3w danych</li> <li>384-512 : Do embeddings tekstu (sentence-transformers)</li> <li>768-1536 : Do zaawansowanych modeli (BERT, etc.)</li> </ul>"},{"location":"Qdrant/Data/PL/07-best-practices/#odlegosc","title":"Odleg\u0142o\u015b\u0107","text":"<ul> <li>COSINE : Zalecana do tekst\u00f3w, embeddings znormalizowane</li> <li>EUCLID : Do danych numerycznych</li> <li>DOT : Do wektor\u00f3w nieznormalizowanych</li> </ul> <p>Nast\u0119pny krok : Projekty praktyczne</p>"},{"location":"Qdrant/Data/PL/08-projets/","title":"8. Projekty praktyczne","text":""},{"location":"Qdrant/Data/PL/08-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 kompletne projekty</li> <li>Zastosowa\u0107 wiedz\u0119</li> <li>Integrowa\u0107 z AI</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> </ul>"},{"location":"Qdrant/Data/PL/08-projets/#projekt-1-silnik-wyszukiwania-semantycznego","title":"Projekt 1 : Silnik wyszukiwania semantycznego","text":"<pre><code>class SemanticSearchEngine:\n    def __init__(self):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.client = QdrantClient(host=\"localhost\", port=6333)\n\n    def search(self, query, limit=10):\n        query_embedding = self.model.encode([query])[0]\n        results = self.client.search(\n            collection_name=\"documents\",\n            query_vector=query_embedding.tolist(),\n            limit=limit\n        )\n        return results\n</code></pre>"},{"location":"Qdrant/Data/PL/08-projets/#projekt-2-system-rekomendacji","title":"Projekt 2 : System rekomendacji","text":"<pre><code>def recommend_products(user_id, limit=10):\n    user_history = get_user_history(user_id)\n    user_embedding = model.encode(user_history).mean(axis=0)\n    results = client.search(\n        collection_name=\"products\",\n        query_vector=user_embedding.tolist(),\n        limit=limit\n    )\n    return results\n</code></pre> <p>Gratulacje! Uko\u0144czy\u0142e\u015b szkolenie Qdrant dla Data Analyst! \ud83c\udf89</p>"},{"location":"Qdrant/Dev/EN/","title":"Qdrant Training for Java Developer","text":""},{"location":"Qdrant/Dev/EN/#overview","title":"\ud83d\udcda Overview","text":"<p>This training guides you through integrating Qdrant into your Java applications. You will learn to use Qdrant with Java, the Java client, queries, optimization and best practices.</p>"},{"location":"Qdrant/Dev/EN/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Integrate Qdrant into Java applications</li> <li>Use Qdrant Java client</li> <li>Manage collections and vectors</li> <li>Perform similarity searches</li> <li>Optimize performance</li> <li>Create complete applications</li> </ul>"},{"location":"Qdrant/Dev/EN/#everything-is-free","title":"\ud83d\udcb0 Everything is Free!","text":"<p>This training uses only: - \u2705 Qdrant Community Edition : Free - \u2705 Qdrant Java Client : Open-source - \u2705 Java : OpenJDK free</p> <p>Total budget: $0</p>"},{"location":"Qdrant/Dev/EN/#training-structure","title":"\ud83d\udcd6 Training Structure","text":""},{"location":"Qdrant/Dev/EN/#1-getting-started-with-qdrant-and-java","title":"1. Getting Started with Qdrant and Java","text":""},{"location":"Qdrant/Dev/EN/#2-java-client-and-connection","title":"2. Java Client and Connection","text":""},{"location":"Qdrant/Dev/EN/#3-collections-and-vectors","title":"3. Collections and Vectors","text":""},{"location":"Qdrant/Dev/EN/#4-similarity-search","title":"4. Similarity Search","text":""},{"location":"Qdrant/Dev/EN/#5-filters-and-metadata","title":"5. Filters and Metadata","text":""},{"location":"Qdrant/Dev/EN/#6-performance-and-optimization","title":"6. Performance and Optimization","text":""},{"location":"Qdrant/Dev/EN/#7-best-practices","title":"7. Best Practices","text":""},{"location":"Qdrant/Dev/EN/#8-practical-projects","title":"8. Practical Projects","text":""},{"location":"Qdrant/Dev/EN/#prerequisites","title":"\ud83d\ude80 Prerequisites","text":"<ul> <li>Java knowledge (basics)</li> <li>Database concepts</li> <li>Maven or Gradle</li> </ul>"},{"location":"Qdrant/Dev/EN/#estimated-duration","title":"\u23f1\ufe0f Estimated Duration","text":"<ul> <li>Total : 30-40 hours</li> <li>Per module : 4-5 hours</li> </ul> <p>Happy learning! \ud83d\ude80</p>"},{"location":"Qdrant/Dev/EN/01-getting-started/","title":"1. Getting Started with Qdrant and Java","text":""},{"location":"Qdrant/Dev/EN/01-getting-started/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Understand Qdrant and Java</li> <li>Set up environment</li> <li>Install Java client</li> <li>Create first project</li> </ul>"},{"location":"Qdrant/Dev/EN/01-getting-started/#maven-configuration","title":"Maven Configuration","text":"<pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;io.qdrant&lt;/groupId&gt;\n        &lt;artifactId&gt;qdrant-java-client&lt;/artifactId&gt;\n        &lt;version&gt;1.7.0&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"Qdrant/Dev/EN/01-getting-started/#first-example","title":"First Example","text":"<pre><code>import io.qdrant.client.QdrantClient;\n\nQdrantClient client = new QdrantClient(\n    QdrantClient.newBuilder(\"localhost\", 6334, false).build()\n);\n\nSystem.out.println(\"Connected to Qdrant!\");\nclient.close();\n</code></pre> <p>Next step : Java Client and Connection</p>"},{"location":"Qdrant/Dev/EN/02-java-client/","title":"2. Java Client and Connection","text":""},{"location":"Qdrant/Dev/EN/02-java-client/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Configure connection</li> <li>Manage client</li> <li>Understand connection options</li> </ul>"},{"location":"Qdrant/Dev/EN/02-java-client/#connection","title":"Connection","text":"<pre><code>QdrantClient client = new QdrantClient(\n    QdrantClient.newBuilder(\"localhost\", 6334, false).build()\n);\n\n// With authentication\nQdrantClient client = new QdrantClient(\n    QdrantClient.newBuilder(\"localhost\", 6334, false)\n        .withApiKey(\"your-api-key\")\n        .build()\n);\n</code></pre> <p>Next step : Collections and Vectors</p>"},{"location":"Qdrant/Dev/EN/03-collections-vectors/","title":"3. Collections and Vectors","text":""},{"location":"Qdrant/Dev/EN/03-collections-vectors/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create collections</li> <li>Insert vectors</li> <li>Manage collections</li> </ul>"},{"location":"Qdrant/Dev/EN/03-collections-vectors/#create-collection","title":"Create Collection","text":"<pre><code>CreateCollection createCollection = CreateCollection.newBuilder()\n    .setCollectionName(\"products\")\n    .setVectorsConfig(VectorsConfig.newBuilder()\n        .setParams(VectorParams.newBuilder()\n            .setSize(128)\n            .setDistance(Distance.Cosine)\n            .build())\n        .build())\n    .build();\n\nclient.createCollection(createCollection);\n</code></pre>"},{"location":"Qdrant/Dev/EN/03-collections-vectors/#insert-vectors","title":"Insert Vectors","text":"<pre><code>List&lt;PointStruct&gt; points = new ArrayList&lt;&gt;();\npoints.add(PointStruct.newBuilder()\n    .setId(1)\n    .setVectors(Vectors.newBuilder()\n        .setVector(Vector.newBuilder()\n            .addAllData(Arrays.asList(0.1f, 0.2f, 0.3f, ...))\n            .build())\n        .build())\n    .putPayload(\"name\", Value.newBuilder().setStringValue(\"Product A\").build())\n    .build());\n\nclient.upsert(UpsertPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllPoints(points)\n    .build());\n</code></pre> <p>Next step : Similarity Search</p>"},{"location":"Qdrant/Dev/EN/04-similarity-search/","title":"4. Similarity Search","text":""},{"location":"Qdrant/Dev/EN/04-similarity-search/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Perform searches</li> <li>Use different algorithms</li> <li>Handle results</li> </ul>"},{"location":"Qdrant/Dev/EN/04-similarity-search/#simple-search","title":"Simple Search","text":"<pre><code>List&lt;Float&gt; queryVector = Arrays.asList(0.1f, 0.2f, 0.3f, ...);\n\nSearchPoints searchPoints = SearchPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllVector(queryVector)\n    .setLimit(10)\n    .build();\n\nList&lt;ScoredPoint&gt; results = client.search(searchPoints).getResultList();\n</code></pre> <p>Next step : Filters and Metadata</p>"},{"location":"Qdrant/Dev/EN/05-filters-metadata/","title":"5. Filters and Metadata","text":""},{"location":"Qdrant/Dev/EN/05-filters-metadata/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Use advanced filters</li> <li>Manage metadata</li> <li>Combine multiple conditions</li> </ul>"},{"location":"Qdrant/Dev/EN/05-filters-metadata/#simple-filters","title":"Simple Filters","text":"<pre><code>Filter filter = Filter.newBuilder()\n    .addMust(Condition.newBuilder()\n        .setField(FieldCondition.newBuilder()\n            .setKey(\"category\")\n            .setMatch(Match.newBuilder()\n                .setValue(Value.newBuilder()\n                    .setStringValue(\"electronics\")\n                    .build())\n                .build())\n            .build())\n        .build())\n    .build();\n</code></pre> <p>Next step : Performance and Optimization</p>"},{"location":"Qdrant/Dev/EN/06-performance/","title":"6. Performance and Optimization","text":""},{"location":"Qdrant/Dev/EN/06-performance/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Optimize performance</li> <li>Use batch processing</li> <li>Manage memory</li> </ul>"},{"location":"Qdrant/Dev/EN/06-performance/#batch-insert","title":"Batch Insert","text":"<pre><code>List&lt;PointStruct&gt; points = new ArrayList&lt;&gt;();\nfor (int i = 0; i &lt; 1000; i++) {\n    points.add(createPoint(i, generateVector()));\n}\n\nclient.upsert(UpsertPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllPoints(points)\n    .build());\n</code></pre> <p>Next step : Best Practices</p>"},{"location":"Qdrant/Dev/EN/07-best-practices/","title":"7. Best Practices","text":""},{"location":"Qdrant/Dev/EN/07-best-practices/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Structure code</li> <li>Manage resources</li> <li>Optimize performance</li> </ul>"},{"location":"Qdrant/Dev/EN/07-best-practices/#service-layer","title":"Service Layer","text":"<pre><code>public class QdrantService {\n    private final QdrantClient client;\n\n    public List&lt;ScoredPoint&gt; search(String collection, List&lt;Float&gt; vector, int limit) {\n        SearchPoints searchPoints = SearchPoints.newBuilder()\n            .setCollectionName(collection)\n            .addAllVector(vector)\n            .setLimit(limit)\n            .build();\n        return client.search(searchPoints).getResultList();\n    }\n}\n</code></pre> <p>Next step : Practical Projects</p>"},{"location":"Qdrant/Dev/EN/08-projets/","title":"8. Practical Projects","text":""},{"location":"Qdrant/Dev/EN/08-projets/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Create complete application</li> <li>Integrate Qdrant in real project</li> <li>Apply best practices</li> </ul>"},{"location":"Qdrant/Dev/EN/08-projets/#project-1-semantic-search-service","title":"Project 1 : Semantic Search Service","text":"<pre><code>@Service\npublic class SemanticSearchService {\n    private final QdrantClient client;\n\n    public List&lt;SearchResult&gt; search(String query, int limit) {\n        List&lt;Float&gt; queryVector = embeddingModel.encode(query);\n        SearchPoints searchPoints = SearchPoints.newBuilder()\n            .setCollectionName(\"documents\")\n            .addAllVector(queryVector)\n            .setLimit(limit)\n            .build();\n        return mapToSearchResults(client.search(searchPoints).getResultList());\n    }\n}\n</code></pre> <p>Congratulations! You have completed the Qdrant training for Java Developer! \ud83c\udf89</p>"},{"location":"Qdrant/Dev/FR/","title":"Formation Qdrant pour D\u00e9veloppeur Java","text":""},{"location":"Qdrant/Dev/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation vous guide dans l'int\u00e9gration de Qdrant dans vos applications Java. Vous apprendrez \u00e0 utiliser Qdrant avec Java, le client Java, les requ\u00eates, l'optimisation et les bonnes pratiques.</p>"},{"location":"Qdrant/Dev/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Int\u00e9grer Qdrant dans des applications Java</li> <li>Utiliser le client Java Qdrant</li> <li>G\u00e9rer les collections et vecteurs</li> <li>Effectuer des recherches par similarit\u00e9</li> <li>Optimiser les performances</li> <li>Cr\u00e9er des applications compl\u00e8tes</li> </ul>"},{"location":"Qdrant/Dev/FR/#tout-est-gratuit","title":"\ud83d\udcb0 Tout est gratuit !","text":"<p>Cette formation utilise uniquement : - \u2705 Qdrant Community Edition : Gratuit - \u2705 Qdrant Java Client : Open-source - \u2705 Java : OpenJDK gratuit</p> <p>Budget total : 0\u20ac</p>"},{"location":"Qdrant/Dev/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"Qdrant/Dev/FR/#1-prise-en-main-qdrant-avec-java","title":"1. Prise en main Qdrant avec Java","text":""},{"location":"Qdrant/Dev/FR/#2-client-java-et-connexion","title":"2. Client Java et Connexion","text":""},{"location":"Qdrant/Dev/FR/#3-collections-et-vecteurs","title":"3. Collections et Vecteurs","text":""},{"location":"Qdrant/Dev/FR/#4-recherche-par-similarite","title":"4. Recherche par Similarit\u00e9","text":""},{"location":"Qdrant/Dev/FR/#5-filtres-et-metadonnees","title":"5. Filtres et M\u00e9tadonn\u00e9es","text":""},{"location":"Qdrant/Dev/FR/#6-performance-et-optimisation","title":"6. Performance et Optimisation","text":""},{"location":"Qdrant/Dev/FR/#7-bonnes-pratiques","title":"7. Bonnes Pratiques","text":""},{"location":"Qdrant/Dev/FR/#8-projets-pratiques","title":"8. Projets Pratiques","text":""},{"location":"Qdrant/Dev/FR/#prerequis","title":"\ud83d\ude80 Pr\u00e9requis","text":"<ul> <li>Connaissances Java (basiques)</li> <li>Notions de bases de donn\u00e9es</li> <li>Maven ou Gradle</li> </ul>"},{"location":"Qdrant/Dev/FR/#duree-estimee","title":"\u23f1\ufe0f Dur\u00e9e estim\u00e9e","text":"<ul> <li>Total : 30-40 heures</li> <li>Par module : 4-5 heures</li> </ul> <p>Bon apprentissage ! \ud83d\ude80</p>"},{"location":"Qdrant/Dev/FR/01-getting-started/","title":"1. Prise en main Qdrant avec Java","text":""},{"location":"Qdrant/Dev/FR/01-getting-started/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre Qdrant et Java</li> <li>Configurer l'environnement</li> <li>Installer le client Java</li> <li>Cr\u00e9er un premier projet</li> </ul>"},{"location":"Qdrant/Dev/FR/01-getting-started/#configuration-maven","title":"Configuration Maven","text":""},{"location":"Qdrant/Dev/FR/01-getting-started/#pomxml","title":"pom.xml","text":"<pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;io.qdrant&lt;/groupId&gt;\n        &lt;artifactId&gt;qdrant-java-client&lt;/artifactId&gt;\n        &lt;version&gt;1.7.0&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"Qdrant/Dev/FR/01-getting-started/#premier-exemple","title":"Premier exemple","text":"<pre><code>import io.qdrant.client.QdrantClient;\nimport io.qdrant.client.grpc.Collections;\n\npublic class QdrantExample {\n    public static void main(String[] args) {\n        QdrantClient client = new QdrantClient(\n            QdrantClient.newBuilder(\"localhost\", 6334, false).build()\n        );\n\n        System.out.println(\"Connected to Qdrant!\");\n        client.close();\n    }\n}\n</code></pre> <p>Prochaine \u00e9tape : Client Java et Connexion</p>"},{"location":"Qdrant/Dev/FR/02-java-client/","title":"2. Client Java et Connexion","text":""},{"location":"Qdrant/Dev/FR/02-java-client/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Configurer la connexion</li> <li>G\u00e9rer le client</li> <li>Comprendre les options de connexion</li> </ul>"},{"location":"Qdrant/Dev/FR/02-java-client/#connexion","title":"Connexion","text":"<pre><code>import io.qdrant.client.QdrantClient;\n\n// Connexion simple\nQdrantClient client = new QdrantClient(\n    QdrantClient.newBuilder(\"localhost\", 6334, false).build()\n);\n\n// Avec authentification\nQdrantClient client = new QdrantClient(\n    QdrantClient.newBuilder(\"localhost\", 6334, false)\n        .withApiKey(\"your-api-key\")\n        .build()\n);\n</code></pre>"},{"location":"Qdrant/Dev/FR/02-java-client/#gestion-du-client","title":"Gestion du client","text":"<pre><code>public class QdrantManager {\n    private static QdrantClient client;\n\n    public static QdrantClient getClient() {\n        if (client == null) {\n            client = new QdrantClient(\n                QdrantClient.newBuilder(\"localhost\", 6334, false).build()\n            );\n        }\n        return client;\n    }\n\n    public static void close() {\n        if (client != null) {\n            client.close();\n        }\n    }\n}\n</code></pre> <p>Prochaine \u00e9tape : Collections et Vecteurs</p>"},{"location":"Qdrant/Dev/FR/03-collections-vectors/","title":"3. Collections et Vecteurs","text":""},{"location":"Qdrant/Dev/FR/03-collections-vectors/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er des collections</li> <li>Ins\u00e9rer des vecteurs</li> <li>G\u00e9rer les collections</li> </ul>"},{"location":"Qdrant/Dev/FR/03-collections-vectors/#creer-une-collection","title":"Cr\u00e9er une collection","text":"<pre><code>import io.qdrant.client.QdrantClient;\nimport io.qdrant.client.grpc.Collections.*;\n\nQdrantClient client = QdrantManager.getClient();\n\nCreateCollection createCollection = CreateCollection.newBuilder()\n    .setCollectionName(\"products\")\n    .setVectorsConfig(VectorsConfig.newBuilder()\n        .setParams(VectorParams.newBuilder()\n            .setSize(128)\n            .setDistance(Distance.Cosine)\n            .build())\n        .build())\n    .build();\n\nclient.createCollection(createCollection);\n</code></pre>"},{"location":"Qdrant/Dev/FR/03-collections-vectors/#inserer-des-vecteurs","title":"Ins\u00e9rer des vecteurs","text":"<pre><code>import io.qdrant.client.grpc.Points.*;\n\nList&lt;PointStruct&gt; points = new ArrayList&lt;&gt;();\npoints.add(PointStruct.newBuilder()\n    .setId(1)\n    .setVectors(Vectors.newBuilder()\n        .setVector(Vector.newBuilder()\n            .addAllData(Arrays.asList(0.1f, 0.2f, 0.3f, ...))\n            .build())\n        .build())\n    .putPayload(\"name\", Value.newBuilder().setStringValue(\"Product A\").build())\n    .putPayload(\"category\", Value.newBuilder().setStringValue(\"electronics\").build())\n    .build());\n\nclient.upsert(UpsertPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllPoints(points)\n    .build());\n</code></pre> <p>Prochaine \u00e9tape : Recherche par Similarit\u00e9</p>"},{"location":"Qdrant/Dev/FR/04-similarity-search/","title":"4. Recherche par Similarit\u00e9","text":""},{"location":"Qdrant/Dev/FR/04-similarity-search/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Effectuer des recherches</li> <li>Utiliser diff\u00e9rents algorithmes</li> <li>G\u00e9rer les r\u00e9sultats</li> </ul>"},{"location":"Qdrant/Dev/FR/04-similarity-search/#recherche-simple","title":"Recherche simple","text":"<pre><code>import io.qdrant.client.grpc.Points.*;\n\nList&lt;Float&gt; queryVector = Arrays.asList(0.1f, 0.2f, 0.3f, ...);\n\nSearchPoints searchPoints = SearchPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllVector(queryVector)\n    .setLimit(10)\n    .build();\n\nList&lt;ScoredPoint&gt; results = client.search(searchPoints).getResultList();\n\nfor (ScoredPoint result : results) {\n    System.out.println(\"ID: \" + result.getId() + \", Score: \" + result.getScore());\n}\n</code></pre>"},{"location":"Qdrant/Dev/FR/04-similarity-search/#recherche-avec-filtres","title":"Recherche avec filtres","text":"<pre><code>import io.qdrant.client.grpc.Points.*;\n\nFilter filter = Filter.newBuilder()\n    .addMust(Condition.newBuilder()\n        .setField(FieldCondition.newBuilder()\n            .setKey(\"category\")\n            .setMatch(Match.newBuilder()\n                .setValue(Value.newBuilder()\n                    .setStringValue(\"electronics\")\n                    .build())\n                .build())\n            .build())\n        .build())\n    .build();\n\nSearchPoints searchPoints = SearchPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllVector(queryVector)\n    .setFilter(filter)\n    .setLimit(10)\n    .build();\n</code></pre> <p>Prochaine \u00e9tape : Filtres et M\u00e9tadonn\u00e9es</p>"},{"location":"Qdrant/Dev/FR/05-filters-metadata/","title":"5. Filtres et M\u00e9tadonn\u00e9es","text":""},{"location":"Qdrant/Dev/FR/05-filters-metadata/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Utiliser les filtres avanc\u00e9s</li> <li>G\u00e9rer les m\u00e9tadonn\u00e9es</li> <li>Combiner plusieurs conditions</li> </ul>"},{"location":"Qdrant/Dev/FR/05-filters-metadata/#filtres-simples","title":"Filtres simples","text":"<pre><code>Filter filter = Filter.newBuilder()\n    .addMust(Condition.newBuilder()\n        .setField(FieldCondition.newBuilder()\n            .setKey(\"category\")\n            .setMatch(Match.newBuilder()\n                .setValue(Value.newBuilder()\n                    .setStringValue(\"electronics\")\n                    .build())\n                .build())\n            .build())\n        .build())\n    .build();\n</code></pre>"},{"location":"Qdrant/Dev/FR/05-filters-metadata/#range-filters","title":"Range filters","text":"<pre><code>Filter filter = Filter.newBuilder()\n    .addMust(Condition.newBuilder()\n        .setField(FieldCondition.newBuilder()\n            .setKey(\"price\")\n            .setRange(Range.newBuilder()\n                .setGte(100.0)\n                .setLte(500.0)\n                .build())\n            .build())\n        .build())\n    .build();\n</code></pre>"},{"location":"Qdrant/Dev/FR/05-filters-metadata/#mettre-a-jour-le-payload","title":"Mettre \u00e0 jour le payload","text":"<pre><code>SetPayloadPoints setPayload = SetPayloadPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .putPayload(\"discount\", Value.newBuilder().setDoubleValue(0.1).build())\n    .addAllPoints(Arrays.asList(1L, 2L, 3L))\n    .build();\n\nclient.setPayload(setPayload);\n</code></pre> <p>Prochaine \u00e9tape : Performance et Optimisation</p>"},{"location":"Qdrant/Dev/FR/06-performance/","title":"6. Performance et Optimisation","text":""},{"location":"Qdrant/Dev/FR/06-performance/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Optimiser les performances</li> <li>Utiliser le batch processing</li> <li>G\u00e9rer la m\u00e9moire</li> </ul>"},{"location":"Qdrant/Dev/FR/06-performance/#batch-insert","title":"Batch insert","text":"<pre><code>List&lt;PointStruct&gt; points = new ArrayList&lt;&gt;();\n// Ajouter plusieurs points\nfor (int i = 0; i &lt; 1000; i++) {\n    points.add(createPoint(i, generateVector()));\n}\n\nclient.upsert(UpsertPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllPoints(points)\n    .build());\n</code></pre>"},{"location":"Qdrant/Dev/FR/06-performance/#configuration-hnsw","title":"Configuration HNSW","text":"<pre><code>HnswConfigDiff hnswConfig = HnswConfigDiff.newBuilder()\n    .setM(16)\n    .setEfConstruct(100)\n    .build();\n\nCreateCollection createCollection = CreateCollection.newBuilder()\n    .setCollectionName(\"products\")\n    .setVectorsConfig(VectorsConfig.newBuilder()\n        .setParams(VectorParams.newBuilder()\n            .setSize(128)\n            .setDistance(Distance.Cosine)\n            .build())\n        .build())\n    .setHnswConfig(hnswConfig)\n    .build();\n</code></pre> <p>Prochaine \u00e9tape : Bonnes Pratiques</p>"},{"location":"Qdrant/Dev/FR/07-best-practices/","title":"7. Bonnes Pratiques","text":""},{"location":"Qdrant/Dev/FR/07-best-practices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Structurer le code</li> <li>G\u00e9rer les ressources</li> <li>Optimiser les performances</li> </ul>"},{"location":"Qdrant/Dev/FR/07-best-practices/#service-layer","title":"Service Layer","text":"<pre><code>public class QdrantService {\n    private final QdrantClient client;\n\n    public QdrantService(QdrantClient client) {\n        this.client = client;\n    }\n\n    public List&lt;ScoredPoint&gt; search(String collection, List&lt;Float&gt; vector, int limit) {\n        SearchPoints searchPoints = SearchPoints.newBuilder()\n            .setCollectionName(collection)\n            .addAllVector(vector)\n            .setLimit(limit)\n            .build();\n        return client.search(searchPoints).getResultList();\n    }\n}\n</code></pre>"},{"location":"Qdrant/Dev/FR/07-best-practices/#gestion-des-ressources","title":"Gestion des ressources","text":"<pre><code>// \u2705 Bon\ntry (QdrantClient client = new QdrantClient(...)) {\n    // Utiliser le client\n}\n\n// Ou fermer explicitement\nQdrantClient client = new QdrantClient(...);\ntry {\n    // Utiliser\n} finally {\n    client.close();\n}\n</code></pre> <p>Prochaine \u00e9tape : Projets Pratiques</p>"},{"location":"Qdrant/Dev/FR/08-projets/","title":"8. Projets Pratiques","text":""},{"location":"Qdrant/Dev/FR/08-projets/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Cr\u00e9er une application compl\u00e8te</li> <li>Int\u00e9grer Qdrant dans un projet r\u00e9el</li> <li>Appliquer les bonnes pratiques</li> </ul>"},{"location":"Qdrant/Dev/FR/08-projets/#projet-1-service-de-recherche-semantique","title":"Projet 1 : Service de recherche s\u00e9mantique","text":"<pre><code>@Service\npublic class SemanticSearchService {\n    private final QdrantClient client;\n    private final EmbeddingModel embeddingModel;\n\n    public List&lt;SearchResult&gt; search(String query, int limit) {\n        List&lt;Float&gt; queryVector = embeddingModel.encode(query);\n\n        SearchPoints searchPoints = SearchPoints.newBuilder()\n            .setCollectionName(\"documents\")\n            .addAllVector(queryVector)\n            .setLimit(limit)\n            .build();\n\n        List&lt;ScoredPoint&gt; results = client.search(searchPoints).getResultList();\n        return mapToSearchResults(results);\n    }\n}\n</code></pre>"},{"location":"Qdrant/Dev/FR/08-projets/#projet-2-systeme-de-recommandation","title":"Projet 2 : Syst\u00e8me de recommandation","text":"<pre><code>public List&lt;Product&gt; recommendSimilarProducts(long productId, int limit) {\n    // R\u00e9cup\u00e9rer le vecteur du produit\n    GetPoints getPoints = GetPoints.newBuilder()\n        .setCollectionName(\"products\")\n        .addIds(productId)\n        .build();\n\n    PointStruct product = client.getPoints(getPoints).getResultList().get(0);\n\n    // Rechercher des produits similaires\n    SearchPoints searchPoints = SearchPoints.newBuilder()\n        .setCollectionName(\"products\")\n        .addAllVector(product.getVectors().getVector().getDataList())\n        .setLimit(limit)\n        .build();\n\n    return mapToProducts(client.search(searchPoints).getResultList());\n}\n</code></pre> <p>F\u00e9licitations ! Vous avez termin\u00e9 la formation Qdrant pour D\u00e9veloppeur Java ! \ud83c\udf89</p>"},{"location":"Qdrant/Dev/PL/","title":"Szkolenie Qdrant dla Dewelopera Java","text":""},{"location":"Qdrant/Dev/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>To szkolenie poprowadzi Ci\u0119 przez integracj\u0119 Qdrant w aplikacjach Java. Nauczysz si\u0119 u\u017cywa\u0107 Qdrant z Java, klienta Java, zapyta\u0144, optymalizacji i najlepszych praktyk.</p>"},{"location":"Qdrant/Dev/PL/#cele-szkoleniowe","title":"\ud83c\udfaf Cele szkoleniowe","text":"<ul> <li>Integrowa\u0107 Qdrant w aplikacjach Java</li> <li>U\u017cywa\u0107 klienta Java Qdrant</li> <li>Zarz\u0105dza\u0107 kolekcjami i wektorami</li> <li>Wykonywa\u0107 wyszukiwania podobie\u0144stwa</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> <li>Tworzy\u0107 kompletne aplikacje</li> </ul>"},{"location":"Qdrant/Dev/PL/#wszystko-jest-darmowe","title":"\ud83d\udcb0 Wszystko jest darmowe!","text":"<p>To szkolenie wykorzystuje tylko: - \u2705 Qdrant Community Edition : Darmowe - \u2705 Klient Java Qdrant : Open-source - \u2705 Java : OpenJDK darmowe</p> <p>Ca\u0142kowity bud\u017cet: 0 z\u0142</p>"},{"location":"Qdrant/Dev/PL/#struktura-szkolenia","title":"\ud83d\udcd6 Struktura szkolenia","text":""},{"location":"Qdrant/Dev/PL/#1-rozpoczecie-pracy-z-qdrant-i-java","title":"1. Rozpocz\u0119cie pracy z Qdrant i Java","text":""},{"location":"Qdrant/Dev/PL/#2-klient-java-i-poaczenie","title":"2. Klient Java i po\u0142\u0105czenie","text":""},{"location":"Qdrant/Dev/PL/#3-kolekcje-i-wektory","title":"3. Kolekcje i wektory","text":""},{"location":"Qdrant/Dev/PL/#4-wyszukiwanie-podobienstwa","title":"4. Wyszukiwanie podobie\u0144stwa","text":""},{"location":"Qdrant/Dev/PL/#5-filtry-i-metadane","title":"5. Filtry i metadane","text":""},{"location":"Qdrant/Dev/PL/#6-wydajnosc-i-optymalizacja","title":"6. Wydajno\u015b\u0107 i optymalizacja","text":""},{"location":"Qdrant/Dev/PL/#7-najlepsze-praktyki","title":"7. Najlepsze praktyki","text":""},{"location":"Qdrant/Dev/PL/#8-projekty-praktyczne","title":"8. Projekty praktyczne","text":""},{"location":"Qdrant/Dev/PL/#wymagania-wstepne","title":"\ud83d\ude80 Wymagania wst\u0119pne","text":"<ul> <li>Znajomo\u015b\u0107 Java (podstawy)</li> <li>Poj\u0119cia dotycz\u0105ce baz danych</li> <li>Maven lub Gradle</li> </ul>"},{"location":"Qdrant/Dev/PL/#szacowany-czas-trwania","title":"\u23f1\ufe0f Szacowany czas trwania","text":"<ul> <li>Ca\u0142kowity : 30-40 godzin</li> <li>Na modu\u0142 : 4-5 godzin</li> </ul> <p>Powodzenia w nauce! \ud83d\ude80</p>"},{"location":"Qdrant/Dev/PL/01-getting-started/","title":"1. Rozpocz\u0119cie pracy z Qdrant i Java","text":""},{"location":"Qdrant/Dev/PL/01-getting-started/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 Qdrant i Java</li> <li>Skonfigurowa\u0107 \u015brodowisko</li> <li>Zainstalowa\u0107 klienta Java</li> <li>Utworzy\u0107 pierwszy projekt</li> </ul>"},{"location":"Qdrant/Dev/PL/01-getting-started/#konfiguracja-maven","title":"Konfiguracja Maven","text":"<pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;io.qdrant&lt;/groupId&gt;\n        &lt;artifactId&gt;qdrant-java-client&lt;/artifactId&gt;\n        &lt;version&gt;1.7.0&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"Qdrant/Dev/PL/01-getting-started/#pierwszy-przykad","title":"Pierwszy przyk\u0142ad","text":"<pre><code>import io.qdrant.client.QdrantClient;\n\nQdrantClient client = new QdrantClient(\n    QdrantClient.newBuilder(\"localhost\", 6334, false).build()\n);\n\nSystem.out.println(\"Po\u0142\u0105czono z Qdrant!\");\nclient.close();\n</code></pre> <p>Nast\u0119pny krok : Klient Java i po\u0142\u0105czenie</p>"},{"location":"Qdrant/Dev/PL/02-java-client/","title":"2. Klient Java i po\u0142\u0105czenie","text":""},{"location":"Qdrant/Dev/PL/02-java-client/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Skonfigurowa\u0107 po\u0142\u0105czenie</li> <li>Zarz\u0105dza\u0107 klientem</li> <li>Zrozumie\u0107 opcje po\u0142\u0105czenia</li> </ul>"},{"location":"Qdrant/Dev/PL/02-java-client/#poaczenie","title":"Po\u0142\u0105czenie","text":"<pre><code>QdrantClient client = new QdrantClient(\n    QdrantClient.newBuilder(\"localhost\", 6334, false).build()\n);\n\n// Z uwierzytelnianiem\nQdrantClient client = new QdrantClient(\n    QdrantClient.newBuilder(\"localhost\", 6334, false)\n        .withApiKey(\"your-api-key\")\n        .build()\n);\n</code></pre> <p>Nast\u0119pny krok : Kolekcje i wektory</p>"},{"location":"Qdrant/Dev/PL/03-collections-vectors/","title":"3. Kolekcje i wektory","text":""},{"location":"Qdrant/Dev/PL/03-collections-vectors/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 kolekcje</li> <li>Wstawia\u0107 wektory</li> <li>Zarz\u0105dza\u0107 kolekcjami</li> </ul>"},{"location":"Qdrant/Dev/PL/03-collections-vectors/#utworzenie-kolekcji","title":"Utworzenie kolekcji","text":"<pre><code>CreateCollection createCollection = CreateCollection.newBuilder()\n    .setCollectionName(\"products\")\n    .setVectorsConfig(VectorsConfig.newBuilder()\n        .setParams(VectorParams.newBuilder()\n            .setSize(128)\n            .setDistance(Distance.Cosine)\n            .build())\n        .build())\n    .build();\n\nclient.createCollection(createCollection);\n</code></pre>"},{"location":"Qdrant/Dev/PL/03-collections-vectors/#wstawienie-wektorow","title":"Wstawienie wektor\u00f3w","text":"<pre><code>List&lt;PointStruct&gt; points = new ArrayList&lt;&gt;();\npoints.add(PointStruct.newBuilder()\n    .setId(1)\n    .setVectors(Vectors.newBuilder()\n        .setVector(Vector.newBuilder()\n            .addAllData(Arrays.asList(0.1f, 0.2f, 0.3f, ...))\n            .build())\n        .build())\n    .putPayload(\"name\", Value.newBuilder().setStringValue(\"Product A\").build())\n    .build());\n\nclient.upsert(UpsertPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllPoints(points)\n    .build());\n</code></pre> <p>Nast\u0119pny krok : Wyszukiwanie podobie\u0144stwa</p>"},{"location":"Qdrant/Dev/PL/04-similarity-search/","title":"4. Wyszukiwanie podobie\u0144stwa","text":""},{"location":"Qdrant/Dev/PL/04-similarity-search/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Wykonywa\u0107 wyszukiwania</li> <li>U\u017cywa\u0107 r\u00f3\u017cnych algorytm\u00f3w</li> <li>Obs\u0142ugiwa\u0107 wyniki</li> </ul>"},{"location":"Qdrant/Dev/PL/04-similarity-search/#proste-wyszukiwanie","title":"Proste wyszukiwanie","text":"<pre><code>List&lt;Float&gt; queryVector = Arrays.asList(0.1f, 0.2f, 0.3f, ...);\n\nSearchPoints searchPoints = SearchPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllVector(queryVector)\n    .setLimit(10)\n    .build();\n\nList&lt;ScoredPoint&gt; results = client.search(searchPoints).getResultList();\n</code></pre> <p>Nast\u0119pny krok : Filtry i metadane</p>"},{"location":"Qdrant/Dev/PL/05-filters-metadata/","title":"5. Filtry i metadane","text":""},{"location":"Qdrant/Dev/PL/05-filters-metadata/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>U\u017cywa\u0107 zaawansowanych filtr\u00f3w</li> <li>Zarz\u0105dza\u0107 metadanymi</li> <li>\u0141\u0105czy\u0107 wiele warunk\u00f3w</li> </ul>"},{"location":"Qdrant/Dev/PL/05-filters-metadata/#proste-filtry","title":"Proste filtry","text":"<pre><code>Filter filter = Filter.newBuilder()\n    .addMust(Condition.newBuilder()\n        .setField(FieldCondition.newBuilder()\n            .setKey(\"category\")\n            .setMatch(Match.newBuilder()\n                .setValue(Value.newBuilder()\n                    .setStringValue(\"electronics\")\n                    .build())\n                .build())\n            .build())\n        .build())\n    .build();\n</code></pre> <p>Nast\u0119pny krok : Wydajno\u015b\u0107 i optymalizacja</p>"},{"location":"Qdrant/Dev/PL/06-performance/","title":"6. Wydajno\u015b\u0107 i optymalizacja","text":""},{"location":"Qdrant/Dev/PL/06-performance/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> <li>U\u017cywa\u0107 przetwarzania wsadowego</li> <li>Zarz\u0105dza\u0107 pami\u0119ci\u0105</li> </ul>"},{"location":"Qdrant/Dev/PL/06-performance/#wstawianie-wsadowe","title":"Wstawianie wsadowe","text":"<pre><code>List&lt;PointStruct&gt; points = new ArrayList&lt;&gt;();\nfor (int i = 0; i &lt; 1000; i++) {\n    points.add(createPoint(i, generateVector()));\n}\n\nclient.upsert(UpsertPoints.newBuilder()\n    .setCollectionName(\"products\")\n    .addAllPoints(points)\n    .build());\n</code></pre> <p>Nast\u0119pny krok : Najlepsze praktyki</p>"},{"location":"Qdrant/Dev/PL/07-best-practices/","title":"7. Najlepsze praktyki","text":""},{"location":"Qdrant/Dev/PL/07-best-practices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Strukturyzowa\u0107 kod</li> <li>Zarz\u0105dza\u0107 zasobami</li> <li>Optymalizowa\u0107 wydajno\u015b\u0107</li> </ul>"},{"location":"Qdrant/Dev/PL/07-best-practices/#warstwa-serwisowa","title":"Warstwa serwisowa","text":"<pre><code>public class QdrantService {\n    private final QdrantClient client;\n\n    public List&lt;ScoredPoint&gt; search(String collection, List&lt;Float&gt; vector, int limit) {\n        SearchPoints searchPoints = SearchPoints.newBuilder()\n            .setCollectionName(collection)\n            .addAllVector(vector)\n            .setLimit(limit)\n            .build();\n        return client.search(searchPoints).getResultList();\n    }\n}\n</code></pre> <p>Nast\u0119pny krok : Projekty praktyczne</p>"},{"location":"Qdrant/Dev/PL/08-projets/","title":"8. Projekty praktyczne","text":""},{"location":"Qdrant/Dev/PL/08-projets/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Tworzy\u0107 kompletn\u0105 aplikacj\u0119</li> <li>Integrowa\u0107 Qdrant w rzeczywistym projekcie</li> <li>Stosowa\u0107 najlepsze praktyki</li> </ul>"},{"location":"Qdrant/Dev/PL/08-projets/#projekt-1-serwis-wyszukiwania-semantycznego","title":"Projekt 1 : Serwis wyszukiwania semantycznego","text":"<pre><code>@Service\npublic class SemanticSearchService {\n    private final QdrantClient client;\n\n    public List&lt;SearchResult&gt; search(String query, int limit) {\n        List&lt;Float&gt; queryVector = embeddingModel.encode(query);\n        SearchPoints searchPoints = SearchPoints.newBuilder()\n            .setCollectionName(\"documents\")\n            .addAllVector(queryVector)\n            .setLimit(limit)\n            .build();\n        return mapToSearchResults(client.search(searchPoints).getResultList());\n    }\n}\n</code></pre> <p>Gratulacje! Uko\u0144czy\u0142e\u015b szkolenie Qdrant dla Dewelopera Java! \ud83c\udf89</p>"},{"location":"SQL-avanc%C3%A9/FR/","title":"Formation SQL/PostgreSQL Avanc\u00e9 - Optimisation des Requ\u00eates","text":""},{"location":"SQL-avanc%C3%A9/FR/#vue-densemble","title":"\ud83d\udcda Vue d'ensemble","text":"<p>Cette formation couvre les techniques avanc\u00e9es d'optimisation de requ\u00eates SQL/PostgreSQL, avec un focus particulier sur l'utilisation de Dalibo pour l'analyse et l'optimisation des performances.</p>"},{"location":"SQL-avanc%C3%A9/FR/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ul> <li>Comprendre les m\u00e9canismes d'ex\u00e9cution des requ\u00eates PostgreSQL</li> <li>Ma\u00eetriser les techniques d'optimisation avanc\u00e9es</li> <li>Utiliser Dalibo pour analyser et optimiser les performances</li> <li>Interpr\u00e9ter les indicateurs de performance cl\u00e9s</li> <li>Appliquer les bonnes pratiques dans des cas r\u00e9els</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#structure-de-la-formation","title":"\ud83d\udcd6 Structure de la formation","text":""},{"location":"SQL-avanc%C3%A9/FR/#1-fondamentaux-de-loptimisation","title":"1. Fondamentaux de l'optimisation","text":"<ul> <li>Architecture PostgreSQL et planificateur de requ\u00eates</li> <li>Types d'index et leur utilisation</li> <li>Statistiques et ANALYZE</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#2-analyse-des-plans-dexecution","title":"2. Analyse des plans d'ex\u00e9cution","text":"<ul> <li>EXPLAIN et EXPLAIN ANALYZE</li> <li>Interpr\u00e9tation des op\u00e9rations (Seq Scan, Index Scan, etc.)</li> <li>Co\u00fbts et temps d'ex\u00e9cution</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#3-dalibo-outil-danalyse","title":"3. Dalibo - Outil d'analyse","text":"<ul> <li>Installation et configuration</li> <li>Analyse de requ\u00eates avec pg_stat_statements</li> <li>Rapports de performance</li> <li>Recommandations automatiques</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#4-indicateurs-de-performance","title":"4. Indicateurs de performance","text":"<ul> <li>M\u00e9triques cl\u00e9s \u00e0 surveiller</li> <li>Interpr\u00e9tation des indicateurs Dalibo</li> <li>Seuils d'alerte et bonnes pratiques</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#5-techniques-doptimisation","title":"5. Techniques d'optimisation","text":"<ul> <li>Optimisation des jointures</li> <li>Optimisation des agr\u00e9gations</li> <li>Optimisation des sous-requ\u00eates</li> <li>Partitionnement et parall\u00e9lisme</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#6-cas-pratiques","title":"6. Cas pratiques","text":"<ul> <li>Sc\u00e9narios r\u00e9els d'optimisation</li> <li>Avant/Apr\u00e8s avec m\u00e9triques</li> <li>R\u00e9solution de probl\u00e8mes courants</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#7-exercices","title":"7. Exercices","text":"<ul> <li>Exercices guid\u00e9s</li> <li>Probl\u00e8mes \u00e0 r\u00e9soudre</li> <li>Solutions comment\u00e9es</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#demarrage-rapide","title":"\ud83d\ude80 D\u00e9marrage rapide","text":"<ol> <li>Pr\u00e9requis</li> <li>PostgreSQL 12+ install\u00e9</li> <li>Acc\u00e8s \u00e0 une base de donn\u00e9es de test</li> <li> <p>Extension <code>pg_stat_statements</code> activ\u00e9e</p> </li> <li> <p>Configuration Dalibo <pre><code>-- Activer pg_stat_statements\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n</code></pre></p> </li> <li> <p>Suivre la formation</p> </li> <li>Commencez par le module 1 (Fondamentaux)</li> <li>Suivez l'ordre des modules pour une progression logique</li> <li>Pratiquez avec les exercices du module 7</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/#outils-recommandes","title":"\ud83d\udcca Outils recommand\u00e9s","text":"<ul> <li>Dalibo : Analyse de performance PostgreSQL</li> <li>pgAdmin : Interface graphique pour PostgreSQL</li> <li>psql : Client en ligne de commande</li> <li>EXPLAIN Visualizer : Visualisation des plans d'ex\u00e9cution</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#conventions","title":"\ud83d\udcdd Conventions","text":"<ul> <li>Les exemples SQL sont test\u00e9s sur PostgreSQL 14+</li> <li>Les m\u00e9triques sont bas\u00e9es sur des environnements de production typiques</li> <li>Les temps d'ex\u00e9cution peuvent varier selon la configuration</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/#contribution","title":"\ud83e\udd1d Contribution","text":"<p>Cette formation est con\u00e7ue pour \u00eatre \u00e9volutive. N'h\u00e9sitez pas \u00e0 proposer des am\u00e9liorations ou des cas d'usage suppl\u00e9mentaires.</p>"},{"location":"SQL-avanc%C3%A9/FR/#ressources-complementaires","title":"\ud83d\udcda Ressources compl\u00e9mentaires","text":"<ul> <li>Documentation PostgreSQL officielle</li> <li>Dalibo Documentation</li> <li>PostgreSQL Performance Tuning</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/","title":"Plan de Formation SQL/PostgreSQL Avanc\u00e9 - Optimisation des Requ\u00eates","text":""},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#vue-densemble","title":"\ud83d\udccb Vue d'ensemble","text":"<p>Ce document pr\u00e9sente le plan complet de la formation sur l'optimisation SQL/PostgreSQL avec focus sur Dalibo et les indicateurs de performance.</p>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#objectifs-pedagogiques","title":"\ud83c\udfaf Objectifs p\u00e9dagogiques","text":"<ol> <li>Comprendre les m\u00e9canismes internes de PostgreSQL</li> <li>Analyser les plans d'ex\u00e9cution et identifier les probl\u00e8mes</li> <li>Utiliser Dalibo pour l'analyse automatique</li> <li>Interpr\u00e9ter les indicateurs de performance</li> <li>Appliquer les techniques d'optimisation avanc\u00e9es</li> <li>R\u00e9soudre des probl\u00e8mes r\u00e9els de performance</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#structure-de-la-formation","title":"\ud83d\udcda Structure de la formation","text":""},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#module-1-fondamentaux-de-loptimisation","title":"Module 1 : Fondamentaux de l'optimisation","text":"<p>Dur\u00e9e estim\u00e9e : 2-3 heures</p> <p>Contenu : - Architecture PostgreSQL et planificateur de requ\u00eates - Types d'index (B-tree, Hash, GIN, GiST, BRIN) - Statistiques et ANALYZE - Param\u00e8tres de co\u00fbt</p> <p>Comp\u00e9tences acquises : - Comprendre comment PostgreSQL ex\u00e9cute les requ\u00eates - Choisir le bon type d'index - Maintenir les statistiques \u00e0 jour</p>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#module-2-analyse-des-plans-dexecution","title":"Module 2 : Analyse des plans d'ex\u00e9cution","text":"<p>Dur\u00e9e estim\u00e9e : 2-3 heures</p> <p>Contenu : - EXPLAIN et EXPLAIN ANALYZE - Types d'op\u00e9rations (Seq Scan, Index Scan, Hash Join, etc.) - Interpr\u00e9tation des co\u00fbts - Signaux d'alerte</p> <p>Comp\u00e9tences acquises : - Lire et interpr\u00e9ter les plans d'ex\u00e9cution - Identifier les op\u00e9rations probl\u00e9matiques - Comprendre les m\u00e9triques de performance</p>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#module-3-dalibo-outil-danalyse","title":"Module 3 : Dalibo - Outil d'analyse","text":"<p>Dur\u00e9e estim\u00e9e : 3-4 heures</p> <p>Contenu : - Installation et configuration - pg_stat_statements - pg_qualstats - pg_stat_monitor - Rapports et visualisations - Recommandations automatiques</p> <p>Comp\u00e9tences acquises : - Installer et configurer les outils Dalibo - Analyser les statistiques de requ\u00eates - Identifier automatiquement les index manquants - G\u00e9n\u00e9rer des rapports de performance</p>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#module-4-indicateurs-de-performance","title":"Module 4 : Indicateurs de performance","text":"<p>Dur\u00e9e estim\u00e9e : 2-3 heures</p> <p>Contenu : - M\u00e9triques syst\u00e8me (CPU, m\u00e9moire, connexions) - M\u00e9triques de requ\u00eates (temps, fr\u00e9quence, cache) - M\u00e9triques d'index (utilisation, bloat) - M\u00e9triques d'I/O - Seuils d'alerte - Tableaux de bord</p> <p>Comp\u00e9tences acquises : - Surveiller les m\u00e9triques cl\u00e9s - D\u00e9finir des seuils d'alerte appropri\u00e9s - Cr\u00e9er des tableaux de bord de monitoring</p>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#module-5-techniques-doptimisation","title":"Module 5 : Techniques d'optimisation","text":"<p>Dur\u00e9e estim\u00e9e : 3-4 heures</p> <p>Contenu : - Optimisation des jointures - Optimisation des agr\u00e9gations - Optimisation des sous-requ\u00eates - Partitionnement (Range, List, Hash) - Parall\u00e9lisme - Optimisation des types de donn\u00e9es</p> <p>Comp\u00e9tences acquises : - Optimiser diff\u00e9rents types de requ\u00eates - Utiliser le partitionnement efficacement - Exploiter le parall\u00e9lisme PostgreSQL</p>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#module-6-cas-pratiques","title":"Module 6 : Cas pratiques","text":"<p>Dur\u00e9e estim\u00e9e : 3-4 heures</p> <p>Contenu : - 6 cas r\u00e9els d'optimisation - Analyse avant/apr\u00e8s avec m\u00e9triques - Utilisation de Dalibo pour l'analyse - R\u00e9solution de probl\u00e8mes courants</p> <p>Comp\u00e9tences acquises : - Appliquer les techniques sur des cas r\u00e9els - Mesurer l'impact des optimisations - R\u00e9soudre des probl\u00e8mes complexes</p>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#module-7-exercices","title":"Module 7 : Exercices","text":"<p>Dur\u00e9e estim\u00e9e : 4-6 heures</p> <p>Contenu : - 6 exercices progressifs (D\u00e9butant \u2192 Avanc\u00e9) - Solutions comment\u00e9es - Probl\u00e8mes \u00e0 r\u00e9soudre</p> <p>Comp\u00e9tences acquises : - Pratiquer les techniques apprises - R\u00e9soudre des probl\u00e8mes de mani\u00e8re autonome - Consolider les connaissances</p>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#indicateurs-dalibo-couverts","title":"\ud83d\udcca Indicateurs Dalibo couverts","text":""},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#outils-principaux","title":"Outils principaux","text":"<ol> <li>pg_stat_statements</li> <li>Identification des requ\u00eates lentes</li> <li>Analyse des temps d'ex\u00e9cution</li> <li>D\u00e9tection des I/O \u00e9lev\u00e9s</li> <li> <p>Cache hit ratio par requ\u00eate</p> </li> <li> <p>pg_qualstats</p> </li> <li>Statistiques sur les pr\u00e9dicats</li> <li>Identification automatique d'index manquants</li> <li>Recommandations d'index</li> <li> <p>Analyse des conditions WHERE/JOIN</p> </li> <li> <p>pg_stat_monitor</p> </li> <li>Monitoring avec agr\u00e9gation temporelle</li> <li>Analyse des erreurs</li> <li>Plans d'ex\u00e9cution multiples</li> <li>Buckets temporels</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#metriques-cles-surveillees","title":"M\u00e9triques cl\u00e9s surveill\u00e9es","text":"M\u00e9trique Outil Seuil d'alerte Temps d'ex\u00e9cution moyen pg_stat_statements &gt; 1000ms Cache hit ratio pg_stat_statements &lt; 95% Index manquants pg_qualstats Fr\u00e9quence &gt; 1000 Requ\u00eates avec I/O temporaire pg_stat_statements &gt; 0 Connexions idle in transaction pg_stat_activity &gt; 5%"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#parcours-dapprentissage-recommande","title":"\ud83c\udf93 Parcours d'apprentissage recommand\u00e9","text":""},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#parcours-complet-16-20-heures","title":"Parcours complet (16-20 heures)","text":"<ol> <li>Module 1 : Fondamentaux</li> <li>Module 2 : Plans d'ex\u00e9cution</li> <li>Module 3 : Dalibo</li> <li>Module 4 : Indicateurs</li> <li>Module 5 : Techniques</li> <li>Module 6 : Cas pratiques</li> <li>Module 7 : Exercices</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#parcours-accelere-8-10-heures","title":"Parcours acc\u00e9l\u00e9r\u00e9 (8-10 heures)","text":"<ol> <li>Module 1 : Fondamentaux (r\u00e9vision rapide)</li> <li>Module 2 : Plans d'ex\u00e9cution</li> <li>Module 3 : Dalibo (focus sur pg_stat_statements et pg_qualstats)</li> <li>Module 4 : Indicateurs (m\u00e9triques essentielles)</li> <li>Module 6 : Cas pratiques (2-3 cas)</li> <li>Module 7 : Exercices (niveau interm\u00e9diaire)</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#parcours-expert-4-6-heures","title":"Parcours expert (4-6 heures)","text":"<ol> <li>Module 3 : Dalibo (approfondissement)</li> <li>Module 4 : Indicateurs (tableaux de bord avanc\u00e9s)</li> <li>Module 5 : Techniques (partitionnement, parall\u00e9lisme)</li> <li>Module 7 : Exercices (niveau avanc\u00e9)</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#prerequis-techniques","title":"\ud83d\udee0\ufe0f Pr\u00e9requis techniques","text":""},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#connaissances-requises","title":"Connaissances requises","text":"<ul> <li>SQL de base (SELECT, JOIN, GROUP BY, etc.)</li> <li>Notions de base sur PostgreSQL</li> <li>Acc\u00e8s \u00e0 une instance PostgreSQL (12+)</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#environnement-recommande","title":"Environnement recommand\u00e9","text":"<ul> <li>PostgreSQL 12+ install\u00e9</li> <li>Acc\u00e8s superutilisateur pour installer les extensions</li> <li>Base de donn\u00e9es de test avec donn\u00e9es r\u00e9alistes</li> <li>Outils : psql, pgAdmin (optionnel)</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#extensions-necessaires","title":"Extensions n\u00e9cessaires","text":"<pre><code>CREATE EXTENSION IF NOT EXISTS pg_stat_statements;\nCREATE EXTENSION IF NOT EXISTS pg_qualstats;  -- Optionnel mais recommand\u00e9\nCREATE EXTENSION IF NOT EXISTS pg_stat_monitor;  -- Optionnel\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#progression-et-evaluation","title":"\ud83d\udcc8 Progression et \u00e9valuation","text":""},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#points-de-controle","title":"Points de contr\u00f4le","text":"<ol> <li>Apr\u00e8s Module 2 : Capable d'interpr\u00e9ter un plan d'ex\u00e9cution</li> <li>Apr\u00e8s Module 3 : Capable d'utiliser Dalibo pour identifier les probl\u00e8mes</li> <li>Apr\u00e8s Module 5 : Capable d'optimiser diff\u00e9rents types de requ\u00eates</li> <li>Apr\u00e8s Module 7 : Capable de r\u00e9soudre des probl\u00e8mes complexes de mani\u00e8re autonome</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#criteres-de-reussite","title":"Crit\u00e8res de r\u00e9ussite","text":"<ul> <li>\u2705 Interpr\u00e9ter correctement un plan d'ex\u00e9cution</li> <li>\u2705 Identifier les probl\u00e8mes de performance avec Dalibo</li> <li>\u2705 Cr\u00e9er les index appropri\u00e9s</li> <li>\u2705 Optimiser une requ\u00eate lente (am\u00e9lioration &gt; 50%)</li> <li>\u2705 Configurer le monitoring des indicateurs cl\u00e9s</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#ressources-complementaires","title":"\ud83d\udd17 Ressources compl\u00e9mentaires","text":""},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#documentation-officielle","title":"Documentation officielle","text":"<ul> <li>PostgreSQL Documentation</li> <li>Dalibo GitHub</li> <li>pg_stat_statements</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#outils-recommandes","title":"Outils recommand\u00e9s","text":"<ul> <li>pgBadger : Analyse des logs PostgreSQL</li> <li>pg_activity : Monitoring en temps r\u00e9el</li> <li>HypoPG : Test d'index hypoth\u00e9tiques</li> <li>explain.dalibo.com : Visualisation des plans</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#communautes","title":"Communaut\u00e9s","text":"<ul> <li>PostgreSQL France</li> <li>Stack Overflow (tag: postgresql)</li> <li>Reddit r/PostgreSQL</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#notes-pedagogiques","title":"\ud83d\udcdd Notes p\u00e9dagogiques","text":""},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#approche-pedagogique","title":"Approche p\u00e9dagogique","text":"<ul> <li>Th\u00e9orique : Concepts expliqu\u00e9s avec exemples</li> <li>Pratique : Cas r\u00e9els et exercices</li> <li>Progressive : Du simple au complexe</li> <li>Autonome : Documentation compl\u00e8te pour auto-formation</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#conseils-pour-les-formateurs","title":"Conseils pour les formateurs","text":"<ol> <li>Commencer par des exemples concrets</li> <li>Utiliser EXPLAIN ANALYZE syst\u00e9matiquement</li> <li>Montrer l'impact avant/apr\u00e8s les optimisations</li> <li>Encourager l'exp\u00e9rimentation</li> <li>Faire des liens entre les modules</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#conseils-pour-les-apprenants","title":"Conseils pour les apprenants","text":"<ol> <li>Pratiquer r\u00e9guli\u00e8rement</li> <li>Tester sur des donn\u00e9es r\u00e9alistes</li> <li>Documenter vos optimisations</li> <li>Mesurer l'impact syst\u00e9matiquement</li> <li>Revenir aux fondamentaux si n\u00e9cessaire</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/PLAN_FORMATION/#resultats-attendus","title":"\ud83c\udfaf R\u00e9sultats attendus","text":"<p>\u00c0 la fin de cette formation, vous serez capable de :</p> <ol> <li>\u2705 Analyser et optimiser des requ\u00eates SQL complexes</li> <li>\u2705 Utiliser Dalibo pour identifier automatiquement les probl\u00e8mes</li> <li>\u2705 Interpr\u00e9ter les indicateurs de performance et d\u00e9finir des alertes</li> <li>\u2705 Appliquer les techniques d'optimisation appropri\u00e9es</li> <li>\u2705 R\u00e9soudre des probl\u00e8mes de performance en production</li> <li>\u2705 Mettre en place un syst\u00e8me de monitoring efficace</li> </ol> <p>Derni\u00e8re mise \u00e0 jour : 2024 Version : 1.0</p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/","title":"1. Fondamentaux de l'optimisation PostgreSQL","text":""},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre l'architecture PostgreSQL et le planificateur de requ\u00eates</li> <li>Ma\u00eetriser les diff\u00e9rents types d'index et leur utilisation optimale</li> <li>Comprendre le r\u00f4le des statistiques dans l'optimisation</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Architecture PostgreSQL</li> <li>Le planificateur de requ\u00eates</li> <li>Types d'index</li> <li>Statistiques et ANALYZE</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#architecture-postgresql","title":"Architecture PostgreSQL","text":""},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#composants-cles","title":"Composants cl\u00e9s","text":"<p>PostgreSQL utilise une architecture multi-processus avec plusieurs composants importants :</p> <ul> <li>Postmaster : Processus principal qui g\u00e8re les connexions</li> <li>Backend processes : Un processus par connexion client</li> <li>Planificateur (Planner) : Optimise les requ\u00eates SQL</li> <li>Ex\u00e9cuteur (Executor) : Ex\u00e9cute les plans de requ\u00eates</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#flux-dexecution-dune-requete","title":"Flux d'ex\u00e9cution d'une requ\u00eate","text":"<pre><code>Requ\u00eate SQL\n    \u2193\nParser (analyse syntaxique)\n    \u2193\nRewriter (r\u00e9\u00e9criture des vues/r\u00e8gles)\n    \u2193\nPlanner (g\u00e9n\u00e9ration du plan d'ex\u00e9cution)\n    \u2193\nExecutor (ex\u00e9cution du plan)\n    \u2193\nR\u00e9sultat\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#le-planificateur-de-requetes","title":"Le planificateur de requ\u00eates","text":"<p>Le planificateur est responsable de : - Choisir le meilleur plan d'ex\u00e9cution - Estimer les co\u00fbts de chaque op\u00e9ration - Utiliser les statistiques de la base de donn\u00e9es - Optimiser les jointures, tri, agr\u00e9gations</p> <p>Facteurs influen\u00e7ant le planificateur : - Statistiques des tables (<code>pg_stat_user_tables</code>) - Statistiques des colonnes (<code>pg_stats</code>) - Configuration des co\u00fbts (<code>random_page_cost</code>, <code>seq_page_cost</code>, etc.) - Param\u00e8tres de m\u00e9moire (<code>work_mem</code>, <code>shared_buffers</code>)</p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#le-planificateur-de-requetes_1","title":"Le planificateur de requ\u00eates","text":""},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#parametres-de-cout","title":"Param\u00e8tres de co\u00fbt","text":"<p>PostgreSQL utilise un syst\u00e8me de co\u00fbts pour comparer les plans :</p> <pre><code>-- Voir les param\u00e8tres de co\u00fbt actuels\nSHOW random_page_cost;\nSHOW seq_page_cost;\nSHOW cpu_tuple_cost;\nSHOW cpu_index_tuple_cost;\n</code></pre> <p>Param\u00e8tres importants : - <code>seq_page_cost</code> : Co\u00fbt de lecture s\u00e9quentielle (d\u00e9faut: 1.0) - <code>random_page_cost</code> : Co\u00fbt de lecture al\u00e9atoire (d\u00e9faut: 4.0) - <code>cpu_tuple_cost</code> : Co\u00fbt de traitement d'une ligne (d\u00e9faut: 0.01) - <code>cpu_index_tuple_cost</code> : Co\u00fbt d'utilisation d'un index (d\u00e9faut: 0.005)</p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#estimation-des-couts","title":"Estimation des co\u00fbts","text":"<p>Le planificateur estime : - Nombre de lignes : Bas\u00e9 sur les statistiques - Co\u00fbt d'E/S : Lecture/\u00e9criture disque - Co\u00fbt CPU : Traitement des donn\u00e9es - Co\u00fbt total : Somme des co\u00fbts</p> <p>Limitation importante : Les estimations peuvent \u00eatre impr\u00e9cises si les statistiques sont obsol\u00e8tes.</p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#types-dindex","title":"Types d'index","text":""},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#index-b-tree-par-defaut","title":"Index B-tree (par d\u00e9faut)","text":"<p>Utilisation : - \u00c9galit\u00e9 (<code>=</code>) - Comparaisons (<code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>) - BETWEEN, IN - LIKE avec pr\u00e9fixe fixe</p> <p>Exemple : <pre><code>CREATE INDEX idx_user_email ON users(email);\n-- Utilis\u00e9 pour: WHERE email = 'user@example.com'\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#index-hash","title":"Index Hash","text":"<p>Utilisation : - Uniquement pour l'\u00e9galit\u00e9 (<code>=</code>) - Plus rapide que B-tree pour l'\u00e9galit\u00e9 simple - Ne supporte pas les comparaisons</p> <p>Exemple : <pre><code>CREATE INDEX idx_user_id_hash ON users USING hash(id);\n-- Utilis\u00e9 pour: WHERE id = 123\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#index-gin-generalized-inverted-index","title":"Index GIN (Generalized Inverted Index)","text":"<p>Utilisation : - Types de donn\u00e9es complexes (tableaux, JSONB, full-text) - Op\u00e9rateurs de recherche avanc\u00e9s</p> <p>Exemple : <pre><code>CREATE INDEX idx_product_tags_gin ON products USING gin(tags);\n-- Utilis\u00e9 pour: WHERE tags @&gt; ARRAY['electronics']\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#index-gist-generalized-search-tree","title":"Index GiST (Generalized Search Tree)","text":"<p>Utilisation : - Types g\u00e9om\u00e9triques - Full-text search - Types personnalis\u00e9s</p> <p>Exemple : <pre><code>CREATE INDEX idx_location_gist ON places USING gist(location);\n-- Utilis\u00e9 pour: WHERE location &lt;-&gt; point(0,0) &lt; 1000\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#index-brin-block-range-index","title":"Index BRIN (Block Range Index)","text":"<p>Utilisation : - Grandes tables avec donn\u00e9es tri\u00e9es - Tr\u00e8s compact (peu d'espace) - Efficace pour les plages de valeurs</p> <p>Exemple : <pre><code>CREATE INDEX idx_orders_date_brin ON orders USING brin(order_date);\n-- Utilis\u00e9 pour: WHERE order_date BETWEEN '2024-01-01' AND '2024-12-31'\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#index-partiel","title":"Index partiel","text":"<p>Utilisation : - R\u00e9duire la taille de l'index - Am\u00e9liorer les performances pour des conditions sp\u00e9cifiques</p> <p>Exemple : <pre><code>CREATE INDEX idx_active_users ON users(email) WHERE active = true;\n-- Index uniquement sur les utilisateurs actifs\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#index-composite","title":"Index composite","text":"<p>Utilisation : - Plusieurs colonnes - Ordre des colonnes important</p> <p>Exemple : <pre><code>CREATE INDEX idx_user_name_email ON users(last_name, first_name, email);\n-- Utilis\u00e9 pour: WHERE last_name = 'Doe' AND first_name = 'John'\n</code></pre></p> <p>R\u00e8gle importante : L'index peut \u00eatre utilis\u00e9 si la requ\u00eate utilise les colonnes dans l'ordre de l'index, en commen\u00e7ant par la premi\u00e8re.</p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#statistiques-et-analyze","title":"Statistiques et ANALYZE","text":""},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#pourquoi-les-statistiques-sont-essentielles","title":"Pourquoi les statistiques sont essentielles","text":"<p>Le planificateur utilise les statistiques pour : - Estimer le nombre de lignes retourn\u00e9es - Choisir entre diff\u00e9rents plans d'ex\u00e9cution - D\u00e9terminer l'ordre des jointures</p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#collecte-des-statistiques","title":"Collecte des statistiques","text":"<pre><code>-- Analyser une table sp\u00e9cifique\nANALYZE table_name;\n\n-- Analyser toutes les tables\nANALYZE;\n\n-- Analyser avec un niveau de d\u00e9tail\nANALYZE VERBOSE table_name;\n</code></pre> <p>Quand ex\u00e9cuter ANALYZE : - Apr\u00e8s des modifications importantes (INSERT, UPDATE, DELETE) - Apr\u00e8s la cr\u00e9ation d'index - Automatiquement par autovacuum (configurable)</p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#configuration-autovacuum","title":"Configuration autovacuum","text":"<pre><code>-- Voir la configuration actuelle\nSHOW autovacuum;\nSHOW autovacuum_analyze_scale_factor;\nSHOW autovacuum_analyze_threshold;\n\n-- Modifier pour une table sp\u00e9cifique\nALTER TABLE large_table SET (\n    autovacuum_analyze_scale_factor = 0.05,\n    autovacuum_analyze_threshold = 10000\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#consulter-les-statistiques","title":"Consulter les statistiques","text":"<pre><code>-- Statistiques des tables\nSELECT \n    schemaname,\n    tablename,\n    n_tup_ins AS inserts,\n    n_tup_upd AS updates,\n    n_tup_del AS deletes,\n    n_live_tup AS live_rows,\n    n_dead_tup AS dead_rows,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables\nORDER BY n_live_tup DESC;\n\n-- Statistiques des colonnes\nSELECT \n    schemaname,\n    tablename,\n    attname AS column_name,\n    n_distinct,\n    correlation,\n    most_common_vals\nFROM pg_stats\nWHERE tablename = 'your_table';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#statistiques-etendues","title":"Statistiques \u00e9tendues","text":"<p>PostgreSQL 10+ supporte les statistiques \u00e9tendues :</p> <pre><code>-- Cr\u00e9er des statistiques sur plusieurs colonnes\nCREATE STATISTICS stats_user_name_email \nON users(last_name, first_name);\n\n-- Analyser pour collecter les statistiques\nANALYZE users;\n\n-- Consulter les statistiques \u00e9tendues\nSELECT * FROM pg_statistic_ext;\n</code></pre> <p>Utilit\u00e9 : Am\u00e9liore les estimations pour les requ\u00eates avec plusieurs colonnes corr\u00e9l\u00e9es.</p>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Le planificateur d\u00e9pend des statistiques : Des statistiques obsol\u00e8tes = mauvais plans</li> <li>Choisir le bon type d'index : Chaque type a ses avantages</li> <li>ANALYZE r\u00e9gulier : Essentiel pour maintenir de bonnes performances</li> <li>Comprendre les co\u00fbts : Aide \u00e0 interpr\u00e9ter les plans d'ex\u00e9cution</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/01-fondamentaux/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 2. Analyse des plans d'ex\u00e9cution pour apprendre \u00e0 interpr\u00e9ter les plans d'ex\u00e9cution.</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/","title":"2. Analyse des plans d'ex\u00e9cution","text":""},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Ma\u00eetriser EXPLAIN et EXPLAIN ANALYZE</li> <li>Interpr\u00e9ter les diff\u00e9rents types d'op\u00e9rations</li> <li>Comprendre les co\u00fbts et temps d'ex\u00e9cution</li> <li>Identifier les probl\u00e8mes de performance dans les plans</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>EXPLAIN et EXPLAIN ANALYZE</li> <li>Types d'op\u00e9rations</li> <li>Interpr\u00e9tation des co\u00fbts</li> <li>Signaux d'alerte</li> <li>Bonnes pratiques</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#explain-et-explain-analyze","title":"EXPLAIN et EXPLAIN ANALYZE","text":""},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#explain-sans-execution","title":"EXPLAIN (sans ex\u00e9cution)","text":"<p>Affiche le plan d'ex\u00e9cution estim\u00e9 sans ex\u00e9cuter la requ\u00eate :</p> <pre><code>EXPLAIN SELECT * FROM users WHERE email = 'user@example.com';\n</code></pre> <p>R\u00e9sultat : <pre><code>Seq Scan on users  (cost=0.00..25.00 rows=1 width=64)\n  Filter: (email = 'user@example.com'::text)\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#explain-analyze-avec-execution","title":"EXPLAIN ANALYZE (avec ex\u00e9cution)","text":"<p>Ex\u00e9cute la requ\u00eate et affiche les temps r\u00e9els :</p> <pre><code>EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'user@example.com';\n</code></pre> <p>R\u00e9sultat : <pre><code>Seq Scan on users  (cost=0.00..25.00 rows=1 width=64) \n  (actual time=0.123..15.456 rows=1 loops=1)\n  Filter: (email = 'user@example.com'::text)\n  Rows Removed by Filter: 9999\nPlanning Time: 0.234 ms\nExecution Time: 15.678 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#options-utiles","title":"Options utiles","text":"<pre><code>-- Format JSON (pour outils externes)\nEXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) \nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Afficher les buffers\nEXPLAIN (ANALYZE, BUFFERS) \nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Afficher les param\u00e8tres de planification\nEXPLAIN (ANALYZE, VERBOSE, SETTINGS) \nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Format YAML\nEXPLAIN (ANALYZE, FORMAT YAML) \nSELECT * FROM users WHERE email = 'user@example.com';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#interpretation-des-metriques","title":"Interpr\u00e9tation des m\u00e9triques","text":"<p>Co\u00fbts estim\u00e9s : - <code>cost=0.00..25.00</code> : Co\u00fbt de d\u00e9marrage..Co\u00fbt total - <code>rows=1</code> : Nombre de lignes estim\u00e9es - <code>width=64</code> : Taille moyenne d'une ligne en octets</p> <p>Temps r\u00e9els (ANALYZE) : - <code>actual time=0.123..15.456</code> : Temps de d\u00e9marrage..Temps total (ms) - <code>rows=1</code> : Nombre r\u00e9el de lignes retourn\u00e9es - <code>loops=1</code> : Nombre d'ex\u00e9cutions de cette op\u00e9ration - <code>Planning Time</code> : Temps de planification - <code>Execution Time</code> : Temps total d'ex\u00e9cution</p> <p>Buffers (avec BUFFERS) : - <code>shared hit=15</code> : Pages lues depuis le cache partag\u00e9 - <code>shared read=3</code> : Pages lues depuis le disque - <code>shared written=0</code> : Pages \u00e9crites - <code>temp read/written</code> : Pages temporaires</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#types-doperations","title":"Types d'op\u00e9rations","text":""},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#seq-scan-scan-sequentiel","title":"Seq Scan (Scan s\u00e9quentiel)","text":"<p>Quand utilis\u00e9 : - Pas d'index appropri\u00e9 - Table petite (&lt; 10% de la table) - Index non s\u00e9lectif</p> <p>Exemple : <pre><code>EXPLAIN ANALYZE SELECT * FROM users WHERE status = 'inactive';\n</code></pre></p> <p>Interpr\u00e9tation : - \u2705 Acceptable pour petites tables - \u26a0\ufe0f Probl\u00e9matique pour grandes tables - \ud83d\udd0d Action : Cr\u00e9er un index si la table est grande</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#index-scan","title":"Index Scan","text":"<p>Quand utilis\u00e9 : - Index disponible et s\u00e9lectif - Acc\u00e8s direct par index</p> <p>Exemple : <pre><code>EXPLAIN ANALYZE \nSELECT * FROM users WHERE email = 'user@example.com';\n</code></pre></p> <p>R\u00e9sultat typique : <pre><code>Index Scan using idx_users_email on users  \n  (cost=0.42..8.44 rows=1 width=64)\n  (actual time=0.123..0.125 rows=1 loops=1)\n  Index Cond: (email = 'user@example.com'::text)\n</code></pre></p> <p>Interpr\u00e9tation : - \u2705 Bonne performance - \u2705 Acc\u00e8s direct aux lignes</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#index-only-scan","title":"Index Only Scan","text":"<p>Quand utilis\u00e9 : - Toutes les colonnes n\u00e9cessaires sont dans l'index - Pas besoin d'acc\u00e9der \u00e0 la table</p> <p>Exemple : <pre><code>-- Index sur (id, email)\nCREATE INDEX idx_users_id_email ON users(id, email);\n\nEXPLAIN ANALYZE \nSELECT id, email FROM users WHERE id BETWEEN 1 AND 100;\n</code></pre></p> <p>R\u00e9sultat typique : <pre><code>Index Only Scan using idx_users_id_email on users\n  (cost=0.42..5.44 rows=100 width=64)\n  (actual time=0.123..0.456 rows=100 loops=1)\n  Index Cond: ((id &gt;= 1) AND (id &lt;= 100))\n  Heap Fetches: 0\n</code></pre></p> <p>Interpr\u00e9tation : - \u2705 Performance optimale - \u2705 <code>Heap Fetches: 0</code> = pas d'acc\u00e8s \u00e0 la table</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#bitmap-index-scan-bitmap-heap-scan","title":"Bitmap Index Scan + Bitmap Heap Scan","text":"<p>Quand utilis\u00e9 : - Conditions multiples avec plusieurs index - Retourne plusieurs lignes</p> <p>Exemple : <pre><code>EXPLAIN ANALYZE \nSELECT * FROM users \nWHERE status = 'active' AND created_at &gt; '2024-01-01';\n</code></pre></p> <p>R\u00e9sultat typique : <pre><code>Bitmap Heap Scan on users\n  (cost=4.44..25.67 rows=50 width=64)\n  (actual time=0.234..1.456 rows=45 loops=1)\n  Recheck Cond: ((status = 'active'::text) AND (created_at &gt; '2024-01-01'::date))\n  Heap Blocks: exact=12\n  -&gt; Bitmap Index Scan on idx_users_status\n      (cost=0.00..4.43 rows=50 width=0)\n      (actual time=0.123..0.123 rows=45 loops=1)\n      Index Cond: (status = 'active'::text)\n</code></pre></p> <p>Interpr\u00e9tation : - \u2705 Efficace pour plusieurs conditions - \u26a0\ufe0f <code>Recheck Cond</code> = v\u00e9rification suppl\u00e9mentaire</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#nested-loop","title":"Nested Loop","text":"<p>Quand utilis\u00e9 : - Petites tables ou r\u00e9sultats limit\u00e9s - Une table externe petite</p> <p>Exemple : <pre><code>EXPLAIN ANALYZE \nSELECT u.*, o.* \nFROM users u \nJOIN orders o ON u.id = o.user_id \nWHERE u.id = 123;\n</code></pre></p> <p>R\u00e9sultat typique : <pre><code>Nested Loop\n  (cost=0.85..25.67 rows=10 width=128)\n  (actual time=0.123..2.456 rows=8 loops=1)\n  -&gt; Index Scan using idx_users_id on users\n      (cost=0.42..8.44 rows=1 width=64)\n      (actual time=0.089..0.090 rows=1 loops=1)\n      Index Cond: (id = 123)\n  -&gt; Index Scan using idx_orders_user_id on orders\n      (cost=0.42..17.23 rows=10 width=64)\n      (actual time=0.234..2.345 rows=8 loops=1)\n      Index Cond: (user_id = 123)\n</code></pre></p> <p>Interpr\u00e9tation : - \u2705 Efficace pour petites boucles - \u26a0\ufe0f Peut \u00eatre lent si la boucle externe est grande</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#hash-join","title":"Hash Join","text":"<p>Quand utilis\u00e9 : - Tables de taille similaire - Pas d'index sur la cl\u00e9 de jointure - \u00c9galit\u00e9 simple</p> <p>Exemple : <pre><code>EXPLAIN ANALYZE \nSELECT u.*, o.* \nFROM users u \nJOIN orders o ON u.id = o.user_id;\n</code></pre></p> <p>R\u00e9sultat typique : <pre><code>Hash Join\n  (cost=125.67..456.78 rows=10000 width=128)\n  (actual time=2.345..15.678 rows=9876 loops=1)\n  Hash Cond: (o.user_id = u.id)\n  -&gt; Seq Scan on orders o\n      (cost=0.00..234.56 rows=10000 width=64)\n      (actual time=0.123..5.678 rows=10000 loops=1)\n  -&gt; Hash\n      (cost=123.45..123.45 rows=1000 width=64)\n      (actual time=1.234..1.234 rows=1000 loops=1)\n      Buckets: 1024  Batches: 1  Memory Usage: 64kB\n      -&gt; Seq Scan on users u\n          (cost=0.00..123.45 rows=1000 width=64)\n          (actual time=0.089..0.567 rows=1000 loops=1)\n</code></pre></p> <p>Interpr\u00e9tation : - \u2705 Efficace pour jointures d'\u00e9galit\u00e9 - \u26a0\ufe0f N\u00e9cessite de la m\u00e9moire (<code>work_mem</code>) - \ud83d\udd0d Action : Augmenter <code>work_mem</code> si \"Batches &gt; 1\"</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#merge-join","title":"Merge Join","text":"<p>Quand utilis\u00e9 : - Donn\u00e9es d\u00e9j\u00e0 tri\u00e9es - Jointures sur cl\u00e9s tri\u00e9es - Op\u00e9rateurs de comparaison (&lt;, &gt;, &lt;=, &gt;=)</p> <p>Exemple : <pre><code>EXPLAIN ANALYZE \nSELECT u.*, o.* \nFROM users u \nJOIN orders o ON u.id = o.user_id \nORDER BY u.id;\n</code></pre></p> <p>Interpr\u00e9tation : - \u2705 Efficace si les donn\u00e9es sont tri\u00e9es - \u26a0\ufe0f N\u00e9cessite un tri si les donn\u00e9es ne le sont pas</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#sort","title":"Sort","text":"<p>Quand utilis\u00e9 : - ORDER BY - GROUP BY (parfois) - Op\u00e9rations n\u00e9cessitant un tri</p> <p>Exemple : <pre><code>EXPLAIN ANALYZE \nSELECT * FROM users ORDER BY created_at DESC LIMIT 100;\n</code></pre></p> <p>R\u00e9sultat typique : <pre><code>Limit\n  (cost=234.56..256.78 rows=100 width=64)\n  (actual time=12.345..15.678 rows=100 loops=1)\n  -&gt; Sort\n      (cost=234.56..256.78 rows=10000 width=64)\n      (actual time=12.345..15.234 rows=100 loops=1)\n      Sort Key: created_at DESC\n      Sort Method: top-N heapsort  Memory: 32kB\n      -&gt; Seq Scan on users\n          (cost=0.00..123.45 rows=10000 width=64)\n          (actual time=0.089..5.678 rows=10000 loops=1)\n</code></pre></p> <p>Interpr\u00e9tation : - \u26a0\ufe0f <code>Sort Method: external merge</code> = tri sur disque (lent) - \u2705 <code>Sort Method: quicksort</code> = tri en m\u00e9moire (rapide) - \ud83d\udd0d Action : Augmenter <code>work_mem</code> si tri sur disque</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#aggregate","title":"Aggregate","text":"<p>Quand utilis\u00e9 : - Fonctions d'agr\u00e9gation (COUNT, SUM, AVG, etc.) - GROUP BY</p> <p>Exemple : <pre><code>EXPLAIN ANALYZE \nSELECT status, COUNT(*) \nFROM users \nGROUP BY status;\n</code></pre></p> <p>R\u00e9sultat typique : <pre><code>HashAggregate\n  (cost=123.45..145.67 rows=5 width=12)\n  (actual time=2.345..2.456 rows=5 loops=1)\n  Group Key: status\n  Batches: 1  Memory Usage: 24kB\n  -&gt; Seq Scan on users\n      (cost=0.00..98.76 rows=10000 width=4)\n      (actual time=0.089..1.234 rows=10000 loops=1)\n</code></pre></p> <p>Interpr\u00e9tation : - \u2705 <code>HashAggregate</code> = efficace - \u26a0\ufe0f <code>GroupAggregate</code> = peut \u00eatre lent - \ud83d\udd0d Action : Augmenter <code>work_mem</code> si \"Batches &gt; 1\"</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#interpretation-des-couts","title":"Interpr\u00e9tation des co\u00fbts","text":""},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#structure-des-couts","title":"Structure des co\u00fbts","text":"<pre><code>cost=0.00..25.00\n  \u2191      \u2191\n  |      \u2514\u2500 Co\u00fbt total\n  \u2514\u2500 Co\u00fbt de d\u00e9marrage\n</code></pre> <p>Co\u00fbt de d\u00e9marrage : Co\u00fbt avant de retourner la premi\u00e8re ligne Co\u00fbt total : Co\u00fbt pour retourner toutes les lignes</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#comparaison-des-couts","title":"Comparaison des co\u00fbts","text":"<p>R\u00e8gle g\u00e9n\u00e9rale : - Co\u00fbt &lt; 100 : Tr\u00e8s rapide - Co\u00fbt 100-1000 : Rapide - Co\u00fbt 1000-10000 : Mod\u00e9r\u00e9 - Co\u00fbt &gt; 10000 : Potentiellement lent</p> <p>\u26a0\ufe0f Important : Les co\u00fbts sont relatifs et d\u00e9pendent de la configuration.</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#ecart-entre-estimation-et-realite","title":"\u00c9cart entre estimation et r\u00e9alit\u00e9","text":"<p>Comparer : - <code>rows</code> (estim\u00e9) vs <code>rows</code> (r\u00e9el dans ANALYZE) - <code>cost</code> (estim\u00e9) vs <code>actual time</code> (r\u00e9el)</p> <p>Exemple probl\u00e9matique : <pre><code>Seq Scan on users\n  (cost=0.00..25.00 rows=1 width=64)\n  (actual time=0.123..1500.456 rows=100000 loops=1)\n</code></pre></p> <p>Probl\u00e8me : Estimation tr\u00e8s incorrecte (1 ligne estim\u00e9e, 100000 r\u00e9elles) Action : Ex\u00e9cuter <code>ANALYZE users;</code></p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#signaux-dalerte","title":"Signaux d'alerte","text":""},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#alertes-critiques","title":"\ud83d\udd34 Alertes critiques","text":"<ol> <li> <p>Seq Scan sur grande table <pre><code>Seq Scan on large_table (cost=0.00..50000.00 rows=1000000)\n</code></pre> Action : Cr\u00e9er un index appropri\u00e9</p> </li> <li> <p>Tri sur disque <pre><code>Sort Method: external merge  Disk: 50000kB\n</code></pre> Action : Augmenter <code>work_mem</code></p> </li> <li> <p>Estimation tr\u00e8s incorrecte <pre><code>rows=1 (estimated) vs rows=100000 (actual)\n</code></pre> Action : Ex\u00e9cuter <code>ANALYZE</code></p> </li> <li> <p>Hash Join avec plusieurs batches <pre><code>Hash Join\n  Batches: 16  Memory Usage: 512kB\n</code></pre> Action : Augmenter <code>work_mem</code></p> </li> <li> <p>Nested Loop avec grande boucle externe <pre><code>Nested Loop (loops=100000)\n</code></pre> Action : V\u00e9rifier les index ou changer le type de jointure</p> </li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#alertes-moderees","title":"\ud83d\udfe1 Alertes mod\u00e9r\u00e9es","text":"<ol> <li> <p>Index Scan avec beaucoup de Heap Fetches <pre><code>Index Only Scan\n  Heap Fetches: 50000\n</code></pre> Action : V\u00e9rifier la visibilit\u00e9 des tuples</p> </li> <li> <p>Bitmap Heap Scan avec beaucoup de rechecks <pre><code>Rows Removed by Filter: 50000\n</code></pre> Action : Am\u00e9liorer la s\u00e9lectivit\u00e9 de l'index</p> </li> <li> <p>Temps de planification \u00e9lev\u00e9 <pre><code>Planning Time: 500.234 ms\n</code></pre> Action : Simplifier la requ\u00eate ou augmenter <code>plan_cache_mode</code></p> </li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#bonnes-pratiques","title":"Bonnes pratiques","text":""},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#1-toujours-utiliser-explain-analyze-pour-les-requetes-lentes","title":"1. Toujours utiliser EXPLAIN ANALYZE pour les requ\u00eates lentes","text":"<pre><code>EXPLAIN (ANALYZE, BUFFERS, VERBOSE) \nSELECT ...;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#2-comparer-les-estimations-et-la-realite","title":"2. Comparer les estimations et la r\u00e9alit\u00e9","text":"<p>V\u00e9rifier si <code>rows</code> (estim\u00e9) \u2248 <code>rows</code> (r\u00e9el)</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#3-surveiller-les-buffers","title":"3. Surveiller les buffers","text":"<pre><code>EXPLAIN (ANALYZE, BUFFERS) SELECT ...;\n</code></pre> <ul> <li><code>shared hit</code> \u00e9lev\u00e9 = bon (cache)</li> <li><code>shared read</code> \u00e9lev\u00e9 = peut \u00eatre am\u00e9lior\u00e9 (I/O disque)</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#4-identifier-les-operations-couteuses","title":"4. Identifier les op\u00e9rations co\u00fbteuses","text":"<p>Chercher les op\u00e9rations avec : - <code>actual time</code> \u00e9lev\u00e9 - <code>loops</code> \u00e9lev\u00e9 - <code>rows</code> beaucoup plus \u00e9lev\u00e9 que l'estimation</p>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#5-utiliser-des-outils-de-visualisation","title":"5. Utiliser des outils de visualisation","text":"<ul> <li>pgAdmin : Visualisation graphique des plans</li> <li>explain.dalibo.com : Analyse en ligne</li> <li>pev : PostgreSQL Explain Visualizer</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>EXPLAIN = estimation, EXPLAIN ANALYZE = r\u00e9alit\u00e9</li> <li>Seq Scan sur grande table = probl\u00e8me potentiel</li> <li>Tri sur disque = augmenter <code>work_mem</code></li> <li>Estimation incorrecte = ex\u00e9cuter <code>ANALYZE</code></li> <li>Comparer toujours estimation vs r\u00e9alit\u00e9</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/02-plans-execution/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 3. Dalibo - Outil d'analyse pour apprendre \u00e0 utiliser Dalibo pour l'analyse de performance.</p>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/","title":"3. Dalibo - Outil d'analyse de performance","text":""},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Comprendre l'\u00e9cosyst\u00e8me Dalibo</li> <li>Installer et configurer les outils Dalibo</li> <li>Utiliser pg_stat_statements pour l'analyse</li> <li>G\u00e9n\u00e9rer et interpr\u00e9ter les rapports de performance</li> <li>Utiliser les recommandations automatiques</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Pr\u00e9sentation de Dalibo</li> <li>Installation et configuration</li> <li>pg_stat_statements</li> <li>pg_qualstats</li> <li>pg_stat_monitor</li> <li>Rapports et visualisations</li> <li>Recommandations automatiques</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#presentation-de-dalibo","title":"Pr\u00e9sentation de Dalibo","text":""},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#ecosysteme-dalibo","title":"\u00c9cosyst\u00e8me Dalibo","text":"<p>Dalibo propose une suite d'outils open-source pour l'analyse de performance PostgreSQL :</p> <ul> <li>pg_stat_statements : Statistiques sur les requ\u00eates SQL</li> <li>pg_qualstats : Statistiques sur les pr\u00e9dicats (WHERE, JOIN)</li> <li>pg_stat_monitor : Monitoring avanc\u00e9 avec agr\u00e9gation temporelle</li> <li>pg_wait_sampling : Analyse des attentes</li> <li>HypoPG : Test d'index hypoth\u00e9tiques</li> <li>pgBadger : Analyse des logs PostgreSQL</li> <li>pg_activity : Monitoring en temps r\u00e9el</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#avantages","title":"Avantages","text":"<p>\u2705 Open-source : Gratuit et modifiable \u2705 Complet : Couvre tous les aspects de performance \u2705 Int\u00e9gr\u00e9 : Fonctionne avec PostgreSQL natif \u2705 Communaut\u00e9 : Support actif et documentation</p>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#installation-et-configuration","title":"Installation et configuration","text":""},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>PostgreSQL 12+ (certains outils n\u00e9cessitent des versions sp\u00e9cifiques)</li> <li>Acc\u00e8s superutilisateur pour installer les extensions</li> <li>Compilateur C pour certaines extensions</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#installation-de-pg_stat_statements","title":"Installation de pg_stat_statements","text":"<p>PostgreSQL 9.2+ : Inclus par d\u00e9faut</p> <pre><code>-- Activer l'extension\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- V\u00e9rifier l'installation\nSELECT * FROM pg_extension WHERE extname = 'pg_stat_statements';\n</code></pre> <p>Configuration dans postgresql.conf :</p> <pre><code># Charger l'extension au d\u00e9marrage\nshared_preload_libraries = 'pg_stat_statements'\n\n# Nombre de requ\u00eates uniques \u00e0 suivre (d\u00e9faut: 10000)\npg_stat_statements.max = 10000\n\n# Taille maximale de la requ\u00eate stock\u00e9e (d\u00e9faut: 1024)\npg_stat_statements.track = all\npg_stat_statements.track_utility = on\npg_stat_statements.save = on\n</code></pre> <p>Red\u00e9marrer PostgreSQL apr\u00e8s modification</p>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#installation-de-pg_qualstats","title":"Installation de pg_qualstats","text":"<p>T\u00e9l\u00e9chargement et compilation :</p> <pre><code># Cloner le repository\ngit clone https://github.com/dalibo/pg_qualstats.git\ncd pg_qualstats\n\n# Compiler et installer\nmake\nsudo make install\n</code></pre> <p>Activation :</p> <pre><code>-- Ajouter \u00e0 shared_preload_libraries\n-- Dans postgresql.conf:\n-- shared_preload_libraries = 'pg_stat_statements,pg_qualstats'\n\n-- Cr\u00e9er l'extension\nCREATE EXTENSION IF NOT EXISTS pg_qualstats;\n\n-- V\u00e9rifier\nSELECT * FROM pg_extension WHERE extname = 'pg_qualstats';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#installation-de-pg_stat_monitor","title":"Installation de pg_stat_monitor","text":"<p>Pour PostgreSQL 12+ :</p> <pre><code># Installation via package manager (exemple Ubuntu)\nsudo apt-get install postgresql-14-pg-stat-monitor\n\n# Ou compilation depuis source\ngit clone https://github.com/percona/pg_stat_monitor.git\ncd pg_stat_monitor\nmake\nsudo make install\n</code></pre> <p>Activation :</p> <pre><code>-- Dans postgresql.conf:\n-- shared_preload_libraries = 'pg_stat_monitor'\n\nCREATE EXTENSION IF NOT EXISTS pg_stat_monitor;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#pg_stat_statements","title":"pg_stat_statements","text":""},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#vue-densemble","title":"Vue d'ensemble","text":"<p><code>pg_stat_statements</code> collecte des statistiques sur toutes les requ\u00eates SQL ex\u00e9cut\u00e9es.</p>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#requetes-les-plus-couteuses","title":"Requ\u00eates les plus co\u00fbteuses","text":"<pre><code>-- Top 10 requ\u00eates par temps total\nSELECT \n    query,\n    calls,\n    total_exec_time,\n    mean_exec_time,\n    max_exec_time,\n    stddev_exec_time,\n    rows,\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#requetes-les-plus-frequentes","title":"Requ\u00eates les plus fr\u00e9quentes","text":"<pre><code>-- Top 10 requ\u00eates par nombre d'appels\nSELECT \n    query,\n    calls,\n    total_exec_time,\n    mean_exec_time,\n    (total_exec_time / sum(total_exec_time) OVER ()) * 100 AS percent_total_time\nFROM pg_stat_statements\nORDER BY calls DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#requetes-avec-io-eleve","title":"Requ\u00eates avec I/O \u00e9lev\u00e9","text":"<pre><code>-- Requ\u00eates avec beaucoup de lectures disque\nSELECT \n    query,\n    calls,\n    shared_blks_read,\n    shared_blks_hit,\n    shared_blks_dirtied,\n    shared_blks_written,\n    temp_blks_read,\n    temp_blks_written\nFROM pg_stat_statements\nWHERE shared_blks_read &gt; 1000\nORDER BY shared_blks_read DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#analyse-detaillee-dune-requete","title":"Analyse d\u00e9taill\u00e9e d'une requ\u00eate","text":"<pre><code>-- Statistiques compl\u00e8tes pour une requ\u00eate sp\u00e9cifique\nSELECT \n    query,\n    calls,\n    total_exec_time,\n    min_exec_time,\n    max_exec_time,\n    mean_exec_time,\n    stddev_exec_time,\n    rows,\n    shared_blks_hit,\n    shared_blks_read,\n    shared_blks_dirtied,\n    shared_blks_written,\n    temp_blks_read,\n    temp_blks_written,\n    blk_read_time,\n    blk_write_time\nFROM pg_stat_statements\nWHERE query LIKE '%SELECT * FROM users%'\nORDER BY total_exec_time DESC;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#reinitialiser-les-statistiques","title":"R\u00e9initialiser les statistiques","text":"<pre><code>-- R\u00e9initialiser toutes les statistiques\nSELECT pg_stat_statements_reset();\n\n-- R\u00e9initialiser pour une base sp\u00e9cifique\nSELECT pg_stat_statements_reset(userid, dbid, queryid);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#normalisation-des-requetes","title":"Normalisation des requ\u00eates","text":"<p><code>pg_stat_statements</code> normalise les requ\u00eates en rempla\u00e7ant les valeurs par <code>$1</code>, <code>$2</code>, etc.</p> <p>Exemple : <pre><code>-- Requ\u00eate originale\nSELECT * FROM users WHERE id = 123;\n\n-- Normalis\u00e9e dans pg_stat_statements\nSELECT * FROM users WHERE id = $1;\n</code></pre></p> <p>Avantage : Regroupe les requ\u00eates similaires avec des param\u00e8tres diff\u00e9rents.</p>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#pg_qualstats","title":"pg_qualstats","text":""},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#vue-densemble_1","title":"Vue d'ensemble","text":"<p><code>pg_qualstats</code> collecte des statistiques sur les pr\u00e9dicats (conditions WHERE, JOIN) pour identifier les index manquants.</p>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#statistiques-sur-les-predicats","title":"Statistiques sur les pr\u00e9dicats","text":"<pre><code>-- Top pr\u00e9dicats les plus utilis\u00e9s\nSELECT \n    left_schema,\n    left_table,\n    left_column,\n    operator,\n    count(*) AS execution_count,\n    n_distinct,\n    most_common_vals\nFROM pg_qualstats\nGROUP BY left_schema, left_table, left_column, operator\nORDER BY execution_count DESC\nLIMIT 20;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#identification-dindex-manquants","title":"Identification d'index manquants","text":"<pre><code>-- Pr\u00e9dicats sans index correspondant\nSELECT \n    qs.left_schema,\n    qs.left_table,\n    qs.left_column,\n    qs.operator,\n    qs.execution_count,\n    pg_size_pretty(pg_relation_size(qs.left_schema||'.'||qs.left_table)) AS table_size\nFROM pg_qualstats qs\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM pg_index i\n    JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n    WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n    AND a.attname = qs.left_column\n)\nORDER BY qs.execution_count DESC\nLIMIT 20;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#recommandations-dindex","title":"Recommandations d'index","text":"<pre><code>-- G\u00e9n\u00e9rer des commandes CREATE INDEX\nSELECT \n    'CREATE INDEX idx_' || \n    left_table || '_' || \n    left_column || \n    ' ON ' || left_schema || '.' || left_table || \n    ' (' || left_column || ');' AS create_index_command,\n    execution_count,\n    n_distinct\nFROM (\n    SELECT \n        qs.left_schema,\n        qs.left_table,\n        qs.left_column,\n        COUNT(*) AS execution_count,\n        COUNT(DISTINCT qs.most_common_vals) AS n_distinct\n    FROM pg_qualstats qs\n    WHERE NOT EXISTS (\n        SELECT 1\n        FROM pg_index i\n        JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n        WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n        AND a.attname = qs.left_column\n    )\n    GROUP BY qs.left_schema, qs.left_table, qs.left_column\n) AS missing_indexes\nORDER BY execution_count DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#reinitialiser-les-statistiques_1","title":"R\u00e9initialiser les statistiques","text":"<pre><code>-- R\u00e9initialiser pg_qualstats\nSELECT pg_qualstats_reset();\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#pg_stat_monitor","title":"pg_stat_monitor","text":""},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#vue-densemble_2","title":"Vue d'ensemble","text":"<p><code>pg_stat_monitor</code> offre un monitoring avanc\u00e9 avec agr\u00e9gation temporelle et analyse des buckets.</p>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#configuration","title":"Configuration","text":"<pre><code>-- Voir la configuration\nSELECT * FROM pg_stat_monitor_settings;\n\n-- Modifier la configuration\nALTER SYSTEM SET pg_stat_monitor.pgsm_max_buckets = 10;\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#requetes-par-bucket-periode","title":"Requ\u00eates par bucket (p\u00e9riode)","text":"<pre><code>-- Requ\u00eates group\u00e9es par p\u00e9riode\nSELECT \n    bucket,\n    bucket_start_time,\n    query,\n    calls,\n    total_exec_time,\n    mean_exec_time,\n    max_exec_time\nFROM pg_stat_monitor\nORDER BY bucket DESC, total_exec_time DESC\nLIMIT 20;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#analyse-des-erreurs","title":"Analyse des erreurs","text":"<pre><code>-- Requ\u00eates avec erreurs\nSELECT \n    query,\n    calls,\n    errors,\n    error_count,\n    error_code\nFROM pg_stat_monitor\nWHERE errors &gt; 0\nORDER BY errors DESC;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#analyse-des-plans","title":"Analyse des plans","text":"<pre><code>-- Plans d'ex\u00e9cution les plus utilis\u00e9s\nSELECT \n    query,\n    planid,\n    calls,\n    mean_exec_time,\n    plans\nFROM pg_stat_monitor\nWHERE plans IS NOT NULL\nORDER BY calls DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#rapports-et-visualisations","title":"Rapports et visualisations","text":""},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#pgbadger-analyse-des-logs","title":"pgBadger - Analyse des logs","text":"<p>Installation :</p> <pre><code># Ubuntu/Debian\nsudo apt-get install pgbadger\n\n# Ou via Perl CPAN\ncpanm pgbadger\n</code></pre> <p>G\u00e9n\u00e9ration de rapport :</p> <pre><code># G\u00e9n\u00e9rer un rapport HTML\npgbadger /var/log/postgresql/postgresql-*.log -o report.html\n\n# Avec options avanc\u00e9es\npgbadger \\\n  --prefix '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h' \\\n  --outdir /var/www/pgbadger \\\n  /var/log/postgresql/postgresql-*.log\n</code></pre> <p>Configuration PostgreSQL pour pgBadger :</p> <pre><code># Dans postgresql.conf\nlogging_collector = on\nlog_directory = 'log'\nlog_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'\nlog_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '\nlog_checkpoints = on\nlog_connections = on\nlog_disconnections = on\nlog_lock_waits = on\nlog_temp_files = 0\nlog_autovacuum_min_duration = 0\nlog_error_verbosity = default\nlog_min_duration_statement = 1000  # Log les requ\u00eates &gt; 1s\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#pg_activity-monitoring-temps-reel","title":"pg_activity - Monitoring temps r\u00e9el","text":"<p>Installation :</p> <pre><code>pip install pg_activity\n</code></pre> <p>Utilisation :</p> <pre><code># Connexion simple\npg_activity -U postgres -d mydb\n\n# Avec options\npg_activity -U postgres -d mydb --refresh 2 --no-database-size\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#visualisation-avec-metabasegrafana","title":"Visualisation avec Metabase/Grafana","text":"<p>Int\u00e9gration avec Grafana :</p> <ol> <li>Installer le plugin PostgreSQL</li> <li>Cr\u00e9er des dashboards avec les vues syst\u00e8me</li> <li>Surveiller les m\u00e9triques en temps r\u00e9el</li> </ol> <p>Requ\u00eates utiles pour Grafana :</p> <pre><code>-- Temps d'ex\u00e9cution moyen par minute\nSELECT \n    date_trunc('minute', now()) AS time,\n    AVG(mean_exec_time) AS avg_exec_time\nFROM pg_stat_statements\nGROUP BY time;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#recommandations-automatiques","title":"Recommandations automatiques","text":""},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#script-de-recommandations-basique","title":"Script de recommandations basique","text":"<pre><code>-- Requ\u00eates lentes sans index appropri\u00e9\nWITH slow_queries AS (\n    SELECT \n        query,\n        calls,\n        mean_exec_time,\n        total_exec_time\n    FROM pg_stat_statements\n    WHERE mean_exec_time &gt; 100  -- &gt; 100ms\n    ORDER BY total_exec_time DESC\n    LIMIT 10\n),\nmissing_indexes AS (\n    SELECT \n        qs.left_schema,\n        qs.left_table,\n        qs.left_column,\n        qs.operator,\n        COUNT(*) AS execution_count\n    FROM pg_qualstats qs\n    WHERE NOT EXISTS (\n        SELECT 1\n        FROM pg_index i\n        JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n        WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n        AND a.attname = qs.left_column\n    )\n    GROUP BY qs.left_schema, qs.left_table, qs.left_column, qs.operator\n)\nSELECT \n    'MISSING INDEX' AS recommendation_type,\n    'CREATE INDEX idx_' || left_table || '_' || left_column || \n    ' ON ' || left_schema || '.' || left_table || \n    ' (' || left_column || ');' AS recommendation,\n    execution_count AS priority_score\nFROM missing_indexes\nORDER BY execution_count DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#utiliser-hypopg-pour-tester-les-index","title":"Utiliser HypoPG pour tester les index","text":"<p>Installation :</p> <pre><code>CREATE EXTENSION IF NOT EXISTS hypopg;\n</code></pre> <p>Tester un index hypoth\u00e9tique :</p> <pre><code>-- Cr\u00e9er un index hypoth\u00e9tique\nSELECT * FROM hypopg_create_index('CREATE INDEX ON users(email)');\n\n-- Voir les index hypoth\u00e9tiques\nSELECT * FROM hypopg_list_indexes();\n\n-- Tester le plan avec l'index hypoth\u00e9tique\nEXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';\n\n-- Supprimer l'index hypoth\u00e9tique\nSELECT hypopg_drop_index(oid) FROM hypopg_list_indexes();\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>pg_stat_statements : Essentiel pour identifier les requ\u00eates lentes</li> <li>pg_qualstats : Identifie les index manquants automatiquement</li> <li>pg_stat_monitor : Monitoring avanc\u00e9 avec agr\u00e9gation temporelle</li> <li>pgBadger : Analyse compl\u00e8te des logs PostgreSQL</li> <li>HypoPG : Teste les index avant de les cr\u00e9er</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/03-dalibo/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 4. Indicateurs de performance pour apprendre \u00e0 interpr\u00e9ter les indicateurs cl\u00e9s de performance.</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/","title":"4. Indicateurs de performance","text":""},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Identifier les m\u00e9triques cl\u00e9s \u00e0 surveiller</li> <li>Interpr\u00e9ter les indicateurs Dalibo</li> <li>D\u00e9finir des seuils d'alerte</li> <li>Comprendre les corr\u00e9lations entre indicateurs</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>M\u00e9triques syst\u00e8me</li> <li>M\u00e9triques de requ\u00eates</li> <li>M\u00e9triques d'index</li> <li>M\u00e9triques d'I/O</li> <li>Seuils d'alerte</li> <li>Tableau de bord des indicateurs</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#metriques-systeme","title":"M\u00e9triques syst\u00e8me","text":""},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#utilisation-cpu","title":"Utilisation CPU","text":"<pre><code>-- Voir l'utilisation CPU par processus\nSELECT \n    pid,\n    usename,\n    application_name,\n    state,\n    query_start,\n    state_change,\n    wait_event_type,\n    wait_event,\n    query\nFROM pg_stat_activity\nWHERE state = 'active'\nORDER BY query_start;\n</code></pre> <p>Indicateurs cl\u00e9s : - CPU \u00e9lev\u00e9 : Requ\u00eates en cours d'ex\u00e9cution - wait_event_type = CPU : Processus en attente CPU</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#utilisation-memoire","title":"Utilisation m\u00e9moire","text":"<pre><code>-- M\u00e9moire partag\u00e9e utilis\u00e9e\nSELECT \n    setting AS shared_buffers,\n    pg_size_pretty(setting::bigint * 8192) AS shared_buffers_size\nFROM pg_settings\nWHERE name = 'shared_buffers';\n\n-- M\u00e9moire de travail utilis\u00e9e\nSELECT \n    setting AS work_mem,\n    pg_size_pretty(setting::bigint * 1024) AS work_mem_size\nFROM pg_settings\nWHERE name = 'work_mem';\n\n-- Statistiques de m\u00e9moire\nSELECT \n    name,\n    setting,\n    unit,\n    short_desc\nFROM pg_settings\nWHERE name IN ('shared_buffers', 'work_mem', 'maintenance_work_mem', 'effective_cache_size')\nORDER BY name;\n</code></pre> <p>Indicateurs cl\u00e9s : - shared_buffers : Cache partag\u00e9 (recommand\u00e9: 25% RAM) - work_mem : M\u00e9moire par op\u00e9ration de tri/hash - effective_cache_size : Estimation du cache OS (recommand\u00e9: 50-75% RAM)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#connexions-actives","title":"Connexions actives","text":"<pre><code>-- Nombre de connexions par \u00e9tat\nSELECT \n    state,\n    COUNT(*) AS count,\n    COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () AS percent\nFROM pg_stat_activity\nGROUP BY state\nORDER BY count DESC;\n\n-- Connexions par base de donn\u00e9es\nSELECT \n    datname,\n    COUNT(*) AS connections,\n    COUNT(*) FILTER (WHERE state = 'active') AS active,\n    COUNT(*) FILTER (WHERE state = 'idle') AS idle,\n    COUNT(*) FILTER (WHERE state = 'idle in transaction') AS idle_in_transaction\nFROM pg_stat_activity\nWHERE datname IS NOT NULL\nGROUP BY datname\nORDER BY connections DESC;\n\n-- Limite de connexions\nSELECT \n    setting AS max_connections,\n    (SELECT COUNT(*) FROM pg_stat_activity) AS current_connections,\n    ROUND(100.0 * (SELECT COUNT(*) FROM pg_stat_activity) / setting::numeric, 2) AS percent_used\nFROM pg_settings\nWHERE name = 'max_connections';\n</code></pre> <p>Indicateurs cl\u00e9s : - max_connections : Limite configur\u00e9e - idle in transaction : Connexions bloquantes (alerte si &gt; 5%) - active : Requ\u00eates en cours</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#verrous-locks","title":"Verrous (Locks)","text":"<pre><code>-- Verrous en attente\nSELECT \n    blocked_locks.pid AS blocked_pid,\n    blocked_activity.usename AS blocked_user,\n    blocking_locks.pid AS blocking_pid,\n    blocking_activity.usename AS blocking_user,\n    blocked_activity.query AS blocked_statement,\n    blocking_activity.query AS blocking_statement\nFROM pg_catalog.pg_locks blocked_locks\nJOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid\nJOIN pg_catalog.pg_locks blocking_locks \n    ON blocking_locks.locktype = blocked_locks.locktype\n    AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database\n    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation\n    AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page\n    AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple\n    AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid\n    AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid\n    AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid\n    AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid\n    AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid\n    AND blocking_locks.pid != blocked_locks.pid\nJOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid\nWHERE NOT blocked_locks.granted;\n</code></pre> <p>Indicateurs cl\u00e9s : - Verrous en attente : Blocages actifs (alerte si &gt; 0) - Dur\u00e9e des verrous : Temps d'attente (alerte si &gt; 1s)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#metriques-de-requetes","title":"M\u00e9triques de requ\u00eates","text":""},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#temps-dexecution","title":"Temps d'ex\u00e9cution","text":"<pre><code>-- Temps d'ex\u00e9cution moyen par requ\u00eate (top 20)\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    ROUND(total_exec_time::numeric, 2) AS total_time_ms,\n    ROUND(mean_exec_time::numeric, 2) AS mean_time_ms,\n    ROUND(max_exec_time::numeric, 2) AS max_time_ms,\n    ROUND(stddev_exec_time::numeric, 2) AS stddev_time_ms,\n    ROUND((total_exec_time / SUM(total_exec_time) OVER ()) * 100, 2) AS percent_total_time\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 20;\n</code></pre> <p>Indicateurs cl\u00e9s : - mean_exec_time : Temps moyen (alerte si &gt; 1000ms) - max_exec_time : Temps maximum (alerte si &gt; 5000ms) - stddev_exec_time : Variabilit\u00e9 (alerte si stddev &gt; mean)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#frequence-dexecution","title":"Fr\u00e9quence d'ex\u00e9cution","text":"<pre><code>-- Requ\u00eates les plus fr\u00e9quentes\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    ROUND(mean_exec_time::numeric, 2) AS mean_time_ms,\n    ROUND((calls * mean_exec_time)::numeric, 2) AS total_time_ms,\n    ROUND((calls::numeric / (SELECT SUM(calls) FROM pg_stat_statements)) * 100, 2) AS percent_calls\nFROM pg_stat_statements\nORDER BY calls DESC\nLIMIT 20;\n</code></pre> <p>Indicateurs cl\u00e9s : - calls : Nombre d'appels (identifier les requ\u00eates N+1) - percent_calls : Pourcentage du total (alerte si &gt; 50% pour une seule requ\u00eate)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#efficacite-du-cache","title":"Efficacit\u00e9 du cache","text":"<pre><code>-- Taux de hit du cache par requ\u00eate\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    shared_blks_hit,\n    shared_blks_read,\n    ROUND(100.0 * shared_blks_hit / NULLIF(shared_blks_hit + shared_blks_read, 0), 2) AS cache_hit_ratio,\n    ROUND((shared_blks_read * 8)::numeric / 1024, 2) AS disk_read_mb\nFROM pg_stat_statements\nWHERE shared_blks_hit + shared_blks_read &gt; 0\nORDER BY shared_blks_read DESC\nLIMIT 20;\n</code></pre> <p>Indicateurs cl\u00e9s : - cache_hit_ratio : Taux de hit (objectif: &gt; 95%) - disk_read_mb : Lectures disque (alerte si &gt; 100MB)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#requetes-avec-io-temporaire","title":"Requ\u00eates avec I/O temporaire","text":"<pre><code>-- Requ\u00eates utilisant des fichiers temporaires\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    temp_blks_read,\n    temp_blks_written,\n    ROUND((temp_blks_read + temp_blks_written) * 8.0 / 1024, 2) AS temp_mb,\n    ROUND(mean_exec_time::numeric, 2) AS mean_time_ms\nFROM pg_stat_statements\nWHERE temp_blks_read &gt; 0 OR temp_blks_written &gt; 0\nORDER BY (temp_blks_read + temp_blks_written) DESC\nLIMIT 20;\n</code></pre> <p>Indicateurs cl\u00e9s : - temp_blks_read/written : I/O temporaire (alerte si &gt; 0) - Action : Augmenter <code>work_mem</code> si pr\u00e9sent</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#metriques-dindex","title":"M\u00e9triques d'index","text":""},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#utilisation-des-index","title":"Utilisation des index","text":"<pre><code>-- Statistiques d'utilisation des index\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan AS index_scans,\n    idx_tup_read AS tuples_read,\n    idx_tup_fetch AS tuples_fetched,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size\nFROM pg_stat_user_indexes\nORDER BY idx_scan ASC, pg_relation_size(indexrelid) DESC\nLIMIT 20;\n</code></pre> <p>Indicateurs cl\u00e9s : - idx_scan = 0 : Index non utilis\u00e9 (candidat \u00e0 suppression) - index_size : Taille de l'index (co\u00fbt de maintenance)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#index-manquants-via-pg_qualstats","title":"Index manquants (via pg_qualstats)","text":"<pre><code>-- Pr\u00e9dicats fr\u00e9quents sans index\nSELECT \n    qs.left_schema,\n    qs.left_table,\n    qs.left_column,\n    qs.operator,\n    COUNT(*) AS execution_count,\n    pg_size_pretty(pg_relation_size((qs.left_schema||'.'||qs.left_table)::regclass)) AS table_size\nFROM pg_qualstats qs\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM pg_index i\n    JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n    WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n    AND a.attname = qs.left_column\n)\nGROUP BY qs.left_schema, qs.left_table, qs.left_column, qs.operator\nORDER BY execution_count DESC\nLIMIT 20;\n</code></pre> <p>Indicateurs cl\u00e9s : - execution_count : Fr\u00e9quence d'utilisation (priorit\u00e9) - table_size : Taille de la table (impact de l'index)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#bloat-des-index","title":"Bloat des index","text":"<pre><code>-- D\u00e9tection du bloat des index\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,\n    idx_scan AS index_scans,\n    CASE \n        WHEN idx_scan = 0 THEN 'UNUSED'\n        WHEN idx_scan &lt; 100 THEN 'LOW'\n        ELSE 'OK'\n    END AS usage_status\nFROM pg_stat_user_indexes\nWHERE pg_relation_size(indexrelid) &gt; 1048576  -- &gt; 1MB\nORDER BY pg_relation_size(indexrelid) DESC;\n</code></pre> <p>Indicateurs cl\u00e9s : - UNUSED : Index jamais utilis\u00e9 (candidat \u00e0 suppression) - LOW : Index peu utilis\u00e9 (\u00e0 \u00e9valuer)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#metriques-dio","title":"M\u00e9triques d'I/O","text":""},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#statistiques-dio-par-table","title":"Statistiques d'I/O par table","text":"<pre><code>-- I/O par table\nSELECT \n    schemaname,\n    tablename,\n    seq_scan,\n    seq_tup_read,\n    idx_scan,\n    idx_tup_fetch,\n    n_tup_ins AS inserts,\n    n_tup_upd AS updates,\n    n_tup_del AS deletes,\n    n_live_tup AS live_rows,\n    n_dead_tup AS dead_rows,\n    ROUND(n_dead_tup::numeric / NULLIF(n_live_tup + n_dead_tup, 0) * 100, 2) AS dead_tuple_percent,\n    last_vacuum,\n    last_autovacuum,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables\nORDER BY seq_scan DESC\nLIMIT 20;\n</code></pre> <p>Indicateurs cl\u00e9s : - seq_scan \u00e9lev\u00e9 : Beaucoup de scans s\u00e9quentiels (cr\u00e9er des index) - dead_tuple_percent : Pourcentage de tuples morts (alerte si &gt; 10%) - last_vacuum : Dernier vacuum (alerte si &gt; 7 jours)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#statistiques-dio-globales","title":"Statistiques d'I/O globales","text":"<pre><code>-- Statistiques d'I/O globales\nSELECT \n    datname,\n    blks_read,\n    blks_hit,\n    ROUND(100.0 * blks_hit / NULLIF(blks_hit + blks_read, 0), 2) AS cache_hit_ratio,\n    tup_returned,\n    tup_fetched,\n    tup_inserted,\n    tup_updated,\n    tup_deleted\nFROM pg_stat_database\nWHERE datname NOT IN ('template0', 'template1', 'postgres')\nORDER BY blks_read DESC;\n</code></pre> <p>Indicateurs cl\u00e9s : - cache_hit_ratio : Taux de hit global (objectif: &gt; 95%) - blks_read : Lectures disque (alerte si \u00e9lev\u00e9)</p>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#seuils-dalerte","title":"Seuils d'alerte","text":""},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#tableau-des-seuils-recommandes","title":"Tableau des seuils recommand\u00e9s","text":"M\u00e9trique Seuil d'alerte Seuil critique Action Temps d'ex\u00e9cution moyen &gt; 1000ms &gt; 5000ms Analyser le plan, cr\u00e9er index Taux de hit cache &lt; 95% &lt; 90% Augmenter shared_buffers Tuples morts &gt; 10% &gt; 20% Ex\u00e9cuter VACUUM I/O temporaire &gt; 0 &gt; 100MB Augmenter work_mem Connexions idle in transaction &gt; 5% &gt; 10% Identifier et corriger Verrous en attente &gt; 0 &gt; 10 Analyser les blocages Index non utilis\u00e9s Taille &gt; 100MB Taille &gt; 1GB \u00c9valuer suppression Dernier VACUUM &gt; 7 jours &gt; 30 jours Configurer autovacuum"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#requete-de-monitoring-global","title":"Requ\u00eate de monitoring global","text":"<pre><code>-- Dashboard de sant\u00e9 PostgreSQL\nWITH metrics AS (\n    SELECT \n        (SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'active') AS active_queries,\n        (SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'idle in transaction') AS idle_in_transaction,\n        (SELECT COUNT(*) FROM pg_locks WHERE NOT granted) AS waiting_locks,\n        (SELECT AVG(mean_exec_time) FROM pg_stat_statements) AS avg_query_time,\n        (SELECT SUM(shared_blks_read) FROM pg_stat_statements) AS total_disk_reads,\n        (SELECT SUM(shared_blks_hit) FROM pg_stat_statements) AS total_cache_hits,\n        (SELECT COUNT(*) FROM pg_stat_user_indexes WHERE idx_scan = 0) AS unused_indexes,\n        (SELECT SUM(n_dead_tup)::numeric / NULLIF(SUM(n_live_tup + n_dead_tup), 0) * 100 \n         FROM pg_stat_user_tables) AS dead_tuple_percent\n)\nSELECT \n    active_queries,\n    idle_in_transaction,\n    waiting_locks,\n    ROUND(avg_query_time::numeric, 2) AS avg_query_time_ms,\n    ROUND(100.0 * total_cache_hits / NULLIF(total_cache_hits + total_disk_reads, 0), 2) AS cache_hit_ratio,\n    unused_indexes,\n    ROUND(dead_tuple_percent::numeric, 2) AS dead_tuple_percent,\n    CASE \n        WHEN idle_in_transaction &gt; (SELECT setting::int FROM pg_settings WHERE name = 'max_connections') * 0.1 \n        THEN 'ALERT'\n        WHEN waiting_locks &gt; 0 THEN 'WARNING'\n        WHEN avg_query_time &gt; 1000 THEN 'WARNING'\n        WHEN dead_tuple_percent &gt; 10 THEN 'WARNING'\n        ELSE 'OK'\n    END AS health_status\nFROM metrics;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#tableau-de-bord-des-indicateurs","title":"Tableau de bord des indicateurs","text":""},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#vue-consolidee-pour-dalibo","title":"Vue consolid\u00e9e pour Dalibo","text":"<pre><code>-- Vue consolid\u00e9e des indicateurs cl\u00e9s\nCREATE OR REPLACE VIEW v_performance_dashboard AS\nSELECT \n    'QUERIES' AS category,\n    COUNT(*) AS metric_count,\n    ROUND(AVG(mean_exec_time)::numeric, 2) AS avg_value,\n    ROUND(MAX(mean_exec_time)::numeric, 2) AS max_value,\n    'ms' AS unit\nFROM pg_stat_statements\nWHERE mean_exec_time &gt; 100\n\nUNION ALL\n\nSELECT \n    'CACHE_HIT',\n    NULL,\n    ROUND(100.0 * SUM(shared_blks_hit) / NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2),\n    NULL,\n    '%'\nFROM pg_stat_statements\n\nUNION ALL\n\nSELECT \n    'IDLE_IN_TRANSACTION',\n    COUNT(*),\n    NULL,\n    NULL,\n    'count'\nFROM pg_stat_activity\nWHERE state = 'idle in transaction'\n\nUNION ALL\n\nSELECT \n    'WAITING_LOCKS',\n    COUNT(*),\n    NULL,\n    NULL,\n    'count'\nFROM pg_locks\nWHERE NOT granted\n\nUNION ALL\n\nSELECT \n    'UNUSED_INDEXES',\n    COUNT(*),\n    NULL,\n    NULL,\n    'count'\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\n  AND pg_relation_size(indexrelid) &gt; 1048576;  -- &gt; 1MB\n\n-- Utilisation\nSELECT * FROM v_performance_dashboard;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#export-pour-monitoring-externe","title":"Export pour monitoring externe","text":"<pre><code>-- Export JSON pour outils de monitoring\nSELECT json_build_object(\n    'timestamp', NOW(),\n    'metrics', json_build_object(\n        'active_queries', (SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'active'),\n        'cache_hit_ratio', (\n            SELECT ROUND(100.0 * SUM(shared_blks_hit) / NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2)\n            FROM pg_stat_statements\n        ),\n        'avg_query_time_ms', (\n            SELECT ROUND(AVG(mean_exec_time)::numeric, 2)\n            FROM pg_stat_statements\n        ),\n        'unused_indexes', (\n            SELECT COUNT(*) FROM pg_stat_user_indexes WHERE idx_scan = 0\n        )\n    )\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Surveiller r\u00e9guli\u00e8rement : Les m\u00e9triques changent avec le temps</li> <li>Seuils contextuels : Adapter selon l'environnement</li> <li>Corr\u00e9lations : Analyser plusieurs m\u00e9triques ensemble</li> <li>Tendances : Surveiller l'\u00e9volution dans le temps</li> <li>Actions prioritaires : Agir sur les m\u00e9triques critiques d'abord</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/04-indicateurs/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 5. Techniques d'optimisation pour apprendre les techniques pratiques d'optimisation.</p>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/","title":"5. Techniques d'optimisation","text":""},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Ma\u00eetriser les techniques d'optimisation des jointures</li> <li>Optimiser les agr\u00e9gations et sous-requ\u00eates</li> <li>Utiliser le partitionnement efficacement</li> <li>Exploiter le parall\u00e9lisme PostgreSQL</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Optimisation des jointures</li> <li>Optimisation des agr\u00e9gations</li> <li>Optimisation des sous-requ\u00eates</li> <li>Partitionnement</li> <li>Parall\u00e9lisme</li> <li>Optimisation des types de donn\u00e9es</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#optimisation-des-jointures","title":"Optimisation des jointures","text":""},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#choix-du-type-de-jointure","title":"Choix du type de jointure","text":"<p>PostgreSQL choisit automatiquement, mais vous pouvez influencer :</p> <p>Nested Loop : - \u2705 Petite table externe (&lt; 1000 lignes) - \u2705 Index sur la cl\u00e9 de jointure - \u274c Grande table externe</p> <p>Hash Join : - \u2705 Tables de taille similaire - \u2705 Jointure d'\u00e9galit\u00e9 - \u2705 Suffisamment de <code>work_mem</code> - \u274c Pas d'index n\u00e9cessaire</p> <p>Merge Join : - \u2705 Donn\u00e9es d\u00e9j\u00e0 tri\u00e9es - \u2705 Jointures sur cl\u00e9s tri\u00e9es - \u274c N\u00e9cessite un tri si non tri\u00e9</p>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#optimiser-avec-des-index","title":"Optimiser avec des index","text":"<pre><code>-- Avant : Jointure lente\nEXPLAIN ANALYZE\nSELECT u.*, o.*\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01';\n\n-- Cr\u00e9er des index sur les cl\u00e9s de jointure\nCREATE INDEX idx_orders_user_id ON orders(user_id);\nCREATE INDEX idx_users_created_at ON users(created_at);\n\n-- Apr\u00e8s : Jointure optimis\u00e9e\nEXPLAIN ANALYZE\nSELECT u.*, o.*\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#reduire-la-taille-des-tables-de-jointure","title":"R\u00e9duire la taille des tables de jointure","text":"<pre><code>-- Avant : Jointure sur toutes les lignes\nSELECT u.*, o.*\nFROM users u\nJOIN orders o ON u.id = o.user_id;\n\n-- Apr\u00e8s : Filtrer avant la jointure\nSELECT u.*, o.*\nFROM (\n    SELECT * FROM users WHERE active = true\n) u\nJOIN (\n    SELECT * FROM orders WHERE status = 'completed'\n) o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#ordre-des-jointures","title":"Ordre des jointures","text":"<p>Le planificateur choisit l'ordre, mais vous pouvez influencer :</p> <pre><code>-- Utiliser des CTE pour forcer l'ordre\nWITH filtered_users AS (\n    SELECT * FROM users WHERE active = true\n),\nfiltered_orders AS (\n    SELECT * FROM orders WHERE status = 'completed'\n)\nSELECT u.*, o.*\nFROM filtered_users u\nJOIN filtered_orders o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#jointures-multiples","title":"Jointures multiples","text":"<pre><code>-- Optimiser l'ordre des jointures\n-- PostgreSQL choisit g\u00e9n\u00e9ralement le bon ordre, mais v\u00e9rifiez avec EXPLAIN\n\n-- Mauvais : Jointure sur grande table en premier\nSELECT *\nFROM large_table l\nJOIN small_table1 s1 ON l.id = s1.large_id\nJOIN small_table2 s2 ON l.id = s2.large_id;\n\n-- Meilleur : Filtrer d'abord\nSELECT *\nFROM large_table l\nJOIN (\n    SELECT large_id FROM small_table1 WHERE condition = true\n) s1 ON l.id = s1.large_id\nJOIN (\n    SELECT large_id FROM small_table2 WHERE condition = true\n) s2 ON l.id = s2.large_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#optimisation-des-agregations","title":"Optimisation des agr\u00e9gations","text":""},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#group-by-optimise","title":"GROUP BY optimis\u00e9","text":"<pre><code>-- Avant : Agr\u00e9gation sur toutes les lignes\nSELECT status, COUNT(*), AVG(amount)\nFROM orders\nGROUP BY status;\n\n-- Apr\u00e8s : Filtrer avant l'agr\u00e9gation\nSELECT status, COUNT(*), AVG(amount)\nFROM orders\nWHERE created_at &gt; '2024-01-01'\nGROUP BY status;\n\n-- Index pour acc\u00e9l\u00e9rer\nCREATE INDEX idx_orders_status_created ON orders(status, created_at);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#agregations-avec-having","title":"Agr\u00e9gations avec HAVING","text":"<pre><code>-- Filtrer avec WHERE avant GROUP BY (plus efficace)\n-- Mauvais\nSELECT status, COUNT(*)\nFROM orders\nGROUP BY status\nHAVING COUNT(*) &gt; 100;\n\n-- Meilleur : Utiliser une sous-requ\u00eate\nSELECT status, cnt\nFROM (\n    SELECT status, COUNT(*) AS cnt\n    FROM orders\n    GROUP BY status\n) sub\nWHERE cnt &gt; 100;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#distinct-optimise","title":"DISTINCT optimis\u00e9","text":"<pre><code>-- DISTINCT peut \u00eatre co\u00fbteux\nSELECT DISTINCT user_id FROM orders;\n\n-- Parfois GROUP BY est plus rapide\nSELECT user_id FROM orders GROUP BY user_id;\n\n-- Avec index, les deux peuvent \u00eatre rapides\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#agregations-avec-fenetres","title":"Agr\u00e9gations avec fen\u00eatres","text":"<pre><code>-- Utiliser des fonctions de fen\u00eatre pour \u00e9viter les sous-requ\u00eates\n-- Avant : Sous-requ\u00eate corr\u00e9l\u00e9e\nSELECT \n    o.*,\n    (SELECT AVG(amount) FROM orders o2 WHERE o2.user_id = o.user_id) AS avg_user_amount\nFROM orders o;\n\n-- Apr\u00e8s : Fonction de fen\u00eatre\nSELECT \n    o.*,\n    AVG(amount) OVER (PARTITION BY user_id) AS avg_user_amount\nFROM orders o;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#optimisation-des-sous-requetes","title":"Optimisation des sous-requ\u00eates","text":""},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#sous-requetes-correlees-join","title":"Sous-requ\u00eates corr\u00e9l\u00e9es \u2192 JOIN","text":"<pre><code>-- Avant : Sous-requ\u00eate corr\u00e9l\u00e9e (lente)\nSELECT \n    u.*,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) AS order_count\nFROM users u;\n\n-- Apr\u00e8s : JOIN avec agr\u00e9gation (plus rapide)\nSELECT \n    u.*,\n    COALESCE(o.order_count, 0) AS order_count\nFROM users u\nLEFT JOIN (\n    SELECT user_id, COUNT(*) AS order_count\n    FROM orders\n    GROUP BY user_id\n) o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#exists-vs-in-vs-join","title":"EXISTS vs IN vs JOIN","text":"<pre><code>-- EXISTS : G\u00e9n\u00e9ralement le plus rapide\nSELECT *\nFROM users u\nWHERE EXISTS (\n    SELECT 1 FROM orders o WHERE o.user_id = u.id\n);\n\n-- IN : Peut \u00eatre lent si la liste est grande\nSELECT *\nFROM users u\nWHERE u.id IN (\n    SELECT user_id FROM orders\n);\n\n-- JOIN : Bon compromis\nSELECT DISTINCT u.*\nFROM users u\nJOIN orders o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#sous-requetes-dans-select","title":"Sous-requ\u00eates dans SELECT","text":"<pre><code>-- \u00c9viter les sous-requ\u00eates dans SELECT si possible\n-- Avant : Sous-requ\u00eate ex\u00e9cut\u00e9e pour chaque ligne\nSELECT \n    u.*,\n    (SELECT MAX(created_at) FROM orders WHERE user_id = u.id) AS last_order_date\nFROM users u;\n\n-- Apr\u00e8s : JOIN avec agr\u00e9gation\nSELECT \n    u.*,\n    o.last_order_date\nFROM users u\nLEFT JOIN (\n    SELECT user_id, MAX(created_at) AS last_order_date\n    FROM orders\n    GROUP BY user_id\n) o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#cte-common-table-expressions","title":"CTE (Common Table Expressions)","text":"<pre><code>-- CTE pour am\u00e9liorer la lisibilit\u00e9 et parfois la performance\nWITH active_users AS (\n    SELECT * FROM users WHERE active = true\n),\nrecent_orders AS (\n    SELECT * FROM orders WHERE created_at &gt; '2024-01-01'\n)\nSELECT \n    u.*,\n    COUNT(o.id) AS order_count\nFROM active_users u\nLEFT JOIN recent_orders o ON u.id = o.user_id\nGROUP BY u.id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#partitionnement","title":"Partitionnement","text":""},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#partitionnement-par-plage-range","title":"Partitionnement par plage (Range)","text":"<pre><code>-- Cr\u00e9er une table partitionn\u00e9e\nCREATE TABLE orders (\n    id SERIAL,\n    user_id INTEGER,\n    amount DECIMAL,\n    created_at DATE\n) PARTITION BY RANGE (created_at);\n\n-- Cr\u00e9er les partitions\nCREATE TABLE orders_2024_q1 PARTITION OF orders\n    FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');\n\nCREATE TABLE orders_2024_q2 PARTITION OF orders\n    FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');\n\nCREATE TABLE orders_2024_q3 PARTITION OF orders\n    FOR VALUES FROM ('2024-07-01') TO ('2024-10-01');\n\nCREATE TABLE orders_2024_q4 PARTITION OF orders\n    FOR VALUES FROM ('2024-10-01') TO ('2025-01-01');\n</code></pre> <p>Avantages : - \u2705 Partition pruning (seules les partitions pertinentes sont scann\u00e9es) - \u2705 Maintenance par partition (VACUUM, ANALYZE) - \u2705 Suppression rapide de partitions enti\u00e8res</p>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#partitionnement-par-liste-list","title":"Partitionnement par liste (List)","text":"<pre><code>-- Partitionnement par r\u00e9gion\nCREATE TABLE users (\n    id SERIAL,\n    name TEXT,\n    region TEXT\n) PARTITION BY LIST (region);\n\nCREATE TABLE users_europe PARTITION OF users\n    FOR VALUES IN ('FR', 'DE', 'UK', 'IT');\n\nCREATE TABLE users_america PARTITION OF users\n    FOR VALUES IN ('US', 'CA', 'MX');\n\nCREATE TABLE users_asia PARTITION OF users\n    FOR VALUES IN ('JP', 'CN', 'IN');\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#partitionnement-par-hash","title":"Partitionnement par hash","text":"<pre><code>-- Partitionnement par hash (pour distribution uniforme)\nCREATE TABLE events (\n    id SERIAL,\n    user_id INTEGER,\n    event_type TEXT,\n    created_at TIMESTAMP\n) PARTITION BY HASH (user_id);\n\nCREATE TABLE events_0 PARTITION OF events\n    FOR VALUES WITH (MODULUS 4, REMAINDER 0);\n\nCREATE TABLE events_1 PARTITION OF events\n    FOR VALUES WITH (MODULUS 4, REMAINDER 1);\n\nCREATE TABLE events_2 PARTITION OF events\n    FOR VALUES WITH (MODULUS 4, REMAINDER 2);\n\nCREATE TABLE events_3 PARTITION OF events\n    FOR VALUES WITH (MODULUS 4, REMAINDER 3);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#index-sur-tables-partitionnees","title":"Index sur tables partitionn\u00e9es","text":"<pre><code>-- Cr\u00e9er un index sur la table partitionn\u00e9e (cr\u00e9\u00e9 sur toutes les partitions)\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n\n-- Ou cr\u00e9er des index sp\u00e9cifiques par partition\nCREATE INDEX idx_orders_2024_q1_user_id ON orders_2024_q1(user_id);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#maintenance-des-partitions","title":"Maintenance des partitions","text":"<pre><code>-- V\u00e9rifier les partitions\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\nFROM pg_tables\nWHERE schemaname = 'public'\n  AND tablename LIKE 'orders_%'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\n-- Supprimer une partition (tr\u00e8s rapide)\nDROP TABLE orders_2024_q1;  -- Supprime la partition et ses donn\u00e9es\n\n-- D\u00e9tacher une partition (garder les donn\u00e9es)\nALTER TABLE orders DETACH PARTITION orders_2024_q1;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#parallelisme","title":"Parall\u00e9lisme","text":""},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#configuration-du-parallelisme","title":"Configuration du parall\u00e9lisme","text":"<pre><code>-- V\u00e9rifier la configuration\nSHOW max_parallel_workers_per_gather;\nSHOW max_parallel_workers;\nSHOW max_worker_processes;\n\n-- Modifier (dans postgresql.conf)\n-- max_parallel_workers_per_gather = 4\n-- max_parallel_workers = 8\n-- max_worker_processes = 8\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#quand-le-parallelisme-est-utilise","title":"Quand le parall\u00e9lisme est utilis\u00e9","text":"<p>PostgreSQL utilise le parall\u00e9lisme pour : - \u2705 Scans s\u00e9quentiels de grandes tables - \u2705 Jointures sur grandes tables - \u2705 Agr\u00e9gations sur grandes tables - \u274c Petites tables (&lt; 8MB par d\u00e9faut) - \u274c Requ\u00eates avec verrous</p>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#forcer-le-parallelisme","title":"Forcer le parall\u00e9lisme","text":"<pre><code>-- Augmenter min_parallel_table_scan_size pour forcer le parall\u00e9lisme\nSET min_parallel_table_scan_size = 0;  -- Toujours consid\u00e9rer le parall\u00e9lisme\n\n-- Voir le plan avec parall\u00e9lisme\nEXPLAIN ANALYZE\nSELECT COUNT(*) FROM large_table WHERE condition = 'value';\n</code></pre> <p>R\u00e9sultat typique : <pre><code>Finalize Aggregate\n  -&gt; Gather\n      Workers Planned: 4\n      -&gt; Partial Aggregate\n          -&gt; Parallel Seq Scan on large_table\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#optimiser-pour-le-parallelisme","title":"Optimiser pour le parall\u00e9lisme","text":"<pre><code>-- Tables avec beaucoup de colonnes : r\u00e9duire work_mem par worker\nSET work_mem = '64MB';\n\n-- Tables partitionn\u00e9es : parall\u00e9lisme par partition\n-- Chaque partition peut \u00eatre scann\u00e9e en parall\u00e8le\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#optimisation-des-types-de-donnees","title":"Optimisation des types de donn\u00e9es","text":""},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#choisir-le-bon-type","title":"Choisir le bon type","text":"<pre><code>-- \u00c9viter TEXT pour des valeurs limit\u00e9es\n-- Avant\nCREATE TABLE users (\n    id SERIAL,\n    status TEXT  -- 'active', 'inactive', 'pending'\n);\n\n-- Apr\u00e8s : Utiliser ENUM ou VARCHAR\nCREATE TYPE user_status AS ENUM ('active', 'inactive', 'pending');\nCREATE TABLE users (\n    id SERIAL,\n    status user_status\n);\n\n-- Ou VARCHAR avec contrainte\nCREATE TABLE users (\n    id SERIAL,\n    status VARCHAR(20) CHECK (status IN ('active', 'inactive', 'pending'))\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#types-numeriques","title":"Types num\u00e9riques","text":"<pre><code>-- Utiliser le type le plus petit possible\n-- Avant\nCREATE TABLE products (\n    id BIGINT,  -- Si jamais &gt; 2 milliards\n    price DECIMAL(10,2)\n);\n\n-- Apr\u00e8s : Adapter selon les besoins\nCREATE TABLE products (\n    id INTEGER,  -- Suffisant pour &lt; 2 milliards\n    price NUMERIC(10,2)  -- NUMERIC = DECIMAL\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#types-de-dateheure","title":"Types de date/heure","text":"<pre><code>-- Utiliser TIMESTAMP WITH TIME ZONE pour les dates/heures\nCREATE TABLE events (\n    id SERIAL,\n    created_at TIMESTAMPTZ,  -- Stocke avec timezone\n    event_date DATE  -- Pour les dates uniquement\n);\n\n-- Index sur les dates\nCREATE INDEX idx_events_created_at ON events(created_at);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#json-vs-colonnes-normales","title":"JSON vs colonnes normales","text":"<pre><code>-- JSON : Flexible mais plus lent\nCREATE TABLE products (\n    id SERIAL,\n    metadata JSONB\n);\n\n-- Colonnes normales : Plus rapide si structure fixe\nCREATE TABLE products (\n    id SERIAL,\n    brand TEXT,\n    category TEXT,\n    tags TEXT[]\n);\n\n-- Index GIN pour JSONB\nCREATE INDEX idx_products_metadata_gin ON products USING gin(metadata);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Index sur les cl\u00e9s de jointure : Essentiel pour les jointures rapides</li> <li>Filtrer avant d'agr\u00e9ger : R\u00e9duire la taille des donn\u00e9es</li> <li>\u00c9viter les sous-requ\u00eates corr\u00e9l\u00e9es : Utiliser JOIN \u00e0 la place</li> <li>Partitionner les grandes tables : Am\u00e9liore les performances et la maintenance</li> <li>Parall\u00e9lisme : Automatique, mais configurable</li> <li>Types de donn\u00e9es : Choisir le plus appropri\u00e9</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/05-techniques/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 6. Cas pratiques pour voir des exemples concrets d'optimisation.</p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/","title":"6. Cas pratiques d'optimisation","text":""},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Appliquer les techniques d'optimisation sur des cas r\u00e9els</li> <li>Analyser les probl\u00e8mes de performance</li> <li>Mesurer l'impact des optimisations</li> <li>Utiliser Dalibo pour identifier et r\u00e9soudre les probl\u00e8mes</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#table-des-matieres","title":"\ud83d\udccb Table des mati\u00e8res","text":"<ol> <li>Cas 1 : Requ\u00eate lente avec scan s\u00e9quentiel</li> <li>Cas 2 : Jointure lente sur grande table</li> <li>Cas 3 : Agr\u00e9gation lente</li> <li>Cas 4 : Sous-requ\u00eate corr\u00e9l\u00e9e</li> <li>Cas 5 : Probl\u00e8me de cache hit ratio</li> <li>Cas 6 : Index non utilis\u00e9s</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#cas-1-requete-lente-avec-scan-sequentiel","title":"Cas 1 : Requ\u00eate lente avec scan s\u00e9quentiel","text":""},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#probleme-initial","title":"Probl\u00e8me initial","text":"<p>Requ\u00eate : <pre><code>SELECT * FROM users WHERE email = 'user@example.com';\n</code></pre></p> <p>Plan d'ex\u00e9cution : <pre><code>Seq Scan on users  (cost=0.00..25000.00 rows=1 width=64)\n  (actual time=0.123..1500.456 rows=1 loops=1)\n  Filter: (email = 'user@example.com'::text)\n  Rows Removed by Filter: 999999\nPlanning Time: 0.234 ms\nExecution Time: 1500.678 ms\n</code></pre></p> <p>Probl\u00e8mes identifi\u00e9s : - \ud83d\udd34 Scan s\u00e9quentiel sur 1 million de lignes - \ud83d\udd34 Temps d'ex\u00e9cution : 1.5 secondes - \ud83d\udd34 999999 lignes filtr\u00e9es</p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#analyse-avec-dalibo","title":"Analyse avec Dalibo","text":"<pre><code>-- V\u00e9rifier dans pg_stat_statements\nSELECT \n    query,\n    calls,\n    mean_exec_time,\n    shared_blks_read,\n    shared_blks_hit\nFROM pg_stat_statements\nWHERE query LIKE '%users WHERE email%'\nORDER BY mean_exec_time DESC;\n\n-- V\u00e9rifier avec pg_qualstats\nSELECT \n    left_table,\n    left_column,\n    operator,\n    execution_count\nFROM pg_qualstats\nWHERE left_table = 'users' AND left_column = 'email';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#solution","title":"Solution","text":"<pre><code>-- Cr\u00e9er un index sur email\nCREATE INDEX idx_users_email ON users(email);\n\n-- V\u00e9rifier le nouveau plan\nEXPLAIN ANALYZE\nSELECT * FROM users WHERE email = 'user@example.com';\n</code></pre> <p>Plan optimis\u00e9 : <pre><code>Index Scan using idx_users_email on users\n  (cost=0.42..8.44 rows=1 width=64)\n  (actual time=0.123..0.125 rows=1 loops=1)\n  Index Cond: (email = 'user@example.com'::text)\nPlanning Time: 0.234 ms\nExecution Time: 0.125 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#resultats","title":"R\u00e9sultats","text":"M\u00e9trique Avant Apr\u00e8s Am\u00e9lioration Temps d'ex\u00e9cution 1500ms 0.125ms 99.99% Type de scan Seq Scan Index Scan \u2705 Lignes scann\u00e9es 1,000,000 1 \u2705"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#cas-2-jointure-lente-sur-grande-table","title":"Cas 2 : Jointure lente sur grande table","text":""},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#probleme-initial_1","title":"Probl\u00e8me initial","text":"<p>Requ\u00eate : <pre><code>SELECT \n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_amount\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01'\nGROUP BY u.id, u.name, u.email;\n</code></pre></p> <p>Plan d'ex\u00e9cution : <pre><code>Hash Join\n  (cost=125000.00..250000.00 rows=50000 width=64)\n  (actual time=5000.123..15000.456 rows=45000 loops=1)\n  Hash Cond: (o.user_id = u.id)\n  -&gt; Seq Scan on orders o\n      (cost=0.00..100000.00 rows=1000000 width=16)\n      (actual time=0.123..5000.456 rows=1000000 loops=1)\n  -&gt; Hash\n      (cost=25000.00..25000.00 rows=100000 width=48)\n      (actual time=2000.123..2000.123 rows=100000 loops=1)\n      Buckets: 131072  Batches: 8  Memory Usage: 5120kB\n      -&gt; Seq Scan on users u\n          (cost=0.00..25000.00 rows=100000 width=48)\n          (actual time=0.089..1000.567 rows=100000 loops=1)\n          Filter: (created_at &gt; '2024-01-01'::date)\nPlanning Time: 50.234 ms\nExecution Time: 15000.678 ms\n</code></pre></p> <p>Probl\u00e8mes identifi\u00e9s : - \ud83d\udd34 Hash Join avec 8 batches (tri sur disque) - \ud83d\udd34 Scan s\u00e9quentiel sur orders (1 million de lignes) - \ud83d\udd34 Temps d'ex\u00e9cution : 15 secondes</p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#analyse-avec-dalibo_1","title":"Analyse avec Dalibo","text":"<pre><code>-- Identifier les index manquants\nSELECT \n    qs.left_table,\n    qs.left_column,\n    qs.operator,\n    COUNT(*) AS execution_count\nFROM pg_qualstats qs\nWHERE qs.left_table IN ('users', 'orders')\nGROUP BY qs.left_table, qs.left_column, qs.operator\nORDER BY execution_count DESC;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#solution_1","title":"Solution","text":"<pre><code>-- Cr\u00e9er des index sur les cl\u00e9s de jointure et filtres\nCREATE INDEX idx_orders_user_id ON orders(user_id);\nCREATE INDEX idx_users_created_at ON users(created_at);\n\n-- Index composite pour la requ\u00eate compl\u00e8te\nCREATE INDEX idx_orders_user_id_amount ON orders(user_id, amount);\n\n-- Augmenter work_mem pour \u00e9viter les batches\nSET work_mem = '256MB';\n\n-- V\u00e9rifier le nouveau plan\nEXPLAIN ANALYZE\nSELECT \n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_amount\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01'\nGROUP BY u.id, u.name, u.email;\n</code></pre> <p>Plan optimis\u00e9 : <pre><code>Hash Join\n  (cost=5000.00..15000.00 rows=50000 width=64)\n  (actual time=200.123..800.456 rows=45000 loops=1)\n  Hash Cond: (o.user_id = u.id)\n  -&gt; Index Scan using idx_orders_user_id on orders o\n      (cost=0.42..8000.00 rows=500000 width=16)\n      (actual time=0.123..300.456 rows=500000 loops=1)\n  -&gt; Hash\n      (cost=2500.00..2500.00 rows=100000 width=48)\n      (actual time=100.123..100.123 rows=100000 loops=1)\n      Buckets: 131072  Batches: 1  Memory Usage: 5120kB\n      -&gt; Index Scan using idx_users_created_at on users u\n          (cost=0.42..2500.00 rows=100000 width=48)\n          (actual time=0.089..50.567 rows=100000 loops=1)\n          Index Cond: (created_at &gt; '2024-01-01'::date)\nPlanning Time: 5.234 ms\nExecution Time: 800.678 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#resultats_1","title":"R\u00e9sultats","text":"M\u00e9trique Avant Apr\u00e8s Am\u00e9lioration Temps d'ex\u00e9cution 15000ms 800ms 94.7% Batches Hash Join 8 1 \u2705 Type de scan orders Seq Scan Index Scan \u2705 Lignes scann\u00e9es 1,000,000 500,000 \u2705"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#cas-3-agregation-lente","title":"Cas 3 : Agr\u00e9gation lente","text":""},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#probleme-initial_2","title":"Probl\u00e8me initial","text":"<p>Requ\u00eate : <pre><code>SELECT \n    status,\n    COUNT(*) AS count,\n    AVG(amount) AS avg_amount,\n    SUM(amount) AS total_amount\nFROM orders\nWHERE created_at BETWEEN '2024-01-01' AND '2024-12-31'\nGROUP BY status\nORDER BY count DESC;\n</code></pre></p> <p>Plan d'ex\u00e9cution : <pre><code>Sort\n  (cost=50000.00..50000.00 rows=5 width=32)\n  (actual time=10000.123..10000.234 rows=5 loops=1)\n  Sort Key: (count(*)) DESC\n  Sort Method: quicksort  Memory: 25kB\n  -&gt; HashAggregate\n      (cost=45000.00..45000.00 rows=5 width=32)\n      (actual time=8000.123..8000.456 rows=5 loops=1)\n      Group Key: status\n      Batches: 1  Memory Usage: 24kB\n      -&gt; Seq Scan on orders\n          (cost=0.00..40000.00 rows=2000000 width=16)\n          (actual time=0.123..5000.456 rows=2000000 loops=1)\n          Filter: ((created_at &gt;= '2024-01-01'::date) \n                   AND (created_at &lt;= '2024-12-31'::date))\n          Rows Removed by Filter: 0\nPlanning Time: 10.234 ms\nExecution Time: 10000.678 ms\n</code></pre></p> <p>Probl\u00e8mes identifi\u00e9s : - \ud83d\udd34 Scan s\u00e9quentiel sur 2 millions de lignes - \ud83d\udd34 Temps d'ex\u00e9cution : 10 secondes - \ud83d\udd34 Pas d'index sur created_at</p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#solution_2","title":"Solution","text":"<pre><code>-- Cr\u00e9er un index sur created_at et status\nCREATE INDEX idx_orders_created_at_status ON orders(created_at, status);\n\n-- Alternative : Index partiel si certaines status sont rares\nCREATE INDEX idx_orders_created_at_status_partial \nON orders(created_at, status) \nWHERE status IN ('pending', 'processing');\n\n-- V\u00e9rifier le nouveau plan\nEXPLAIN ANALYZE\nSELECT \n    status,\n    COUNT(*) AS count,\n    AVG(amount) AS avg_amount,\n    SUM(amount) AS total_amount\nFROM orders\nWHERE created_at BETWEEN '2024-01-01' AND '2024-12-31'\nGROUP BY status\nORDER BY count DESC;\n</code></pre> <p>Plan optimis\u00e9 : <pre><code>Sort\n  (cost=5000.00..5000.00 rows=5 width=32)\n  (actual time=500.123..500.234 rows=5 loops=1)\n  Sort Key: (count(*)) DESC\n  Sort Method: quicksort  Memory: 25kB\n  -&gt; HashAggregate\n      (cost=4500.00..4500.00 rows=5 width=32)\n      (actual time=400.123..400.456 rows=5 loops=1)\n      Group Key: status\n      Batches: 1  Memory Usage: 24kB\n      -&gt; Index Scan using idx_orders_created_at_status on orders\n          (cost=0.42..4000.00 rows=2000000 width=16)\n          (actual time=0.123..200.456 rows=2000000 loops=1)\n          Index Cond: ((created_at &gt;= '2024-01-01'::date) \n                       AND (created_at &lt;= '2024-12-31'::date))\nPlanning Time: 5.234 ms\nExecution Time: 500.678 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#resultats_2","title":"R\u00e9sultats","text":"M\u00e9trique Avant Apr\u00e8s Am\u00e9lioration Temps d'ex\u00e9cution 10000ms 500ms 95% Type de scan Seq Scan Index Scan \u2705 Lignes scann\u00e9es 2,000,000 2,000,000 (m\u00eame nombre, mais index)"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#cas-4-sous-requete-correlee","title":"Cas 4 : Sous-requ\u00eate corr\u00e9l\u00e9e","text":""},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#probleme-initial_3","title":"Probl\u00e8me initial","text":"<p>Requ\u00eate : <pre><code>SELECT \n    u.id,\n    u.name,\n    u.email,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) AS order_count,\n    (SELECT MAX(created_at) FROM orders o WHERE o.user_id = u.id) AS last_order_date,\n    (SELECT SUM(amount) FROM orders o WHERE o.user_id = u.id) AS total_spent\nFROM users u\nWHERE u.active = true;\n</code></pre></p> <p>Plan d'ex\u00e9cution : <pre><code>Seq Scan on users u\n  (cost=0.00..250000.00 rows=100000 width=64)\n  (actual time=0.123..50000.456 rows=100000 loops=1)\n  Filter: (active = true)\n  SubPlan 1\n    -&gt; Aggregate\n        (cost=2.50..2.50 rows=1 width=8)\n        (actual time=0.100..0.100 rows=1 loops=100000)\n        -&gt; Seq Scan on orders o\n            (cost=0.00..2.25 rows=10 width=0)\n            (actual time=0.050..0.050 rows=5 loops=100000)\n            Filter: (user_id = u.id)\n  SubPlan 2\n    -&gt; Result\n        (cost=2.50..2.50 rows=1 width=8)\n        (actual time=0.100..0.100 rows=1 loops=100000)\n        -&gt; Aggregate\n            (cost=2.50..2.50 rows=1 width=8)\n            (actual time=0.100..0.100 rows=1 loops=100000)\n            -&gt; Seq Scan on orders o\n                (cost=0.00..2.25 rows=10 width=0)\n                (actual time=0.050..0.050 rows=5 loops=100000)\n                Filter: (user_id = u.id)\n  SubPlan 3\n    -&gt; Aggregate\n        (cost=2.50..2.50 rows=1 width=8)\n        (actual time=0.100..0.100 rows=1 loops=100000)\n        -&gt; Seq Scan on orders o\n            (cost=0.00..2.25 rows=10 width=0)\n            (actual time=0.050..0.050 rows=5 loops=100000)\n            Filter: (user_id = u.id)\nPlanning Time: 5.234 ms\nExecution Time: 50000.678 ms\n</code></pre></p> <p>Probl\u00e8mes identifi\u00e9s : - \ud83d\udd34 3 sous-requ\u00eates corr\u00e9l\u00e9es ex\u00e9cut\u00e9es 100,000 fois chacune - \ud83d\udd34 300,000 scans s\u00e9quentiels sur orders - \ud83d\udd34 Temps d'ex\u00e9cution : 50 secondes</p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#solution_3","title":"Solution","text":"<pre><code>-- Remplacer par des JOIN avec agr\u00e9gation\nSELECT \n    u.id,\n    u.name,\n    u.email,\n    COALESCE(o.order_count, 0) AS order_count,\n    o.last_order_date,\n    COALESCE(o.total_spent, 0) AS total_spent\nFROM users u\nLEFT JOIN (\n    SELECT \n        user_id,\n        COUNT(*) AS order_count,\n        MAX(created_at) AS last_order_date,\n        SUM(amount) AS total_spent\n    FROM orders\n    GROUP BY user_id\n) o ON u.id = o.user_id\nWHERE u.active = true;\n\n-- Cr\u00e9er un index pour acc\u00e9l\u00e9rer la jointure\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n</code></pre> <p>Plan optimis\u00e9 : <pre><code>Hash Right Join\n  (cost=5000.00..15000.00 rows=100000 width=64)\n  (actual time=200.123..800.456 rows=100000 loops=1)\n  Hash Cond: (o.user_id = u.id)\n  -&gt; HashAggregate\n      (cost=4000.00..4500.00 rows=50000 width=24)\n      (actual time=150.123..200.456 rows=50000 loops=1)\n      Group Key: orders.user_id\n      Batches: 1  Memory Usage: 5120kB\n      -&gt; Index Scan using idx_orders_user_id on orders\n          (cost=0.42..3000.00 rows=500000 width=16)\n          (actual time=0.123..100.456 rows=500000 loops=1)\n  -&gt; Hash\n      (cost=2000.00..2000.00 rows=100000 width=48)\n      (actual time=50.123..50.123 rows=100000 loops=1)\n      Buckets: 131072  Batches: 1  Memory Usage: 5120kB\n      -&gt; Seq Scan on users u\n          (cost=0.00..2000.00 rows=100000 width=48)\n          (actual time=0.089..25.567 rows=100000 loops=1)\n          Filter: (active = true)\nPlanning Time: 5.234 ms\nExecution Time: 800.678 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#resultats_3","title":"R\u00e9sultats","text":"M\u00e9trique Avant Apr\u00e8s Am\u00e9lioration Temps d'ex\u00e9cution 50000ms 800ms 98.4% Scans sur orders 300,000 1 \u2705 Type d'op\u00e9ration Sous-requ\u00eates Hash Join \u2705"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#cas-5-probleme-de-cache-hit-ratio","title":"Cas 5 : Probl\u00e8me de cache hit ratio","text":""},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#probleme-initial_4","title":"Probl\u00e8me initial","text":"<p>M\u00e9triques : <pre><code>-- Cache hit ratio global\nSELECT \n    ROUND(100.0 * SUM(shared_blks_hit) / \n          NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2) AS cache_hit_ratio\nFROM pg_stat_statements;\n-- R\u00e9sultat: 75% (objectif: &gt; 95%)\n</code></pre></p> <p>Requ\u00eates avec beaucoup de lectures disque : <pre><code>SELECT \n    LEFT(query, 100) AS query_preview,\n    shared_blks_read,\n    shared_blks_hit,\n    ROUND(100.0 * shared_blks_hit / \n          NULLIF(shared_blks_hit + shared_blks_read, 0), 2) AS cache_hit_ratio,\n    ROUND((shared_blks_read * 8)::numeric / 1024, 2) AS disk_read_mb\nFROM pg_stat_statements\nWHERE shared_blks_read &gt; 1000\nORDER BY shared_blks_read DESC\nLIMIT 10;\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#solution_4","title":"Solution","text":"<pre><code>-- 1. Augmenter shared_buffers (dans postgresql.conf)\n-- shared_buffers = 4GB  (25% de RAM pour serveur d\u00e9di\u00e9)\n\n-- 2. Pr\u00e9charger les tables fr\u00e9quemment utilis\u00e9es\n-- Cr\u00e9er une fonction de pr\u00e9chargement\nCREATE OR REPLACE FUNCTION pg_prewarm_table(table_name TEXT)\nRETURNS void AS $$\nBEGIN\n    EXECUTE format('SELECT * FROM %I LIMIT 1', table_name);\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Pr\u00e9charger les tables importantes\nSELECT pg_prewarm_table('users');\nSELECT pg_prewarm_table('orders');\nSELECT pg_prewarm_table('products');\n\n-- 3. Utiliser pg_prewarm extension\nCREATE EXTENSION IF NOT EXISTS pg_prewarm;\n\n-- Pr\u00e9charger une table compl\u00e8te\nSELECT pg_prewarm('users');\nSELECT pg_prewarm('orders');\n</code></pre> <p>Apr\u00e8s optimisation : <pre><code>-- V\u00e9rifier l'am\u00e9lioration\nSELECT \n    ROUND(100.0 * SUM(shared_blks_hit) / \n          NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2) AS cache_hit_ratio\nFROM pg_stat_statements;\n-- R\u00e9sultat: 98% \u2705\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#resultats_4","title":"R\u00e9sultats","text":"M\u00e9trique Avant Apr\u00e8s Am\u00e9lioration Cache hit ratio 75% 98% +23% Lectures disque \u00c9lev\u00e9es Faibles \u2705 Temps de r\u00e9ponse Variable Stable \u2705"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#cas-6-index-non-utilises","title":"Cas 6 : Index non utilis\u00e9s","text":""},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#probleme-initial_5","title":"Probl\u00e8me initial","text":"<p>Identification des index non utilis\u00e9s : <pre><code>-- Index jamais utilis\u00e9s\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,\n    idx_scan AS index_scans,\n    pg_relation_size(indexrelid) AS size_bytes\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\n  AND pg_relation_size(indexrelid) &gt; 1048576  -- &gt; 1MB\nORDER BY pg_relation_size(indexrelid) DESC;\n</code></pre></p> <p>R\u00e9sultat : <pre><code> schemaname | tablename |      indexname       | index_size | index_scans | size_bytes\n------------+-----------+----------------------+------------+-------------+------------\n public     | orders    | idx_orders_old_field | 250 MB     |           0 |  262144000\n public     | users     | idx_users_old_email  | 150 MB     |           0 |  157286400\n</code></pre></p> <p>Impact : - \ud83d\udd34 400 MB d'espace disque perdu - \ud83d\udd34 Ralentissement des INSERT/UPDATE - \ud83d\udd34 Maintenance inutile</p>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#solution_5","title":"Solution","text":"<pre><code>-- 1. V\u00e9rifier avec HypoPG si l'index est vraiment inutile\nCREATE EXTENSION IF NOT EXISTS hypopg;\n\n-- 2. Analyser les requ\u00eates qui pourraient utiliser l'index\nSELECT \n    query,\n    calls,\n    mean_exec_time\nFROM pg_stat_statements\nWHERE query LIKE '%old_field%' OR query LIKE '%old_email%';\n\n-- 3. Si vraiment inutile, supprimer l'index\nDROP INDEX idx_orders_old_field;\nDROP INDEX idx_users_old_email;\n\n-- 4. V\u00e9rifier l'espace lib\u00e9r\u00e9\nSELECT \n    pg_size_pretty(pg_database_size(current_database())) AS database_size;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#resultats_5","title":"R\u00e9sultats","text":"M\u00e9trique Avant Apr\u00e8s Am\u00e9lioration Espace index 400 MB 0 MB -400 MB Temps INSERT +10% Normal \u2705 Temps UPDATE +15% Normal \u2705"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#points-cles-a-retenir","title":"\ud83d\udcca Points cl\u00e9s \u00e0 retenir","text":"<ol> <li>Toujours analyser avec EXPLAIN ANALYZE avant d'optimiser</li> <li>Utiliser Dalibo pour identifier les probl\u00e8mes automatiquement</li> <li>Mesurer l'impact avant et apr\u00e8s chaque optimisation</li> <li>Index appropri\u00e9s : Solution la plus courante</li> <li>\u00c9viter les sous-requ\u00eates corr\u00e9l\u00e9es : Utiliser JOIN</li> <li>Surveiller r\u00e9guli\u00e8rement : Les probl\u00e8mes \u00e9voluent</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/06-cas-pratiques/#prochain-module","title":"\ud83d\udd17 Prochain module","text":"<p>Passer au module 7. Exercices pour pratiquer avec des exercices guid\u00e9s.</p>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/","title":"7. Exercices pratiques","text":""},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<ul> <li>Appliquer les connaissances acquises</li> <li>R\u00e9soudre des probl\u00e8mes de performance</li> <li>Analyser et optimiser des requ\u00eates r\u00e9elles</li> <li>Utiliser Dalibo pour l'analyse</li> </ul>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#structure-des-exercices","title":"\ud83d\udccb Structure des exercices","text":"<p>Les exercices sont organis\u00e9s par niveau de difficult\u00e9 : - \ud83d\udfe2 D\u00e9butant : Concepts de base - \ud83d\udfe1 Interm\u00e9diaire : Techniques avanc\u00e9es - \ud83d\udd34 Avanc\u00e9 : Optimisations complexes</p>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#exercice-1-analyse-dune-requete-simple-debutant","title":"Exercice 1 : Analyse d'une requ\u00eate simple (\ud83d\udfe2 D\u00e9butant)","text":""},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#contexte","title":"Contexte","text":"<p>Vous avez une table <code>products</code> avec 1 million de lignes et une requ\u00eate lente :</p> <pre><code>SELECT * FROM products WHERE category = 'electronics';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#taches","title":"T\u00e2ches","text":"<ol> <li>Analyser la requ\u00eate avec <code>EXPLAIN ANALYZE</code></li> <li>Identifier le probl\u00e8me dans le plan d'ex\u00e9cution</li> <li>Proposer une solution avec un index</li> <li>V\u00e9rifier l'am\u00e9lioration avec <code>EXPLAIN ANALYZE</code></li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#donnees-de-test","title":"Donn\u00e9es de test","text":"<pre><code>-- Cr\u00e9er la table\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    category TEXT,\n    price DECIMAL(10,2),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Ins\u00e9rer des donn\u00e9es de test (1 million de lignes)\nINSERT INTO products (name, category, price)\nSELECT \n    'Product ' || i,\n    CASE (i % 10)\n        WHEN 0 THEN 'electronics'\n        WHEN 1 THEN 'clothing'\n        WHEN 2 THEN 'books'\n        WHEN 3 THEN 'food'\n        WHEN 4 THEN 'toys'\n        WHEN 5 THEN 'electronics'\n        WHEN 6 THEN 'furniture'\n        WHEN 7 THEN 'electronics'\n        WHEN 8 THEN 'sports'\n        ELSE 'other'\n    END,\n    (RANDOM() * 1000)::DECIMAL(10,2)\nFROM generate_series(1, 1000000) i;\n\n-- Analyser la table\nANALYZE products;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#solution-attendue","title":"Solution attendue","text":"Cliquez pour voir la solution <pre><code>-- 1. Analyser la requ\u00eate\nEXPLAIN ANALYZE\nSELECT * FROM products WHERE category = 'electronics';\n\n-- Probl\u00e8me identifi\u00e9 : Seq Scan sur 1 million de lignes\n\n-- 2. Cr\u00e9er un index\nCREATE INDEX idx_products_category ON products(category);\n\n-- 3. Analyser \u00e0 nouveau\nEXPLAIN ANALYZE\nSELECT * FROM products WHERE category = 'electronics';\n\n-- R\u00e9sultat attendu : Index Scan avec temps d'ex\u00e9cution &lt; 100ms\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#exercice-2-optimisation-dune-jointure-intermediaire","title":"Exercice 2 : Optimisation d'une jointure (\ud83d\udfe1 Interm\u00e9diaire)","text":""},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#contexte_1","title":"Contexte","text":"<p>Vous avez deux tables <code>users</code> et <code>orders</code> avec une jointure lente :</p> <pre><code>SELECT \n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_amount\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01'\nGROUP BY u.id, u.name, u.email\nHAVING COUNT(o.id) &gt; 5;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#taches_1","title":"T\u00e2ches","text":"<ol> <li>Analyser la requ\u00eate avec <code>EXPLAIN ANALYZE</code></li> <li>Identifier les probl\u00e8mes (scans, jointures, agr\u00e9gations)</li> <li>Cr\u00e9er les index n\u00e9cessaires</li> <li>Optimiser la requ\u00eate (HAVING \u2192 WHERE si possible)</li> <li>Mesurer l'am\u00e9lioration</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#donnees-de-test_1","title":"Donn\u00e9es de test","text":"<pre><code>-- Cr\u00e9er les tables\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    email TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER REFERENCES users(id),\n    amount DECIMAL(10,2),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Ins\u00e9rer des donn\u00e9es\nINSERT INTO users (name, email, created_at)\nSELECT \n    'User ' || i,\n    'user' || i || '@example.com',\n    NOW() - (RANDOM() * 365 || ' days')::INTERVAL\nFROM generate_series(1, 100000) i;\n\nINSERT INTO orders (user_id, amount, created_at)\nSELECT \n    (RANDOM() * 100000)::INTEGER + 1,\n    (RANDOM() * 1000)::DECIMAL(10,2),\n    NOW() - (RANDOM() * 365 || ' days')::INTERVAL\nFROM generate_series(1, 1000000) i;\n\nANALYZE users, orders;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#solution-attendue_1","title":"Solution attendue","text":"Cliquez pour voir la solution <pre><code>-- 1. Analyser la requ\u00eate\nEXPLAIN ANALYZE\nSELECT \n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_amount\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01'\nGROUP BY u.id, u.name, u.email\nHAVING COUNT(o.id) &gt; 5;\n\n-- Probl\u00e8mes identifi\u00e9s :\n-- - Pas d'index sur users.created_at\n-- - Pas d'index sur orders.user_id\n-- - HAVING peut \u00eatre optimis\u00e9\n\n-- 2. Cr\u00e9er les index\nCREATE INDEX idx_users_created_at ON users(created_at);\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n\n-- 3. Optimiser la requ\u00eate (filtrer dans la sous-requ\u00eate)\nSELECT \n    u.name,\n    u.email,\n    o.order_count,\n    o.total_amount\nFROM users u\nJOIN (\n    SELECT \n        user_id,\n        COUNT(*) AS order_count,\n        SUM(amount) AS total_amount\n    FROM orders\n    GROUP BY user_id\n    HAVING COUNT(*) &gt; 5\n) o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01';\n\n-- 4. Analyser \u00e0 nouveau\nEXPLAIN ANALYZE [requ\u00eate optimis\u00e9e];\n\n-- R\u00e9sultat attendu : Temps d'ex\u00e9cution r\u00e9duit de 80%+\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#exercice-3-sous-requete-correlee-intermediaire","title":"Exercice 3 : Sous-requ\u00eate corr\u00e9l\u00e9e (\ud83d\udfe1 Interm\u00e9diaire)","text":""},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#contexte_2","title":"Contexte","text":"<p>Une requ\u00eate avec sous-requ\u00eates corr\u00e9l\u00e9es tr\u00e8s lente :</p> <pre><code>SELECT \n    u.id,\n    u.name,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) AS order_count,\n    (SELECT MAX(created_at) FROM orders o WHERE o.user_id = u.id) AS last_order_date,\n    (SELECT AVG(amount) FROM orders o WHERE o.user_id = u.id) AS avg_order_amount\nFROM users u\nWHERE u.active = true;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#taches_2","title":"T\u00e2ches","text":"<ol> <li>Analyser la requ\u00eate et identifier les sous-requ\u00eates corr\u00e9l\u00e9es</li> <li>Convertir en JOIN avec agr\u00e9gation</li> <li>Cr\u00e9er les index n\u00e9cessaires</li> <li>Comparer les performances</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#solution-attendue_2","title":"Solution attendue","text":"Cliquez pour voir la solution <pre><code>-- 1. Analyser\nEXPLAIN ANALYZE [requ\u00eate originale];\n-- Probl\u00e8me : 3 sous-requ\u00eates ex\u00e9cut\u00e9es pour chaque ligne\n\n-- 2. Convertir en JOIN\nSELECT \n    u.id,\n    u.name,\n    COALESCE(o.order_count, 0) AS order_count,\n    o.last_order_date,\n    COALESCE(o.avg_order_amount, 0) AS avg_order_amount\nFROM users u\nLEFT JOIN (\n    SELECT \n        user_id,\n        COUNT(*) AS order_count,\n        MAX(created_at) AS last_order_date,\n        AVG(amount) AS avg_order_amount\n    FROM orders\n    GROUP BY user_id\n) o ON u.id = o.user_id\nWHERE u.active = true;\n\n-- 3. Cr\u00e9er index\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n\n-- 4. Comparer\n-- R\u00e9sultat attendu : Am\u00e9lioration de 95%+\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#exercice-4-analyse-avec-dalibo-avance","title":"Exercice 4 : Analyse avec Dalibo (\ud83d\udd34 Avanc\u00e9)","text":""},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#contexte_3","title":"Contexte","text":"<p>Vous devez analyser les performances d'une base de donn\u00e9es de production (simul\u00e9e) et identifier les probl\u00e8mes principaux.</p>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#taches_3","title":"T\u00e2ches","text":"<ol> <li>Activer pg_stat_statements et pg_qualstats</li> <li>Ex\u00e9cuter des requ\u00eates de test pour g\u00e9n\u00e9rer des statistiques</li> <li>Identifier les requ\u00eates lentes avec pg_stat_statements</li> <li>Identifier les index manquants avec pg_qualstats</li> <li>G\u00e9n\u00e9rer un rapport avec recommandations</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#donnees-de-test_2","title":"Donn\u00e9es de test","text":"<pre><code>-- Utiliser les tables de l'exercice 2\n-- Ex\u00e9cuter diverses requ\u00eates pour g\u00e9n\u00e9rer des statistiques\n\n-- Requ\u00eates de test\nSELECT * FROM users WHERE email = 'user50000@example.com';\nSELECT * FROM orders WHERE user_id = 12345;\nSELECT * FROM users WHERE created_at &gt; '2024-06-01';\nSELECT COUNT(*) FROM orders WHERE amount &gt; 500;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#solution-attendue_3","title":"Solution attendue","text":"Cliquez pour voir la solution <pre><code>-- 1. Activer les extensions\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\nCREATE EXTENSION IF NOT EXISTS pg_qualstats;\n\n-- 2. Ex\u00e9cuter les requ\u00eates de test\n[requ\u00eates de test]\n\n-- 3. Identifier les requ\u00eates lentes\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    mean_exec_time,\n    total_exec_time,\n    (total_exec_time / SUM(total_exec_time) OVER ()) * 100 AS percent_total_time\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 10;\n\n-- 4. Identifier les index manquants\nSELECT \n    qs.left_schema,\n    qs.left_table,\n    qs.left_column,\n    qs.operator,\n    COUNT(*) AS execution_count\nFROM pg_qualstats qs\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM pg_index i\n    JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n    WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n    AND a.attname = qs.left_column\n)\nGROUP BY qs.left_schema, qs.left_table, qs.left_column, qs.operator\nORDER BY execution_count DESC;\n\n-- 5. G\u00e9n\u00e9rer des recommandations\nSELECT \n    'CREATE INDEX idx_' || left_table || '_' || left_column || \n    ' ON ' || left_schema || '.' || left_table || \n    ' (' || left_column || ');' AS recommendation,\n    execution_count AS priority\nFROM [requ\u00eate pg_qualstats ci-dessus]\nORDER BY execution_count DESC\nLIMIT 5;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#exercice-5-optimisation-complete-avance","title":"Exercice 5 : Optimisation compl\u00e8te (\ud83d\udd34 Avanc\u00e9)","text":""},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#contexte_4","title":"Contexte","text":"<p>Vous avez une application e-commerce avec des performances d\u00e9grad\u00e9es. Analysez et optimisez le syst\u00e8me complet.</p>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#schema-de-base-de-donnees","title":"Sch\u00e9ma de base de donn\u00e9es","text":"<pre><code>CREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    email TEXT,\n    name TEXT,\n    created_at TIMESTAMP,\n    active BOOLEAN\n);\n\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    category TEXT,\n    price DECIMAL(10,2),\n    stock INTEGER\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    created_at TIMESTAMP,\n    status TEXT,\n    total_amount DECIMAL(10,2)\n);\n\nCREATE TABLE order_items (\n    id SERIAL PRIMARY KEY,\n    order_id INTEGER REFERENCES orders(id),\n    product_id INTEGER REFERENCES products(id),\n    quantity INTEGER,\n    price DECIMAL(10,2)\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#requetes-a-optimiser","title":"Requ\u00eates \u00e0 optimiser","text":"<ol> <li> <p>Requ\u00eate de dashboard : <pre><code>SELECT \n    c.name,\n    c.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.total_amount) AS total_spent,\n    MAX(o.created_at) AS last_order_date\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nWHERE c.active = true\nGROUP BY c.id, c.name, c.email\nHAVING COUNT(o.id) &gt; 0\nORDER BY total_spent DESC\nLIMIT 100;\n</code></pre></p> </li> <li> <p>Requ\u00eate de produits populaires : <pre><code>SELECT \n    p.name,\n    p.category,\n    SUM(oi.quantity) AS total_sold,\n    SUM(oi.quantity * oi.price) AS total_revenue\nFROM products p\nJOIN order_items oi ON p.id = oi.product_id\nJOIN orders o ON oi.order_id = o.id\nWHERE o.created_at &gt; '2024-01-01'\n  AND o.status = 'completed'\nGROUP BY p.id, p.name, p.category\nORDER BY total_sold DESC\nLIMIT 50;\n</code></pre></p> </li> <li> <p>Requ\u00eate de recherche : <pre><code>SELECT \n    p.*,\n    (SELECT COUNT(*) FROM order_items oi WHERE oi.product_id = p.id) AS times_ordered\nFROM products p\nWHERE p.category = 'electronics'\n  AND p.price BETWEEN 100 AND 500\nORDER BY times_ordered DESC;\n</code></pre></p> </li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#taches_4","title":"T\u00e2ches","text":"<ol> <li>Analyser chaque requ\u00eate avec EXPLAIN ANALYZE</li> <li>Identifier tous les probl\u00e8mes</li> <li>Cr\u00e9er un plan d'optimisation</li> <li>Impl\u00e9menter les optimisations</li> <li>Mesurer l'impact global</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#solution-attendue_4","title":"Solution attendue","text":"Cliquez pour voir la solution <pre><code>-- 1. Analyser toutes les requ\u00eates\nEXPLAIN ANALYZE [chaque requ\u00eate];\n\n-- 2. Cr\u00e9er les index n\u00e9cessaires\nCREATE INDEX idx_customers_active ON customers(active);\nCREATE INDEX idx_orders_customer_id ON orders(customer_id);\nCREATE INDEX idx_orders_created_at_status ON orders(created_at, status);\nCREATE INDEX idx_order_items_order_id ON order_items(order_id);\nCREATE INDEX idx_order_items_product_id ON order_items(product_id);\nCREATE INDEX idx_products_category_price ON products(category, price);\n\n-- 3. Optimiser la requ\u00eate 1 (HAVING \u2192 WHERE)\nSELECT \n    c.name,\n    c.email,\n    o.order_count,\n    o.total_spent,\n    o.last_order_date\nFROM customers c\nJOIN (\n    SELECT \n        customer_id,\n        COUNT(*) AS order_count,\n        SUM(total_amount) AS total_spent,\n        MAX(created_at) AS last_order_date\n    FROM orders\n    GROUP BY customer_id\n) o ON c.id = o.customer_id\nWHERE c.active = true\nORDER BY o.total_spent DESC\nLIMIT 100;\n\n-- 4. Optimiser la requ\u00eate 3 (sous-requ\u00eate \u2192 JOIN)\nSELECT \n    p.*,\n    COALESCE(oi.times_ordered, 0) AS times_ordered\nFROM products p\nLEFT JOIN (\n    SELECT product_id, COUNT(*) AS times_ordered\n    FROM order_items\n    GROUP BY product_id\n) oi ON p.id = oi.product_id\nWHERE p.category = 'electronics'\n  AND p.price BETWEEN 100 AND 500\nORDER BY oi.times_ordered DESC NULLS LAST;\n\n-- 5. Mesurer l'impact\n-- Comparer les temps d'ex\u00e9cution avant/apr\u00e8s\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#exercice-6-monitoring-et-alertes-avance","title":"Exercice 6 : Monitoring et alertes (\ud83d\udd34 Avanc\u00e9)","text":""},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#contexte_5","title":"Contexte","text":"<p>Cr\u00e9ez un syst\u00e8me de monitoring pour d\u00e9tecter automatiquement les probl\u00e8mes de performance.</p>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#taches_5","title":"T\u00e2ches","text":"<ol> <li>Cr\u00e9er une vue consolidant les m\u00e9triques cl\u00e9s</li> <li>Cr\u00e9er une fonction de d\u00e9tection d'alertes</li> <li>Cr\u00e9er un rapport automatique</li> <li>Tester le syst\u00e8me avec des donn\u00e9es r\u00e9elles</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#solution-attendue_5","title":"Solution attendue","text":"Cliquez pour voir la solution <pre><code>-- 1. Vue consolid\u00e9e\nCREATE OR REPLACE VIEW v_performance_metrics AS\nSELECT \n    'slow_queries' AS metric_type,\n    COUNT(*) AS count,\n    AVG(mean_exec_time) AS avg_value,\n    MAX(mean_exec_time) AS max_value\nFROM pg_stat_statements\nWHERE mean_exec_time &gt; 1000\n\nUNION ALL\n\nSELECT \n    'cache_hit_ratio',\n    NULL,\n    ROUND(100.0 * SUM(shared_blks_hit) / \n          NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2),\n    NULL\nFROM pg_stat_statements\n\nUNION ALL\n\nSELECT \n    'idle_in_transaction',\n    COUNT(*),\n    NULL,\n    NULL\nFROM pg_stat_activity\nWHERE state = 'idle in transaction';\n\n-- 2. Fonction d'alertes\nCREATE OR REPLACE FUNCTION check_performance_alerts()\nRETURNS TABLE(alert_type TEXT, message TEXT, severity TEXT) AS $$\nBEGIN\n    -- Alertes sur requ\u00eates lentes\n    RETURN QUERY\n    SELECT \n        'SLOW_QUERY'::TEXT,\n        format('Query with mean_exec_time: %s ms', ROUND(mean_exec_time::numeric, 2)),\n        CASE WHEN mean_exec_time &gt; 5000 THEN 'CRITICAL' ELSE 'WARNING' END\n    FROM pg_stat_statements\n    WHERE mean_exec_time &gt; 1000\n    ORDER BY mean_exec_time DESC\n    LIMIT 5;\n\n    -- Alerte sur cache hit ratio\n    RETURN QUERY\n    SELECT \n        'LOW_CACHE_HIT'::TEXT,\n        format('Cache hit ratio: %s%%', \n               ROUND(100.0 * SUM(shared_blks_hit) / \n                     NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2)),\n        CASE \n            WHEN 100.0 * SUM(shared_blks_hit) / \n                 NULLIF(SUM(shared_blks_hit + shared_blks_read), 0) &lt; 90 \n            THEN 'CRITICAL'\n            WHEN 100.0 * SUM(shared_blks_hit) / \n                 NULLIF(SUM(shared_blks_hit + shared_blks_read), 0) &lt; 95 \n            THEN 'WARNING'\n            ELSE 'INFO'\n        END\n    FROM pg_stat_statements;\n\n    -- Alerte sur connexions idle in transaction\n    RETURN QUERY\n    SELECT \n        'IDLE_IN_TRANSACTION'::TEXT,\n        format('%s connections idle in transaction', COUNT(*)),\n        CASE WHEN COUNT(*) &gt; 10 THEN 'CRITICAL' ELSE 'WARNING' END\n    FROM pg_stat_activity\n    WHERE state = 'idle in transaction';\nEND;\n$$ LANGUAGE plpgsql;\n\n-- 3. Utilisation\nSELECT * FROM check_performance_alerts();\n</code></pre>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#conseils-pour-les-exercices","title":"\ud83d\udcdd Conseils pour les exercices","text":"<ol> <li>Toujours commencer par EXPLAIN ANALYZE : Comprendre avant d'optimiser</li> <li>Mesurer avant et apr\u00e8s : Quantifier l'am\u00e9lioration</li> <li>Tester avec des donn\u00e9es r\u00e9alistes : Utiliser des volumes similaires \u00e0 la production</li> <li>Documenter vos d\u00e9cisions : Noter pourquoi vous avez choisi telle optimisation</li> <li>V\u00e9rifier les effets secondaires : Un index am\u00e9liore les SELECT mais ralentit les INSERT/UPDATE</li> </ol>"},{"location":"SQL-avanc%C3%A9/FR/07-exercices/#retour-aux-modules","title":"\ud83d\udd17 Retour aux modules","text":"<ul> <li>Module 1 : Fondamentaux</li> <li>Module 2 : Plans d'ex\u00e9cution</li> <li>Module 3 : Dalibo</li> <li>Module 4 : Indicateurs</li> <li>Module 5 : Techniques</li> <li>Module 6 : Cas pratiques</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/","title":"Zaawansowane SQL/PostgreSQL - Optymalizacja Zapyta\u0144","text":""},{"location":"SQL-avanc%C3%A9/PL/#przeglad","title":"\ud83d\udcda Przegl\u0105d","text":"<p>Ten kurs obejmuje zaawansowane techniki optymalizacji zapyta\u0144 SQL/PostgreSQL, ze szczeg\u00f3lnym naciskiem na wykorzystanie Dalibo do analizy i optymalizacji wydajno\u015bci.</p>"},{"location":"SQL-avanc%C3%A9/PL/#cele-edukacyjne","title":"\ud83c\udfaf Cele edukacyjne","text":"<ul> <li>Zrozumie\u0107 mechanizmy wykonywania zapyta\u0144 PostgreSQL</li> <li>Opanowa\u0107 zaawansowane techniki optymalizacji</li> <li>U\u017cywa\u0107 Dalibo do analizy i optymalizacji wydajno\u015bci</li> <li>Interpretowa\u0107 kluczowe wska\u017aniki wydajno\u015bci</li> <li>Stosowa\u0107 najlepsze praktyki w rzeczywistych przypadkach</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#struktura-kursu","title":"\ud83d\udcd6 Struktura kursu","text":""},{"location":"SQL-avanc%C3%A9/PL/#1-podstawy-optymalizacji","title":"1. Podstawy optymalizacji","text":"<ul> <li>Architektura PostgreSQL i planista zapyta\u0144</li> <li>Typy indeks\u00f3w i ich wykorzystanie</li> <li>Statystyki i ANALYZE</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#2-analiza-planow-wykonania","title":"2. Analiza plan\u00f3w wykonania","text":"<ul> <li>EXPLAIN i EXPLAIN ANALYZE</li> <li>Interpretacja operacji (Seq Scan, Index Scan, itp.)</li> <li>Koszty i czasy wykonania</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#3-dalibo-narzedzie-analityczne","title":"3. Dalibo - Narz\u0119dzie analityczne","text":"<ul> <li>Instalacja i konfiguracja</li> <li>Analiza zapyta\u0144 z pg_stat_statements</li> <li>Raporty wydajno\u015bci</li> <li>Automatyczne rekomendacje</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#4-wskazniki-wydajnosci","title":"4. Wska\u017aniki wydajno\u015bci","text":"<ul> <li>Kluczowe metryki do monitorowania</li> <li>Interpretacja wska\u017anik\u00f3w Dalibo</li> <li>Progi alarmowe i najlepsze praktyki</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#5-techniki-optymalizacji","title":"5. Techniki optymalizacji","text":"<ul> <li>Optymalizacja z\u0142\u0105cze\u0144</li> <li>Optymalizacja agregacji</li> <li>Optymalizacja podzapyta\u0144</li> <li>Partycjonowanie i r\u00f3wnoleg\u0142o\u015b\u0107</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#6-przypadki-praktyczne","title":"6. Przypadki praktyczne","text":"<ul> <li>Rzeczywiste scenariusze optymalizacji</li> <li>Przed/po z metrykami</li> <li>Rozwi\u0105zywanie typowych problem\u00f3w</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#7-cwiczenia","title":"7. \u0106wiczenia","text":"<ul> <li>\u0106wiczenia prowadzone</li> <li>Problemy do rozwi\u0105zania</li> <li>Skomentowane rozwi\u0105zania</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#szybki-start","title":"\ud83d\ude80 Szybki start","text":"<ol> <li>Wymagania wst\u0119pne</li> <li>PostgreSQL 12+ zainstalowany</li> <li>Dost\u0119p do bazy danych testowej</li> <li> <p>Rozszerzenie <code>pg_stat_statements</code> w\u0142\u0105czone</p> </li> <li> <p>Konfiguracja Dalibo <pre><code>-- W\u0142\u0105cz pg_stat_statements\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n</code></pre></p> </li> <li> <p>Przej\u015bcie przez kurs</p> </li> <li>Zacznij od modu\u0142u 1 (Podstawy)</li> <li>Post\u0119puj zgodnie z kolejno\u015bci\u0105 modu\u0142\u00f3w dla logicznej progresji</li> <li>\u0106wicz z \u0107wiczeniami z modu\u0142u 7</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/#zalecane-narzedzia","title":"\ud83d\udcca Zalecane narz\u0119dzia","text":"<ul> <li>Dalibo : Analiza wydajno\u015bci PostgreSQL</li> <li>pgAdmin : Interfejs graficzny dla PostgreSQL</li> <li>psql : Klient wiersza polece\u0144</li> <li>EXPLAIN Visualizer : Wizualizacja plan\u00f3w wykonania</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#konwencje","title":"\ud83d\udcdd Konwencje","text":"<ul> <li>Przyk\u0142ady SQL s\u0105 testowane na PostgreSQL 14+</li> <li>Metryki oparte s\u0105 na typowych \u015brodowiskach produkcyjnych</li> <li>Czasy wykonania mog\u0105 si\u0119 r\u00f3\u017cni\u0107 w zale\u017cno\u015bci od konfiguracji</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/#wkad","title":"\ud83e\udd1d Wk\u0142ad","text":"<p>Ten kurs jest zaprojektowany tak, aby by\u0142 rozwijany. Nie wahaj si\u0119 proponowa\u0107 ulepsze\u0144 lub dodatkowych przypadk\u00f3w u\u017cycia.</p>"},{"location":"SQL-avanc%C3%A9/PL/#dodatkowe-zasoby","title":"\ud83d\udcda Dodatkowe zasoby","text":"<ul> <li>Oficjalna dokumentacja PostgreSQL</li> <li>Dokumentacja Dalibo</li> <li>PostgreSQL Performance Tuning</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/","title":"Plan Kursu Zaawansowanego SQL/PostgreSQL - Optymalizacja Zapyta\u0144","text":""},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#przeglad","title":"\ud83d\udccb Przegl\u0105d","text":"<p>Ten dokument przedstawia kompletny plan kursu dotycz\u0105cego optymalizacji SQL/PostgreSQL ze szczeg\u00f3lnym naciskiem na Dalibo i wska\u017aniki wydajno\u015bci.</p>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#cele-edukacyjne","title":"\ud83c\udfaf Cele edukacyjne","text":"<ol> <li>Zrozumie\u0107 wewn\u0119trzne mechanizmy PostgreSQL</li> <li>Analizowa\u0107 plany wykonania i identyfikowa\u0107 problemy</li> <li>U\u017cywa\u0107 Dalibo do automatycznej analizy</li> <li>Interpretowa\u0107 wska\u017aniki wydajno\u015bci</li> <li>Stosowa\u0107 zaawansowane techniki optymalizacji</li> <li>Rozwi\u0105zywa\u0107 rzeczywiste problemy wydajno\u015bciowe</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#struktura-kursu","title":"\ud83d\udcda Struktura kursu","text":""},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#modu-1-podstawy-optymalizacji","title":"Modu\u0142 1 : Podstawy optymalizacji","text":"<p>Szacowany czas : 2-3 godziny</p> <p>Tre\u015b\u0107 : - Architektura PostgreSQL i planista zapyta\u0144 - Typy indeks\u00f3w (B-tree, Hash, GIN, GiST, BRIN) - Statystyki i ANALYZE - Parametry koszt\u00f3w</p> <p>Nabyte umiej\u0119tno\u015bci : - Zrozumie\u0107, jak PostgreSQL wykonuje zapytania - Wybra\u0107 odpowiedni typ indeksu - Utrzymywa\u0107 statystyki na bie\u017c\u0105co</p>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#modu-2-analiza-planow-wykonania","title":"Modu\u0142 2 : Analiza plan\u00f3w wykonania","text":"<p>Szacowany czas : 2-3 godziny</p> <p>Tre\u015b\u0107 : - EXPLAIN i EXPLAIN ANALYZE - Typy operacji (Seq Scan, Index Scan, Hash Join, itp.) - Interpretacja koszt\u00f3w - Sygna\u0142y alarmowe</p> <p>Nabyte umiej\u0119tno\u015bci : - Czyta\u0107 i interpretowa\u0107 plany wykonania - Identyfikowa\u0107 problematyczne operacje - Rozumie\u0107 metryki wydajno\u015bci</p>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#modu-3-dalibo-narzedzie-analityczne","title":"Modu\u0142 3 : Dalibo - Narz\u0119dzie analityczne","text":"<p>Szacowany czas : 3-4 godziny</p> <p>Tre\u015b\u0107 : - Instalacja i konfiguracja - pg_stat_statements - pg_qualstats - pg_stat_monitor - Raporty i wizualizacje - Automatyczne rekomendacje</p> <p>Nabyte umiej\u0119tno\u015bci : - Instalowa\u0107 i konfigurowa\u0107 narz\u0119dzia Dalibo - Analizowa\u0107 statystyki zapyta\u0144 - Automatycznie identyfikowa\u0107 brakuj\u0105ce indeksy - Generowa\u0107 raporty wydajno\u015bci</p>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#modu-4-wskazniki-wydajnosci","title":"Modu\u0142 4 : Wska\u017aniki wydajno\u015bci","text":"<p>Szacowany czas : 2-3 godziny</p> <p>Tre\u015b\u0107 : - Metryki systemowe (CPU, pami\u0119\u0107, po\u0142\u0105czenia) - Metryki zapyta\u0144 (czas, cz\u0119stotliwo\u015b\u0107, cache) - Metryki indeks\u00f3w (u\u017cycie, bloat) - Metryki I/O - Progi alarmowe - Panele kontrolne</p> <p>Nabyte umiej\u0119tno\u015bci : - Monitorowa\u0107 kluczowe metryki - Definiowa\u0107 odpowiednie progi alarmowe - Tworzy\u0107 panele kontrolne monitoringu</p>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#modu-5-techniki-optymalizacji","title":"Modu\u0142 5 : Techniki optymalizacji","text":"<p>Szacowany czas : 3-4 godziny</p> <p>Tre\u015b\u0107 : - Optymalizacja z\u0142\u0105cze\u0144 - Optymalizacja agregacji - Optymalizacja podzapyta\u0144 - Partycjonowanie (Range, List, Hash) - R\u00f3wnoleg\u0142o\u015b\u0107 - Optymalizacja typ\u00f3w danych</p> <p>Nabyte umiej\u0119tno\u015bci : - Optymalizowa\u0107 r\u00f3\u017cne typy zapyta\u0144 - Skutecznie wykorzystywa\u0107 partycjonowanie - Wykorzystywa\u0107 r\u00f3wnoleg\u0142o\u015b\u0107 PostgreSQL</p>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#modu-6-przypadki-praktyczne","title":"Modu\u0142 6 : Przypadki praktyczne","text":"<p>Szacowany czas : 3-4 godziny</p> <p>Tre\u015b\u0107 : - 6 rzeczywistych przypadk\u00f3w optymalizacji - Analiza przed/po z metrykami - Wykorzystanie Dalibo do analizy - Rozwi\u0105zywanie typowych problem\u00f3w</p> <p>Nabyte umiej\u0119tno\u015bci : - Stosowa\u0107 techniki na rzeczywistych przypadkach - Mierzy\u0107 wp\u0142yw optymalizacji - Rozwi\u0105zywa\u0107 z\u0142o\u017cone problemy</p>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#modu-7-cwiczenia","title":"Modu\u0142 7 : \u0106wiczenia","text":"<p>Szacowany czas : 4-6 godzin</p> <p>Tre\u015b\u0107 : - 6 progresywnych \u0107wicze\u0144 (Pocz\u0105tkuj\u0105cy \u2192 Zaawansowany) - Skomentowane rozwi\u0105zania - Problemy do rozwi\u0105zania</p> <p>Nabyte umiej\u0119tno\u015bci : - \u0106wiczy\u0107 poznane techniki - Rozwi\u0105zywa\u0107 problemy samodzielnie - Konsolidowa\u0107 wiedz\u0119</p>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#wskazniki-dalibo-objete-kursem","title":"\ud83d\udcca Wska\u017aniki Dalibo obj\u0119te kursem","text":""},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#gowne-narzedzia","title":"G\u0142\u00f3wne narz\u0119dzia","text":"<ol> <li>pg_stat_statements</li> <li>Identyfikacja wolnych zapyta\u0144</li> <li>Analiza czas\u00f3w wykonania</li> <li>Wykrywanie wysokiego I/O</li> <li> <p>Wsp\u00f3\u0142czynnik trafie\u0144 cache na zapytanie</p> </li> <li> <p>pg_qualstats</p> </li> <li>Statystyki dotycz\u0105ce predykat\u00f3w</li> <li>Automatyczna identyfikacja brakuj\u0105cych indeks\u00f3w</li> <li>Rekomendacje indeks\u00f3w</li> <li> <p>Analiza warunk\u00f3w WHERE/JOIN</p> </li> <li> <p>pg_stat_monitor</p> </li> <li>Monitoring z agregacj\u0105 czasow\u0105</li> <li>Analiza b\u0142\u0119d\u00f3w</li> <li>Wiele plan\u00f3w wykonania</li> <li>Wiadra czasowe</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#kluczowe-monitorowane-metryki","title":"Kluczowe monitorowane metryki","text":"Metryka Narz\u0119dzie Pr\u00f3g alarmowy \u015aredni czas wykonania pg_stat_statements &gt; 1000ms Wsp\u00f3\u0142czynnik trafie\u0144 cache pg_stat_statements &lt; 95% Brakuj\u0105ce indeksy pg_qualstats Cz\u0119stotliwo\u015b\u0107 &gt; 1000 Zapytania z I/O tymczasowym pg_stat_statements &gt; 0 Po\u0142\u0105czenia idle in transaction pg_stat_activity &gt; 5%"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#zalecane-sciezki-uczenia-sie","title":"\ud83c\udf93 Zalecane \u015bcie\u017cki uczenia si\u0119","text":""},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#pena-sciezka-16-20-godzin","title":"Pe\u0142na \u015bcie\u017cka (16-20 godzin)","text":"<ol> <li>Modu\u0142 1 : Podstawy</li> <li>Modu\u0142 2 : Plany wykonania</li> <li>Modu\u0142 3 : Dalibo</li> <li>Modu\u0142 4 : Wska\u017aniki</li> <li>Modu\u0142 5 : Techniki</li> <li>Modu\u0142 6 : Przypadki praktyczne</li> <li>Modu\u0142 7 : \u0106wiczenia</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#przyspieszona-sciezka-8-10-godzin","title":"Przyspieszona \u015bcie\u017cka (8-10 godzin)","text":"<ol> <li>Modu\u0142 1 : Podstawy (szybka powt\u00f3rka)</li> <li>Modu\u0142 2 : Plany wykonania</li> <li>Modu\u0142 3 : Dalibo (fokus na pg_stat_statements i pg_qualstats)</li> <li>Modu\u0142 4 : Wska\u017aniki (kluczowe metryki)</li> <li>Modu\u0142 6 : Przypadki praktyczne (2-3 przypadki)</li> <li>Modu\u0142 7 : \u0106wiczenia (poziom \u015bredni)</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#sciezka-ekspercka-4-6-godzin","title":"\u015acie\u017cka ekspercka (4-6 godzin)","text":"<ol> <li>Modu\u0142 3 : Dalibo (pog\u0142\u0119bienie)</li> <li>Modu\u0142 4 : Wska\u017aniki (zaawansowane panele)</li> <li>Modu\u0142 5 : Techniki (partycjonowanie, r\u00f3wnoleg\u0142o\u015b\u0107)</li> <li>Modu\u0142 7 : \u0106wiczenia (poziom zaawansowany)</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#wymagania-techniczne","title":"\ud83d\udee0\ufe0f Wymagania techniczne","text":""},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#wymagana-wiedza","title":"Wymagana wiedza","text":"<ul> <li>Podstawy SQL (SELECT, JOIN, GROUP BY, itp.)</li> <li>Podstawowa znajomo\u015b\u0107 PostgreSQL</li> <li>Dost\u0119p do instancji PostgreSQL (12+)</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#zalecane-srodowisko","title":"Zalecane \u015brodowisko","text":"<ul> <li>PostgreSQL 12+ zainstalowany</li> <li>Dost\u0119p superu\u017cytkownika do instalacji rozszerze\u0144</li> <li>Baza danych testowa z realistycznymi danymi</li> <li>Narz\u0119dzia : psql, pgAdmin (opcjonalnie)</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#wymagane-rozszerzenia","title":"Wymagane rozszerzenia","text":"<pre><code>CREATE EXTENSION IF NOT EXISTS pg_stat_statements;\nCREATE EXTENSION IF NOT EXISTS pg_qualstats;  -- Opcjonalne ale zalecane\nCREATE EXTENSION IF NOT EXISTS pg_stat_monitor;  -- Opcjonalne\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#postep-i-ocena","title":"\ud83d\udcc8 Post\u0119p i ocena","text":""},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#punkty-kontrolne","title":"Punkty kontrolne","text":"<ol> <li>Po Module 2 : Zdolno\u015b\u0107 do interpretacji planu wykonania</li> <li>Po Module 3 : Zdolno\u015b\u0107 do u\u017cycia Dalibo do identyfikacji problem\u00f3w</li> <li>Po Module 5 : Zdolno\u015b\u0107 do optymalizacji r\u00f3\u017cnych typ\u00f3w zapyta\u0144</li> <li>Po Module 7 : Zdolno\u015b\u0107 do samodzielnego rozwi\u0105zywania z\u0142o\u017conych problem\u00f3w</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#kryteria-sukcesu","title":"Kryteria sukcesu","text":"<ul> <li>\u2705 Poprawnie interpretowa\u0107 plan wykonania</li> <li>\u2705 Identyfikowa\u0107 problemy wydajno\u015bciowe z Dalibo</li> <li>\u2705 Tworzy\u0107 odpowiednie indeksy</li> <li>\u2705 Optymalizowa\u0107 wolne zapytanie (poprawa &gt; 50%)</li> <li>\u2705 Konfigurowa\u0107 monitoring kluczowych wska\u017anik\u00f3w</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#dodatkowe-zasoby","title":"\ud83d\udd17 Dodatkowe zasoby","text":""},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#oficjalna-dokumentacja","title":"Oficjalna dokumentacja","text":"<ul> <li>Dokumentacja PostgreSQL</li> <li>Dalibo GitHub</li> <li>pg_stat_statements</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#zalecane-narzedzia","title":"Zalecane narz\u0119dzia","text":"<ul> <li>pgBadger : Analiza log\u00f3w PostgreSQL</li> <li>pg_activity : Monitoring w czasie rzeczywistym</li> <li>HypoPG : Test hipotetycznych indeks\u00f3w</li> <li>explain.dalibo.com : Wizualizacja plan\u00f3w</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#spoecznosci","title":"Spo\u0142eczno\u015bci","text":"<ul> <li>PostgreSQL Polska</li> <li>Stack Overflow (tag: postgresql)</li> <li>Reddit r/PostgreSQL</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#uwagi-pedagogiczne","title":"\ud83d\udcdd Uwagi pedagogiczne","text":""},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#podejscie-pedagogiczne","title":"Podej\u015bcie pedagogiczne","text":"<ul> <li>Teoretyczne : Koncepcje wyja\u015bnione z przyk\u0142adami</li> <li>Praktyczne : Rzeczywiste przypadki i \u0107wiczenia</li> <li>Progresywne : Od prostego do z\u0142o\u017conego</li> <li>Autonomiczne : Kompletna dokumentacja do samokszta\u0142cenia</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#wskazowki-dla-trenerow","title":"Wskaz\u00f3wki dla trener\u00f3w","text":"<ol> <li>Zacznij od konkretnych przyk\u0142ad\u00f3w</li> <li>U\u017cywaj systematycznie EXPLAIN ANALYZE</li> <li>Poka\u017c wp\u0142yw przed/po optymalizacjach</li> <li>Zach\u0119caj do eksperymentowania</li> <li>Tw\u00f3rz powi\u0105zania mi\u0119dzy modu\u0142ami</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#wskazowki-dla-uczniow","title":"Wskaz\u00f3wki dla uczni\u00f3w","text":"<ol> <li>\u0106wicz regularnie</li> <li>Testuj na realistycznych danych</li> <li>Dokumentuj swoje optymalizacje</li> <li>Mierz wp\u0142yw systematycznie</li> <li>Wracaj do podstaw w razie potrzeby</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/PLAN_FORMATION/#oczekiwane-rezultaty","title":"\ud83c\udfaf Oczekiwane rezultaty","text":"<p>Po uko\u0144czeniu tego kursu b\u0119dziesz w stanie :</p> <ol> <li>\u2705 Analizowa\u0107 i optymalizowa\u0107 z\u0142o\u017cone zapytania SQL</li> <li>\u2705 U\u017cywa\u0107 Dalibo do automatycznej identyfikacji problem\u00f3w</li> <li>\u2705 Interpretowa\u0107 wska\u017aniki wydajno\u015bci i definiowa\u0107 alerty</li> <li>\u2705 Stosowa\u0107 odpowiednie techniki optymalizacji</li> <li>\u2705 Rozwi\u0105zywa\u0107 problemy wydajno\u015bciowe w produkcji</li> <li>\u2705 Wdro\u017cy\u0107 skuteczny system monitoringu</li> </ol> <p>Ostatnia aktualizacja : 2024 Wersja : 1.0</p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/","title":"1. Podstawy optymalizacji PostgreSQL","text":""},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 architektur\u0119 PostgreSQL i planist\u0119 zapyta\u0144</li> <li>Opanowa\u0107 r\u00f3\u017cne typy indeks\u00f3w i ich optymalne wykorzystanie</li> <li>Zrozumie\u0107 rol\u0119 statystyk w optymalizacji</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Architektura PostgreSQL</li> <li>Planista zapyta\u0144</li> <li>Typy indeks\u00f3w</li> <li>Statystyki i ANALYZE</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#architektura-postgresql","title":"Architektura PostgreSQL","text":""},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#kluczowe-komponenty","title":"Kluczowe komponenty","text":"<p>PostgreSQL wykorzystuje architektur\u0119 wieloprocesow\u0105 z kilkoma wa\u017cnymi komponentami :</p> <ul> <li>Postmaster : G\u0142\u00f3wny proces zarz\u0105dzaj\u0105cy po\u0142\u0105czeniami</li> <li>Backend processes : Jeden proces na po\u0142\u0105czenie klienta</li> <li>Planista (Planner) : Optymalizuje zapytania SQL</li> <li>Wykonawca (Executor) : Wykonuje plany zapyta\u0144</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#przepyw-wykonania-zapytania","title":"Przep\u0142yw wykonania zapytania","text":"<pre><code>Zapytanie SQL\n    \u2193\nParser (analiza sk\u0142adniowa)\n    \u2193\nRewriter (przepisywanie widok\u00f3w/regu\u0142)\n    \u2193\nPlanner (generowanie planu wykonania)\n    \u2193\nExecutor (wykonanie planu)\n    \u2193\nWynik\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#planista-zapytan","title":"Planista zapyta\u0144","text":"<p>Planista jest odpowiedzialny za : - Wyb\u00f3r najlepszego planu wykonania - Szacowanie koszt\u00f3w ka\u017cdej operacji - Wykorzystanie statystyk bazy danych - Optymalizacj\u0119 z\u0142\u0105cze\u0144, sortowania, agregacji</p> <p>Czynniki wp\u0142ywaj\u0105ce na planist\u0119 : - Statystyki tabel (<code>pg_stat_user_tables</code>) - Statystyki kolumn (<code>pg_stats</code>) - Konfiguracja koszt\u00f3w (<code>random_page_cost</code>, <code>seq_page_cost</code>, itp.) - Parametry pami\u0119ci (<code>work_mem</code>, <code>shared_buffers</code>)</p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#planista-zapytan_1","title":"Planista zapyta\u0144","text":""},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#parametry-kosztow","title":"Parametry koszt\u00f3w","text":"<p>PostgreSQL wykorzystuje system koszt\u00f3w do por\u00f3wnywania plan\u00f3w :</p> <pre><code>-- Zobaczy\u0107 aktualne parametry koszt\u00f3w\nSHOW random_page_cost;\nSHOW seq_page_cost;\nSHOW cpu_tuple_cost;\nSHOW cpu_index_tuple_cost;\n</code></pre> <p>Wa\u017cne parametry : - <code>seq_page_cost</code> : Koszt odczytu sekwencyjnego (domy\u015blnie: 1.0) - <code>random_page_cost</code> : Koszt odczytu losowego (domy\u015blnie: 4.0) - <code>cpu_tuple_cost</code> : Koszt przetworzenia wiersza (domy\u015blnie: 0.01) - <code>cpu_index_tuple_cost</code> : Koszt wykorzystania indeksu (domy\u015blnie: 0.005)</p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#szacowanie-kosztow","title":"Szacowanie koszt\u00f3w","text":"<p>Planista szacuje : - Liczb\u0119 wierszy : Na podstawie statystyk - Koszt I/O : Odczyt/zapis dysku - Koszt CPU : Przetwarzanie danych - Koszt ca\u0142kowity : Suma koszt\u00f3w</p> <p>Wa\u017cne ograniczenie : Szacowania mog\u0105 by\u0107 niedok\u0142adne, je\u015bli statystyki s\u0105 przestarza\u0142e.</p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#typy-indeksow","title":"Typy indeks\u00f3w","text":""},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#indeks-b-tree-domyslny","title":"Indeks B-tree (domy\u015blny)","text":"<p>Wykorzystanie : - R\u00f3wno\u015b\u0107 (<code>=</code>) - Por\u00f3wnania (<code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>) - BETWEEN, IN - LIKE z ustalonym prefiksem</p> <p>Przyk\u0142ad : <pre><code>CREATE INDEX idx_user_email ON users(email);\n-- U\u017cywany dla: WHERE email = 'user@example.com'\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#indeks-hash","title":"Indeks Hash","text":"<p>Wykorzystanie : - Tylko dla r\u00f3wno\u015bci (<code>=</code>) - Szybszy ni\u017c B-tree dla prostej r\u00f3wno\u015bci - Nie obs\u0142uguje por\u00f3wna\u0144</p> <p>Przyk\u0142ad : <pre><code>CREATE INDEX idx_user_id_hash ON users USING hash(id);\n-- U\u017cywany dla: WHERE id = 123\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#indeks-gin-generalized-inverted-index","title":"Indeks GIN (Generalized Inverted Index)","text":"<p>Wykorzystanie : - Z\u0142o\u017cone typy danych (tablice, JSONB, full-text) - Zaawansowane operatory wyszukiwania</p> <p>Przyk\u0142ad : <pre><code>CREATE INDEX idx_product_tags_gin ON products USING gin(tags);\n-- U\u017cywany dla: WHERE tags @&gt; ARRAY['electronics']\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#indeks-gist-generalized-search-tree","title":"Indeks GiST (Generalized Search Tree)","text":"<p>Wykorzystanie : - Typy geometryczne - Wyszukiwanie pe\u0142notekstowe - Typy niestandardowe</p> <p>Przyk\u0142ad : <pre><code>CREATE INDEX idx_location_gist ON places USING gist(location);\n-- U\u017cywany dla: WHERE location &lt;-&gt; point(0,0) &lt; 1000\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#indeks-brin-block-range-index","title":"Indeks BRIN (Block Range Index)","text":"<p>Wykorzystanie : - Du\u017ce tabele z posortowanymi danymi - Bardzo kompaktowy (ma\u0142o miejsca) - Skuteczny dla zakres\u00f3w warto\u015bci</p> <p>Przyk\u0142ad : <pre><code>CREATE INDEX idx_orders_date_brin ON orders USING brin(order_date);\n-- U\u017cywany dla: WHERE order_date BETWEEN '2024-01-01' AND '2024-12-31'\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#indeks-czesciowy","title":"Indeks cz\u0119\u015bciowy","text":"<p>Wykorzystanie : - Zmniejszenie rozmiaru indeksu - Poprawa wydajno\u015bci dla okre\u015blonych warunk\u00f3w</p> <p>Przyk\u0142ad : <pre><code>CREATE INDEX idx_active_users ON users(email) WHERE active = true;\n-- Indeks tylko dla aktywnych u\u017cytkownik\u00f3w\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#indeks-zozony","title":"Indeks z\u0142o\u017cony","text":"<p>Wykorzystanie : - Wiele kolumn - Kolejno\u015b\u0107 kolumn jest wa\u017cna</p> <p>Przyk\u0142ad : <pre><code>CREATE INDEX idx_user_name_email ON users(last_name, first_name, email);\n-- U\u017cywany dla: WHERE last_name = 'Doe' AND first_name = 'John'\n</code></pre></p> <p>Wa\u017cna zasada : Indeks mo\u017ce by\u0107 u\u017cyty, je\u015bli zapytanie wykorzystuje kolumny w kolejno\u015bci indeksu, zaczynaj\u0105c od pierwszej.</p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#statystyki-i-analyze","title":"Statystyki i ANALYZE","text":""},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#dlaczego-statystyki-sa-istotne","title":"Dlaczego statystyki s\u0105 istotne","text":"<p>Planista wykorzystuje statystyki do : - Szacowania liczby zwracanych wierszy - Wyboru mi\u0119dzy r\u00f3\u017cnymi planami wykonania - Okre\u015blenia kolejno\u015bci z\u0142\u0105cze\u0144</p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#zbieranie-statystyk","title":"Zbieranie statystyk","text":"<pre><code>-- Analizowa\u0107 konkretn\u0105 tabel\u0119\nANALYZE table_name;\n\n-- Analizowa\u0107 wszystkie tabele\nANALYZE;\n\n-- Analizowa\u0107 z poziomem szczeg\u00f3\u0142owo\u015bci\nANALYZE VERBOSE table_name;\n</code></pre> <p>Kiedy wykona\u0107 ANALYZE : - Po znacz\u0105cych modyfikacjach (INSERT, UPDATE, DELETE) - Po utworzeniu indeksu - Automatycznie przez autovacuum (konfigurowalne)</p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#konfiguracja-autovacuum","title":"Konfiguracja autovacuum","text":"<pre><code>-- Zobaczy\u0107 aktualn\u0105 konfiguracj\u0119\nSHOW autovacuum;\nSHOW autovacuum_analyze_scale_factor;\nSHOW autovacuum_analyze_threshold;\n\n-- Zmodyfikowa\u0107 dla konkretnej tabeli\nALTER TABLE large_table SET (\n    autovacuum_analyze_scale_factor = 0.05,\n    autovacuum_analyze_threshold = 10000\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#konsultowanie-statystyk","title":"Konsultowanie statystyk","text":"<pre><code>-- Statystyki tabel\nSELECT \n    schemaname,\n    tablename,\n    n_tup_ins AS inserts,\n    n_tup_upd AS updates,\n    n_tup_del AS deletes,\n    n_live_tup AS live_rows,\n    n_dead_tup AS dead_rows,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables\nORDER BY n_live_tup DESC;\n\n-- Statystyki kolumn\nSELECT \n    schemaname,\n    tablename,\n    attname AS column_name,\n    n_distinct,\n    correlation,\n    most_common_vals\nFROM pg_stats\nWHERE tablename = 'your_table';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#statystyki-rozszerzone","title":"Statystyki rozszerzone","text":"<p>PostgreSQL 10+ obs\u0142uguje statystyki rozszerzone :</p> <pre><code>-- Utworzy\u0107 statystyki na wielu kolumnach\nCREATE STATISTICS stats_user_name_email \nON users(last_name, first_name);\n\n-- Analizowa\u0107, aby zebra\u0107 statystyki\nANALYZE users;\n\n-- Konsultowa\u0107 statystyki rozszerzone\nSELECT * FROM pg_statistic_ext;\n</code></pre> <p>U\u017cyteczno\u015b\u0107 : Poprawia szacowania dla zapyta\u0144 z wieloma skorelowanymi kolumnami.</p>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Planista zale\u017cy od statystyk : Przestarza\u0142e statystyki = z\u0142e plany</li> <li>Wyb\u00f3r odpowiedniego typu indeksu : Ka\u017cdy typ ma swoje zalety</li> <li>Regularne ANALYZE : Istotne dla utrzymania dobrych wydajno\u015bci</li> <li>Zrozumienie koszt\u00f3w : Pomaga interpretowa\u0107 plany wykonania</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/01-fondamentaux/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 2. Analiza plan\u00f3w wykonania, aby nauczy\u0107 si\u0119 interpretowa\u0107 plany wykonania.</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/","title":"2. Analiza plan\u00f3w wykonania","text":""},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Opanowa\u0107 EXPLAIN i EXPLAIN ANALYZE</li> <li>Interpretowa\u0107 r\u00f3\u017cne typy operacji</li> <li>Rozumie\u0107 koszty i czasy wykonania</li> <li>Identyfikowa\u0107 problemy wydajno\u015bciowe w planach</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>EXPLAIN i EXPLAIN ANALYZE</li> <li>Typy operacji</li> <li>Interpretacja koszt\u00f3w</li> <li>Sygna\u0142y alarmowe</li> <li>Dobre praktyki</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#explain-i-explain-analyze","title":"EXPLAIN i EXPLAIN ANALYZE","text":""},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#explain-bez-wykonania","title":"EXPLAIN (bez wykonania)","text":"<p>Wy\u015bwietla szacowany plan wykonania bez wykonywania zapytania :</p> <pre><code>EXPLAIN SELECT * FROM users WHERE email = 'user@example.com';\n</code></pre> <p>Wynik : <pre><code>Seq Scan on users  (cost=0.00..25.00 rows=1 width=64)\n  Filter: (email = 'user@example.com'::text)\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#explain-analyze-z-wykonaniem","title":"EXPLAIN ANALYZE (z wykonaniem)","text":"<p>Wykonuje zapytanie i wy\u015bwietla rzeczywiste czasy :</p> <pre><code>EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'user@example.com';\n</code></pre> <p>Wynik : <pre><code>Seq Scan on users  (cost=0.00..25.00 rows=1 width=64) \n  (actual time=0.123..15.456 rows=1 loops=1)\n  Filter: (email = 'user@example.com'::text)\n  Rows Removed by Filter: 9999\nPlanning Time: 0.234 ms\nExecution Time: 15.678 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#przydatne-opcje","title":"Przydatne opcje","text":"<pre><code>-- Format JSON (dla narz\u0119dzi zewn\u0119trznych)\nEXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) \nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Wy\u015bwietli\u0107 bufory\nEXPLAIN (ANALYZE, BUFFERS) \nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Wy\u015bwietli\u0107 parametry planowania\nEXPLAIN (ANALYZE, VERBOSE, SETTINGS) \nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- Format YAML\nEXPLAIN (ANALYZE, FORMAT YAML) \nSELECT * FROM users WHERE email = 'user@example.com';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#interpretacja-metryk","title":"Interpretacja metryk","text":"<p>Szacowane koszty : - <code>cost=0.00..25.00</code> : Koszt startowy..Koszt ca\u0142kowity - <code>rows=1</code> : Szacowana liczba wierszy - <code>width=64</code> : \u015aredni rozmiar wiersza w bajtach</p> <p>Rzeczywiste czasy (ANALYZE) : - <code>actual time=0.123..15.456</code> : Czas startowy..Czas ca\u0142kowity (ms) - <code>rows=1</code> : Rzeczywista liczba zwr\u00f3conych wierszy - <code>loops=1</code> : Liczba wykona\u0144 tej operacji - <code>Planning Time</code> : Czas planowania - <code>Execution Time</code> : Ca\u0142kowity czas wykonania</p> <p>Bufory (z BUFFERS) : - <code>shared hit=15</code> : Strony odczytane z cache wsp\u00f3\u0142dzielonego - <code>shared read=3</code> : Strony odczytane z dysku - <code>shared written=0</code> : Strony zapisane - <code>temp read/written</code> : Strony tymczasowe</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#typy-operacji","title":"Typy operacji","text":""},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#seq-scan-skanowanie-sekwencyjne","title":"Seq Scan (Skanowanie sekwencyjne)","text":"<p>Kiedy u\u017cywane : - Brak odpowiedniego indeksu - Ma\u0142a tabela (&lt; 10% tabeli) - Indeks nie selektywny</p> <p>Przyk\u0142ad : <pre><code>EXPLAIN ANALYZE SELECT * FROM users WHERE status = 'inactive';\n</code></pre></p> <p>Interpretacja : - \u2705 Akceptowalne dla ma\u0142ych tabel - \u26a0\ufe0f Problematyczne dla du\u017cych tabel - \ud83d\udd0d Dzia\u0142anie : Utworzy\u0107 indeks je\u015bli tabela jest du\u017ca</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#index-scan","title":"Index Scan","text":"<p>Kiedy u\u017cywane : - Indeks dost\u0119pny i selektywny - Bezpo\u015bredni dost\u0119p przez indeks</p> <p>Przyk\u0142ad : <pre><code>EXPLAIN ANALYZE \nSELECT * FROM users WHERE email = 'user@example.com';\n</code></pre></p> <p>Typowy wynik : <pre><code>Index Scan using idx_users_email on users  \n  (cost=0.42..8.44 rows=1 width=64)\n  (actual time=0.123..0.125 rows=1 loops=1)\n  Index Cond: (email = 'user@example.com'::text)\n</code></pre></p> <p>Interpretacja : - \u2705 Dobra wydajno\u015b\u0107 - \u2705 Bezpo\u015bredni dost\u0119p do wierszy</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#index-only-scan","title":"Index Only Scan","text":"<p>Kiedy u\u017cywane : - Wszystkie potrzebne kolumny s\u0105 w indeksie - Nie ma potrzeby dost\u0119pu do tabeli</p> <p>Przyk\u0142ad : <pre><code>-- Indeks na (id, email)\nCREATE INDEX idx_users_id_email ON users(id, email);\n\nEXPLAIN ANALYZE \nSELECT id, email FROM users WHERE id BETWEEN 1 AND 100;\n</code></pre></p> <p>Typowy wynik : <pre><code>Index Only Scan using idx_users_id_email on users\n  (cost=0.42..5.44 rows=100 width=64)\n  (actual time=0.123..0.456 rows=100 loops=1)\n  Index Cond: ((id &gt;= 1) AND (id &lt;= 100))\n  Heap Fetches: 0\n</code></pre></p> <p>Interpretacja : - \u2705 Optymalna wydajno\u015b\u0107 - \u2705 <code>Heap Fetches: 0</code> = brak dost\u0119pu do tabeli</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#bitmap-index-scan-bitmap-heap-scan","title":"Bitmap Index Scan + Bitmap Heap Scan","text":"<p>Kiedy u\u017cywane : - Wiele warunk\u00f3w z kilkoma indeksami - Zwraca wiele wierszy</p> <p>Przyk\u0142ad : <pre><code>EXPLAIN ANALYZE \nSELECT * FROM users \nWHERE status = 'active' AND created_at &gt; '2024-01-01';\n</code></pre></p> <p>Typowy wynik : <pre><code>Bitmap Heap Scan on users\n  (cost=4.44..25.67 rows=50 width=64)\n  (actual time=0.234..1.456 rows=45 loops=1)\n  Recheck Cond: ((status = 'active'::text) AND (created_at &gt; '2024-01-01'::date))\n  Heap Blocks: exact=12\n  -&gt; Bitmap Index Scan on idx_users_status\n      (cost=0.00..4.43 rows=50 width=0)\n      (actual time=0.123..0.123 rows=45 loops=1)\n      Index Cond: (status = 'active'::text)\n</code></pre></p> <p>Interpretacja : - \u2705 Skuteczne dla wielu warunk\u00f3w - \u26a0\ufe0f <code>Recheck Cond</code> = dodatkowa weryfikacja</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#nested-loop","title":"Nested Loop","text":"<p>Kiedy u\u017cywane : - Ma\u0142e tabele lub ograniczone wyniki - Ma\u0142a tabela zewn\u0119trzna</p> <p>Przyk\u0142ad : <pre><code>EXPLAIN ANALYZE \nSELECT u.*, o.* \nFROM users u \nJOIN orders o ON u.id = o.user_id \nWHERE u.id = 123;\n</code></pre></p> <p>Typowy wynik : <pre><code>Nested Loop\n  (cost=0.85..25.67 rows=10 width=128)\n  (actual time=0.123..2.456 rows=8 loops=1)\n  -&gt; Index Scan using idx_users_id on users\n      (cost=0.42..8.44 rows=1 width=64)\n      (actual time=0.089..0.090 rows=1 loops=1)\n      Index Cond: (id = 123)\n  -&gt; Index Scan using idx_orders_user_id on orders\n      (cost=0.42..17.23 rows=10 width=64)\n      (actual time=0.234..2.345 rows=8 loops=1)\n      Index Cond: (user_id = 123)\n</code></pre></p> <p>Interpretacja : - \u2705 Skuteczne dla ma\u0142ych p\u0119tli - \u26a0\ufe0f Mo\u017ce by\u0107 wolne je\u015bli p\u0119tla zewn\u0119trzna jest du\u017ca</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#hash-join","title":"Hash Join","text":"<p>Kiedy u\u017cywane : - Tabele podobnej wielko\u015bci - Brak indeksu na kluczu z\u0142\u0105czenia - Prosta r\u00f3wno\u015b\u0107</p> <p>Przyk\u0142ad : <pre><code>EXPLAIN ANALYZE \nSELECT u.*, o.* \nFROM users u \nJOIN orders o ON u.id = o.user_id;\n</code></pre></p> <p>Typowy wynik : <pre><code>Hash Join\n  (cost=125.67..456.78 rows=10000 width=128)\n  (actual time=2.345..15.678 rows=9876 loops=1)\n  Hash Cond: (o.user_id = u.id)\n  -&gt; Seq Scan on orders o\n      (cost=0.00..234.56 rows=10000 width=64)\n      (actual time=0.123..5.678 rows=10000 loops=1)\n  -&gt; Hash\n      (cost=123.45..123.45 rows=1000 width=64)\n      (actual time=1.234..1.234 rows=1000 loops=1)\n      Buckets: 1024  Batches: 1  Memory Usage: 64kB\n      -&gt; Seq Scan on users u\n          (cost=0.00..123.45 rows=1000 width=64)\n          (actual time=0.089..0.567 rows=1000 loops=1)\n</code></pre></p> <p>Interpretacja : - \u2705 Skuteczne dla z\u0142\u0105cze\u0144 r\u00f3wno\u015bciowych - \u26a0\ufe0f Wymaga pami\u0119ci (<code>work_mem</code>) - \ud83d\udd0d Dzia\u0142anie : Zwi\u0119kszy\u0107 <code>work_mem</code> je\u015bli \"Batches &gt; 1\"</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#merge-join","title":"Merge Join","text":"<p>Kiedy u\u017cywane : - Dane ju\u017c posortowane - Z\u0142\u0105czenia na posortowanych kluczach - Operatory por\u00f3wnania (&lt;, &gt;, &lt;=, &gt;=)</p> <p>Przyk\u0142ad : <pre><code>EXPLAIN ANALYZE \nSELECT u.*, o.* \nFROM users u \nJOIN orders o ON u.id = o.user_id \nORDER BY u.id;\n</code></pre></p> <p>Interpretacja : - \u2705 Skuteczne je\u015bli dane s\u0105 posortowane - \u26a0\ufe0f Wymaga sortowania je\u015bli dane nie s\u0105 posortowane</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#sort","title":"Sort","text":"<p>Kiedy u\u017cywane : - ORDER BY - GROUP BY (czasami) - Operacje wymagaj\u0105ce sortowania</p> <p>Przyk\u0142ad : <pre><code>EXPLAIN ANALYZE \nSELECT * FROM users ORDER BY created_at DESC LIMIT 100;\n</code></pre></p> <p>Typowy wynik : <pre><code>Limit\n  (cost=234.56..256.78 rows=100 width=64)\n  (actual time=12.345..15.678 rows=100 loops=1)\n  -&gt; Sort\n      (cost=234.56..256.78 rows=10000 width=64)\n      (actual time=12.345..15.234 rows=100 loops=1)\n      Sort Key: created_at DESC\n      Sort Method: top-N heapsort  Memory: 32kB\n      -&gt; Seq Scan on users\n          (cost=0.00..123.45 rows=10000 width=64)\n          (actual time=0.089..5.678 rows=10000 loops=1)\n</code></pre></p> <p>Interpretacja : - \u26a0\ufe0f <code>Sort Method: external merge</code> = sortowanie na dysku (wolne) - \u2705 <code>Sort Method: quicksort</code> = sortowanie w pami\u0119ci (szybkie) - \ud83d\udd0d Dzia\u0142anie : Zwi\u0119kszy\u0107 <code>work_mem</code> je\u015bli sortowanie na dysku</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#aggregate","title":"Aggregate","text":"<p>Kiedy u\u017cywane : - Funkcje agreguj\u0105ce (COUNT, SUM, AVG, etc.) - GROUP BY</p> <p>Przyk\u0142ad : <pre><code>EXPLAIN ANALYZE \nSELECT status, COUNT(*) \nFROM users \nGROUP BY status;\n</code></pre></p> <p>Typowy wynik : <pre><code>HashAggregate\n  (cost=123.45..145.67 rows=5 width=12)\n  (actual time=2.345..2.456 rows=5 loops=1)\n  Group Key: status\n  Batches: 1  Memory Usage: 24kB\n  -&gt; Seq Scan on users\n      (cost=0.00..98.76 rows=10000 width=4)\n      (actual time=0.089..1.234 rows=10000 loops=1)\n</code></pre></p> <p>Interpretacja : - \u2705 <code>HashAggregate</code> = skuteczne - \u26a0\ufe0f <code>GroupAggregate</code> = mo\u017ce by\u0107 wolne - \ud83d\udd0d Dzia\u0142anie : Zwi\u0119kszy\u0107 <code>work_mem</code> je\u015bli \"Batches &gt; 1\"</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#interpretacja-kosztow","title":"Interpretacja koszt\u00f3w","text":""},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#struktura-kosztow","title":"Struktura koszt\u00f3w","text":"<pre><code>cost=0.00..25.00\n  \u2191      \u2191\n  |      \u2514\u2500 Koszt ca\u0142kowity\n  \u2514\u2500 Koszt startowy\n</code></pre> <p>Koszt startowy : Koszt przed zwr\u00f3ceniem pierwszego wiersza Koszt ca\u0142kowity : Koszt dla zwr\u00f3cenia wszystkich wierszy</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#porownanie-kosztow","title":"Por\u00f3wnanie koszt\u00f3w","text":"<p>Og\u00f3lna zasada : - Koszt &lt; 100 : Bardzo szybko - Koszt 100-1000 : Szybko - Koszt 1000-10000 : Umiarkowanie - Koszt &gt; 10000 : Potencjalnie wolno</p> <p>\u26a0\ufe0f Wa\u017cne : Koszty s\u0105 wzgl\u0119dne i zale\u017c\u0105 od konfiguracji.</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#roznica-miedzy-szacunkiem-a-rzeczywistoscia","title":"R\u00f3\u017cnica mi\u0119dzy szacunkiem a rzeczywisto\u015bci\u0105","text":"<p>Por\u00f3wna\u0107 : - <code>rows</code> (szacowane) vs <code>rows</code> (rzeczywiste w ANALYZE) - <code>cost</code> (szacowane) vs <code>actual time</code> (rzeczywiste)</p> <p>Przyk\u0142ad problematyczny : <pre><code>Seq Scan on users\n  (cost=0.00..25.00 rows=1 width=64)\n  (actual time=0.123..1500.456 rows=100000 loops=1)\n</code></pre></p> <p>Problem : Szacowanie bardzo nieprawid\u0142owe (1 wiersz szacowany, 100000 rzeczywistych) Dzia\u0142anie : Wykona\u0107 <code>ANALYZE users;</code></p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#sygnay-alarmowe","title":"Sygna\u0142y alarmowe","text":""},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#alarmy-krytyczne","title":"\ud83d\udd34 Alarmy krytyczne","text":"<ol> <li> <p>Seq Scan na du\u017cej tabeli <pre><code>Seq Scan on large_table (cost=0.00..50000.00 rows=1000000)\n</code></pre> Dzia\u0142anie : Utworzy\u0107 odpowiedni indeks</p> </li> <li> <p>Sortowanie na dysku <pre><code>Sort Method: external merge  Disk: 50000kB\n</code></pre> Dzia\u0142anie : Zwi\u0119kszy\u0107 <code>work_mem</code></p> </li> <li> <p>Bardzo nieprawid\u0142owe szacowanie <pre><code>rows=1 (szacowane) vs rows=100000 (rzeczywiste)\n</code></pre> Dzia\u0142anie : Wykona\u0107 <code>ANALYZE</code></p> </li> <li> <p>Hash Join z wieloma batchami <pre><code>Hash Join\n  Batches: 16  Memory Usage: 512kB\n</code></pre> Dzia\u0142anie : Zwi\u0119kszy\u0107 <code>work_mem</code></p> </li> <li> <p>Nested Loop z du\u017c\u0105 p\u0119tl\u0105 zewn\u0119trzn\u0105 <pre><code>Nested Loop (loops=100000)\n</code></pre> Dzia\u0142anie : Sprawdzi\u0107 indeksy lub zmieni\u0107 typ z\u0142\u0105czenia</p> </li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#alarmy-umiarkowane","title":"\ud83d\udfe1 Alarmy umiarkowane","text":"<ol> <li> <p>Index Scan z wieloma Heap Fetches <pre><code>Index Only Scan\n  Heap Fetches: 50000\n</code></pre> Dzia\u0142anie : Sprawdzi\u0107 widoczno\u015b\u0107 krotek</p> </li> <li> <p>Bitmap Heap Scan z wieloma rechecks <pre><code>Rows Removed by Filter: 50000\n</code></pre> Dzia\u0142anie : Poprawi\u0107 selektywno\u015b\u0107 indeksu</p> </li> <li> <p>Wysoki czas planowania <pre><code>Planning Time: 500.234 ms\n</code></pre> Dzia\u0142anie : Upro\u015bci\u0107 zapytanie lub zwi\u0119kszy\u0107 <code>plan_cache_mode</code></p> </li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#dobre-praktyki","title":"Dobre praktyki","text":""},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#1-zawsze-uzywac-explain-analyze-dla-wolnych-zapytan","title":"1. Zawsze u\u017cywa\u0107 EXPLAIN ANALYZE dla wolnych zapyta\u0144","text":"<pre><code>EXPLAIN (ANALYZE, BUFFERS, VERBOSE) \nSELECT ...;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#2-porownywac-szacunki-i-rzeczywistosc","title":"2. Por\u00f3wnywa\u0107 szacunki i rzeczywisto\u015b\u0107","text":"<p>Sprawdzi\u0107 czy <code>rows</code> (szacowane) \u2248 <code>rows</code> (rzeczywiste)</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#3-monitorowac-bufory","title":"3. Monitorowa\u0107 bufory","text":"<pre><code>EXPLAIN (ANALYZE, BUFFERS) SELECT ...;\n</code></pre> <ul> <li><code>shared hit</code> wysoki = dobrze (cache)</li> <li><code>shared read</code> wysoki = mo\u017cna poprawi\u0107 (I/O dysku)</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#4-identyfikowac-kosztowne-operacje","title":"4. Identyfikowa\u0107 kosztowne operacje","text":"<p>Szuka\u0107 operacji z : - <code>actual time</code> wysokim - <code>loops</code> wysokim - <code>rows</code> znacznie wy\u017cszym ni\u017c szacunek</p>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#5-uzywac-narzedzi-wizualizacji","title":"5. U\u017cywa\u0107 narz\u0119dzi wizualizacji","text":"<ul> <li>pgAdmin : Wizualizacja graficzna plan\u00f3w</li> <li>explain.dalibo.com : Analiza online</li> <li>pev : PostgreSQL Explain Visualizer</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>EXPLAIN = szacunek, EXPLAIN ANALYZE = rzeczywisto\u015b\u0107</li> <li>Seq Scan na du\u017cej tabeli = potencjalny problem</li> <li>Sortowanie na dysku = zwi\u0119kszy\u0107 <code>work_mem</code></li> <li>Nieprawid\u0142owe szacowanie = wykona\u0107 <code>ANALYZE</code></li> <li>Zawsze por\u00f3wnywa\u0107 szacunek vs rzeczywisto\u015b\u0107</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/02-plans-execution/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 3. Dalibo - Narz\u0119dzie analizy, aby nauczy\u0107 si\u0119 u\u017cywa\u0107 Dalibo do analizy wydajno\u015bci.</p>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/","title":"3. Dalibo - Narz\u0119dzie analizy wydajno\u015bci","text":""},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Zrozumie\u0107 ekosystem Dalibo</li> <li>Zainstalowa\u0107 i skonfigurowa\u0107 narz\u0119dzia Dalibo</li> <li>U\u017cywa\u0107 pg_stat_statements do analizy</li> <li>Generowa\u0107 i interpretowa\u0107 raporty wydajno\u015bci</li> <li>U\u017cywa\u0107 automatycznych rekomendacji</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Prezentacja Dalibo</li> <li>Instalacja i konfiguracja</li> <li>pg_stat_statements</li> <li>pg_qualstats</li> <li>pg_stat_monitor</li> <li>Raporty i wizualizacje</li> <li>Rekomendacje automatyczne</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#prezentacja-dalibo","title":"Prezentacja Dalibo","text":""},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#ekosystem-dalibo","title":"Ekosystem Dalibo","text":"<p>Dalibo oferuje zestaw narz\u0119dzi open-source do analizy wydajno\u015bci PostgreSQL :</p> <ul> <li>pg_stat_statements : Statystyki zapyta\u0144 SQL</li> <li>pg_qualstats : Statystyki predykat\u00f3w (WHERE, JOIN)</li> <li>pg_stat_monitor : Zaawansowany monitoring z agregacj\u0105 czasow\u0105</li> <li>pg_wait_sampling : Analiza oczekiwa\u0144</li> <li>HypoPG : Test indeks\u00f3w hipotetycznych</li> <li>pgBadger : Analiza log\u00f3w PostgreSQL</li> <li>pg_activity : Monitoring w czasie rzeczywistym</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#zalety","title":"Zalety","text":"<p>\u2705 Open-source : Darmowe i modyfikowalne \u2705 Kompletne : Obejmuje wszystkie aspekty wydajno\u015bci \u2705 Zintegrowane : Dzia\u0142a z natywnym PostgreSQL \u2705 Spo\u0142eczno\u015b\u0107 : Aktywne wsparcie i dokumentacja</p>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#instalacja-i-konfiguracja","title":"Instalacja i konfiguracja","text":""},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#wymagania-wstepne","title":"Wymagania wst\u0119pne","text":"<ul> <li>PostgreSQL 12+ (niekt\u00f3re narz\u0119dzia wymagaj\u0105 okre\u015blonych wersji)</li> <li>Dost\u0119p superu\u017cytkownika do instalacji rozszerze\u0144</li> <li>Kompilator C dla niekt\u00f3rych rozszerze\u0144</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#instalacja-pg_stat_statements","title":"Instalacja pg_stat_statements","text":"<p>PostgreSQL 9.2+ : W\u0142\u0105czone domy\u015blnie</p> <pre><code>-- Aktywowa\u0107 rozszerzenie\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- Sprawdzi\u0107 instalacj\u0119\nSELECT * FROM pg_extension WHERE extname = 'pg_stat_statements';\n</code></pre> <p>Konfiguracja w postgresql.conf :</p> <pre><code># Za\u0142adowa\u0107 rozszerzenie przy starcie\nshared_preload_libraries = 'pg_stat_statements'\n\n# Liczba unikalnych zapyta\u0144 do \u015bledzenia (domy\u015blnie: 10000)\npg_stat_statements.max = 10000\n\n# Maksymalny rozmiar zapytania przechowywanego (domy\u015blnie: 1024)\npg_stat_statements.track = all\npg_stat_statements.track_utility = on\npg_stat_statements.save = on\n</code></pre> <p>Uruchomi\u0107 ponownie PostgreSQL po modyfikacji</p>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#instalacja-pg_qualstats","title":"Instalacja pg_qualstats","text":"<p>Pobieranie i kompilacja :</p> <pre><code># Sklonowa\u0107 repozytorium\ngit clone https://github.com/dalibo/pg_qualstats.git\ncd pg_qualstats\n\n# Skompilowa\u0107 i zainstalowa\u0107\nmake\nsudo make install\n</code></pre> <p>Aktywacja :</p> <pre><code>-- Doda\u0107 do shared_preload_libraries\n-- W postgresql.conf:\n-- shared_preload_libraries = 'pg_stat_statements,pg_qualstats'\n\n-- Utworzy\u0107 rozszerzenie\nCREATE EXTENSION IF NOT EXISTS pg_qualstats;\n\n-- Sprawdzi\u0107\nSELECT * FROM pg_extension WHERE extname = 'pg_qualstats';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#instalacja-pg_stat_monitor","title":"Instalacja pg_stat_monitor","text":"<p>Dla PostgreSQL 12+ :</p> <pre><code># Instalacja przez mened\u017cer pakiet\u00f3w (przyk\u0142ad Ubuntu)\nsudo apt-get install postgresql-14-pg-stat-monitor\n\n# Lub kompilacja ze \u017ar\u00f3de\u0142\ngit clone https://github.com/percona/pg_stat_monitor.git\ncd pg_stat_monitor\nmake\nsudo make install\n</code></pre> <p>Aktywacja :</p> <pre><code>-- W postgresql.conf:\n-- shared_preload_libraries = 'pg_stat_monitor'\n\nCREATE EXTENSION IF NOT EXISTS pg_stat_monitor;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#pg_stat_statements","title":"pg_stat_statements","text":""},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#przeglad","title":"Przegl\u0105d","text":"<p><code>pg_stat_statements</code> zbiera statystyki o wszystkich wykonywanych zapytaniach SQL.</p>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#najbardziej-kosztowne-zapytania","title":"Najbardziej kosztowne zapytania","text":"<pre><code>-- Top 10 zapyta\u0144 wed\u0142ug ca\u0142kowitego czasu\nSELECT \n    query,\n    calls,\n    total_exec_time,\n    mean_exec_time,\n    max_exec_time,\n    stddev_exec_time,\n    rows,\n    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#najczestsze-zapytania","title":"Najcz\u0119stsze zapytania","text":"<pre><code>-- Top 10 zapyta\u0144 wed\u0142ug liczby wywo\u0142a\u0144\nSELECT \n    query,\n    calls,\n    total_exec_time,\n    mean_exec_time,\n    (total_exec_time / sum(total_exec_time) OVER ()) * 100 AS percent_total_time\nFROM pg_stat_statements\nORDER BY calls DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#zapytania-z-wysokim-io","title":"Zapytania z wysokim I/O","text":"<pre><code>-- Zapytania z wieloma odczytami z dysku\nSELECT \n    query,\n    calls,\n    shared_blks_read,\n    shared_blks_hit,\n    shared_blks_dirtied,\n    shared_blks_written,\n    temp_blks_read,\n    temp_blks_written\nFROM pg_stat_statements\nWHERE shared_blks_read &gt; 1000\nORDER BY shared_blks_read DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#szczegoowa-analiza-zapytania","title":"Szczeg\u00f3\u0142owa analiza zapytania","text":"<pre><code>-- Pe\u0142ne statystyki dla konkretnego zapytania\nSELECT \n    query,\n    calls,\n    total_exec_time,\n    min_exec_time,\n    max_exec_time,\n    mean_exec_time,\n    stddev_exec_time,\n    rows,\n    shared_blks_hit,\n    shared_blks_read,\n    shared_blks_dirtied,\n    shared_blks_written,\n    temp_blks_read,\n    temp_blks_written,\n    blk_read_time,\n    blk_write_time\nFROM pg_stat_statements\nWHERE query LIKE '%SELECT * FROM users%'\nORDER BY total_exec_time DESC;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#resetowanie-statystyk","title":"Resetowanie statystyk","text":"<pre><code>-- Resetowa\u0107 wszystkie statystyki\nSELECT pg_stat_statements_reset();\n\n-- Resetowa\u0107 dla konkretnej bazy\nSELECT pg_stat_statements_reset(userid, dbid, queryid);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#normalizacja-zapytan","title":"Normalizacja zapyta\u0144","text":"<p><code>pg_stat_statements</code> normalizuje zapytania, zast\u0119puj\u0105c warto\u015bci przez <code>$1</code>, <code>$2</code>, etc.</p> <p>Przyk\u0142ad : <pre><code>-- Oryginalne zapytanie\nSELECT * FROM users WHERE id = 123;\n\n-- Znormalizowane w pg_stat_statements\nSELECT * FROM users WHERE id = $1;\n</code></pre></p> <p>Zaleta : Grupuje podobne zapytania z r\u00f3\u017cnymi parametrami.</p>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#pg_qualstats","title":"pg_qualstats","text":""},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#przeglad_1","title":"Przegl\u0105d","text":"<p><code>pg_qualstats</code> zbiera statystyki o predykatach (warunki WHERE, JOIN) w celu identyfikacji brakuj\u0105cych indeks\u00f3w.</p>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#statystyki-predykatow","title":"Statystyki predykat\u00f3w","text":"<pre><code>-- Top najcz\u0119\u015bciej u\u017cywanych predykat\u00f3w\nSELECT \n    left_schema,\n    left_table,\n    left_column,\n    operator,\n    count(*) AS execution_count,\n    n_distinct,\n    most_common_vals\nFROM pg_qualstats\nGROUP BY left_schema, left_table, left_column, operator\nORDER BY execution_count DESC\nLIMIT 20;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#identyfikacja-brakujacych-indeksow","title":"Identyfikacja brakuj\u0105cych indeks\u00f3w","text":"<pre><code>-- Predykaty bez odpowiadaj\u0105cego indeksu\nSELECT \n    qs.left_schema,\n    qs.left_table,\n    qs.left_column,\n    qs.operator,\n    qs.execution_count,\n    pg_size_pretty(pg_relation_size(qs.left_schema||'.'||qs.left_table)) AS table_size\nFROM pg_qualstats qs\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM pg_index i\n    JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n    WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n    AND a.attname = qs.left_column\n)\nORDER BY qs.execution_count DESC\nLIMIT 20;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#rekomendacje-indeksow","title":"Rekomendacje indeks\u00f3w","text":"<pre><code>-- Generowa\u0107 polecenia CREATE INDEX\nSELECT \n    'CREATE INDEX idx_' || \n    left_table || '_' || \n    left_column || \n    ' ON ' || left_schema || '.' || left_table || \n    ' (' || left_column || ');' AS create_index_command,\n    execution_count,\n    n_distinct\nFROM (\n    SELECT \n        qs.left_schema,\n        qs.left_table,\n        qs.left_column,\n        COUNT(*) AS execution_count,\n        COUNT(DISTINCT qs.most_common_vals) AS n_distinct\n    FROM pg_qualstats qs\n    WHERE NOT EXISTS (\n        SELECT 1\n        FROM pg_index i\n        JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n        WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n        AND a.attname = qs.left_column\n    )\n    GROUP BY qs.left_schema, qs.left_table, qs.left_column\n) AS missing_indexes\nORDER BY execution_count DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#resetowanie-statystyk_1","title":"Resetowanie statystyk","text":"<pre><code>-- Resetowa\u0107 pg_qualstats\nSELECT pg_qualstats_reset();\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#pg_stat_monitor","title":"pg_stat_monitor","text":""},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#przeglad_2","title":"Przegl\u0105d","text":"<p><code>pg_stat_monitor</code> oferuje zaawansowany monitoring z agregacj\u0105 czasow\u0105 i analiz\u0105 bucket\u00f3w.</p>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#konfiguracja","title":"Konfiguracja","text":"<pre><code>-- Zobaczy\u0107 konfiguracj\u0119\nSELECT * FROM pg_stat_monitor_settings;\n\n-- Zmodyfikowa\u0107 konfiguracj\u0119\nALTER SYSTEM SET pg_stat_monitor.pgsm_max_buckets = 10;\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#zapytania-wedug-bucketa-okres","title":"Zapytania wed\u0142ug bucketa (okres)","text":"<pre><code>-- Zapytania pogrupowane wed\u0142ug okresu\nSELECT \n    bucket,\n    bucket_start_time,\n    query,\n    calls,\n    total_exec_time,\n    mean_exec_time,\n    max_exec_time\nFROM pg_stat_monitor\nORDER BY bucket DESC, total_exec_time DESC\nLIMIT 20;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#analiza-bedow","title":"Analiza b\u0142\u0119d\u00f3w","text":"<pre><code>-- Zapytania z b\u0142\u0119dami\nSELECT \n    query,\n    calls,\n    errors,\n    error_count,\n    error_code\nFROM pg_stat_monitor\nWHERE errors &gt; 0\nORDER BY errors DESC;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#analiza-planow","title":"Analiza plan\u00f3w","text":"<pre><code>-- Najcz\u0119\u015bciej u\u017cywane plany wykonania\nSELECT \n    query,\n    planid,\n    calls,\n    mean_exec_time,\n    plans\nFROM pg_stat_monitor\nWHERE plans IS NOT NULL\nORDER BY calls DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#raporty-i-wizualizacje","title":"Raporty i wizualizacje","text":""},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#pgbadger-analiza-logow","title":"pgBadger - Analiza log\u00f3w","text":"<p>Instalacja :</p> <pre><code># Ubuntu/Debian\nsudo apt-get install pgbadger\n\n# Lub przez Perl CPAN\ncpanm pgbadger\n</code></pre> <p>Generowanie raportu :</p> <pre><code># Generowa\u0107 raport HTML\npgbadger /var/log/postgresql/postgresql-*.log -o report.html\n\n# Z opcjami zaawansowanymi\npgbadger \\\n  --prefix '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h' \\\n  --outdir /var/www/pgbadger \\\n  /var/log/postgresql/postgresql-*.log\n</code></pre> <p>Konfiguracja PostgreSQL dla pgBadger :</p> <pre><code># W postgresql.conf\nlogging_collector = on\nlog_directory = 'log'\nlog_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'\nlog_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '\nlog_checkpoints = on\nlog_connections = on\nlog_disconnections = on\nlog_lock_waits = on\nlog_temp_files = 0\nlog_autovacuum_min_duration = 0\nlog_error_verbosity = default\nlog_min_duration_statement = 1000  # Log zapytania &gt; 1s\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#pg_activity-monitoring-czasu-rzeczywistego","title":"pg_activity - Monitoring czasu rzeczywistego","text":"<p>Instalacja :</p> <pre><code>pip install pg_activity\n</code></pre> <p>U\u017cycie :</p> <pre><code># Proste po\u0142\u0105czenie\npg_activity -U postgres -d mydb\n\n# Z opcjami\npg_activity -U postgres -d mydb --refresh 2 --no-database-size\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#wizualizacja-z-metabasegrafana","title":"Wizualizacja z Metabase/Grafana","text":"<p>Integracja z Grafana :</p> <ol> <li>Zainstalowa\u0107 plugin PostgreSQL</li> <li>Utworzy\u0107 dashboardy z widokami systemowymi</li> <li>Monitorowa\u0107 metryki w czasie rzeczywistym</li> </ol> <p>Przydatne zapytania dla Grafana :</p> <pre><code>-- \u015aredni czas wykonania na minut\u0119\nSELECT \n    date_trunc('minute', now()) AS time,\n    AVG(mean_exec_time) AS avg_exec_time\nFROM pg_stat_statements\nGROUP BY time;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#rekomendacje-automatyczne","title":"Rekomendacje automatyczne","text":""},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#podstawowy-skrypt-rekomendacji","title":"Podstawowy skrypt rekomendacji","text":"<pre><code>-- Wolne zapytania bez odpowiedniego indeksu\nWITH slow_queries AS (\n    SELECT \n        query,\n        calls,\n        mean_exec_time,\n        total_exec_time\n    FROM pg_stat_statements\n    WHERE mean_exec_time &gt; 100  -- &gt; 100ms\n    ORDER BY total_exec_time DESC\n    LIMIT 10\n),\nmissing_indexes AS (\n    SELECT \n        qs.left_schema,\n        qs.left_table,\n        qs.left_column,\n        qs.operator,\n        COUNT(*) AS execution_count\n    FROM pg_qualstats qs\n    WHERE NOT EXISTS (\n        SELECT 1\n        FROM pg_index i\n        JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n        WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n        AND a.attname = qs.left_column\n    )\n    GROUP BY qs.left_schema, qs.left_table, qs.left_column, qs.operator\n)\nSELECT \n    'MISSING INDEX' AS recommendation_type,\n    'CREATE INDEX idx_' || left_table || '_' || left_column || \n    ' ON ' || left_schema || '.' || left_table || \n    ' (' || left_column || ');' AS recommendation,\n    execution_count AS priority_score\nFROM missing_indexes\nORDER BY execution_count DESC\nLIMIT 10;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#uzywac-hypopg-do-testowania-indeksow","title":"U\u017cywa\u0107 HypoPG do testowania indeks\u00f3w","text":"<p>Instalacja :</p> <pre><code>CREATE EXTENSION IF NOT EXISTS hypopg;\n</code></pre> <p>Testowa\u0107 indeks hipotetyczny :</p> <pre><code>-- Utworzy\u0107 indeks hipotetyczny\nSELECT * FROM hypopg_create_index('CREATE INDEX ON users(email)');\n\n-- Zobaczy\u0107 indeksy hipotetyczne\nSELECT * FROM hypopg_list_indexes();\n\n-- Testowa\u0107 plan z indeksem hipotetycznym\nEXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';\n\n-- Usun\u0105\u0107 indeks hipotetyczny\nSELECT hypopg_drop_index(oid) FROM hypopg_list_indexes();\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>pg_stat_statements : Niezb\u0119dne do identyfikacji wolnych zapyta\u0144</li> <li>pg_qualstats : Identyfikuje brakuj\u0105ce indeksy automatycznie</li> <li>pg_stat_monitor : Zaawansowany monitoring z agregacj\u0105 czasow\u0105</li> <li>pgBadger : Kompletna analiza log\u00f3w PostgreSQL</li> <li>HypoPG : Testuje indeksy przed ich utworzeniem</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/03-dalibo/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 4. Wska\u017aniki wydajno\u015bci, aby nauczy\u0107 si\u0119 interpretowa\u0107 kluczowe wska\u017aniki wydajno\u015bci.</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/","title":"4. Wska\u017aniki wydajno\u015bci","text":""},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Identyfikowa\u0107 kluczowe metryki do monitorowania</li> <li>Interpretowa\u0107 wska\u017aniki Dalibo</li> <li>Definiowa\u0107 progi alarmowe</li> <li>Rozumie\u0107 korelacje mi\u0119dzy wska\u017anikami</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Metryki systemowe</li> <li>Metryki zapyta\u0144</li> <li>Metryki indeks\u00f3w</li> <li>Metryki I/O</li> <li>Progi alarmowe</li> <li>Pulpit nawigacyjny wska\u017anik\u00f3w</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#metryki-systemowe","title":"Metryki systemowe","text":""},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#wykorzystanie-cpu","title":"Wykorzystanie CPU","text":"<pre><code>-- Zobaczy\u0107 wykorzystanie CPU wed\u0142ug procesu\nSELECT \n    pid,\n    usename,\n    application_name,\n    state,\n    query_start,\n    state_change,\n    wait_event_type,\n    wait_event,\n    query\nFROM pg_stat_activity\nWHERE state = 'active'\nORDER BY query_start;\n</code></pre> <p>Kluczowe wska\u017aniki : - Wysokie CPU : Zapytania w trakcie wykonywania - wait_event_type = CPU : Proces oczekuj\u0105cy na CPU</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#wykorzystanie-pamieci","title":"Wykorzystanie pami\u0119ci","text":"<pre><code>-- U\u017cywana pami\u0119\u0107 wsp\u00f3\u0142dzielona\nSELECT \n    setting AS shared_buffers,\n    pg_size_pretty(setting::bigint * 8192) AS shared_buffers_size\nFROM pg_settings\nWHERE name = 'shared_buffers';\n\n-- U\u017cywana pami\u0119\u0107 robocza\nSELECT \n    setting AS work_mem,\n    pg_size_pretty(setting::bigint * 1024) AS work_mem_size\nFROM pg_settings\nWHERE name = 'work_mem';\n\n-- Statystyki pami\u0119ci\nSELECT \n    name,\n    setting,\n    unit,\n    short_desc\nFROM pg_settings\nWHERE name IN ('shared_buffers', 'work_mem', 'maintenance_work_mem', 'effective_cache_size')\nORDER BY name;\n</code></pre> <p>Kluczowe wska\u017aniki : - shared_buffers : Cache wsp\u00f3\u0142dzielony (zalecane: 25% RAM) - work_mem : Pami\u0119\u0107 na operacj\u0119 sortowania/hash - effective_cache_size : Szacunek cache OS (zalecane: 50-75% RAM)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#aktywne-poaczenia","title":"Aktywne po\u0142\u0105czenia","text":"<pre><code>-- Liczba po\u0142\u0105cze\u0144 wed\u0142ug stanu\nSELECT \n    state,\n    COUNT(*) AS count,\n    COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () AS percent\nFROM pg_stat_activity\nGROUP BY state\nORDER BY count DESC;\n\n-- Po\u0142\u0105czenia wed\u0142ug bazy danych\nSELECT \n    datname,\n    COUNT(*) AS connections,\n    COUNT(*) FILTER (WHERE state = 'active') AS active,\n    COUNT(*) FILTER (WHERE state = 'idle') AS idle,\n    COUNT(*) FILTER (WHERE state = 'idle in transaction') AS idle_in_transaction\nFROM pg_stat_activity\nWHERE datname IS NOT NULL\nGROUP BY datname\nORDER BY connections DESC;\n\n-- Limit po\u0142\u0105cze\u0144\nSELECT \n    setting AS max_connections,\n    (SELECT COUNT(*) FROM pg_stat_activity) AS current_connections,\n    ROUND(100.0 * (SELECT COUNT(*) FROM pg_stat_activity) / setting::numeric, 2) AS percent_used\nFROM pg_settings\nWHERE name = 'max_connections';\n</code></pre> <p>Kluczowe wska\u017aniki : - max_connections : Skonfigurowany limit - idle in transaction : Blokuj\u0105ce po\u0142\u0105czenia (alarm je\u015bli &gt; 5%) - active : Zapytania w trakcie</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#blokady-locks","title":"Blokady (Locks)","text":"<pre><code>-- Blokady oczekuj\u0105ce\nSELECT \n    blocked_locks.pid AS blocked_pid,\n    blocked_activity.usename AS blocked_user,\n    blocking_locks.pid AS blocking_pid,\n    blocking_activity.usename AS blocking_user,\n    blocked_activity.query AS blocked_statement,\n    blocking_activity.query AS blocking_statement\nFROM pg_catalog.pg_locks blocked_locks\nJOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid\nJOIN pg_catalog.pg_locks blocking_locks \n    ON blocking_locks.locktype = blocked_locks.locktype\n    AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database\n    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation\n    AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page\n    AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple\n    AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid\n    AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid\n    AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid\n    AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid\n    AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid\n    AND blocking_locks.pid != blocked_locks.pid\nJOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid\nWHERE NOT blocked_locks.granted;\n</code></pre> <p>Kluczowe wska\u017aniki : - Blokady oczekuj\u0105ce : Aktywne blokady (alarm je\u015bli &gt; 0) - Czas trwania blokad : Czas oczekiwania (alarm je\u015bli &gt; 1s)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#metryki-zapytan","title":"Metryki zapyta\u0144","text":""},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#czas-wykonania","title":"Czas wykonania","text":"<pre><code>-- \u015aredni czas wykonania wed\u0142ug zapytania (top 20)\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    ROUND(total_exec_time::numeric, 2) AS total_time_ms,\n    ROUND(mean_exec_time::numeric, 2) AS mean_time_ms,\n    ROUND(max_exec_time::numeric, 2) AS max_time_ms,\n    ROUND(stddev_exec_time::numeric, 2) AS stddev_time_ms,\n    ROUND((total_exec_time / SUM(total_exec_time) OVER ()) * 100, 2) AS percent_total_time\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 20;\n</code></pre> <p>Kluczowe wska\u017aniki : - mean_exec_time : \u015aredni czas (alarm je\u015bli &gt; 1000ms) - max_exec_time : Maksymalny czas (alarm je\u015bli &gt; 5000ms) - stddev_exec_time : Zmienno\u015b\u0107 (alarm je\u015bli stddev &gt; mean)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#czestotliwosc-wykonania","title":"Cz\u0119stotliwo\u015b\u0107 wykonania","text":"<pre><code>-- Najcz\u0119stsze zapytania\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    ROUND(mean_exec_time::numeric, 2) AS mean_time_ms,\n    ROUND((calls * mean_exec_time)::numeric, 2) AS total_time_ms,\n    ROUND((calls::numeric / (SELECT SUM(calls) FROM pg_stat_statements)) * 100, 2) AS percent_calls\nFROM pg_stat_statements\nORDER BY calls DESC\nLIMIT 20;\n</code></pre> <p>Kluczowe wska\u017aniki : - calls : Liczba wywo\u0142a\u0144 (identyfikowa\u0107 zapytania N+1) - percent_calls : Procent ca\u0142kowity (alarm je\u015bli &gt; 50% dla jednego zapytania)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#skutecznosc-cache","title":"Skuteczno\u015b\u0107 cache","text":"<pre><code>-- Wsp\u00f3\u0142czynnik trafie\u0144 cache wed\u0142ug zapytania\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    shared_blks_hit,\n    shared_blks_read,\n    ROUND(100.0 * shared_blks_hit / NULLIF(shared_blks_hit + shared_blks_read, 0), 2) AS cache_hit_ratio,\n    ROUND((shared_blks_read * 8)::numeric / 1024, 2) AS disk_read_mb\nFROM pg_stat_statements\nWHERE shared_blks_hit + shared_blks_read &gt; 0\nORDER BY shared_blks_read DESC\nLIMIT 20;\n</code></pre> <p>Kluczowe wska\u017aniki : - cache_hit_ratio : Wsp\u00f3\u0142czynnik trafie\u0144 (cel: &gt; 95%) - disk_read_mb : Odczyty z dysku (alarm je\u015bli &gt; 100MB)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#zapytania-z-io-tymczasowym","title":"Zapytania z I/O tymczasowym","text":"<pre><code>-- Zapytania u\u017cywaj\u0105ce plik\u00f3w tymczasowych\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    temp_blks_read,\n    temp_blks_written,\n    ROUND((temp_blks_read + temp_blks_written) * 8.0 / 1024, 2) AS temp_mb,\n    ROUND(mean_exec_time::numeric, 2) AS mean_time_ms\nFROM pg_stat_statements\nWHERE temp_blks_read &gt; 0 OR temp_blks_written &gt; 0\nORDER BY (temp_blks_read + temp_blks_written) DESC\nLIMIT 20;\n</code></pre> <p>Kluczowe wska\u017aniki : - temp_blks_read/written : I/O tymczasowe (alarm je\u015bli &gt; 0) - Dzia\u0142anie : Zwi\u0119kszy\u0107 <code>work_mem</code> je\u015bli obecne</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#metryki-indeksow","title":"Metryki indeks\u00f3w","text":""},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#wykorzystanie-indeksow","title":"Wykorzystanie indeks\u00f3w","text":"<pre><code>-- Statystyki wykorzystania indeks\u00f3w\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan AS index_scans,\n    idx_tup_read AS tuples_read,\n    idx_tup_fetch AS tuples_fetched,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size\nFROM pg_stat_user_indexes\nORDER BY idx_scan ASC, pg_relation_size(indexrelid) DESC\nLIMIT 20;\n</code></pre> <p>Kluczowe wska\u017aniki : - idx_scan = 0 : Indeks nieu\u017cywany (kandydat do usuni\u0119cia) - index_size : Rozmiar indeksu (koszt utrzymania)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#brakujace-indeksy-przez-pg_qualstats","title":"Brakuj\u0105ce indeksy (przez pg_qualstats)","text":"<pre><code>-- Cz\u0119ste predykaty bez indeksu\nSELECT \n    qs.left_schema,\n    qs.left_table,\n    qs.left_column,\n    qs.operator,\n    COUNT(*) AS execution_count,\n    pg_size_pretty(pg_relation_size((qs.left_schema||'.'||qs.left_table)::regclass)) AS table_size\nFROM pg_qualstats qs\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM pg_index i\n    JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n    WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n    AND a.attname = qs.left_column\n)\nGROUP BY qs.left_schema, qs.left_table, qs.left_column, qs.operator\nORDER BY execution_count DESC\nLIMIT 20;\n</code></pre> <p>Kluczowe wska\u017aniki : - execution_count : Cz\u0119stotliwo\u015b\u0107 u\u017cycia (priorytet) - table_size : Rozmiar tabeli (wp\u0142yw indeksu)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#bloat-indeksow","title":"Bloat indeks\u00f3w","text":"<pre><code>-- Wykrywanie bloatu indeks\u00f3w\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,\n    idx_scan AS index_scans,\n    CASE \n        WHEN idx_scan = 0 THEN 'UNUSED'\n        WHEN idx_scan &lt; 100 THEN 'LOW'\n        ELSE 'OK'\n    END AS usage_status\nFROM pg_stat_user_indexes\nWHERE pg_relation_size(indexrelid) &gt; 1048576  -- &gt; 1MB\nORDER BY pg_relation_size(indexrelid) DESC;\n</code></pre> <p>Kluczowe wska\u017aniki : - UNUSED : Indeks nigdy nieu\u017cywany (kandydat do usuni\u0119cia) - LOW : Indeks rzadko u\u017cywany (do oceny)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#metryki-io","title":"Metryki I/O","text":""},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#statystyki-io-wedug-tabeli","title":"Statystyki I/O wed\u0142ug tabeli","text":"<pre><code>-- I/O wed\u0142ug tabeli\nSELECT \n    schemaname,\n    tablename,\n    seq_scan,\n    seq_tup_read,\n    idx_scan,\n    idx_tup_fetch,\n    n_tup_ins AS inserts,\n    n_tup_upd AS updates,\n    n_tup_del AS deletes,\n    n_live_tup AS live_rows,\n    n_dead_tup AS dead_rows,\n    ROUND(n_dead_tup::numeric / NULLIF(n_live_tup + n_dead_tup, 0) * 100, 2) AS dead_tuple_percent,\n    last_vacuum,\n    last_autovacuum,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables\nORDER BY seq_scan DESC\nLIMIT 20;\n</code></pre> <p>Kluczowe wska\u017aniki : - seq_scan wysoki : Wiele skanowa\u0144 sekwencyjnych (utworzy\u0107 indeksy) - dead_tuple_percent : Procent martwych krotek (alarm je\u015bli &gt; 10%) - last_vacuum : Ostatni vacuum (alarm je\u015bli &gt; 7 dni)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#statystyki-io-globalne","title":"Statystyki I/O globalne","text":"<pre><code>-- Statystyki I/O globalne\nSELECT \n    datname,\n    blks_read,\n    blks_hit,\n    ROUND(100.0 * blks_hit / NULLIF(blks_hit + blks_read, 0), 2) AS cache_hit_ratio,\n    tup_returned,\n    tup_fetched,\n    tup_inserted,\n    tup_updated,\n    tup_deleted\nFROM pg_stat_database\nWHERE datname NOT IN ('template0', 'template1', 'postgres')\nORDER BY blks_read DESC;\n</code></pre> <p>Kluczowe wska\u017aniki : - cache_hit_ratio : Globalny wsp\u00f3\u0142czynnik trafie\u0144 (cel: &gt; 95%) - blks_read : Odczyty z dysku (alarm je\u015bli wysoki)</p>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#progi-alarmowe","title":"Progi alarmowe","text":""},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#tabela-zalecanych-progow","title":"Tabela zalecanych prog\u00f3w","text":"Metryka Pr\u00f3g alarmowy Pr\u00f3g krytyczny Dzia\u0142anie \u015aredni czas wykonania &gt; 1000ms &gt; 5000ms Analizowa\u0107 plan, utworzy\u0107 indeks Wsp\u00f3\u0142czynnik trafie\u0144 cache &lt; 95% &lt; 90% Zwi\u0119kszy\u0107 shared_buffers Martwe krotki &gt; 10% &gt; 20% Wykona\u0107 VACUUM I/O tymczasowe &gt; 0 &gt; 100MB Zwi\u0119kszy\u0107 work_mem Po\u0142\u0105czenia idle in transaction &gt; 5% &gt; 10% Identyfikowa\u0107 i poprawi\u0107 Blokady oczekuj\u0105ce &gt; 0 &gt; 10 Analizowa\u0107 blokady Indeksy nieu\u017cywane Rozmiar &gt; 100MB Rozmiar &gt; 1GB Oceni\u0107 usuni\u0119cie Ostatni VACUUM &gt; 7 dni &gt; 30 dni Skonfigurowa\u0107 autovacuum"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#zapytanie-monitoringu-globalnego","title":"Zapytanie monitoringu globalnego","text":"<pre><code>-- Pulpit nawigacyjny zdrowia PostgreSQL\nWITH metrics AS (\n    SELECT \n        (SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'active') AS active_queries,\n        (SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'idle in transaction') AS idle_in_transaction,\n        (SELECT COUNT(*) FROM pg_locks WHERE NOT granted) AS waiting_locks,\n        (SELECT AVG(mean_exec_time) FROM pg_stat_statements) AS avg_query_time,\n        (SELECT SUM(shared_blks_read) FROM pg_stat_statements) AS total_disk_reads,\n        (SELECT SUM(shared_blks_hit) FROM pg_stat_statements) AS total_cache_hits,\n        (SELECT COUNT(*) FROM pg_stat_user_indexes WHERE idx_scan = 0) AS unused_indexes,\n        (SELECT SUM(n_dead_tup)::numeric / NULLIF(SUM(n_live_tup + n_dead_tup), 0) * 100 \n         FROM pg_stat_user_tables) AS dead_tuple_percent\n)\nSELECT \n    active_queries,\n    idle_in_transaction,\n    waiting_locks,\n    ROUND(avg_query_time::numeric, 2) AS avg_query_time_ms,\n    ROUND(100.0 * total_cache_hits / NULLIF(total_cache_hits + total_disk_reads, 0), 2) AS cache_hit_ratio,\n    unused_indexes,\n    ROUND(dead_tuple_percent::numeric, 2) AS dead_tuple_percent,\n    CASE \n        WHEN idle_in_transaction &gt; (SELECT setting::int FROM pg_settings WHERE name = 'max_connections') * 0.1 \n        THEN 'ALERT'\n        WHEN waiting_locks &gt; 0 THEN 'WARNING'\n        WHEN avg_query_time &gt; 1000 THEN 'WARNING'\n        WHEN dead_tuple_percent &gt; 10 THEN 'WARNING'\n        ELSE 'OK'\n    END AS health_status\nFROM metrics;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#pulpit-nawigacyjny-wskaznikow","title":"Pulpit nawigacyjny wska\u017anik\u00f3w","text":""},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#skonsolidowany-widok-dla-dalibo","title":"Skonsolidowany widok dla Dalibo","text":"<pre><code>-- Skonsolidowany widok kluczowych wska\u017anik\u00f3w\nCREATE OR REPLACE VIEW v_performance_dashboard AS\nSELECT \n    'QUERIES' AS category,\n    COUNT(*) AS metric_count,\n    ROUND(AVG(mean_exec_time)::numeric, 2) AS avg_value,\n    ROUND(MAX(mean_exec_time)::numeric, 2) AS max_value,\n    'ms' AS unit\nFROM pg_stat_statements\nWHERE mean_exec_time &gt; 100\n\nUNION ALL\n\nSELECT \n    'CACHE_HIT',\n    NULL,\n    ROUND(100.0 * SUM(shared_blks_hit) / NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2),\n    NULL,\n    '%'\nFROM pg_stat_statements\n\nUNION ALL\n\nSELECT \n    'IDLE_IN_TRANSACTION',\n    COUNT(*),\n    NULL,\n    NULL,\n    'count'\nFROM pg_stat_activity\nWHERE state = 'idle in transaction'\n\nUNION ALL\n\nSELECT \n    'WAITING_LOCKS',\n    COUNT(*),\n    NULL,\n    NULL,\n    'count'\nFROM pg_locks\nWHERE NOT granted\n\nUNION ALL\n\nSELECT \n    'UNUSED_INDEXES',\n    COUNT(*),\n    NULL,\n    NULL,\n    'count'\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\n  AND pg_relation_size(indexrelid) &gt; 1048576;  -- &gt; 1MB\n\n-- U\u017cycie\nSELECT * FROM v_performance_dashboard;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#eksport-dla-monitoringu-zewnetrznego","title":"Eksport dla monitoringu zewn\u0119trznego","text":"<pre><code>-- Eksport JSON dla narz\u0119dzi monitoringu\nSELECT json_build_object(\n    'timestamp', NOW(),\n    'metrics', json_build_object(\n        'active_queries', (SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'active'),\n        'cache_hit_ratio', (\n            SELECT ROUND(100.0 * SUM(shared_blks_hit) / NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2)\n            FROM pg_stat_statements\n        ),\n        'avg_query_time_ms', (\n            SELECT ROUND(AVG(mean_exec_time)::numeric, 2)\n            FROM pg_stat_statements\n        ),\n        'unused_indexes', (\n            SELECT COUNT(*) FROM pg_stat_user_indexes WHERE idx_scan = 0\n        )\n    )\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Monitorowa\u0107 regularnie : Metryki zmieniaj\u0105 si\u0119 z czasem</li> <li>Progi kontekstowe : Dostosowa\u0107 wed\u0142ug \u015brodowiska</li> <li>Korelacje : Analizowa\u0107 kilka metryk razem</li> <li>Tendencje : Monitorowa\u0107 ewolucj\u0119 w czasie</li> <li>Dzia\u0142ania priorytetowe : Dzia\u0142a\u0107 na krytycznych metrykach najpierw</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/04-indicateurs/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 5. Techniki optymalizacji, aby nauczy\u0107 si\u0119 praktycznych technik optymalizacji.</p>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/","title":"5. Techniki optymalizacji","text":""},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Opanowa\u0107 techniki optymalizacji z\u0142\u0105cze\u0144</li> <li>Optymalizowa\u0107 agregacje i podzapytania</li> <li>U\u017cywa\u0107 partycjonowania skutecznie</li> <li>Wykorzysta\u0107 r\u00f3wnoleg\u0142o\u015b\u0107 PostgreSQL</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Optymalizacja z\u0142\u0105cze\u0144</li> <li>Optymalizacja agregacji</li> <li>Optymalizacja podzapyta\u0144</li> <li>Partycjonowanie</li> <li>R\u00f3wnoleg\u0142o\u015b\u0107</li> <li>Optymalizacja typ\u00f3w danych</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#optymalizacja-zaczen","title":"Optymalizacja z\u0142\u0105cze\u0144","text":""},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#wybor-typu-zaczenia","title":"Wyb\u00f3r typu z\u0142\u0105czenia","text":"<p>PostgreSQL wybiera automatycznie, ale mo\u017cesz wp\u0142ywa\u0107 :</p> <p>Nested Loop : - \u2705 Ma\u0142a tabela zewn\u0119trzna (&lt; 1000 wierszy) - \u2705 Indeks na kluczu z\u0142\u0105czenia - \u274c Du\u017ca tabela zewn\u0119trzna</p> <p>Hash Join : - \u2705 Tabele podobnej wielko\u015bci - \u2705 Z\u0142\u0105czenie r\u00f3wno\u015bciowe - \u2705 Wystarczaj\u0105co du\u017co <code>work_mem</code> - \u274c Indeks nie jest konieczny</p> <p>Merge Join : - \u2705 Dane ju\u017c posortowane - \u2705 Z\u0142\u0105czenia na posortowanych kluczach - \u274c Wymaga sortowania je\u015bli nie posortowane</p>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#optymalizowac-z-indeksami","title":"Optymalizowa\u0107 z indeksami","text":"<pre><code>-- Przed : Wolne z\u0142\u0105czenie\nEXPLAIN ANALYZE\nSELECT u.*, o.*\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01';\n\n-- Utworzy\u0107 indeksy na kluczach z\u0142\u0105czenia\nCREATE INDEX idx_orders_user_id ON orders(user_id);\nCREATE INDEX idx_users_created_at ON users(created_at);\n\n-- Po : Zoptymalizowane z\u0142\u0105czenie\nEXPLAIN ANALYZE\nSELECT u.*, o.*\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#zmniejszyc-rozmiar-tabel-zaczenia","title":"Zmniejszy\u0107 rozmiar tabel z\u0142\u0105czenia","text":"<pre><code>-- Przed : Z\u0142\u0105czenie na wszystkich wierszach\nSELECT u.*, o.*\nFROM users u\nJOIN orders o ON u.id = o.user_id;\n\n-- Po : Filtrowa\u0107 przed z\u0142\u0105czeniem\nSELECT u.*, o.*\nFROM (\n    SELECT * FROM users WHERE active = true\n) u\nJOIN (\n    SELECT * FROM orders WHERE status = 'completed'\n) o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#kolejnosc-zaczen","title":"Kolejno\u015b\u0107 z\u0142\u0105cze\u0144","text":"<p>Planista wybiera kolejno\u015b\u0107, ale mo\u017cesz wp\u0142ywa\u0107 :</p> <pre><code>-- U\u017cywa\u0107 CTE do wymuszenia kolejno\u015bci\nWITH filtered_users AS (\n    SELECT * FROM users WHERE active = true\n),\nfiltered_orders AS (\n    SELECT * FROM orders WHERE status = 'completed'\n)\nSELECT u.*, o.*\nFROM filtered_users u\nJOIN filtered_orders o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#wiele-zaczen","title":"Wiele z\u0142\u0105cze\u0144","text":"<pre><code>-- Optymalizowa\u0107 kolejno\u015b\u0107 z\u0142\u0105cze\u0144\n-- PostgreSQL zazwyczaj wybiera dobr\u0105 kolejno\u015b\u0107, ale sprawd\u017a z EXPLAIN\n\n-- Z\u0142e : Z\u0142\u0105czenie na du\u017cej tabeli najpierw\nSELECT *\nFROM large_table l\nJOIN small_table1 s1 ON l.id = s1.large_id\nJOIN small_table2 s2 ON l.id = s2.large_id;\n\n-- Lepsze : Filtrowa\u0107 najpierw\nSELECT *\nFROM large_table l\nJOIN (\n    SELECT large_id FROM small_table1 WHERE condition = true\n) s1 ON l.id = s1.large_id\nJOIN (\n    SELECT large_id FROM small_table2 WHERE condition = true\n) s2 ON l.id = s2.large_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#optymalizacja-agregacji","title":"Optymalizacja agregacji","text":""},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#zoptymalizowany-group-by","title":"Zoptymalizowany GROUP BY","text":"<pre><code>-- Przed : Agregacja na wszystkich wierszach\nSELECT status, COUNT(*), AVG(amount)\nFROM orders\nGROUP BY status;\n\n-- Po : Filtrowa\u0107 przed agregacj\u0105\nSELECT status, COUNT(*), AVG(amount)\nFROM orders\nWHERE created_at &gt; '2024-01-01'\nGROUP BY status;\n\n-- Indeks do przyspieszenia\nCREATE INDEX idx_orders_status_created ON orders(status, created_at);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#agregacje-z-having","title":"Agregacje z HAVING","text":"<pre><code>-- Filtrowa\u0107 z WHERE przed GROUP BY (bardziej skuteczne)\n-- Z\u0142e\nSELECT status, COUNT(*)\nFROM orders\nGROUP BY status\nHAVING COUNT(*) &gt; 100;\n\n-- Lepsze : U\u017cywa\u0107 podzapytania\nSELECT status, cnt\nFROM (\n    SELECT status, COUNT(*) AS cnt\n    FROM orders\n    GROUP BY status\n) sub\nWHERE cnt &gt; 100;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#zoptymalizowany-distinct","title":"Zoptymalizowany DISTINCT","text":"<pre><code>-- DISTINCT mo\u017ce by\u0107 kosztowne\nSELECT DISTINCT user_id FROM orders;\n\n-- Czasami GROUP BY jest szybsze\nSELECT user_id FROM orders GROUP BY user_id;\n\n-- Z indeksem, oba mog\u0105 by\u0107 szybkie\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#agregacje-z-oknami","title":"Agregacje z oknami","text":"<pre><code>-- U\u017cywa\u0107 funkcji okienkowych do unikania podzapyta\u0144\n-- Przed : Podzapytanie skorelowane\nSELECT \n    o.*,\n    (SELECT AVG(amount) FROM orders o2 WHERE o2.user_id = o.user_id) AS avg_user_amount\nFROM orders o;\n\n-- Po : Funkcja okienkowa\nSELECT \n    o.*,\n    AVG(amount) OVER (PARTITION BY user_id) AS avg_user_amount\nFROM orders o;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#optymalizacja-podzapytan","title":"Optymalizacja podzapyta\u0144","text":""},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#podzapytania-skorelowane-join","title":"Podzapytania skorelowane \u2192 JOIN","text":"<pre><code>-- Przed : Podzapytanie skorelowane (wolne)\nSELECT \n    u.*,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) AS order_count\nFROM users u;\n\n-- Po : JOIN z agregacj\u0105 (szybsze)\nSELECT \n    u.*,\n    COALESCE(o.order_count, 0) AS order_count\nFROM users u\nLEFT JOIN (\n    SELECT user_id, COUNT(*) AS order_count\n    FROM orders\n    GROUP BY user_id\n) o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#exists-vs-in-vs-join","title":"EXISTS vs IN vs JOIN","text":"<pre><code>-- EXISTS : Zazwyczaj najszybsze\nSELECT *\nFROM users u\nWHERE EXISTS (\n    SELECT 1 FROM orders o WHERE o.user_id = u.id\n);\n\n-- IN : Mo\u017ce by\u0107 wolne je\u015bli lista jest du\u017ca\nSELECT *\nFROM users u\nWHERE u.id IN (\n    SELECT user_id FROM orders\n);\n\n-- JOIN : Dobry kompromis\nSELECT DISTINCT u.*\nFROM users u\nJOIN orders o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#podzapytania-w-select","title":"Podzapytania w SELECT","text":"<pre><code>-- Unika\u0107 podzapyta\u0144 w SELECT je\u015bli mo\u017cliwe\n-- Przed : Podzapytanie wykonywane dla ka\u017cdego wiersza\nSELECT \n    u.*,\n    (SELECT MAX(created_at) FROM orders WHERE user_id = u.id) AS last_order_date\nFROM users u;\n\n-- Po : JOIN z agregacj\u0105\nSELECT \n    u.*,\n    o.last_order_date\nFROM users u\nLEFT JOIN (\n    SELECT user_id, MAX(created_at) AS last_order_date\n    FROM orders\n    GROUP BY user_id\n) o ON u.id = o.user_id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#cte-common-table-expressions","title":"CTE (Common Table Expressions)","text":"<pre><code>-- CTE do poprawy czytelno\u015bci i czasami wydajno\u015bci\nWITH active_users AS (\n    SELECT * FROM users WHERE active = true\n),\nrecent_orders AS (\n    SELECT * FROM orders WHERE created_at &gt; '2024-01-01'\n)\nSELECT \n    u.*,\n    COUNT(o.id) AS order_count\nFROM active_users u\nLEFT JOIN recent_orders o ON u.id = o.user_id\nGROUP BY u.id;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#partycjonowanie","title":"Partycjonowanie","text":""},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#partycjonowanie-wedug-zakresu-range","title":"Partycjonowanie wed\u0142ug zakresu (Range)","text":"<pre><code>-- Utworzy\u0107 tabel\u0119 partycjonowan\u0105\nCREATE TABLE orders (\n    id SERIAL,\n    user_id INTEGER,\n    amount DECIMAL,\n    created_at DATE\n) PARTITION BY RANGE (created_at);\n\n-- Utworzy\u0107 partycje\nCREATE TABLE orders_2024_q1 PARTITION OF orders\n    FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');\n\nCREATE TABLE orders_2024_q2 PARTITION OF orders\n    FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');\n\nCREATE TABLE orders_2024_q3 PARTITION OF orders\n    FOR VALUES FROM ('2024-07-01') TO ('2024-10-01');\n\nCREATE TABLE orders_2024_q4 PARTITION OF orders\n    FOR VALUES FROM ('2024-10-01') TO ('2025-01-01');\n</code></pre> <p>Zalety : - \u2705 Partition pruning (tylko odpowiednie partycje s\u0105 skanowane) - \u2705 Konserwacja wed\u0142ug partycji (VACUUM, ANALYZE) - \u2705 Szybkie usuwanie ca\u0142ych partycji</p>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#partycjonowanie-wedug-listy-list","title":"Partycjonowanie wed\u0142ug listy (List)","text":"<pre><code>-- Partycjonowanie wed\u0142ug regionu\nCREATE TABLE users (\n    id SERIAL,\n    name TEXT,\n    region TEXT\n) PARTITION BY LIST (region);\n\nCREATE TABLE users_europe PARTITION OF users\n    FOR VALUES IN ('FR', 'DE', 'UK', 'IT');\n\nCREATE TABLE users_america PARTITION OF users\n    FOR VALUES IN ('US', 'CA', 'MX');\n\nCREATE TABLE users_asia PARTITION OF users\n    FOR VALUES IN ('JP', 'CN', 'IN');\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#partycjonowanie-wedug-hash","title":"Partycjonowanie wed\u0142ug hash","text":"<pre><code>-- Partycjonowanie wed\u0142ug hash (dla r\u00f3wnomiernej dystrybucji)\nCREATE TABLE events (\n    id SERIAL,\n    user_id INTEGER,\n    event_type TEXT,\n    created_at TIMESTAMP\n) PARTITION BY HASH (user_id);\n\nCREATE TABLE events_0 PARTITION OF events\n    FOR VALUES WITH (MODULUS 4, REMAINDER 0);\n\nCREATE TABLE events_1 PARTITION OF events\n    FOR VALUES WITH (MODULUS 4, REMAINDER 1);\n\nCREATE TABLE events_2 PARTITION OF events\n    FOR VALUES WITH (MODULUS 4, REMAINDER 2);\n\nCREATE TABLE events_3 PARTITION OF events\n    FOR VALUES WITH (MODULUS 4, REMAINDER 3);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#indeksy-na-tabelach-partycjonowanych","title":"Indeksy na tabelach partycjonowanych","text":"<pre><code>-- Utworzy\u0107 indeks na tabeli partycjonowanej (utworzony na wszystkich partycjach)\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n\n-- Lub utworzy\u0107 indeksy specyficzne wed\u0142ug partycji\nCREATE INDEX idx_orders_2024_q1_user_id ON orders_2024_q1(user_id);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#konserwacja-partycji","title":"Konserwacja partycji","text":"<pre><code>-- Sprawdzi\u0107 partycje\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\nFROM pg_tables\nWHERE schemaname = 'public'\n  AND tablename LIKE 'orders_%'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\n-- Usun\u0105\u0107 partycj\u0119 (bardzo szybko)\nDROP TABLE orders_2024_q1;  -- Usuwa partycj\u0119 i jej dane\n\n-- Od\u0142\u0105czy\u0107 partycj\u0119 (zachowa\u0107 dane)\nALTER TABLE orders DETACH PARTITION orders_2024_q1;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#rownolegosc","title":"R\u00f3wnoleg\u0142o\u015b\u0107","text":""},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#konfiguracja-rownolegosci","title":"Konfiguracja r\u00f3wnoleg\u0142o\u015bci","text":"<pre><code>-- Sprawdzi\u0107 konfiguracj\u0119\nSHOW max_parallel_workers_per_gather;\nSHOW max_parallel_workers;\nSHOW max_worker_processes;\n\n-- Zmodyfikowa\u0107 (w postgresql.conf)\n-- max_parallel_workers_per_gather = 4\n-- max_parallel_workers = 8\n-- max_worker_processes = 8\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#kiedy-rownolegosc-jest-uzywana","title":"Kiedy r\u00f3wnoleg\u0142o\u015b\u0107 jest u\u017cywana","text":"<p>PostgreSQL u\u017cywa r\u00f3wnoleg\u0142o\u015bci dla : - \u2705 Skanowa\u0144 sekwencyjnych du\u017cych tabel - \u2705 Z\u0142\u0105cze\u0144 na du\u017cych tabelach - \u2705 Agregacji na du\u017cych tabelach - \u274c Ma\u0142ych tabel (&lt; 8MB domy\u015blnie) - \u274c Zapyta\u0144 z blokadami</p>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#wymusic-rownolegosc","title":"Wymusi\u0107 r\u00f3wnoleg\u0142o\u015b\u0107","text":"<pre><code>-- Zwi\u0119kszy\u0107 min_parallel_table_scan_size do wymuszenia r\u00f3wnoleg\u0142o\u015bci\nSET min_parallel_table_scan_size = 0;  -- Zawsze rozwa\u017ca\u0107 r\u00f3wnoleg\u0142o\u015b\u0107\n\n-- Zobaczy\u0107 plan z r\u00f3wnoleg\u0142o\u015bci\u0105\nEXPLAIN ANALYZE\nSELECT COUNT(*) FROM large_table WHERE condition = 'value';\n</code></pre> <p>Typowy wynik : <pre><code>Finalize Aggregate\n  -&gt; Gather\n      Workers Planned: 4\n      -&gt; Partial Aggregate\n          -&gt; Parallel Seq Scan on large_table\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#optymalizowac-dla-rownolegosci","title":"Optymalizowa\u0107 dla r\u00f3wnoleg\u0142o\u015bci","text":"<pre><code>-- Tabele z wieloma kolumnami : zmniejszy\u0107 work_mem na worker\nSET work_mem = '64MB';\n\n-- Tabele partycjonowane : r\u00f3wnoleg\u0142o\u015b\u0107 wed\u0142ug partycji\n-- Ka\u017cda partycja mo\u017ce by\u0107 skanowana r\u00f3wnolegle\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#optymalizacja-typow-danych","title":"Optymalizacja typ\u00f3w danych","text":""},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#wybrac-odpowiedni-typ","title":"Wybra\u0107 odpowiedni typ","text":"<pre><code>-- Unika\u0107 TEXT dla warto\u015bci ograniczonych\n-- Przed\nCREATE TABLE users (\n    id SERIAL,\n    status TEXT  -- 'active', 'inactive', 'pending'\n);\n\n-- Po : U\u017cywa\u0107 ENUM lub VARCHAR\nCREATE TYPE user_status AS ENUM ('active', 'inactive', 'pending');\nCREATE TABLE users (\n    id SERIAL,\n    status user_status\n);\n\n-- Lub VARCHAR z ograniczeniem\nCREATE TABLE users (\n    id SERIAL,\n    status VARCHAR(20) CHECK (status IN ('active', 'inactive', 'pending'))\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#typy-numeryczne","title":"Typy numeryczne","text":"<pre><code>-- U\u017cywa\u0107 najmniejszego mo\u017cliwego typu\n-- Przed\nCREATE TABLE products (\n    id BIGINT,  -- Je\u015bli nigdy &gt; 2 miliardy\n    price DECIMAL(10,2)\n);\n\n-- Po : Dostosowa\u0107 wed\u0142ug potrzeb\nCREATE TABLE products (\n    id INTEGER,  -- Wystarczaj\u0105ce dla &lt; 2 miliardy\n    price NUMERIC(10,2)  -- NUMERIC = DECIMAL\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#typy-datygodziny","title":"Typy daty/godziny","text":"<pre><code>-- U\u017cywa\u0107 TIMESTAMP WITH TIME ZONE dla dat/godzin\nCREATE TABLE events (\n    id SERIAL,\n    created_at TIMESTAMPTZ,  -- Przechowuje z timezone\n    event_date DATE  -- Dla dat tylko\n);\n\n-- Indeks na datach\nCREATE INDEX idx_events_created_at ON events(created_at);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#json-vs-kolumny-normalne","title":"JSON vs kolumny normalne","text":"<pre><code>-- JSON : Elastyczne ale wolniejsze\nCREATE TABLE products (\n    id SERIAL,\n    metadata JSONB\n);\n\n-- Kolumny normalne : Szybsze je\u015bli struktura sta\u0142a\nCREATE TABLE products (\n    id SERIAL,\n    brand TEXT,\n    category TEXT,\n    tags TEXT[]\n);\n\n-- Indeks GIN dla JSONB\nCREATE INDEX idx_products_metadata_gin ON products USING gin(metadata);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Indeksy na kluczach z\u0142\u0105czenia : Niezb\u0119dne dla szybkich z\u0142\u0105cze\u0144</li> <li>Filtrowa\u0107 przed agregacj\u0105 : Zmniejszy\u0107 rozmiar danych</li> <li>Unika\u0107 podzapyta\u0144 skorelowanych : U\u017cywa\u0107 JOIN zamiast</li> <li>Partycjonowa\u0107 du\u017ce tabele : Poprawia wydajno\u015b\u0107 i konserwacj\u0119</li> <li>R\u00f3wnoleg\u0142o\u015b\u0107 : Automatyczna, ale konfigurowalna</li> <li>Typy danych : Wybra\u0107 najbardziej odpowiedni</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/05-techniques/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 6. Przypadki praktyczne, aby zobaczy\u0107 konkretne przyk\u0142ady optymalizacji.</p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/","title":"6. Przypadki praktyczne optymalizacji","text":""},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Stosowa\u0107 techniki optymalizacji na rzeczywistych przypadkach</li> <li>Analizowa\u0107 problemy wydajno\u015bciowe</li> <li>Mierzy\u0107 wp\u0142yw optymalizacji</li> <li>U\u017cywa\u0107 Dalibo do identyfikacji i rozwi\u0105zywania problem\u00f3w</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#spis-tresci","title":"\ud83d\udccb Spis tre\u015bci","text":"<ol> <li>Przypadek 1 : Wolne zapytanie z skanowaniem sekwencyjnym</li> <li>Przypadek 2 : Wolne z\u0142\u0105czenie na du\u017cej tabeli</li> <li>Przypadek 3 : Wolna agregacja</li> <li>Przypadek 4 : Podzapytanie skorelowane</li> <li>Przypadek 5 : Problem wsp\u00f3\u0142czynnika trafie\u0144 cache</li> <li>Przypadek 6 : Indeksy nieu\u017cywane</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#przypadek-1-wolne-zapytanie-z-skanowaniem-sekwencyjnym","title":"Przypadek 1 : Wolne zapytanie z skanowaniem sekwencyjnym","text":""},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#problem-poczatkowy","title":"Problem pocz\u0105tkowy","text":"<p>Zapytanie : <pre><code>SELECT * FROM users WHERE email = 'user@example.com';\n</code></pre></p> <p>Plan wykonania : <pre><code>Seq Scan on users  (cost=0.00..25000.00 rows=1 width=64)\n  (actual time=0.123..1500.456 rows=1 loops=1)\n  Filter: (email = 'user@example.com'::text)\n  Rows Removed by Filter: 999999\nPlanning Time: 0.234 ms\nExecution Time: 1500.678 ms\n</code></pre></p> <p>Zidentyfikowane problemy : - \ud83d\udd34 Skanowanie sekwencyjne na 1 milionie wierszy - \ud83d\udd34 Czas wykonania : 1.5 sekundy - \ud83d\udd34 999999 wierszy przefiltrowanych</p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#analiza-z-dalibo","title":"Analiza z Dalibo","text":"<pre><code>-- Sprawdzi\u0107 w pg_stat_statements\nSELECT \n    query,\n    calls,\n    mean_exec_time,\n    shared_blks_read,\n    shared_blks_hit\nFROM pg_stat_statements\nWHERE query LIKE '%users WHERE email%'\nORDER BY mean_exec_time DESC;\n\n-- Sprawdzi\u0107 z pg_qualstats\nSELECT \n    left_table,\n    left_column,\n    operator,\n    execution_count\nFROM pg_qualstats\nWHERE left_table = 'users' AND left_column = 'email';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#rozwiazanie","title":"Rozwi\u0105zanie","text":"<pre><code>-- Utworzy\u0107 indeks na email\nCREATE INDEX idx_users_email ON users(email);\n\n-- Sprawdzi\u0107 nowy plan\nEXPLAIN ANALYZE\nSELECT * FROM users WHERE email = 'user@example.com';\n</code></pre> <p>Zoptymalizowany plan : <pre><code>Index Scan using idx_users_email on users\n  (cost=0.42..8.44 rows=1 width=64)\n  (actual time=0.123..0.125 rows=1 loops=1)\n  Index Cond: (email = 'user@example.com'::text)\nPlanning Time: 0.234 ms\nExecution Time: 0.125 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#wyniki","title":"Wyniki","text":"Metryka Przed Po Poprawa Czas wykonania 1500ms 0.125ms 99.99% Typ skanowania Seq Scan Index Scan \u2705 Wiersze skanowane 1,000,000 1 \u2705"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#przypadek-2-wolne-zaczenie-na-duzej-tabeli","title":"Przypadek 2 : Wolne z\u0142\u0105czenie na du\u017cej tabeli","text":""},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#problem-poczatkowy_1","title":"Problem pocz\u0105tkowy","text":"<p>Zapytanie : <pre><code>SELECT \n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_amount\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01'\nGROUP BY u.id, u.name, u.email;\n</code></pre></p> <p>Plan wykonania : <pre><code>Hash Join\n  (cost=125000.00..250000.00 rows=50000 width=64)\n  (actual time=5000.123..15000.456 rows=45000 loops=1)\n  Hash Cond: (o.user_id = u.id)\n  -&gt; Seq Scan on orders o\n      (cost=0.00..100000.00 rows=1000000 width=16)\n      (actual time=0.123..5000.456 rows=1000000 loops=1)\n  -&gt; Hash\n      (cost=25000.00..25000.00 rows=100000 width=48)\n      (actual time=2000.123..2000.123 rows=100000 loops=1)\n      Buckets: 131072  Batches: 8  Memory Usage: 5120kB\n      -&gt; Seq Scan on users u\n          (cost=0.00..25000.00 rows=100000 width=48)\n          (actual time=0.089..1000.567 rows=100000 loops=1)\n          Filter: (created_at &gt; '2024-01-01'::date)\nPlanning Time: 50.234 ms\nExecution Time: 15000.678 ms\n</code></pre></p> <p>Zidentyfikowane problemy : - \ud83d\udd34 Hash Join z 8 batchami (sortowanie na dysku) - \ud83d\udd34 Skanowanie sekwencyjne na orders (1 milion wierszy) - \ud83d\udd34 Czas wykonania : 15 sekund</p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#analiza-z-dalibo_1","title":"Analiza z Dalibo","text":"<pre><code>-- Identyfikowa\u0107 brakuj\u0105ce indeksy\nSELECT \n    qs.left_table,\n    qs.left_column,\n    qs.operator,\n    COUNT(*) AS execution_count\nFROM pg_qualstats qs\nWHERE qs.left_table IN ('users', 'orders')\nGROUP BY qs.left_table, qs.left_column, qs.operator\nORDER BY execution_count DESC;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#rozwiazanie_1","title":"Rozwi\u0105zanie","text":"<pre><code>-- Utworzy\u0107 indeksy na kluczach z\u0142\u0105czenia i filtrach\nCREATE INDEX idx_orders_user_id ON orders(user_id);\nCREATE INDEX idx_users_created_at ON users(created_at);\n\n-- Indeks z\u0142o\u017cony dla pe\u0142nego zapytania\nCREATE INDEX idx_orders_user_id_amount ON orders(user_id, amount);\n\n-- Zwi\u0119kszy\u0107 work_mem do unikni\u0119cia batch\u00f3w\nSET work_mem = '256MB';\n\n-- Sprawdzi\u0107 nowy plan\nEXPLAIN ANALYZE\nSELECT \n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_amount\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01'\nGROUP BY u.id, u.name, u.email;\n</code></pre> <p>Zoptymalizowany plan : <pre><code>Hash Join\n  (cost=5000.00..15000.00 rows=50000 width=64)\n  (actual time=200.123..800.456 rows=45000 loops=1)\n  Hash Cond: (o.user_id = u.id)\n  -&gt; Index Scan using idx_orders_user_id on orders o\n      (cost=0.42..8000.00 rows=500000 width=16)\n      (actual time=0.123..300.456 rows=500000 loops=1)\n  -&gt; Hash\n      (cost=2500.00..2500.00 rows=100000 width=48)\n      (actual time=100.123..100.123 rows=100000 loops=1)\n      Buckets: 131072  Batches: 1  Memory Usage: 5120kB\n      -&gt; Index Scan using idx_users_created_at on users u\n          (cost=0.42..2500.00 rows=100000 width=48)\n          (actual time=0.089..50.567 rows=100000 loops=1)\n          Index Cond: (created_at &gt; '2024-01-01'::date)\nPlanning Time: 5.234 ms\nExecution Time: 800.678 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#wyniki_1","title":"Wyniki","text":"Metryka Przed Po Poprawa Czas wykonania 15000ms 800ms 94.7% Batches Hash Join 8 1 \u2705 Typ skanowania orders Seq Scan Index Scan \u2705 Wiersze skanowane 1,000,000 500,000 \u2705"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#przypadek-3-wolna-agregacja","title":"Przypadek 3 : Wolna agregacja","text":""},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#problem-poczatkowy_2","title":"Problem pocz\u0105tkowy","text":"<p>Zapytanie : <pre><code>SELECT \n    status,\n    COUNT(*) AS count,\n    AVG(amount) AS avg_amount,\n    SUM(amount) AS total_amount\nFROM orders\nWHERE created_at BETWEEN '2024-01-01' AND '2024-12-31'\nGROUP BY status\nORDER BY count DESC;\n</code></pre></p> <p>Plan wykonania : <pre><code>Sort\n  (cost=50000.00..50000.00 rows=5 width=32)\n  (actual time=10000.123..10000.234 rows=5 loops=1)\n  Sort Key: (count(*)) DESC\n  Sort Method: quicksort  Memory: 25kB\n  -&gt; HashAggregate\n      (cost=45000.00..45000.00 rows=5 width=32)\n      (actual time=8000.123..8000.456 rows=5 loops=1)\n      Group Key: status\n      Batches: 1  Memory Usage: 24kB\n      -&gt; Seq Scan on orders\n          (cost=0.00..40000.00 rows=2000000 width=16)\n          (actual time=0.123..5000.456 rows=2000000 loops=1)\n          Filter: ((created_at &gt;= '2024-01-01'::date) \n                   AND (created_at &lt;= '2024-12-31'::date))\n          Rows Removed by Filter: 0\nPlanning Time: 10.234 ms\nExecution Time: 10000.678 ms\n</code></pre></p> <p>Zidentyfikowane problemy : - \ud83d\udd34 Skanowanie sekwencyjne na 2 milionach wierszy - \ud83d\udd34 Czas wykonania : 10 sekund - \ud83d\udd34 Brak indeksu na created_at</p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#rozwiazanie_2","title":"Rozwi\u0105zanie","text":"<pre><code>-- Utworzy\u0107 indeks na created_at i status\nCREATE INDEX idx_orders_created_at_status ON orders(created_at, status);\n\n-- Alternatywa : Indeks cz\u0119\u015bciowy je\u015bli niekt\u00f3re status s\u0105 rzadkie\nCREATE INDEX idx_orders_created_at_status_partial \nON orders(created_at, status) \nWHERE status IN ('pending', 'processing');\n\n-- Sprawdzi\u0107 nowy plan\nEXPLAIN ANALYZE\nSELECT \n    status,\n    COUNT(*) AS count,\n    AVG(amount) AS avg_amount,\n    SUM(amount) AS total_amount\nFROM orders\nWHERE created_at BETWEEN '2024-01-01' AND '2024-12-31'\nGROUP BY status\nORDER BY count DESC;\n</code></pre> <p>Zoptymalizowany plan : <pre><code>Sort\n  (cost=5000.00..5000.00 rows=5 width=32)\n  (actual time=500.123..500.234 rows=5 loops=1)\n  Sort Key: (count(*)) DESC\n  Sort Method: quicksort  Memory: 25kB\n  -&gt; HashAggregate\n      (cost=4500.00..4500.00 rows=5 width=32)\n      (actual time=400.123..400.456 rows=5 loops=1)\n      Group Key: status\n      Batches: 1  Memory Usage: 24kB\n      -&gt; Index Scan using idx_orders_created_at_status on orders\n          (cost=0.42..4000.00 rows=2000000 width=16)\n          (actual time=0.123..200.456 rows=2000000 loops=1)\n          Index Cond: ((created_at &gt;= '2024-01-01'::date) \n                       AND (created_at &lt;= '2024-12-31'::date))\nPlanning Time: 5.234 ms\nExecution Time: 500.678 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#wyniki_2","title":"Wyniki","text":"Metryka Przed Po Poprawa Czas wykonania 10000ms 500ms 95% Typ skanowania Seq Scan Index Scan \u2705 Wiersze skanowane 2,000,000 2,000,000 (ta sama liczba, ale indeks)"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#przypadek-4-podzapytanie-skorelowane","title":"Przypadek 4 : Podzapytanie skorelowane","text":""},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#problem-poczatkowy_3","title":"Problem pocz\u0105tkowy","text":"<p>Zapytanie : <pre><code>SELECT \n    u.id,\n    u.name,\n    u.email,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) AS order_count,\n    (SELECT MAX(created_at) FROM orders o WHERE o.user_id = u.id) AS last_order_date,\n    (SELECT SUM(amount) FROM orders o WHERE o.user_id = u.id) AS total_spent\nFROM users u\nWHERE u.active = true;\n</code></pre></p> <p>Plan wykonania : <pre><code>Seq Scan on users u\n  (cost=0.00..250000.00 rows=100000 width=64)\n  (actual time=0.123..50000.456 rows=100000 loops=1)\n  Filter: (active = true)\n  SubPlan 1\n    -&gt; Aggregate\n        (cost=2.50..2.50 rows=1 width=8)\n        (actual time=0.100..0.100 rows=1 loops=100000)\n        -&gt; Seq Scan on orders o\n            (cost=0.00..2.25 rows=10 width=0)\n            (actual time=0.050..0.050 rows=5 loops=100000)\n            Filter: (user_id = u.id)\n  SubPlan 2\n    -&gt; Result\n        (cost=2.50..2.50 rows=1 width=8)\n        (actual time=0.100..0.100 rows=1 loops=100000)\n        -&gt; Aggregate\n            (cost=2.50..2.50 rows=1 width=8)\n            (actual time=0.100..0.100 rows=1 loops=100000)\n            -&gt; Seq Scan on orders o\n                (cost=0.00..2.25 rows=10 width=0)\n                (actual time=0.050..0.050 rows=5 loops=100000)\n                Filter: (user_id = u.id)\n  SubPlan 3\n    -&gt; Aggregate\n        (cost=2.50..2.50 rows=1 width=8)\n        (actual time=0.100..0.100 rows=1 loops=100000)\n        -&gt; Seq Scan on orders o\n            (cost=0.00..2.25 rows=10 width=0)\n            (actual time=0.050..0.050 rows=5 loops=100000)\n            Filter: (user_id = u.id)\nPlanning Time: 5.234 ms\nExecution Time: 50000.678 ms\n</code></pre></p> <p>Zidentyfikowane problemy : - \ud83d\udd34 3 podzapytania skorelowane wykonywane 100,000 razy ka\u017cde - \ud83d\udd34 300,000 skanowa\u0144 sekwencyjnych na orders - \ud83d\udd34 Czas wykonania : 50 sekund</p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#rozwiazanie_3","title":"Rozwi\u0105zanie","text":"<pre><code>-- Zast\u0105pi\u0107 przez JOIN z agregacj\u0105\nSELECT \n    u.id,\n    u.name,\n    u.email,\n    COALESCE(o.order_count, 0) AS order_count,\n    o.last_order_date,\n    COALESCE(o.total_spent, 0) AS total_spent\nFROM users u\nLEFT JOIN (\n    SELECT \n        user_id,\n        COUNT(*) AS order_count,\n        MAX(created_at) AS last_order_date,\n        SUM(amount) AS total_spent\n    FROM orders\n    GROUP BY user_id\n) o ON u.id = o.user_id\nWHERE u.active = true;\n\n-- Utworzy\u0107 indeks do przyspieszenia z\u0142\u0105czenia\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n</code></pre> <p>Zoptymalizowany plan : <pre><code>Hash Right Join\n  (cost=5000.00..15000.00 rows=100000 width=64)\n  (actual time=200.123..800.456 rows=100000 loops=1)\n  Hash Cond: (o.user_id = u.id)\n  -&gt; HashAggregate\n      (cost=4000.00..4500.00 rows=50000 width=24)\n      (actual time=150.123..200.456 rows=50000 loops=1)\n      Group Key: orders.user_id\n      Batches: 1  Memory Usage: 5120kB\n      -&gt; Index Scan using idx_orders_user_id on orders\n          (cost=0.42..3000.00 rows=500000 width=16)\n          (actual time=0.123..100.456 rows=500000 loops=1)\n  -&gt; Hash\n      (cost=2000.00..2000.00 rows=100000 width=48)\n      (actual time=50.123..50.123 rows=100000 loops=1)\n      Buckets: 131072  Batches: 1  Memory Usage: 5120kB\n      -&gt; Seq Scan on users u\n          (cost=0.00..2000.00 rows=100000 width=48)\n          (actual time=0.089..25.567 rows=100000 loops=1)\n          Filter: (active = true)\nPlanning Time: 5.234 ms\nExecution Time: 800.678 ms\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#wyniki_3","title":"Wyniki","text":"Metryka Przed Po Poprawa Czas wykonania 50000ms 800ms 98.4% Skanowania na orders 300,000 1 \u2705 Typ operacji Podzapytania Hash Join \u2705"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#przypadek-5-problem-wspoczynnika-trafien-cache","title":"Przypadek 5 : Problem wsp\u00f3\u0142czynnika trafie\u0144 cache","text":""},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#problem-poczatkowy_4","title":"Problem pocz\u0105tkowy","text":"<p>Metryki : <pre><code>-- Globalny wsp\u00f3\u0142czynnik trafie\u0144 cache\nSELECT \n    ROUND(100.0 * SUM(shared_blks_hit) / \n          NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2) AS cache_hit_ratio\nFROM pg_stat_statements;\n-- Wynik: 75% (cel: &gt; 95%)\n</code></pre></p> <p>Zapytania z wieloma odczytami z dysku : <pre><code>SELECT \n    LEFT(query, 100) AS query_preview,\n    shared_blks_read,\n    shared_blks_hit,\n    ROUND(100.0 * shared_blks_hit / \n          NULLIF(shared_blks_hit + shared_blks_read, 0), 2) AS cache_hit_ratio,\n    ROUND((shared_blks_read * 8)::numeric / 1024, 2) AS disk_read_mb\nFROM pg_stat_statements\nWHERE shared_blks_read &gt; 1000\nORDER BY shared_blks_read DESC\nLIMIT 10;\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#rozwiazanie_4","title":"Rozwi\u0105zanie","text":"<pre><code>-- 1. Zwi\u0119kszy\u0107 shared_buffers (w postgresql.conf)\n-- shared_buffers = 4GB  (25% RAM dla serwera dedykowanego)\n\n-- 2. Wst\u0119pnie za\u0142adowa\u0107 cz\u0119sto u\u017cywane tabele\n-- Utworzy\u0107 funkcj\u0119 wst\u0119pnego \u0142adowania\nCREATE OR REPLACE FUNCTION pg_prewarm_table(table_name TEXT)\nRETURNS void AS $$\nBEGIN\n    EXECUTE format('SELECT * FROM %I LIMIT 1', table_name);\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Wst\u0119pnie za\u0142adowa\u0107 wa\u017cne tabele\nSELECT pg_prewarm_table('users');\nSELECT pg_prewarm_table('orders');\nSELECT pg_prewarm_table('products');\n\n-- 3. U\u017cywa\u0107 rozszerzenia pg_prewarm\nCREATE EXTENSION IF NOT EXISTS pg_prewarm;\n\n-- Wst\u0119pnie za\u0142adowa\u0107 pe\u0142n\u0105 tabel\u0119\nSELECT pg_prewarm('users');\nSELECT pg_prewarm('orders');\n</code></pre> <p>Po optymalizacji : <pre><code>-- Sprawdzi\u0107 popraw\u0119\nSELECT \n    ROUND(100.0 * SUM(shared_blks_hit) / \n          NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2) AS cache_hit_ratio\nFROM pg_stat_statements;\n-- Wynik: 98% \u2705\n</code></pre></p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#wyniki_4","title":"Wyniki","text":"Metryka Przed Po Poprawa Wsp\u00f3\u0142czynnik trafie\u0144 cache 75% 98% +23% Odczyty z dysku Wysokie Niskie \u2705 Czas odpowiedzi Zmienny Stabilny \u2705"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#przypadek-6-indeksy-nieuzywane","title":"Przypadek 6 : Indeksy nieu\u017cywane","text":""},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#problem-poczatkowy_5","title":"Problem pocz\u0105tkowy","text":"<p>Identyfikacja nieu\u017cywanych indeks\u00f3w : <pre><code>-- Indeksy nigdy nieu\u017cywane\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,\n    idx_scan AS index_scans,\n    pg_relation_size(indexrelid) AS size_bytes\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\n  AND pg_relation_size(indexrelid) &gt; 1048576  -- &gt; 1MB\nORDER BY pg_relation_size(indexrelid) DESC;\n</code></pre></p> <p>Wynik : <pre><code> schemaname | tablename |      indexname       | index_size | index_scans | size_bytes\n------------+-----------+----------------------+------------+-------------+------------\n public     | orders    | idx_orders_old_field | 250 MB     |           0 |  262144000\n public     | users     | idx_users_old_email  | 150 MB     |           0 |  157286400\n</code></pre></p> <p>Wp\u0142yw : - \ud83d\udd34 400 MB przestrzeni dyskowej zmarnowanej - \ud83d\udd34 Spowolnienie INSERT/UPDATE - \ud83d\udd34 Niepotrzebna konserwacja</p>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#rozwiazanie_5","title":"Rozwi\u0105zanie","text":"<pre><code>-- 1. Sprawdzi\u0107 z HypoPG czy indeks jest naprawd\u0119 niepotrzebny\nCREATE EXTENSION IF NOT EXISTS hypopg;\n\n-- 2. Analizowa\u0107 zapytania, kt\u00f3re mog\u0142yby u\u017cy\u0107 indeksu\nSELECT \n    query,\n    calls,\n    mean_exec_time\nFROM pg_stat_statements\nWHERE query LIKE '%old_field%' OR query LIKE '%old_email%';\n\n-- 3. Je\u015bli naprawd\u0119 niepotrzebny, usun\u0105\u0107 indeks\nDROP INDEX idx_orders_old_field;\nDROP INDEX idx_users_old_email;\n\n-- 4. Sprawdzi\u0107 zwolnion\u0105 przestrze\u0144\nSELECT \n    pg_size_pretty(pg_database_size(current_database())) AS database_size;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#wyniki_5","title":"Wyniki","text":"Metryka Przed Po Poprawa Przestrze\u0144 indeks\u00f3w 400 MB 0 MB -400 MB Czas INSERT +10% Normalny \u2705 Czas UPDATE +15% Normalny \u2705"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#kluczowe-punkty-do-zapamietania","title":"\ud83d\udcca Kluczowe punkty do zapami\u0119tania","text":"<ol> <li>Zawsze analizowa\u0107 z EXPLAIN ANALYZE przed optymalizacj\u0105</li> <li>U\u017cywa\u0107 Dalibo do automatycznej identyfikacji problem\u00f3w</li> <li>Mierzy\u0107 wp\u0142yw przed i po ka\u017cdej optymalizacji</li> <li>Odpowiednie indeksy : Najcz\u0119stsze rozwi\u0105zanie</li> <li>Unika\u0107 podzapyta\u0144 skorelowanych : U\u017cywa\u0107 JOIN</li> <li>Monitorowa\u0107 regularnie : Problemy ewoluuj\u0105</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/06-cas-pratiques/#nastepny-modu","title":"\ud83d\udd17 Nast\u0119pny modu\u0142","text":"<p>Przejd\u017a do modu\u0142u 7. \u0106wiczenia, aby \u0107wiczy\u0107 z \u0107wiczeniami prowadzonymi.</p>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/","title":"7. \u0106wiczenia praktyczne","text":""},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#cele","title":"\ud83c\udfaf Cele","text":"<ul> <li>Stosowa\u0107 zdobyt\u0105 wiedz\u0119</li> <li>Rozwi\u0105zywa\u0107 problemy wydajno\u015bciowe</li> <li>Analizowa\u0107 i optymalizowa\u0107 rzeczywiste zapytania</li> <li>U\u017cywa\u0107 Dalibo do analizy</li> </ul>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#struktura-cwiczen","title":"\ud83d\udccb Struktura \u0107wicze\u0144","text":"<p>\u0106wiczenia s\u0105 zorganizowane wed\u0142ug poziomu trudno\u015bci : - \ud83d\udfe2 Pocz\u0105tkuj\u0105cy : Podstawowe koncepcje - \ud83d\udfe1 \u015arednio zaawansowany : Zaawansowane techniki - \ud83d\udd34 Zaawansowany : Z\u0142o\u017cone optymalizacje</p>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#cwiczenie-1-analiza-prostego-zapytania-poczatkujacy","title":"\u0106wiczenie 1 : Analiza prostego zapytania (\ud83d\udfe2 Pocz\u0105tkuj\u0105cy)","text":""},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#kontekst","title":"Kontekst","text":"<p>Masz tabel\u0119 <code>products</code> z 1 milionem wierszy i wolne zapytanie :</p> <pre><code>SELECT * FROM products WHERE category = 'electronics';\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#zadania","title":"Zadania","text":"<ol> <li>Analizowa\u0107 zapytanie z <code>EXPLAIN ANALYZE</code></li> <li>Identyfikowa\u0107 problem w planie wykonania</li> <li>Zaproponowa\u0107 rozwi\u0105zanie z indeksem</li> <li>Sprawdzi\u0107 popraw\u0119 z <code>EXPLAIN ANALYZE</code></li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#dane-testowe","title":"Dane testowe","text":"<pre><code>-- Utworzy\u0107 tabel\u0119\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    category TEXT,\n    price DECIMAL(10,2),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Wstawi\u0107 dane testowe (1 milion wierszy)\nINSERT INTO products (name, category, price)\nSELECT \n    'Product ' || i,\n    CASE (i % 10)\n        WHEN 0 THEN 'electronics'\n        WHEN 1 THEN 'clothing'\n        WHEN 2 THEN 'books'\n        WHEN 3 THEN 'food'\n        WHEN 4 THEN 'toys'\n        WHEN 5 THEN 'electronics'\n        WHEN 6 THEN 'furniture'\n        WHEN 7 THEN 'electronics'\n        WHEN 8 THEN 'sports'\n        ELSE 'other'\n    END,\n    (RANDOM() * 1000)::DECIMAL(10,2)\nFROM generate_series(1, 1000000) i;\n\n-- Analizowa\u0107 tabel\u0119\nANALYZE products;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#oczekiwane-rozwiazanie","title":"Oczekiwane rozwi\u0105zanie","text":"Kliknij, aby zobaczy\u0107 rozwi\u0105zanie <pre><code>-- 1. Analizowa\u0107 zapytanie\nEXPLAIN ANALYZE\nSELECT * FROM products WHERE category = 'electronics';\n\n-- Zidentyfikowany problem : Seq Scan na 1 milionie wierszy\n\n-- 2. Utworzy\u0107 indeks\nCREATE INDEX idx_products_category ON products(category);\n\n-- 3. Analizowa\u0107 ponownie\nEXPLAIN ANALYZE\nSELECT * FROM products WHERE category = 'electronics';\n\n-- Oczekiwany wynik : Index Scan z czasem wykonania &lt; 100ms\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#cwiczenie-2-optymalizacja-zaczenia-srednio-zaawansowany","title":"\u0106wiczenie 2 : Optymalizacja z\u0142\u0105czenia (\ud83d\udfe1 \u015arednio zaawansowany)","text":""},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#kontekst_1","title":"Kontekst","text":"<p>Masz dwie tabele <code>users</code> i <code>orders</code> z wolnym z\u0142\u0105czeniem :</p> <pre><code>SELECT \n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_amount\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01'\nGROUP BY u.id, u.name, u.email\nHAVING COUNT(o.id) &gt; 5;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#zadania_1","title":"Zadania","text":"<ol> <li>Analizowa\u0107 zapytanie z <code>EXPLAIN ANALYZE</code></li> <li>Identyfikowa\u0107 problemy (skanowania, z\u0142\u0105czenia, agregacje)</li> <li>Utworzy\u0107 potrzebne indeksy</li> <li>Optymalizowa\u0107 zapytanie (HAVING \u2192 WHERE je\u015bli mo\u017cliwe)</li> <li>Zmierzy\u0107 popraw\u0119</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#dane-testowe_1","title":"Dane testowe","text":"<pre><code>-- Utworzy\u0107 tabele\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    email TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER REFERENCES users(id),\n    amount DECIMAL(10,2),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Wstawi\u0107 dane\nINSERT INTO users (name, email, created_at)\nSELECT \n    'User ' || i,\n    'user' || i || '@example.com',\n    NOW() - (RANDOM() * 365 || ' days')::INTERVAL\nFROM generate_series(1, 100000) i;\n\nINSERT INTO orders (user_id, amount, created_at)\nSELECT \n    (RANDOM() * 100000)::INTEGER + 1,\n    (RANDOM() * 1000)::DECIMAL(10,2),\n    NOW() - (RANDOM() * 365 || ' days')::INTERVAL\nFROM generate_series(1, 1000000) i;\n\nANALYZE users, orders;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#oczekiwane-rozwiazanie_1","title":"Oczekiwane rozwi\u0105zanie","text":"Kliknij, aby zobaczy\u0107 rozwi\u0105zanie <pre><code>-- 1. Analizowa\u0107 zapytanie\nEXPLAIN ANALYZE\nSELECT \n    u.name,\n    u.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.amount) AS total_amount\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01'\nGROUP BY u.id, u.name, u.email\nHAVING COUNT(o.id) &gt; 5;\n\n-- Zidentyfikowane problemy :\n-- - Brak indeksu na users.created_at\n-- - Brak indeksu na orders.user_id\n-- - HAVING mo\u017ce by\u0107 zoptymalizowane\n\n-- 2. Utworzy\u0107 indeksy\nCREATE INDEX idx_users_created_at ON users(created_at);\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n\n-- 3. Optymalizowa\u0107 zapytanie (filtrowa\u0107 w podzapytaniu)\nSELECT \n    u.name,\n    u.email,\n    o.order_count,\n    o.total_amount\nFROM users u\nJOIN (\n    SELECT \n        user_id,\n        COUNT(*) AS order_count,\n        SUM(amount) AS total_amount\n    FROM orders\n    GROUP BY user_id\n    HAVING COUNT(*) &gt; 5\n) o ON u.id = o.user_id\nWHERE u.created_at &gt; '2024-01-01';\n\n-- 4. Analizowa\u0107 ponownie\nEXPLAIN ANALYZE [zoptymalizowane zapytanie];\n\n-- Oczekiwany wynik : Czas wykonania zmniejszony o 80%+\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#cwiczenie-3-podzapytanie-skorelowane-srednio-zaawansowany","title":"\u0106wiczenie 3 : Podzapytanie skorelowane (\ud83d\udfe1 \u015arednio zaawansowany)","text":""},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#kontekst_2","title":"Kontekst","text":"<p>Zapytanie z podzapytaniami skorelowanymi bardzo wolne :</p> <pre><code>SELECT \n    u.id,\n    u.name,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) AS order_count,\n    (SELECT MAX(created_at) FROM orders o WHERE o.user_id = u.id) AS last_order_date,\n    (SELECT AVG(amount) FROM orders o WHERE o.user_id = u.id) AS avg_order_amount\nFROM users u\nWHERE u.active = true;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#zadania_2","title":"Zadania","text":"<ol> <li>Analizowa\u0107 zapytanie i identyfikowa\u0107 podzapytania skorelowane</li> <li>Konwertowa\u0107 na JOIN z agregacj\u0105</li> <li>Utworzy\u0107 potrzebne indeksy</li> <li>Por\u00f3wna\u0107 wydajno\u015b\u0107</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#oczekiwane-rozwiazanie_2","title":"Oczekiwane rozwi\u0105zanie","text":"Kliknij, aby zobaczy\u0107 rozwi\u0105zanie <pre><code>-- 1. Analizowa\u0107\nEXPLAIN ANALYZE [oryginalne zapytanie];\n-- Problem : 3 podzapytania wykonywane dla ka\u017cdego wiersza\n\n-- 2. Konwertowa\u0107 na JOIN\nSELECT \n    u.id,\n    u.name,\n    COALESCE(o.order_count, 0) AS order_count,\n    o.last_order_date,\n    COALESCE(o.avg_order_amount, 0) AS avg_order_amount\nFROM users u\nLEFT JOIN (\n    SELECT \n        user_id,\n        COUNT(*) AS order_count,\n        MAX(created_at) AS last_order_date,\n        AVG(amount) AS avg_order_amount\n    FROM orders\n    GROUP BY user_id\n) o ON u.id = o.user_id\nWHERE u.active = true;\n\n-- 3. Utworzy\u0107 indeks\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n\n-- 4. Por\u00f3wna\u0107\n-- Oczekiwany wynik : Poprawa o 95%+\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#cwiczenie-4-analiza-z-dalibo-zaawansowany","title":"\u0106wiczenie 4 : Analiza z Dalibo (\ud83d\udd34 Zaawansowany)","text":""},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#kontekst_3","title":"Kontekst","text":"<p>Musisz przeanalizowa\u0107 wydajno\u015b\u0107 bazy danych produkcyjnej (symulowanej) i zidentyfikowa\u0107 g\u0142\u00f3wne problemy.</p>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#zadania_3","title":"Zadania","text":"<ol> <li>Aktywowa\u0107 pg_stat_statements i pg_qualstats</li> <li>Wykona\u0107 zapytania testowe do generowania statystyk</li> <li>Identyfikowa\u0107 wolne zapytania z pg_stat_statements</li> <li>Identyfikowa\u0107 brakuj\u0105ce indeksy z pg_qualstats</li> <li>Generowa\u0107 raport z rekomendacjami</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#dane-testowe_2","title":"Dane testowe","text":"<pre><code>-- U\u017cywa\u0107 tabel z \u0107wiczenia 2\n-- Wykona\u0107 r\u00f3\u017cne zapytania do generowania statystyk\n\n-- Zapytania testowe\nSELECT * FROM users WHERE email = 'user50000@example.com';\nSELECT * FROM orders WHERE user_id = 12345;\nSELECT * FROM users WHERE created_at &gt; '2024-06-01';\nSELECT COUNT(*) FROM orders WHERE amount &gt; 500;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#oczekiwane-rozwiazanie_3","title":"Oczekiwane rozwi\u0105zanie","text":"Kliknij, aby zobaczy\u0107 rozwi\u0105zanie <pre><code>-- 1. Aktywowa\u0107 rozszerzenia\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\nCREATE EXTENSION IF NOT EXISTS pg_qualstats;\n\n-- 2. Wykona\u0107 zapytania testowe\n[zapytania testowe]\n\n-- 3. Identyfikowa\u0107 wolne zapytania\nSELECT \n    LEFT(query, 100) AS query_preview,\n    calls,\n    mean_exec_time,\n    total_exec_time,\n    (total_exec_time / SUM(total_exec_time) OVER ()) * 100 AS percent_total_time\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 10;\n\n-- 4. Identyfikowa\u0107 brakuj\u0105ce indeksy\nSELECT \n    qs.left_schema,\n    qs.left_table,\n    qs.left_column,\n    qs.operator,\n    COUNT(*) AS execution_count\nFROM pg_qualstats qs\nWHERE NOT EXISTS (\n    SELECT 1\n    FROM pg_index i\n    JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)\n    WHERE i.indrelid = (qs.left_schema||'.'||qs.left_table)::regclass\n    AND a.attname = qs.left_column\n)\nGROUP BY qs.left_schema, qs.left_table, qs.left_column, qs.operator\nORDER BY execution_count DESC;\n\n-- 5. Generowa\u0107 rekomendacje\nSELECT \n    'CREATE INDEX idx_' || left_table || '_' || left_column || \n    ' ON ' || left_schema || '.' || left_table || \n    ' (' || left_column || ');' AS recommendation,\n    execution_count AS priority\nFROM [zapytanie pg_qualstats powy\u017cej]\nORDER BY execution_count DESC\nLIMIT 5;\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#cwiczenie-5-optymalizacja-kompletna-zaawansowany","title":"\u0106wiczenie 5 : Optymalizacja kompletna (\ud83d\udd34 Zaawansowany)","text":""},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#kontekst_4","title":"Kontekst","text":"<p>Masz aplikacj\u0119 e-commerce z obni\u017con\u0105 wydajno\u015bci\u0105. Przeanalizuj i zoptymalizuj kompletny system.</p>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#schemat-bazy-danych","title":"Schemat bazy danych","text":"<pre><code>CREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    email TEXT,\n    name TEXT,\n    created_at TIMESTAMP,\n    active BOOLEAN\n);\n\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    category TEXT,\n    price DECIMAL(10,2),\n    stock INTEGER\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    created_at TIMESTAMP,\n    status TEXT,\n    total_amount DECIMAL(10,2)\n);\n\nCREATE TABLE order_items (\n    id SERIAL PRIMARY KEY,\n    order_id INTEGER REFERENCES orders(id),\n    product_id INTEGER REFERENCES products(id),\n    quantity INTEGER,\n    price DECIMAL(10,2)\n);\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#zapytania-do-optymalizacji","title":"Zapytania do optymalizacji","text":"<ol> <li> <p>Zapytanie dashboardu : <pre><code>SELECT \n    c.name,\n    c.email,\n    COUNT(o.id) AS order_count,\n    SUM(o.total_amount) AS total_spent,\n    MAX(o.created_at) AS last_order_date\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nWHERE c.active = true\nGROUP BY c.id, c.name, c.email\nHAVING COUNT(o.id) &gt; 0\nORDER BY total_spent DESC\nLIMIT 100;\n</code></pre></p> </li> <li> <p>Zapytanie popularnych produkt\u00f3w : <pre><code>SELECT \n    p.name,\n    p.category,\n    SUM(oi.quantity) AS total_sold,\n    SUM(oi.quantity * oi.price) AS total_revenue\nFROM products p\nJOIN order_items oi ON p.id = oi.product_id\nJOIN orders o ON oi.order_id = o.id\nWHERE o.created_at &gt; '2024-01-01'\n  AND o.status = 'completed'\nGROUP BY p.id, p.name, p.category\nORDER BY total_sold DESC\nLIMIT 50;\n</code></pre></p> </li> <li> <p>Zapytanie wyszukiwania : <pre><code>SELECT \n    p.*,\n    (SELECT COUNT(*) FROM order_items oi WHERE oi.product_id = p.id) AS times_ordered\nFROM products p\nWHERE p.category = 'electronics'\n  AND p.price BETWEEN 100 AND 500\nORDER BY times_ordered DESC;\n</code></pre></p> </li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#zadania_4","title":"Zadania","text":"<ol> <li>Analizowa\u0107 ka\u017cde zapytanie z EXPLAIN ANALYZE</li> <li>Identyfikowa\u0107 wszystkie problemy</li> <li>Utworzy\u0107 plan optymalizacji</li> <li>Zaimplementowa\u0107 optymalizacje</li> <li>Zmierzy\u0107 globalny wp\u0142yw</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#oczekiwane-rozwiazanie_4","title":"Oczekiwane rozwi\u0105zanie","text":"Kliknij, aby zobaczy\u0107 rozwi\u0105zanie <pre><code>-- 1. Analizowa\u0107 wszystkie zapytania\nEXPLAIN ANALYZE [ka\u017cde zapytanie];\n\n-- 2. Utworzy\u0107 potrzebne indeksy\nCREATE INDEX idx_customers_active ON customers(active);\nCREATE INDEX idx_orders_customer_id ON orders(customer_id);\nCREATE INDEX idx_orders_created_at_status ON orders(created_at, status);\nCREATE INDEX idx_order_items_order_id ON order_items(order_id);\nCREATE INDEX idx_order_items_product_id ON order_items(product_id);\nCREATE INDEX idx_products_category_price ON products(category, price);\n\n-- 3. Optymalizowa\u0107 zapytanie 1 (HAVING \u2192 WHERE)\nSELECT \n    c.name,\n    c.email,\n    o.order_count,\n    o.total_spent,\n    o.last_order_date\nFROM customers c\nJOIN (\n    SELECT \n        customer_id,\n        COUNT(*) AS order_count,\n        SUM(total_amount) AS total_spent,\n        MAX(created_at) AS last_order_date\n    FROM orders\n    GROUP BY customer_id\n) o ON c.id = o.customer_id\nWHERE c.active = true\nORDER BY o.total_spent DESC\nLIMIT 100;\n\n-- 4. Optymalizowa\u0107 zapytanie 3 (podzapytanie \u2192 JOIN)\nSELECT \n    p.*,\n    COALESCE(oi.times_ordered, 0) AS times_ordered\nFROM products p\nLEFT JOIN (\n    SELECT product_id, COUNT(*) AS times_ordered\n    FROM order_items\n    GROUP BY product_id\n) oi ON p.id = oi.product_id\nWHERE p.category = 'electronics'\n  AND p.price BETWEEN 100 AND 500\nORDER BY oi.times_ordered DESC NULLS LAST;\n\n-- 5. Zmierzy\u0107 wp\u0142yw\n-- Por\u00f3wna\u0107 czasy wykonania przed/po\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#cwiczenie-6-monitoring-i-alerty-zaawansowany","title":"\u0106wiczenie 6 : Monitoring i alerty (\ud83d\udd34 Zaawansowany)","text":""},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#kontekst_5","title":"Kontekst","text":"<p>Utw\u00f3rz system monitoringu do automatycznego wykrywania problem\u00f3w wydajno\u015bciowych.</p>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#zadania_5","title":"Zadania","text":"<ol> <li>Utworzy\u0107 widok konsoliduj\u0105cy kluczowe metryki</li> <li>Utworzy\u0107 funkcj\u0119 wykrywania alert\u00f3w</li> <li>Utworzy\u0107 raport automatyczny</li> <li>Przetestowa\u0107 system z rzeczywistymi danymi</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#oczekiwane-rozwiazanie_5","title":"Oczekiwane rozwi\u0105zanie","text":"Kliknij, aby zobaczy\u0107 rozwi\u0105zanie <pre><code>-- 1. Skonsolidowany widok\nCREATE OR REPLACE VIEW v_performance_metrics AS\nSELECT \n    'slow_queries' AS metric_type,\n    COUNT(*) AS count,\n    AVG(mean_exec_time) AS avg_value,\n    MAX(mean_exec_time) AS max_value\nFROM pg_stat_statements\nWHERE mean_exec_time &gt; 1000\n\nUNION ALL\n\nSELECT \n    'cache_hit_ratio',\n    NULL,\n    ROUND(100.0 * SUM(shared_blks_hit) / \n          NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2),\n    NULL\nFROM pg_stat_statements\n\nUNION ALL\n\nSELECT \n    'idle_in_transaction',\n    COUNT(*),\n    NULL,\n    NULL\nFROM pg_stat_activity\nWHERE state = 'idle in transaction';\n\n-- 2. Funkcja alert\u00f3w\nCREATE OR REPLACE FUNCTION check_performance_alerts()\nRETURNS TABLE(alert_type TEXT, message TEXT, severity TEXT) AS $$\nBEGIN\n    -- Alerty na wolne zapytania\n    RETURN QUERY\n    SELECT \n        'SLOW_QUERY'::TEXT,\n        format('Query with mean_exec_time: %s ms', ROUND(mean_exec_time::numeric, 2)),\n        CASE WHEN mean_exec_time &gt; 5000 THEN 'CRITICAL' ELSE 'WARNING' END\n    FROM pg_stat_statements\n    WHERE mean_exec_time &gt; 1000\n    ORDER BY mean_exec_time DESC\n    LIMIT 5;\n\n    -- Alert na wsp\u00f3\u0142czynnik trafie\u0144 cache\n    RETURN QUERY\n    SELECT \n        'LOW_CACHE_HIT'::TEXT,\n        format('Cache hit ratio: %s%%', \n               ROUND(100.0 * SUM(shared_blks_hit) / \n                     NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2)),\n        CASE \n            WHEN 100.0 * SUM(shared_blks_hit) / \n                 NULLIF(SUM(shared_blks_hit + shared_blks_read), 0) &lt; 90 \n            THEN 'CRITICAL'\n            WHEN 100.0 * SUM(shared_blks_hit) / \n                 NULLIF(SUM(shared_blks_hit + shared_blks_read), 0) &lt; 95 \n            THEN 'WARNING'\n            ELSE 'INFO'\n        END\n    FROM pg_stat_statements;\n\n    -- Alert na po\u0142\u0105czenia idle in transaction\n    RETURN QUERY\n    SELECT \n        'IDLE_IN_TRANSACTION'::TEXT,\n        format('%s connections idle in transaction', COUNT(*)),\n        CASE WHEN COUNT(*) &gt; 10 THEN 'CRITICAL' ELSE 'WARNING' END\n    FROM pg_stat_activity\n    WHERE state = 'idle in transaction';\nEND;\n$$ LANGUAGE plpgsql;\n\n-- 3. U\u017cycie\nSELECT * FROM check_performance_alerts();\n</code></pre>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#wskazowki-do-cwiczen","title":"\ud83d\udcdd Wskaz\u00f3wki do \u0107wicze\u0144","text":"<ol> <li>Zawsze zaczyna\u0107 od EXPLAIN ANALYZE : Rozumie\u0107 przed optymalizacj\u0105</li> <li>Mierzy\u0107 przed i po : Kwantyfikowa\u0107 popraw\u0119</li> <li>Testowa\u0107 z realistycznymi danymi : U\u017cywa\u0107 obj\u0119to\u015bci podobnych do produkcji</li> <li>Dokumentowa\u0107 decyzje : Notowa\u0107 dlaczego wybra\u0142e\u015b tak\u0105 optymalizacj\u0119</li> <li>Sprawdza\u0107 efekty uboczne : Indeks poprawia SELECT ale spowalnia INSERT/UPDATE</li> </ol>"},{"location":"SQL-avanc%C3%A9/PL/07-exercices/#powrot-do-moduow","title":"\ud83d\udd17 Powr\u00f3t do modu\u0142\u00f3w","text":"<ul> <li>Modu\u0142 1 : Podstawy</li> <li>Modu\u0142 2 : Plany wykonania</li> <li>Modu\u0142 3 : Dalibo</li> <li>Modu\u0142 4 : Wska\u017aniki</li> <li>Modu\u0142 5 : Techniki</li> <li>Modu\u0142 6 : Przypadki praktyczne</li> </ul>"}]}