# 6. Azure Databricks - Big Data Analytics

## üéØ Objectifs

- Comprendre Azure Databricks
- Cr√©er un workspace Databricks
- Utiliser des notebooks Python/SQL
- Traiter des donn√©es avec Spark
- Int√©grer avec autres services Azure

## üìã Table des mati√®res

1. [Introduction √† Databricks](#introduction-√†-databricks)
2. [Cr√©er un workspace Databricks](#cr√©er-un-workspace-databricks)
3. [Cr√©er un cluster](#cr√©er-un-cluster)
4. [Notebooks Python/SQL](#notebooks-pythonsql)
5. [Traitement de donn√©es avec Spark](#traitement-de-donn√©es-avec-spark)
6. [Int√©gration avec autres services](#int√©gration-avec-autres-services)

---

## Introduction √† Databricks

### Qu'est-ce qu'Azure Databricks ?

**Azure Databricks** = Plateforme Big Data bas√©e sur Apache Spark

- **Apache Spark** : Moteur de traitement distribu√©
- **Notebooks** : Python, SQL, Scala, R
- **G√©r√©** : Microsoft g√®re l'infrastructure
- **Scalable** : Clusters auto-scaling

### Cas d'usage pour Data Analyst

- **Big Data processing** : Traiter de grandes quantit√©s
- **ETL** : Transformations complexes
- **Machine Learning** : MLlib int√©gr√©
- **Data Science** : Notebooks interactifs

### Free Tier Databricks

**Gratuit avec cr√©dit Azure :**
- Utiliser les 200$ de cr√©dit gratuit (30 jours)
- Apr√®s : facturation normale

**‚ö†Ô∏è Important :** Databricks peut √™tre co√ªteux. Surveiller attentivement les co√ªts.

---

## Cr√©er un workspace Databricks

### √âtape 1 : Acc√©der √† Databricks

1. Portail Azure ‚Üí Rechercher "Azure Databricks"
2. Cliquer sur "Azure Databricks"
3. Cliquer sur "Create"

### √âtape 2 : Configuration de base

**Informations de base :**
- **Subscription** : Choisir votre abonnement
- **Resource group** : Cr√©er ou utiliser existant
- **Workspace name** : `my-databricks-workspace`
- **Region** : Choisir la r√©gion
- **Pricing tier** : Standard (ou Premium)

**Networking :**
- **Virtual network** : Cr√©er nouveau ou utiliser existant
- **Public IP** : ‚úÖ Enable (pour acc√®s facile)

### √âtape 3 : Cr√©er le workspace

1. Cliquer sur "Review + create"
2. V√©rifier la configuration
3. Cliquer sur "Create"
4. Attendre la cr√©ation (5-10 minutes)

**‚ö†Ô∏è Important :** Noter l'URL du workspace.

### √âtape 4 : Ouvrir Databricks

1. Une fois cr√©√©, cliquer sur "Launch Workspace"
2. Interface web Databricks
3. Se connecter avec Azure AD

---

## Cr√©er un cluster

### √âtape 1 : Acc√©der aux clusters

1. Databricks Workspace ‚Üí "Compute"
2. Cliquer sur "Create Cluster"

### √âtape 2 : Configuration du cluster

**Configuration de base :**
- **Cluster name** : `my-cluster`
- **Cluster mode** : Standard (ou Single Node pour tests)
- **Databricks runtime version** : Latest LTS (recommand√©)
- **Python version** : 3.11

**Node type :**
- **Worker type** : Standard_DS3_v2 (pour d√©buter)
- **Driver type** : Standard_DS3_v2
- **Min workers** : 0 (pour √©conomiser)
- **Max workers** : 2 (pour d√©buter)

**‚ö†Ô∏è Important :** Min workers = 0 permet l'auto-termination quand inactif.

### √âtape 3 : Options avanc√©es

**Auto-termination :**
- ‚úÖ Enable (arr√™te le cluster apr√®s inactivit√©)
- **Terminate after** : 30 minutes

**Tags :**
- Ajouter des tags pour organisation

### √âtape 4 : Cr√©er le cluster

1. Cliquer sur "Create Cluster"
2. Attendre le d√©marrage (3-5 minutes)
3. Cluster pr√™t quand status = "Running"

**‚ö†Ô∏è Important :** Le cluster consomme des ressources m√™me inactif. L'arr√™ter quand non utilis√©.

---

## Notebooks Python/SQL

### Cr√©er un notebook

**√âtape 1 : Cr√©er un notebook**

1. Databricks Workspace ‚Üí "Workspace"
2. Clic droit ‚Üí "Create" ‚Üí "Notebook"
3. Nom : `data-processing`
4. Language : Python (ou SQL)
5. Cluster : Attacher au cluster cr√©√©

### √âtape 2 : Utiliser le notebook

**Cellules Python :**

```python
# Cellule 1 : Importer des biblioth√®ques
import pandas as pd
from pyspark.sql import SparkSession

# Cellule 2 : Cr√©er une session Spark
spark = SparkSession.builder.appName("DataProcessing").getOrCreate()

# Cellule 3 : Lire des donn√©es
df = spark.read.csv("dbfs:/FileStore/data/users.csv", header=True, inferSchema=True)

# Cellule 4 : Afficher les donn√©es
df.show()

# Cellule 5 : Transformer
df_filtered = df.filter(df["status"] == "active")
df_filtered.show()
```

**Cellules SQL :**

```sql
-- Cellule SQL : Cr√©er une vue temporaire
CREATE OR REPLACE TEMPORARY VIEW users AS
SELECT * FROM csv.`dbfs:/FileStore/data/users.csv`

-- Requ√™te SQL
SELECT 
    YEAR(created_at) AS year,
    COUNT(*) AS user_count
FROM users
GROUP BY YEAR(created_at)
ORDER BY year;
```

### Ex√©cuter un notebook

- **Run cell** : Ex√©cuter une cellule
- **Run all** : Ex√©cuter toutes les cellules
- **Run all above** : Ex√©cuter toutes les cellules au-dessus

---

## Traitement de donn√©es avec Spark

### Lire des donn√©es

**Depuis Data Lake Storage :**

```python
# Lire CSV
df = spark.read.csv(
    "abfss://container@account.dfs.core.windows.net/data/users.csv",
    header=True,
    inferSchema=True
)

# Lire Parquet
df = spark.read.parquet(
    "abfss://container@account.dfs.core.windows.net/data/users.parquet"
)

# Lire JSON
df = spark.read.json(
    "abfss://container@account.dfs.core.windows.net/data/users.json"
)
```

**Depuis Azure Blob Storage :**

```python
# Configurer l'acc√®s
spark.conf.set(
    "fs.azure.account.key.accountname.blob.core.windows.net",
    "your-account-key"
)

# Lire
df = spark.read.csv(
    "wasbs://container@accountname.blob.core.windows.net/data/users.csv",
    header=True
)
```

### Transformer des donn√©es

**Filtrer :**

```python
df_filtered = df.filter(df["age"] > 18)
```

**S√©lectionner des colonnes :**

```python
df_selected = df.select("id", "name", "email")
```

**Agr√©gations :**

```python
df_aggregated = df.groupBy("category").agg({
    "amount": "sum",
    "id": "count"
})
```

**Joindre :**

```python
df_joined = df1.join(df2, df1.id == df2.user_id, "inner")
```

### √âcrire des donn√©es

**Vers Data Lake Storage :**

```python
# √âcrire en Parquet
df.write.mode("overwrite").parquet(
    "abfss://container@account.dfs.core.windows.net/processed/users.parquet"
)

# √âcrire en CSV
df.write.mode("overwrite").csv(
    "abfss://container@account.dfs.core.windows.net/processed/users.csv"
)
```

---

## Int√©gration avec autres services

### Databricks + Data Lake Storage

**Acc√®s direct :**

```python
# Configurer l'acc√®s
spark.conf.set(
    "fs.azure.account.auth.type.account.dfs.core.windows.net",
    "OAuth"
)
spark.conf.set(
    "fs.azure.account.oauth.provider.type.account.dfs.core.windows.net",
    "org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider"
)

# Lire
df = spark.read.parquet(
    "abfss://container@account.dfs.core.windows.net/data/users.parquet"
)
```

### Databricks + Azure SQL Database

**Lire depuis SQL Database :**

```python
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:sqlserver://server.database.windows.net:1433;database=db") \
    .option("dbtable", "users") \
    .option("user", "sqladmin") \
    .option("password", "password") \
    .load()
```

**√âcrire vers SQL Database :**

```python
df.write \
    .format("jdbc") \
    .option("url", "jdbc:sqlserver://server.database.windows.net:1433;database=db") \
    .option("dbtable", "users_processed") \
    .option("user", "sqladmin") \
    .option("password", "password") \
    .mode("overwrite") \
    .save()
```

### Databricks + Data Factory

**Pipeline Data Factory :**
1. Source : Azure Blob Storage
2. Activity : Databricks Notebook
3. Sink : Azure SQL Database

**Configuration :**
- Notebook path : `/Workspace/path/to/notebook`
- Parameters : Passer des param√®tres

---

## Bonnes pratiques

### Performance

1. **Utiliser le cache** pour r√©utiliser des donn√©es
2. **Partitionner** les donn√©es pour am√©liorer les performances
3. **Optimiser les transformations** pour r√©duire le temps
4. **Utiliser le bon nombre de workers**

### Co√ªts

1. **Arr√™ter les clusters** quand non utilis√©s
2. **Utiliser auto-termination** pour √©conomiser
3. **Surveiller les co√ªts** dans Azure Cost Management
4. **Utiliser des clusters plus petits** pour d√©buter

### Organisation

1. **Organiser les notebooks** dans des dossiers
2. **Nommer clairement** les notebooks et clusters
3. **Documenter** le code
4. **Versionner** avec Git

### S√©curit√©

1. **Utiliser Azure AD** pour authentification
2. **Limiter les acc√®s** avec RBAC
3. **Chiffrer les donn√©es** en transit et au repos
4. **Auditer** les acc√®s

---

## Exemples pratiques

### Exemple 1 : Pipeline ETL complet

**Notebook Databricks :**

```python
# 1. Lire depuis Data Lake
df = spark.read.parquet(
    "abfss://container@account.dfs.core.windows.net/raw/users.parquet"
)

# 2. Transformer
df_processed = df \
    .filter(df["status"] == "active") \
    .select("id", "name", "email", "created_at") \
    .withColumn("year", year(col("created_at")))

# 3. √âcrire vers Data Lake
df_processed.write.mode("overwrite").parquet(
    "abfss://container@account.dfs.core.windows.net/processed/users.parquet"
)
```

### Exemple 2 : Analyse avec Spark SQL

```python
# Cr√©er une vue temporaire
df.createOrReplaceTempView("users")

# Requ√™te SQL
result = spark.sql("""
    SELECT 
        YEAR(created_at) AS year,
        COUNT(*) AS user_count,
        COUNT(DISTINCT email) AS unique_emails
    FROM users
    GROUP BY YEAR(created_at)
    ORDER BY year
""")

result.show()
```

### Exemple 3 : Int√©gration avec Data Factory

1. Cr√©er un notebook Databricks
2. Dans Data Factory, ajouter activit√© "Databricks Notebook"
3. Configurer le notebook
4. Ex√©cuter le pipeline

---

## üìä Points cl√©s √† retenir

1. **Databricks = Big Data** avec Apache Spark
2. **Notebooks** Python/SQL pour d√©veloppement
3. **Clusters auto-scaling** pour performance
4. **Int√©gration native** avec services Azure
5. **Payant** : Surveiller les co√ªts

## üîó Prochain module

Passer au module [7. Projets pratiques](../07-projets/README.md) pour cr√©er des projets complets avec Azure.

