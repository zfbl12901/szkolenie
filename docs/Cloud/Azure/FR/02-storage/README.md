# 2. Azure Storage - Stockage de donn√©es

## üéØ Objectifs

- Comprendre Azure Storage et son utilisation
- Cr√©er des comptes de stockage
- Utiliser Blob Storage et Data Lake Storage
- Uploader et g√©rer des fichiers
- Organiser les donn√©es

## üìã Table des mati√®res

1. [Introduction √† Azure Storage](#introduction-√†-azure-storage)
2. [Cr√©er un compte de stockage](#cr√©er-un-compte-de-stockage)
3. [Blob Storage](#blob-storage)
4. [Data Lake Storage Gen2](#data-lake-storage-gen2)
5. [Uploader et g√©rer des fichiers](#uploader-et-g√©rer-des-fichiers)
6. [Int√©gration avec autres services](#int√©gration-avec-autres-services)

---

## Introduction √† Azure Storage

### Qu'est-ce qu'Azure Storage ?

**Azure Storage** = Service de stockage cloud g√©r√©

- **Stockage illimit√©** : √âvolutif selon les besoins
- **Haute disponibilit√©** : 99.99% de disponibilit√©
- **S√©curis√©** : Chiffrement par d√©faut
- **Int√©gration** : Avec tous les services Azure

### Types de stockage

1. **Blob Storage** : Fichiers (CSV, JSON, Parquet, etc.)
2. **Data Lake Storage Gen2** : Data Lake avec syst√®me de fichiers hi√©rarchique
3. **File Storage** : Partages de fichiers
4. **Queue Storage** : Files d'attente
5. **Table Storage** : Stockage NoSQL

### Free Tier Azure Storage

**Gratuit 12 mois :**
- 5 Go de stockage Blob
- 5 Go de stockage File
- 5 Go de stockage Table
- 5 Go de stockage Queue

**Gratuit √† vie :**
- 200 Go de transfert de donn√©es sortantes/mois

**‚ö†Ô∏è Important :** Au-del√† de ces limites, facturation normale.

---

## Cr√©er un compte de stockage

### √âtape 1 : Acc√©der √† Azure Storage

1. Portail Azure ‚Üí Rechercher "Storage accounts"
2. Cliquer sur "Storage accounts"
3. Cliquer sur "Create"

### √âtape 2 : Configuration de base

**Informations de base :**
- **Subscription** : Choisir votre abonnement
- **Resource group** : Cr√©er ou utiliser existant
- **Storage account name** : Nom unique globalement (ex: `mydataanalyststorage`)
- **Region** : Choisir la r√©gion la plus proche (ex: `France Central`)

**Options de performance :**
- **Performance** : Standard (recommand√© pour d√©buter)
- **Redundancy** : LRS (Local Redundant Storage) - le moins cher

### √âtape 3 : Options avanc√©es

**S√©curit√© :**
- **Secure transfer required** : ‚úÖ Activer (recommand√©)
- **Allow Blob public access** : ‚ùå D√©sactiver (s√©curit√©)

**Data Lake Storage Gen2 :**
- **Hierarchical namespace** : ‚úÖ Activer si besoin de Data Lake

### √âtape 4 : Cr√©er le compte

1. Cliquer sur "Review + create"
2. V√©rifier la configuration
3. Cliquer sur "Create"
4. Attendre la cr√©ation (1-2 minutes)

**‚ö†Ô∏è Important :** Noter le nom du compte de stockage.

---

## Blob Storage

### Qu'est-ce que Blob Storage ?

**Blob Storage** = Stockage d'objets pour fichiers

- **Containers** : Organisent les fichiers (comme des dossiers)
- **Blobs** : Fichiers individuels
- **Types** : Block blobs, Page blobs, Append blobs

### Cr√©er un container

**Via le portail Azure :**

1. Storage account ‚Üí "Containers"
2. Cliquer sur "+ Container"
3. Nom : `raw-data` (ou autre)
4. Public access level : Private (recommand√©)
5. Cliquer sur "Create"

**Via Azure CLI :**

```bash
az storage container create \
  --name raw-data \
  --account-name mydataanalyststorage \
  --auth-mode login
```

**Via Python :**

```python
from azure.storage.blob import BlobServiceClient

# Connexion
connection_string = "DefaultEndpointsProtocol=https;AccountName=..."
blob_service_client = BlobServiceClient.from_connection_string(connection_string)

# Cr√©er un container
container_client = blob_service_client.create_container("raw-data")
```

### Types de blobs

**Block Blobs :**
- Fichiers (CSV, JSON, Parquet, images, etc.)
- Jusqu'√† 4.75 To par blob
- Recommand√© pour la plupart des cas

**Page Blobs :**
- Disques virtuels
- Jusqu'√† 8 To

**Append Blobs :**
- Logs
- Donn√©es d'ajout uniquement

---

## Data Lake Storage Gen2

### Qu'est-ce que Data Lake Storage Gen2 ?

**Data Lake Storage Gen2** = Blob Storage + syst√®me de fichiers hi√©rarchique

- **Compatible Blob Storage** : Utilise les m√™mes APIs
- **Syst√®me de fichiers** : Organisation hi√©rarchique
- **Optimis√© Big Data** : Pour analytics et ML
- **Int√©gration** : Avec Azure Synapse, Databricks, etc.

### Activer Data Lake Storage Gen2

**Lors de la cr√©ation du compte :**
1. Dans "Advanced" ‚Üí Activer "Hierarchical namespace"
2. Cr√©er le compte

**‚ö†Ô∏è Important :** Ne peut pas √™tre activ√© apr√®s cr√©ation.

### Structure Data Lake

```
data-lake/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ 2024/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 02/
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ 2024/
‚îî‚îÄ‚îÄ analytics/
    ‚îî‚îÄ‚îÄ results/
```

### Cr√©er des fichiers et dossiers

**Via le portail Azure :**

1. Storage account ‚Üí "Data Lake"
2. Naviguer dans la structure
3. Uploader des fichiers
4. Cr√©er des dossiers

**Via Python :**

```python
from azure.storage.filedatalake import DataLakeServiceClient

# Connexion
account_name = "mydataanalyststorage"
account_key = "..."
datalake_service_client = DataLakeServiceClient(
    account_url=f"https://{account_name}.dfs.core.windows.net",
    credential=account_key
)

# Cr√©er un syst√®me de fichiers
file_system_client = datalake_service_client.create_file_system("data-lake")

# Cr√©er un r√©pertoire
directory_client = file_system_client.create_directory("raw/2024")
```

---

## Uploader et g√©rer des fichiers

### Uploader un fichier

**Via le portail Azure :**

1. Container ‚Üí "Upload"
2. S√©lectionner le fichier
3. Cliquer sur "Upload"

**Via Azure CLI :**

```bash
az storage blob upload \
  --account-name mydataanalyststorage \
  --container-name raw-data \
  --name data.csv \
  --file ./local-data.csv \
  --auth-mode login
```

**Via Python :**

```python
from azure.storage.blob import BlobServiceClient

blob_service_client = BlobServiceClient.from_connection_string(connection_string)
container_client = blob_service_client.get_container_client("raw-data")

# Uploader un fichier
with open("local-data.csv", "rb") as data:
    container_client.upload_blob(name="data.csv", data=data)
```

### T√©l√©charger un fichier

**Via Python :**

```python
# T√©l√©charger un blob
blob_client = container_client.get_blob_client("data.csv")
with open("downloaded-data.csv", "wb") as download_file:
    download_file.write(blob_client.download_blob().readall())
```

### Lister les fichiers

**Via Python :**

```python
# Lister tous les blobs dans un container
blob_list = container_client.list_blobs()
for blob in blob_list:
    print(f"Name: {blob.name}, Size: {blob.size}")
```

### Supprimer un fichier

**Via Python :**

```python
# Supprimer un blob
blob_client = container_client.get_blob_client("data.csv")
blob_client.delete_blob()
```

---

## Int√©gration avec autres services

### Azure Storage + Data Factory

**Utilisation :**
- Source de donn√©es pour pipelines ETL
- Destination pour donn√©es transform√©es

**Exemple :**
```json
{
  "type": "AzureBlobStorage",
  "typeProperties": {
    "connectionString": "...",
    "container": "raw-data"
  }
}
```

### Azure Storage + Azure SQL Database

**Utilisation :**
- Importer des donn√©es depuis Blob Storage
- Exporter des donn√©es vers Blob Storage

**Exemple SQL :**
```sql
-- Importer depuis Blob Storage
BULK INSERT my_table
FROM 'https://mystorageaccount.blob.core.windows.net/raw-data/data.csv'
WITH (
    FORMAT = 'CSV',
    FIRSTROW = 2
);
```

### Azure Storage + PowerBI

**Utilisation :**
- Connecter PowerBI √† Blob Storage
- Analyser des fichiers directement

**Configuration :**
1. PowerBI ‚Üí "Get Data"
2. "Azure Blob Storage"
3. Entrer l'URL du container
4. S√©lectionner les fichiers

### Azure Storage + Azure Functions

**Utilisation :**
- D√©clencher Functions lors d'upload
- Traiter automatiquement les fichiers

**Configuration :**
1. Function ‚Üí "Add trigger"
2. "Azure Blob Storage trigger"
3. Configurer le container et le chemin

---

## Bonnes pratiques

### Organisation

1. **Utiliser des containers** pour organiser par projet
2. **Nommer clairement** les fichiers et containers
3. **Organiser par date** : `raw/2024/01/data.csv`
4. **S√©parer par type** : `raw/`, `processed/`, `analytics/`

### Performance

1. **Utiliser des noms al√©atoires** pour les blobs (√©viter les s√©quences)
2. **Activer CDN** si besoin de distribution globale (payant)
3. **Utiliser des blobs de blocs** pour la plupart des cas
4. **Partitionner les donn√©es** pour am√©liorer les performances

### Co√ªts

1. **Surveiller l'utilisation** dans Azure Cost Management
2. **Supprimer les fichiers inutiles**
3. **Utiliser les bonnes classes** de stockage
4. **Configurer des r√®gles de cycle de vie** pour automatiser

### S√©curit√©

1. **Ne jamais rendre publics** les containers (sauf besoin sp√©cifique)
2. **Utiliser SAS (Shared Access Signature)** pour acc√®s temporaire
3. **Activer le chiffrement** par d√©faut
4. **Utiliser Azure AD** pour authentification

---

## Exemples pratiques

### Exemple 1 : Uploader un fichier CSV

```python
from azure.storage.blob import BlobServiceClient
import pandas as pd

# Connexion
connection_string = "DefaultEndpointsProtocol=https;AccountName=..."
blob_service_client = BlobServiceClient.from_connection_string(connection_string)
container_client = blob_service_client.get_container_client("raw-data")

# Lire un fichier local
df = pd.read_csv("local-data.csv")

# Uploader vers Azure Storage
with open("local-data.csv", "rb") as data:
    container_client.upload_blob(name="2024/01/data.csv", data=data)
```

### Exemple 2 : T√©l√©charger et traiter

```python
# T√©l√©charger depuis Azure Storage
blob_client = container_client.get_blob_client("2024/01/data.csv")
with open("downloaded-data.csv", "wb") as download_file:
    download_file.write(blob_client.download_blob().readall())

# Traiter
df = pd.read_csv("downloaded-data.csv")
# ... traitement ...

# Uploader le r√©sultat
df.to_csv("processed-data.csv", index=False)
with open("processed-data.csv", "rb") as data:
    container_client.upload_blob(name="processed/2024/01/data.csv", data=data)
```

### Exemple 3 : Lister et filtrer

```python
# Lister tous les fichiers dans un pr√©fixe
blob_list = container_client.list_blobs(name_starts_with="2024/01/")
for blob in blob_list:
    print(f"File: {blob.name}, Size: {blob.size} bytes, Modified: {blob.last_modified}")
```

---

## üìä Points cl√©s √† retenir

1. **Azure Storage = Stockage illimit√©** et hautement disponible
2. **Free Tier : 5 Go** pendant 12 mois
3. **Blob Storage** pour fichiers, **Data Lake Gen2** pour Big Data
4. **Organiser avec containers** et pr√©fixes
5. **Int√©gration native** avec tous les services Azure data

## üîó Prochain module

Passer au module [3. Azure Data Factory - ETL Cloud](../03-data-factory/README.md) pour apprendre √† cr√©er des pipelines ETL sur Azure.

