# 7. Projekty praktyczne Azure

## ğŸ¯ Cele

- StosowaÄ‡ zdobytÄ… wiedzÄ™
- TworzyÄ‡ kompletne pipeline'y ETL
- IntegrowaÄ‡ z PowerBI
- TworzyÄ‡ projekty dla portfolio
- UÅ¼ywaÄ‡ wielu usÅ‚ug Azure

## ğŸ“‹ Spis treÅ›ci

1. [Projekt 1 : Pipeline ETL Blob â†’ SQL Database](#projekt-1--pipeline-etl-blob---sql-database)
2. [Projekt 2 : Data Lake z Synapse](#projekt-2--data-lake-z-synapse)
3. [Projekt 3 : Analytics z PowerBI](#projekt-3--analytics-z-powerbi)
4. [Projekt 4 : Kompletny zautomatyzowany pipeline](#projekt-4--kompletny-zautomatyzowany-pipeline)
5. [Dobre praktyki dla portfolio](#dobre-praktyki-dla-portfolio)

---

## Projekt 1 : Pipeline ETL Blob â†’ SQL Database

### Cel

UtworzyÄ‡ pipeline ETL ktÃ³ry Å‚aduje pliki CSV z Blob Storage do SQL Database.

### Architektura

```
Blob Storage (CSV) â†’ Data Factory â†’ SQL Database â†’ PowerBI
```

### Kroki

#### 1. PrzygotowaÄ‡ dane

**UtworzyÄ‡ kontener Blob Storage :**
- Nazwa : `raw-data`
- PrzesÅ‚aÄ‡ plik CSV testowy

**PrzykÅ‚ad danych CSV :**
```csv
id,name,email,created_at,status
1,John Doe,john@example.com,2024-01-01,active
2,Jane Smith,jane@example.com,2024-01-02,inactive
```

#### 2. UtworzyÄ‡ bazÄ™ SQL Database

1. Portal Azure â†’ UtworzyÄ‡ SQL Database
2. Konfiguracja :
   - Name : `analytics-db`
   - Server : UtworzyÄ‡ nowy serwer
   - Service tier : Basic (darmowe 12 miesiÄ™cy)
3. UtworzyÄ‡ bazÄ™

#### 3. UtworzyÄ‡ tabelÄ™ w SQL Database

```sql
CREATE TABLE users (
    id INT PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(100),
    created_at DATETIME2,
    status VARCHAR(20)
);
```

#### 4. UtworzyÄ‡ pipeline Data Factory

1. Data Factory Studio â†’ "Author" â†’ "Pipelines"
2. UtworzyÄ‡ nowy pipeline : `LoadCSVToSQL`
3. DodaÄ‡ dziaÅ‚anie "Copy Data"
4. Konfiguracja :
   - **Source** : Azure Blob Storage (CSV)
   - **Sink** : Azure SQL Database (tabela users)
5. OpublikowaÄ‡ pipeline

#### 5. WykonaÄ‡ pipeline

1. KliknÄ…Ä‡ "Trigger now"
2. SprawdziÄ‡ wykonanie w "Monitor"
3. SprawdziÄ‡ dane w SQL Database

### Wynik

- Pliki CSV zaÅ‚adowane w SQL Database
- Funkcjonalny pipeline ETL
- Gotowe do analytics z PowerBI

---

## Projekt 2 : Data Lake z Synapse

### Cel

UtworzyÄ‡ kompletny Data Lake z ingerencjÄ…, przeksztaÅ‚caniem i analytics.

### Architektura

```
Å¹rÃ³dÅ‚a â†’ Data Lake Storage (Raw) â†’ Synapse (Transform) â†’ Data Lake (Processed) â†’ PowerBI
                â†“
        Data Factory (Orkiestracja)
```

### Kroki

#### 1. Struktura Data Lake Storage

```
data-lake/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ users/
â”‚   â”œâ”€â”€ orders/
â”‚   â””â”€â”€ products/
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ users/
â”‚   â”œâ”€â”€ orders/
â”‚   â””â”€â”€ products/
â””â”€â”€ analytics/
    â””â”€â”€ results/
```

#### 2. UtworzyÄ‡ obszar roboczy Synapse

1. Portal Azure â†’ UtworzyÄ‡ Azure Synapse Analytics
2. Konfiguracja :
   - Workspace name : `my-synapse-workspace`
   - Data Lake Storage : UtworzyÄ‡ nowy
3. UtworzyÄ‡ obszar roboczy

#### 3. Pipeline'y Data Factory do przeksztaÅ‚cania

**Pipeline dla users :**

1. Synapse Studio â†’ "Integrate" â†’ "Pipelines"
2. UtworzyÄ‡ pipeline : `TransformUsers`
3. DziaÅ‚ania :
   - Source : Data Lake Storage (raw/users/)
   - Data Flow : PrzeksztaÅ‚ciÄ‡ (filtrowaÄ‡, czyÅ›ciÄ‡)
   - Sink : Data Lake Storage (processed/users/)
4. OpublikowaÄ‡

#### 4. Tabele Synapse do analytics

```sql
-- UtworzyÄ‡ tabelÄ™ zewnÄ™trznÄ…
CREATE EXTERNAL TABLE users_processed (
    id INT,
    name VARCHAR(100),
    email VARCHAR(100),
    created_at DATETIME2
)
WITH (
    LOCATION = 'processed/users/',
    DATA_SOURCE = DataLakeStorage,
    FILE_FORMAT = ParquetFormat
);

-- Zapytanie analityczne
SELECT 
    YEAR(created_at) AS year,
    COUNT(*) AS user_count
FROM users_processed
GROUP BY YEAR(created_at);
```

#### 5. Automatyzacja z Triggers

1. Pipeline â†’ "Add trigger" â†’ "New/Edit"
2. Typ : Schedule
3. Recurrence : Daily
4. Start time : 02:00
5. ZapisaÄ‡

### Wynik

- Funkcjonalny Data Lake
- Zautomatyzowany pipeline
- Analytics z Synapse
- Kompletny projekt dla portfolio

---

## Projekt 3 : Analytics z PowerBI

### Cel

UtworzyÄ‡ kompletny system analytics z PowerBI poÅ‚Ä…czonym z Azure.

### Kroki

#### 1. PrzygotowaÄ‡ dane

**W SQL Database lub Synapse :**
- ÅadowaÄ‡ dane
- TworzyÄ‡ widoki analityczne

#### 2. PoÅ‚Ä…czyÄ‡ PowerBI z Azure SQL Database

1. PowerBI Desktop â†’ "Get Data"
2. "Azure" â†’ "Azure SQL Database"
3. Konfiguracja :
   - Server : `my-sql-server.database.windows.net`
   - Database : `analytics-db`
   - Authentication : Database
4. WybraÄ‡ tabele lub widoki
5. KliknÄ…Ä‡ "Load"

#### 3. TworzyÄ‡ wizualizacje

**PrzykÅ‚ad :**
1. ImportowaÄ‡ tabelÄ™ `users`
2. UtworzyÄ‡ wykres : Liczba uÅ¼ytkownikÃ³w na miesiÄ…c
3. DodaÄ‡ filtry
4. UtworzyÄ‡ dashboard

#### 4. OpublikowaÄ‡ na PowerBI Service

1. PowerBI Desktop â†’ "Publish"
2. WybraÄ‡ obszar roboczy
3. OpublikowaÄ‡
4. DostÄ™p do raportu na powerbi.com

#### 5. OdÅ›wieÅ¼aÄ‡ dane

1. PowerBI Service â†’ Dataset â†’ "Schedule refresh"
2. Konfiguracja :
   - Frequency : Daily
   - Time : 03:00
3. ZapisaÄ‡

### Wynik

- Analytics z PowerBI
- Interaktywne wizualizacje
- Automatyczne odÅ›wieÅ¼anie
- Kompletny projekt dla portfolio

---

## Projekt 4 : Kompletny zautomatyzowany pipeline

### Cel

UtworzyÄ‡ kompletnie zautomatyzowany pipeline ETL z wieloma usÅ‚ugami Azure.

### Kompletna architektura

```
Plik CSV przesÅ‚any â†’ Blob Storage (raw/)
    â†“ (Event)
Azure Function (Walidacja)
    â†“
Blob Storage (validated/)
    â†“ (Trigger)
Data Factory Pipeline (PrzeksztaÅ‚Ä‡ CSV â†’ Parquet)
    â†“
Data Lake Storage (processed/)
    â†“
Synapse (Analytics)
    â†“
SQL Database (Results)
    â†“
PowerBI (Wizualizacja)
```

### Implementacja

#### 1. Azure Function walidacji

```python
import azure.functions as func
import logging
import csv
from azure.storage.blob import BlobServiceClient

def main(blob: func.InputStream):
    logging.info(f'Processing blob: {blob.name}')
    
    # CzytaÄ‡ blob
    content = blob.read().decode('utf-8')
    reader = csv.DictReader(content.splitlines())
    
    # WalidowaÄ‡
    valid_rows = []
    for row in reader:
        if row.get('email') and '@' in row['email']:
            valid_rows.append(row)
    
    # PrzesÅ‚aÄ‡ zwalidowane dane
    if valid_rows:
        # PrzesÅ‚aÄ‡ do validated/
        # ...
    
    logging.info(f'Validated {len(valid_rows)} rows')
```

#### 2. Data Factory Pipeline przeksztaÅ‚cania

**Pipeline :**
1. Source : Blob Storage (validated/)
2. Data Flow : PrzeksztaÅ‚ciÄ‡ (czyÅ›ciÄ‡, wzbogacaÄ‡)
3. Sink : Data Lake Storage (processed/parquet/)

#### 3. Synapse do analytics

```sql
-- UtworzyÄ‡ widok analityczny
CREATE VIEW vw_user_analytics AS
SELECT 
    u.id,
    u.name,
    COUNT(o.id) AS order_count,
    SUM(o.amount) AS total_spent
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
GROUP BY u.id, u.name;
```

#### 4. PowerBI do wizualizacji

1. PoÅ‚Ä…czyÄ‡ PowerBI z Synapse
2. UÅ¼ywaÄ‡ widoku `vw_user_analytics`
3. TworzyÄ‡ wizualizacje
4. OpublikowaÄ‡ raport

### Wynik

- Kompletnie zautomatyzowany pipeline
- Automatyczna walidacja
- Automatyczne przeksztaÅ‚canie
- Analytics dostÄ™pne natychmiast
- Wizualizacje PowerBI

---

## Dobre praktyki dla portfolio

### Dokumentacja

**UtworzyÄ‡ README dla kaÅ¼dego projektu :**

```markdown
# Projekt : Pipeline ETL Azure

## Opis
Zautomatyzowany pipeline ETL do przeksztaÅ‚cania danych CSV w Parquet.

## Architektura
- Blob Storage : Przechowywanie
- Data Factory : PrzeksztaÅ‚canie
- SQL Database : Baza danych
- PowerBI : Wizualizacja

## Wyniki
- Redukcja kosztÃ³w o 50%
- Czas przetwarzania zmniejszony o 70%
```

### Wizualizacje

**TworzyÄ‡ diagramy :**
- Architektura systemu
- PrzepÅ‚yw danych
- Schemat danych

**NarzÄ™dzia :**
- Draw.io
- Lucidchart
- Diagramy ASCII w README

### Metryki

**UwzglÄ™dniaÄ‡ metryki :**
- Czas wykonania przed/po
- Koszty przed/po
- Wolumen przetworzonych danych
- WydajnoÅ›Ä‡ zapytaÅ„

### Kod

**Dobre praktyki :**
- Kod skomentowany
- Zmienne Å›rodowiskowe do konfiguracji
- ObsÅ‚uga bÅ‚Ä™dÃ³w
- Logowanie

### GitHub

**UtworzyÄ‡ repozytorium :**
- README z dokumentacjÄ…
- Skrypty Data Factory (JSON)
- Skrypty SQL
- Konfiguracja
- Diagramy

---

## ğŸ“Š Kluczowe punkty do zapamiÄ™tania

1. **Projekty praktyczne** : NiezbÄ™dne dla portfolio
2. **Dokumentacja** : WyjaÅ›niaÄ‡ architekturÄ™ i wyniki
3. **Metryki** : PokazywaÄ‡ wpÅ‚yw (wydajnoÅ›Ä‡, koszty)
4. **Czysty kod** : Skomentowany i zorganizowany
5. **GitHub** : DzieliÄ‡ siÄ™ projektami

## ğŸ”— Zasoby

- [Azure Architecture Center](https://learn.microsoft.com/azure/architecture/)
- [Azure Solutions](https://azure.microsoft.com/solutions/)
- [GitHub Azure Examples](https://github.com/Azure-Samples)

---

**Gratulacje !** UkoÅ„czyÅ‚eÅ› formacjÄ™ Azure dla Data Analyst. MoÅ¼esz teraz tworzyÄ‡ kompletne projekty na Azure uÅ¼ywajÄ…c dostÄ™pnych darmowych zasobÃ³w.

