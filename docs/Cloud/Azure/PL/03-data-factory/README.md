# 3. Azure Data Factory - ETL w chmurze

## ğŸ¯ Cele

- ZrozumieÄ‡ Azure Data Factory i jego rolÄ™
- TworzyÄ‡ pipeline'y ETL
- UÅ¼ywaÄ‡ dziaÅ‚aÅ„ przeksztaÅ‚cania
- IntegrowaÄ‡ ze ÅºrÃ³dÅ‚ami danych
- OrkiestrowaÄ‡ przepÅ‚ywy pracy

## ğŸ“‹ Spis treÅ›ci

1. [Wprowadzenie do Data Factory](#wprowadzenie-do-data-factory)
2. [UtworzyÄ‡ Data Factory](#utworzyÄ‡-data-factory)
3. [UtworzyÄ‡ pipeline](#utworzyÄ‡-pipeline)
4. [DziaÅ‚ania przeksztaÅ‚cania](#dziaÅ‚ania-przeksztaÅ‚cania)
5. [Integracja ze ÅºrÃ³dÅ‚ami danych](#integracja-ze-ÅºrÃ³dÅ‚ami-danych)
6. [Orkiestracja i harmonogramowanie](#orkiestracja-i-harmonogramowanie)

---

## Wprowadzenie do Data Factory

### Czym jest Azure Data Factory?

**Azure Data Factory** = ZarzÄ…dzana usÅ‚uga ETL w chmurze

- **ETL** : Extract, Transform, Load
- **Chmura** : Brak infrastruktury do zarzÄ…dzania
- **ZarzÄ…dzane** : Microsoft zarzÄ…dza infrastrukturÄ…
- **Skalowalne** : Automatycznie dostosowuje siÄ™

### Komponenty Data Factory

1. **Pipelines** : PrzepÅ‚ywy pracy ETL
2. **Activities** : Kroki w pipeline
3. **Datasets** : Reprezentacje danych
4. **Linked Services** : PoÅ‚Ä…czenia ze ÅºrÃ³dÅ‚ami
5. **Triggers** : Automatyczne wyzwalanie

### Data Factory Free Tier

**Darmowe na zawsze :**
- 5 darmowych pipeline'Ã³w
- Ograniczone dziaÅ‚ania
- Poza tym : rozliczanie wedÅ‚ug uÅ¼ycia

**âš ï¸ WaÅ¼ne :** MonitorowaÄ‡ koszty, zwÅ‚aszcza dla dziaÅ‚aÅ„ przeksztaÅ‚cania.

---

## UtworzyÄ‡ Data Factory

### Krok 1 : DostÄ™p do Data Factory

1. Portal Azure â†’ SzukaÄ‡ "Data Factory"
2. KliknÄ…Ä‡ "Data factories"
3. KliknÄ…Ä‡ "Create"

### Krok 2 : Podstawowa konfiguracja

**Podstawowe informacje :**
- **Subscription** : WybraÄ‡ subskrypcjÄ™
- **Resource group** : UtworzyÄ‡ lub uÅ¼yÄ‡ istniejÄ…cego
- **Name** : `my-data-factory` (unikalne globalnie)
- **Version** : V2 (zalecane)
- **Region** : WybraÄ‡ najbliÅ¼szy region

**Konfiguracja Git (opcjonalne) :**
- **Configure Git later** : Aby szybko rozpoczÄ…Ä‡
- Lub skonfigurowaÄ‡ Git/GitHub do wersjonowania

### Krok 3 : UtworzyÄ‡ Data Factory

1. KliknÄ…Ä‡ "Review + create"
2. SprawdziÄ‡ konfiguracjÄ™
3. KliknÄ…Ä‡ "Create"
4. CzekaÄ‡ na utworzenie (2-3 minuty)

**âš ï¸ WaÅ¼ne :** ZanotowaÄ‡ nazwÄ™ Data Factory.

### Krok 4 : OtworzyÄ‡ Data Factory Studio

1. Po utworzeniu, kliknÄ…Ä‡ "Open Azure Data Factory Studio"
2. Interfejs web do tworzenia pipeline'Ã³w

---

## UtworzyÄ‡ pipeline

### Krok 1 : UtworzyÄ‡ Linked Service

**Linked Service = PoÅ‚Ä…czenie ze ÅºrÃ³dÅ‚em danych**

**PrzykÅ‚ad : Azure Blob Storage**

1. Data Factory Studio â†’ "Manage" â†’ "Linked services"
2. KliknÄ…Ä‡ "+ New"
3. SzukaÄ‡ "Azure Blob Storage"
4. Konfiguracja :
   - **Name** : `AzureBlobStorage1`
   - **Storage account name** : WybraÄ‡ konto
   - **Authentication method** : Account key (lub inny)
5. KliknÄ…Ä‡ "Create"

### Krok 2 : UtworzyÄ‡ Dataset

**Dataset = Reprezentacja danych**

1. Data Factory Studio â†’ "Author" â†’ "Datasets"
2. KliknÄ…Ä‡ "+ New"
3. WybraÄ‡ "Azure Blob Storage"
4. Konfiguracja :
   - **Name** : `CSVData`
   - **Linked service** : `AzureBlobStorage1`
   - **File path** : `raw-data/`
   - **File format** : DelimitedText (CSV)
5. KliknÄ…Ä‡ "Create"

### Krok 3 : UtworzyÄ‡ pipeline

1. Data Factory Studio â†’ "Author" â†’ "Pipelines"
2. KliknÄ…Ä‡ "+ New pipeline"
3. NazwaÄ‡ pipeline : `CopyCSVToParquet`

### Krok 4 : DodaÄ‡ dziaÅ‚anie

**PrzykÅ‚ad : Copy Data**

1. W pipeline, przeciÄ…gnÄ…Ä‡ "Copy Data" z "Move & transform"
2. SkonfigurowaÄ‡ :
   - **Source** : Dataset `CSVData`
   - **Sink (Destination)** : UtworzyÄ‡ nowy dataset Parquet
3. KliknÄ…Ä‡ "Publish" aby zapisaÄ‡

---

## DziaÅ‚ania przeksztaÅ‚cania

### Copy Data

**KopiowaÄ‡ dane ze ÅºrÃ³dÅ‚a do miejsca docelowego**

**Konfiguracja :**
- **Source** : Dataset ÅºrÃ³dÅ‚owy
- **Sink** : Dataset docelowy
- **Mapping** : Mapowanie kolumn

**PrzykÅ‚ad : CSV â†’ Parquet**

```json
{
  "name": "CopyCSVToParquet",
  "type": "Copy",
  "inputs": [{"referenceName": "CSVData"}],
  "outputs": [{"referenceName": "ParquetData"}],
  "typeProperties": {
    "source": {"type": "DelimitedTextSource"},
    "sink": {"type": "ParquetSink"}
  }
}
```

### Data Flow

**PrzeksztaÅ‚canie danych z interfejsem graficznym**

**Kroki :**
1. UtworzyÄ‡ Data Flow
2. DodaÄ‡ ÅºrÃ³dÅ‚o
3. DodaÄ‡ przeksztaÅ‚cenia :
   - **Select** : WybieraÄ‡ kolumny
   - **Filter** : FiltrowaÄ‡ wiersze
   - **Derived Column** : TworzyÄ‡ kolumny obliczane
   - **Aggregate** : Agregacje
   - **Join** : ÅÄ…czyÄ‡ dane
4. DodaÄ‡ sink

**PrzykÅ‚ad przeksztaÅ‚ceÅ„ :**

```
Source (CSV) 
  â†’ Select (kolumny)
  â†’ Filter (status = 'active')
  â†’ Derived Column (nowa kolumna)
  â†’ Aggregate (SUM, COUNT)
  â†’ Sink (Parquet)
```

### Lookup

**WyszukiwaÄ‡ wartoÅ›ci w innym ÅºrÃ³dle**

**UÅ¼ycie :**
- WalidowaÄ‡ dane
- WzbogacaÄ‡ dane
- SprawdzaÄ‡ referencje

### Stored Procedure

**WykonaÄ‡ procedurÄ™ skÅ‚adowanÄ… SQL**

**UÅ¼ycie :**
- Przetwarzanie w SQL Database
- ZÅ‚oÅ¼ona logika biznesowa
- Optymalizacja po stronie bazy

---

## Integracja ze ÅºrÃ³dÅ‚ami danych

### Azure Blob Storage

**Å¹rÃ³dÅ‚o danych :**

```json
{
  "type": "AzureBlobStorage",
  "typeProperties": {
    "connectionString": "...",
    "container": "raw-data"
  }
}
```

### Azure SQL Database

**Å¹rÃ³dÅ‚o danych :**

```json
{
  "type": "AzureSqlDatabase",
  "typeProperties": {
    "connectionString": "...",
    "tableName": "users"
  }
}
```

### Azure Data Lake Storage Gen2

**Å¹rÃ³dÅ‚o danych :**

```json
{
  "type": "AzureBlobFS",
  "typeProperties": {
    "url": "https://account.dfs.core.windows.net",
    "fileSystem": "data-lake"
  }
}
```

### Pliki lokalne (przez Self-hosted IR)

**Integration Runtime :**
- Self-hosted IR do dostÄ™pu do plikÃ³w lokalnych
- ZainstalowaÄ‡ na maszynie lokalnej
- PoÅ‚Ä…czyÄ‡ z Data Factory

---

## Orkiestracja i harmonogramowanie

### WyzwalaÄ‡ rÄ™cznie

1. Data Factory Studio â†’ "Monitor"
2. WybraÄ‡ pipeline
3. KliknÄ…Ä‡ "Trigger now"
4. ZobaczyÄ‡ wykonanie w czasie rzeczywistym

### PlanowaÄ‡ pipeline (Trigger)

**UtworzyÄ‡ trigger :**

1. Pipeline â†’ "Add trigger" â†’ "New/Edit"
2. Typ : "Schedule"
3. Konfiguracja :
   - **Name** : `DailyTrigger`
   - **Type** : Schedule
   - **Recurrence** : Daily
   - **Start time** : 02:00
4. KliknÄ…Ä‡ "OK"

**Typy triggerÃ³w :**
- **Schedule** : Planowane (cron)
- **Event** : Wyzwalane przez zdarzenie
- **Tumbling window** : Okno przesuwne

### WyzwalaÄ‡ przez zdarzenie

**PrzykÅ‚ad : Nowy plik w Blob Storage**

1. UtworzyÄ‡ trigger "Storage event"
2. SkonfigurowaÄ‡ :
   - **Storage account** : Twoje konto
   - **Container** : `raw-data`
   - **Event type** : Blob created
3. PowiÄ…zaÄ‡ z pipeline

---

## Dobre praktyki

### WydajnoÅ›Ä‡

1. **UÅ¼ywaÄ‡ Data Flow** dla zÅ‚oÅ¼onych przeksztaÅ‚ceÅ„
2. **OptymalizowaÄ‡ dziaÅ‚ania** aby zmniejszyÄ‡ czas
3. **UÅ¼ywaÄ‡ rÃ³wnolegÅ‚oÅ›ci** gdy moÅ¼liwe
4. **WybraÄ‡ odpowiednie regiony** aby zmniejszyÄ‡ opÃ³Åºnienie

### Koszty

1. **MonitorowaÄ‡ wykonania** w Monitor
2. **UÅ¼ywaÄ‡ 5 darmowych pipeline'Ã³w** mÄ…drze
3. **OptymalizowaÄ‡ Data Flows** (kosztowne)
4. **ZatrzymywaÄ‡ nieuÅ¼ywane pipeline'y**

### Organizacja

1. **NazywaÄ‡ jasno** pipeline'y i dziaÅ‚ania
2. **DokumentowaÄ‡** przeksztaÅ‚cenia
3. **WersjonowaÄ‡** z Git
4. **TestowaÄ‡** przed publikacjÄ…

### BezpieczeÅ„stwo

1. **UÅ¼ywaÄ‡ Key Vault** dla sekretÃ³w
2. **OgraniczaÄ‡ uprawnienia** Linked Services
3. **AudytowaÄ‡** wykonania
4. **SzyfrowaÄ‡** dane w tranzycie

---

## PrzykÅ‚ady praktyczne

### PrzykÅ‚ad 1 : Prosty pipeline CSV â†’ Parquet

**Pipeline :**
1. Å¹rÃ³dÅ‚o : Azure Blob Storage (CSV)
2. DziaÅ‚anie : Copy Data
3. Sink : Azure Blob Storage (Parquet)

**Konfiguracja :**
- Å¹rÃ³dÅ‚o : `raw-data/data.csv`
- Sink : `processed-data/data.parquet`
- Format : DelimitedText â†’ Parquet

### PrzykÅ‚ad 2 : Pipeline z przeksztaÅ‚ceniem

**Pipeline :**
1. Å¹rÃ³dÅ‚o : Azure SQL Database
2. Data Flow :
   - WybraÄ‡ kolumny
   - FiltrowaÄ‡ wiersze
   - AgregowaÄ‡
3. Sink : Azure Blob Storage (Parquet)

### PrzykÅ‚ad 3 : Pipeline orkiestrowany

**Pipeline :**
1. Lookup : SprawdziÄ‡ czy nowe dane
2. If Condition : JeÅ›li nowe dane
3. Copy Data : SkopiowaÄ‡ do staging
4. Data Flow : PrzeksztaÅ‚ciÄ‡
5. Copy Data : ZaÅ‚adowaÄ‡ do miejsca docelowego

---

## ğŸ“Š Kluczowe punkty do zapamiÄ™tania

1. **Data Factory = ETL w chmurze** zarzÄ…dzane przez Microsoft
2. **Free Tier : 5 pipeline'Ã³w** darmowych
3. **Pipelines** orkiestrujÄ… dziaÅ‚ania
4. **Data Flows** dla zÅ‚oÅ¼onych przeksztaÅ‚ceÅ„
5. **Triggers** umoÅ¼liwiajÄ… automatyzacjÄ™

## ğŸ”— NastÄ™pny moduÅ‚

PrzejdÅº do moduÅ‚u [4. Azure SQL Database - Baza danych](../04-sql-database/README.md), aby nauczyÄ‡ siÄ™ uÅ¼ywaÄ‡ SQL Database na Azure.

