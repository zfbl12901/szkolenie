# 5. Amazon Athena - RequÃªtes SQL sur S3

## ğŸ¯ Objectifs

- Comprendre Amazon Athena et son utilisation
- CrÃ©er des tables externes pointant vers S3
- ExÃ©cuter des requÃªtes SQL sur fichiers S3
- Optimiser les coÃ»ts et performances
- IntÃ©grer avec Glue Data Catalog

## ğŸ“‹ Table des matiÃ¨res

1. [Introduction Ã  Athena](#introduction-Ã -athena)
2. [CrÃ©er des tables externes](#crÃ©er-des-tables-externes)
3. [ExÃ©cuter des requÃªtes](#exÃ©cuter-des-requÃªtes)
4. [Optimisation des coÃ»ts](#optimisation-des-coÃ»ts)
5. [IntÃ©gration avec Glue](#intÃ©gration-avec-glue)
6. [Bonnes pratiques](#bonnes-pratiques)

---

## Introduction Ã  Athena

### Qu'est-ce qu'Amazon Athena ?

**Amazon Athena** = Service de requÃªtes SQL serverless sur S3

- **Serverless** : Pas d'infrastructure Ã  gÃ©rer
- **Pay-per-query** : Payez seulement ce que vous utilisez
- **Standard SQL** : Syntaxe SQL standard
- **S3 directement** : Pas besoin de charger dans base de donnÃ©es

### Cas d'usage pour Data Analyst

- **Exploration de donnÃ©es** : Analyser rapidement des fichiers S3
- **Data Lake queries** : RequÃªtes sur data lake
- **Ad-hoc analysis** : Analyses ponctuelles
- **Log analysis** : Analyser des logs stockÃ©s dans S3

### Free Tier Athena

**Gratuit Ã  vie :**
- 10 Go de donnÃ©es scannÃ©es/mois
- Au-delÃ  : 5$ par TÃ©raoctet scannÃ©

**âš ï¸ Important :** Les coÃ»ts dÃ©pendent de la quantitÃ© de donnÃ©es scannÃ©es. Optimiser les requÃªtes pour rÃ©duire les coÃ»ts.

---

## CrÃ©er des tables externes

### MÃ©thode 1 : Via l'Ã©diteur Athena

**Ã‰tape 1 : AccÃ©der Ã  Athena**

1. Console AWS â†’ Rechercher "Athena"
2. Cliquer sur "Amazon Athena"
3. PremiÃ¨re utilisation : Configurer le rÃ©sultat S3

**Ã‰tape 2 : Configurer le rÃ©sultat**

1. "Settings" â†’ "Manage"
2. "Query result location" : `s3://my-bucket/athena-results/`
3. "Save"

**Ã‰tape 3 : CrÃ©er une table**

```sql
-- Table pour fichiers CSV
CREATE EXTERNAL TABLE users (
    id INT,
    name STRING,
    email STRING,
    created_at TIMESTAMP
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
    'serialization.format' = ',',
    'field.delim' = ','
)
STORED AS TEXTFILE
LOCATION 's3://my-bucket/data/users/'
TBLPROPERTIES ('skip.header.line.count'='1');
```

### MÃ©thode 2 : Via Glue Data Catalog (recommandÃ©)

**Utiliser les tables crÃ©Ã©es par Glue :**

1. Glue â†’ CrÃ©er un crawler pour S3
2. Crawler crÃ©e automatiquement la table
3. Athena utilise directement cette table

**Avantages :**
- SchÃ©ma dÃ©tectÃ© automatiquement
- Pas besoin de dÃ©finir manuellement
- RÃ©utilisable par d'autres services

### Formats supportÃ©s

**CSV :**
```sql
CREATE EXTERNAL TABLE csv_data (
    col1 STRING,
    col2 INT
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
STORED AS TEXTFILE
LOCATION 's3://bucket/csv/';
```

**JSON :**
```sql
CREATE EXTERNAL TABLE json_data (
    id INT,
    name STRING
)
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
STORED AS TEXTFILE
LOCATION 's3://bucket/json/';
```

**Parquet (recommandÃ©) :**
```sql
CREATE EXTERNAL TABLE parquet_data (
    id INT,
    name STRING,
    created_at TIMESTAMP
)
STORED AS PARQUET
LOCATION 's3://bucket/parquet/';
```

---

## ExÃ©cuter des requÃªtes

### RequÃªtes de base

**SELECT simple :**

```sql
SELECT * FROM users LIMIT 10;
```

**Filtrer :**

```sql
SELECT 
    id,
    name,
    email
FROM users
WHERE created_at > DATE '2024-01-01'
ORDER BY created_at DESC;
```

**AgrÃ©gations :**

```sql
SELECT 
    DATE_TRUNC('month', created_at) AS month,
    COUNT(*) AS user_count,
    COUNT(DISTINCT email) AS unique_emails
FROM users
GROUP BY DATE_TRUNC('month', created_at)
ORDER BY month;
```

### RequÃªtes avancÃ©es

**Window functions :**

```sql
SELECT 
    id,
    name,
    created_at,
    ROW_NUMBER() OVER (PARTITION BY DATE_TRUNC('month', created_at) ORDER BY created_at) AS rank
FROM users;
```

**Jointures :**

```sql
SELECT 
    u.name,
    o.amount,
    o.created_at
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE o.created_at > DATE '2024-01-01';
```

### RequÃªtes sur partitions

**Si donnÃ©es partitionnÃ©es :**

```sql
-- Table partitionnÃ©e par date
CREATE EXTERNAL TABLE sales (
    id INT,
    product_id INT,
    amount DECIMAL(10,2)
)
PARTITIONED BY (sale_date DATE)
STORED AS PARQUET
LOCATION 's3://bucket/sales/';

-- Ajouter des partitions
ALTER TABLE sales ADD PARTITION (sale_date='2024-01-01')
LOCATION 's3://bucket/sales/year=2024/month=01/day=01/';

-- RequÃªte avec partition (plus rapide et moins cher)
SELECT * FROM sales
WHERE sale_date = DATE '2024-01-01';
```

---

## Optimisation des coÃ»ts

### RÃ©duire les donnÃ©es scannÃ©es

**1. Utiliser WHERE pour filtrer tÃ´t :**

```sql
-- âŒ Mauvais : Scanne tout puis filtre
SELECT * FROM large_table
WHERE date = '2024-01-01';

-- âœ… Bon : Filtre dÃ¨s le dÃ©but (si partitionnÃ©)
SELECT * FROM large_table
WHERE date = '2024-01-01';
```

**2. SÃ©lectionner uniquement les colonnes nÃ©cessaires :**

```sql
-- âŒ Mauvais : Scanne toutes les colonnes
SELECT * FROM large_table;

-- âœ… Bon : Scanne seulement les colonnes nÃ©cessaires
SELECT id, name FROM large_table;
```

**3. Utiliser LIMIT :**

```sql
-- Limiter le nombre de rÃ©sultats
SELECT * FROM large_table LIMIT 100;
```

### Utiliser Parquet

**Parquet est plus efficace que CSV :**

- **Compression** : Moins de donnÃ©es scannÃ©es
- **Colonnes** : Scanne seulement les colonnes nÃ©cessaires
- **CoÃ»t rÃ©duit** : Jusqu'Ã  90% de rÃ©duction

**Convertir CSV â†’ Parquet avec Glue :**

```python
# Job Glue pour convertir
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "csv_data"
)

glueContext.write_dynamic_frame.from_options(
    frame = datasource,
    connection_type = "s3",
    connection_options = {"path": "s3://bucket/parquet/"},
    format = "parquet"
)
```

### Partitionner les donnÃ©es

**Partitionner par date (recommandÃ©) :**

```
s3://bucket/data/
â”œâ”€â”€ year=2024/
â”‚   â”œâ”€â”€ month=01/
â”‚   â”‚   â”œâ”€â”€ day=01/
â”‚   â”‚   â””â”€â”€ day=02/
â”‚   â””â”€â”€ month=02/
```

**CrÃ©er table partitionnÃ©e :**

```sql
CREATE EXTERNAL TABLE partitioned_data (
    id INT,
    name STRING
)
PARTITIONED BY (year INT, month INT, day INT)
STORED AS PARQUET
LOCATION 's3://bucket/data/';
```

---

## IntÃ©gration avec Glue

### Utiliser les tables Glue

**Tables crÃ©Ã©es par Glue sont automatiquement disponibles dans Athena :**

1. Glue â†’ Crawler crÃ©e une table
2. Athena â†’ "Tables" â†’ Voir toutes les tables Glue
3. Utiliser directement dans les requÃªtes

**Avantages :**
- SchÃ©ma automatique
- Pas de dÃ©finition manuelle
- Synchronisation automatique

### Mettre Ã  jour les partitions

**Si nouvelles donnÃ©es ajoutÃ©es :**

```sql
-- Mettre Ã  jour les partitions
MSCK REPAIR TABLE sales;

-- Ou ajouter manuellement
ALTER TABLE sales ADD PARTITION (sale_date='2024-01-02')
LOCATION 's3://bucket/sales/year=2024/month=01/day=02/';
```

---

## Bonnes pratiques

### Performance

1. **Utiliser Parquet** au lieu de CSV
2. **Partitionner les donnÃ©es** par date/catÃ©gorie
3. **SÃ©lectionner uniquement les colonnes** nÃ©cessaires
4. **Filtrer tÃ´t** avec WHERE
5. **Utiliser LIMIT** pour exploration

### CoÃ»ts

1. **Surveiller les donnÃ©es scannÃ©es** dans les rÃ©sultats
2. **Optimiser les requÃªtes** pour rÃ©duire le scan
3. **Utiliser Parquet** pour compression
4. **Partitionner** pour rÃ©duire le scan
5. **Mettre en cache** les rÃ©sultats frÃ©quents

### Organisation

1. **Organiser S3** avec prÃ©fixes cohÃ©rents
2. **Nommer les tables** de maniÃ¨re claire
3. **Documenter les schÃ©mas**
4. **Utiliser des bases de donnÃ©es** pour organiser

---

## Exemples pratiques

### Exemple 1 : Analyser des logs

```sql
-- Table pour logs
CREATE EXTERNAL TABLE logs (
    timestamp TIMESTAMP,
    level STRING,
    message STRING,
    user_id INT
)
PARTITIONED BY (date DATE)
STORED AS TEXTFILE
LOCATION 's3://bucket/logs/';

-- RequÃªte : Erreurs par jour
SELECT 
    date,
    COUNT(*) AS error_count
FROM logs
WHERE level = 'ERROR'
GROUP BY date
ORDER BY date DESC;
```

### Exemple 2 : Analyser des donnÃ©es CSV

```sql
-- Table CSV
CREATE EXTERNAL TABLE sales_csv (
    id INT,
    product_id INT,
    amount DECIMAL(10,2),
    sale_date DATE
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
STORED AS TEXTFILE
LOCATION 's3://bucket/sales/csv/'
TBLPROPERTIES ('skip.header.line.count'='1');

-- Analyse : Ventes par mois
SELECT 
    DATE_TRUNC('month', sale_date) AS month,
    SUM(amount) AS total_sales,
    COUNT(*) AS transaction_count
FROM sales_csv
GROUP BY DATE_TRUNC('month', sale_date)
ORDER BY month;
```

### Exemple 3 : Jointure de plusieurs tables

```sql
-- Analyser avec jointures
SELECT 
    p.name AS product_name,
    c.name AS category_name,
    SUM(s.amount) AS total_sales
FROM sales s
JOIN products p ON s.product_id = p.id
JOIN categories c ON p.category_id = c.id
WHERE s.sale_date >= DATE '2024-01-01'
GROUP BY p.name, c.name
ORDER BY total_sales DESC
LIMIT 10;
```

---

## ğŸ“Š Points clÃ©s Ã  retenir

1. **Athena = SQL serverless** sur fichiers S3
2. **Free Tier : 10 Go/mois** de donnÃ©es scannÃ©es
3. **Parquet** = format le plus efficace
4. **Partitionner** = rÃ©duire les coÃ»ts
5. **IntÃ©gration Glue** = schÃ©mas automatiques

## ğŸ”— Prochain module

Passer au module [6. AWS Lambda - Serverless Computing](../06-lambda/README.md) pour apprendre Ã  automatiser le traitement de donnÃ©es.

