# 3. AWS Glue - ETL Serverless

## üéØ Objectifs

- Comprendre AWS Glue et son r√¥le dans l'ETL
- Cr√©er des crawlers pour d√©couvrir les donn√©es
- Cr√©er des jobs ETL avec Glue
- Transformer des donn√©es avec PySpark
- Int√©grer Glue avec S3 et autres services

## üìã Table des mati√®res

1. [Introduction √† AWS Glue](#introduction-√†-aws-glue)
2. [Cr√©er un Data Catalog](#cr√©er-un-data-catalog)
3. [Crawlers - D√©couvrir les donn√©es](#crawlers---d√©couvrir-les-donn√©es)
4. [Cr√©er un job ETL](#cr√©er-un-job-etl)
5. [Transformation de donn√©es](#transformation-de-donn√©es)
6. [Orchestration et scheduling](#orchestration-et-scheduling)

---

## Introduction √† AWS Glue

### Qu'est-ce qu'AWS Glue ?

**AWS Glue** = Service ETL serverless g√©r√©

- **ETL** : Extract, Transform, Load
- **Serverless** : Pas de serveurs √† g√©rer
- **G√©r√©** : AWS g√®re l'infrastructure
- **Scalable** : S'adapte automatiquement

### Composants Glue

1. **Data Catalog** : Catalogue de m√©tadonn√©es
2. **Crawlers** : D√©couvrent automatiquement les sch√©mas
3. **ETL Jobs** : Scripts de transformation (Python/PySpark)
4. **Triggers** : D√©clenchement automatique
5. **Workflows** : Orchestration de plusieurs jobs

### Free Tier Glue

**Gratuit √† vie :**
- 10 000 objets/mois dans le Data Catalog
- 1 million requ√™tes/mois au Data Catalog
- 0.44$ par DPU-heure (premier million gratuit)

**‚ö†Ô∏è Important :** Les jobs Glue consomment des DPU (Data Processing Units). Surveiller les co√ªts.

---

## Cr√©er un Data Catalog

### Qu'est-ce que le Data Catalog ?

**Data Catalog** = Catalogue centralis√© de m√©tadonn√©es

- Sch√©mas de donn√©es
- Emplacements (S3, bases de donn√©es)
- Types de donn√©es
- Partitions

### Structure du Data Catalog

- **Databases** : Groupes de tables
- **Tables** : M√©tadonn√©es des donn√©es
- **Partitions** : Organisation des donn√©es

### Cr√©er une base de donn√©es

1. Console AWS ‚Üí Glue ‚Üí "Databases"
2. "Add database"
3. Nom : `data_analyst_db`
4. Description (optionnel)
5. "Create"

**Utilisation :**
- Organiser les tables par projet
- Exemple : `raw_data_db`, `processed_data_db`

---

## Crawlers - D√©couvrir les donn√©es

### Qu'est-ce qu'un Crawler ?

**Crawler** = Service qui scanne les donn√©es et cr√©e automatiquement les tables

- Analyse les fichiers dans S3
- D√©tecte le sch√©ma automatiquement
- Cr√©e les tables dans le Data Catalog
- Supporte : CSV, JSON, Parquet, etc.

### Cr√©er un Crawler

**√âtape 1 : Configuration de base**

1. Glue ‚Üí "Crawlers" ‚Üí "Add crawler"
2. Nom : `s3-csv-crawler`
3. Description (optionnel)

**√âtape 2 : Source de donn√©es**

1. "Add a data source"
2. Type : "S3"
3. Chemin S3 : `s3://my-bucket/raw/`
4. Inclure les sous-dossiers (optionnel)

**√âtape 3 : IAM Role**

1. Cr√©er un nouveau r√¥le ou utiliser existant
2. Nom : `AWSGlueServiceRole-default`
3. Permissions : Acc√®s S3 et Glue

**√âtape 4 : Sortie**

1. Base de donn√©es : `data_analyst_db`
2. Pr√©fixe des tables (optionnel)

**√âtape 5 : Ex√©cuter**

1. "Run crawler now" ou planifier
2. Attendre la fin (quelques minutes)
3. V√©rifier les tables cr√©√©es

### R√©sultat du Crawler

**Table cr√©√©e automatiquement :**
- Colonnes d√©tect√©es
- Types de donn√©es inf√©r√©s
- Emplacement S3
- Format de fichier

**Exemple de table cr√©√©e :**
```
Table: raw_data
Columns:
  - id (bigint)
  - name (string)
  - created_at (timestamp)
Location: s3://my-bucket/raw/
Format: csv
```

---

## Cr√©er un job ETL

### Types de jobs Glue

1. **Spark** : Jobs PySpark (recommand√©)
2. **Python shell** : Scripts Python simples
3. **Ray** : Traitement distribu√© avanc√©

### Cr√©er un job Spark

**√âtape 1 : Configuration**

1. Glue ‚Üí "ETL jobs" ‚Üí "Add job"
2. Nom : `transform-csv-job`
3. IAM Role : `AWSGlueServiceRole-default`
4. Type : "Spark"
5. Glue version : "4.0" (recommand√©)
6. DPU : 2 (minimum, ajustable)

**√âtape 2 : Source de donn√©es**

1. "Data source" : S√©lectionner une table du Data Catalog
2. Ou : Chemin S3 direct

**√âtape 3 : Destination**

1. "Data target" : S3
2. Format : Parquet (recommand√© pour analytics)
3. Chemin : `s3://my-bucket/processed/`

**√âtape 4 : Script**

1. G√©n√©rer un script automatique
2. Ou : √âcrire un script personnalis√©

### Script ETL de base

**Script g√©n√©r√© automatiquement :**

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Lire depuis le Data Catalog
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "raw_data"
)

# Transformer (exemple : filtrer)
filtered = Filter.apply(
    frame = datasource,
    f = lambda x: x["status"] == "active"
)

# √âcrire vers S3
glueContext.write_dynamic_frame.from_options(
    frame = filtered,
    connection_type = "s3",
    connection_options = {"path": "s3://my-bucket/processed/"},
    format = "parquet"
)

job.commit()
```

---

## Transformation de donn√©es

### Transformations courantes

#### 1. Filtrer des lignes

```python
from awsglue.transforms import Filter

filtered = Filter.apply(
    frame = datasource,
    f = lambda x: x["age"] > 18
)
```

#### 2. S√©lectionner des colonnes

```python
from awsglue.transforms import SelectFields

selected = SelectFields.apply(
    frame = datasource,
    paths = ["id", "name", "email"]
)
```

#### 3. Renommer des colonnes

```python
from awsglue.transforms import RenameField

renamed = RenameField.apply(
    frame = datasource,
    old_name = "old_column",
    new_name = "new_column"
)
```

#### 4. Joindre des donn√©es

```python
joined = Join.apply(
    frame1 = datasource1,
    frame2 = datasource2,
    keys1 = ["id"],
    keys2 = ["user_id"]
)
```

#### 5. Agr√©gations

```python
# Convertir en DataFrame Spark pour agr√©gations
df = datasource.toDF()

aggregated = df.groupBy("category").agg({
    "amount": "sum",
    "id": "count"
})

# Reconvertir en DynamicFrame
from awsglue.dynamicframe import DynamicFrame
result = DynamicFrame.fromDF(aggregated, glueContext, "result")
```

### Exemple complet : Transformation CSV ‚Üí Parquet

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# 1. Lire depuis S3 (via Data Catalog)
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "raw_data"
)

# 2. Filtrer les donn√©es
filtered = Filter.apply(
    frame = datasource,
    f = lambda x: x["status"] == "active"
)

# 3. S√©lectionner colonnes
selected = SelectFields.apply(
    frame = filtered,
    paths = ["id", "name", "email", "created_at"]
)

# 4. Convertir en DataFrame pour transformations avanc√©es
df = selected.toDF()

# 5. Ajouter une colonne calcul√©e
from pyspark.sql.functions import col, year
df = df.withColumn("year", year(col("created_at")))

# 6. Reconvertir en DynamicFrame
from awsglue.dynamicframe import DynamicFrame
result = DynamicFrame.fromDF(df, glueContext, "result")

# 7. √âcrire vers S3 en Parquet (partitionn√© par ann√©e)
glueContext.write_dynamic_frame.from_options(
    frame = result,
    connection_type = "s3",
    connection_options = {
        "path": "s3://my-bucket/processed/",
        "partitionKeys": ["year"]
    },
    format = "parquet"
)

job.commit()
```

---

## Orchestration et scheduling

### D√©clencher un job manuellement

1. Glue ‚Üí "ETL jobs"
2. S√©lectionner le job
3. "Run job"
4. Voir les logs en temps r√©el

### Planifier un job (Trigger)

**Cr√©er un trigger :**

1. Glue ‚Üí "Triggers" ‚Üí "Add trigger"
2. Nom : `daily-etl-trigger`
3. Type : "Scheduled"
4. Fr√©quence : "Cron expression"
   - Exemple : `cron(0 2 * * ? *)` = Tous les jours √† 2h
5. Actions : S√©lectionner le job √† ex√©cuter
6. "Add"

**Types de triggers :**
- **On-demand** : D√©clenchement manuel
- **Scheduled** : Planifi√© (cron)
- **Event-driven** : D√©clench√© par √©v√©nement (ex: nouveau fichier S3)

### Workflows (orchestration complexe)

**Cr√©er un workflow :**

1. Glue ‚Üí "Workflows" ‚Üí "Add workflow"
2. Nom : `etl-pipeline-workflow`
3. Ajouter des √©tapes :
   - Crawler ‚Üí Job ETL ‚Üí Autre Job
4. D√©finir les d√©pendances
5. D√©clencher le workflow

**Exemple de workflow :**
```
1. Crawler S3 ‚Üí D√©couvre nouveaux fichiers
2. Job ETL 1 ‚Üí Transforme les donn√©es brutes
3. Job ETL 2 ‚Üí Agr√®ge les donn√©es
4. Job ETL 3 ‚Üí Charge dans Redshift
```

---

## Bonnes pratiques

### Performance

1. **Utiliser Parquet** au lieu de CSV (plus rapide)
2. **Partitionner les donn√©es** (am√©liore les performances)
3. **Ajuster les DPU** selon la taille des donn√©es
4. **Utiliser le cache Spark** pour r√©utiliser des donn√©es

### Co√ªts

1. **Surveiller les DPU-heures** utilis√©es
2. **Optimiser les scripts** pour r√©duire le temps d'ex√©cution
3. **Utiliser les bonnes classes S3** (Standard-IA pour archives)
4. **Arr√™ter les jobs** qui √©chouent rapidement

### Organisation

1. **Nommer les jobs** de mani√®re coh√©rente
2. **Documenter les transformations**
3. **Versionner les scripts** (Git)
4. **Tester localement** avant de d√©ployer

---

## Exemples pratiques

### Exemple 1 : Transformer CSV ‚Üí Parquet

```python
# Lire CSV depuis S3
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "raw_csv_data"
)

# √âcrire en Parquet
glueContext.write_dynamic_frame.from_options(
    frame = datasource,
    connection_type = "s3",
    connection_options = {"path": "s3://my-bucket/parquet/"},
    format = "parquet"
)
```

### Exemple 2 : Nettoyer et valider

```python
# Filtrer les lignes invalides
cleaned = Filter.apply(
    frame = datasource,
    f = lambda x: x["email"] is not None and "@" in x["email"]
)

# Supprimer les doublons (via DataFrame)
df = cleaned.toDF()
df = df.dropDuplicates(["id"])

result = DynamicFrame.fromDF(df, glueContext, "result")
```

### Exemple 3 : Joindre plusieurs sources

```python
# Lire deux tables
users = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "users"
)

orders = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "orders"
)

# Joindre
joined = Join.apply(
    frame1 = users,
    frame2 = orders,
    keys1 = ["id"],
    keys2 = ["user_id"]
)
```

---

## üìä Points cl√©s √† retenir

1. **Glue = ETL serverless** g√©r√© par AWS
2. **Crawlers** d√©couvrent automatiquement les sch√©mas
3. **Jobs ETL** utilisent PySpark pour transformations
4. **Data Catalog** centralise les m√©tadonn√©es
5. **Triggers** permettent l'automatisation

## üîó Prochain module

Passer au module [4. Amazon Redshift - Data Warehouse](../04-redshift/README.md) pour apprendre √† utiliser Redshift pour l'analyse de donn√©es.

