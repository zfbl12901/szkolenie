# 7. Projekty praktyczne AWS

## ðŸŽ¯ Cele

- StosowaÄ‡ zdobytÄ… wiedzÄ™
- TworzyÄ‡ kompletne pipeline'y ETL
- BudowaÄ‡ Data Lake na AWS
- TworzyÄ‡ projekty dla portfolio
- IntegrowaÄ‡ wiele usÅ‚ug AWS

## ðŸ“‹ Spis treÅ›ci

1. [Projekt 1 : Pipeline ETL S3 â†’ Parquet](#projekt-1--pipeline-etl-s3---parquet)
2. [Projekt 2 : Data Lake na AWS](#projekt-2--data-lake-na-aws)
3. [Projekt 3 : Analytics z Athena](#projekt-3--analytics-z-athena)
4. [Projekt 4 : Kompletny zautomatyzowany pipeline](#projekt-4--kompletny-zautomatyzowany-pipeline)
5. [Dobre praktyki dla portfolio](#dobre-praktyki-dla-portfolio)

---

## Projekt 1 : Pipeline ETL S3 â†’ Parquet

### Cel

UtworzyÄ‡ pipeline ETL ktÃ³ry przeksztaÅ‚ca pliki CSV z S3 w zoptymalizowany format Parquet.

### Architektura

```
S3 (raw/) â†’ Glue Crawler â†’ Data Catalog â†’ Glue Job â†’ S3 (processed/parquet/)
```

### Kroki

#### 1. PrzygotowaÄ‡ dane

**UtworzyÄ‡ bucket S3 :**
- Nazwa : `data-analyst-project-1`
- UtworzyÄ‡ folder `raw/`
- PrzesÅ‚aÄ‡ plik CSV testowy

**PrzykÅ‚ad danych CSV :**
```csv
id,name,email,created_at,status
1,John Doe,john@example.com,2024-01-01,active
2,Jane Smith,jane@example.com,2024-01-02,inactive
```

#### 2. UtworzyÄ‡ Crawler Glue

1. Glue â†’ "Crawlers" â†’ "Add crawler"
2. Nazwa : `csv-crawler`
3. Å¹rÃ³dÅ‚o danych : `s3://data-analyst-project-1/raw/`
4. Rola IAM : UtworzyÄ‡ rolÄ™ z dostÄ™pem S3
5. Baza danych : `project1_db`
6. WykonaÄ‡ crawler

#### 3. UtworzyÄ‡ Job Glue

1. Glue â†’ "ETL jobs" â†’ "Add job"
2. Nazwa : `csv-to-parquet-job`
3. Typ : Spark
4. Skrypt :

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# CzytaÄ‡ z Data Catalog
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "project1_db",
    table_name = "raw_data"
)

# FiltrowaÄ‡ aktywne dane
filtered = Filter.apply(
    frame = datasource,
    f = lambda x: x["status"] == "active"
)

# ZapisaÄ‡ w Parquet
glueContext.write_dynamic_frame.from_options(
    frame = filtered,
    connection_type = "s3",
    connection_options = {
        "path": "s3://data-analyst-project-1/processed/parquet/"
    },
    format = "parquet"
)

job.commit()
```

#### 4. WykonaÄ‡ job

1. WybraÄ‡ job
2. "Run job"
3. SprawdziÄ‡ logi
4. SprawdziÄ‡ pliki Parquet w S3

### Wynik

- Pliki CSV przeksztaÅ‚cone w Parquet
- Dane przefiltrowane (tylko aktywne)
- Gotowe do analytics z Athena

---

## Projekt 2 : Data Lake na AWS

### Cel

UtworzyÄ‡ kompletny Data Lake z ingerencjÄ…, przeksztaÅ‚caniem i analytics.

### Architektura

```
Å¹rÃ³dÅ‚a â†’ S3 (Raw) â†’ Glue (Transform) â†’ S3 (Processed) â†’ Athena (Analytics)
                â†“
            Lambda (Trigger)
```

### Kroki

#### 1. Struktura S3

```
data-lake-bucket/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ users/
â”‚   â”œâ”€â”€ orders/
â”‚   â””â”€â”€ products/
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ users/
â”‚   â”œâ”€â”€ orders/
â”‚   â””â”€â”€ products/
â””â”€â”€ analytics/
    â””â”€â”€ results/
```

#### 2. Crawlery dla kaÅ¼dego ÅºrÃ³dÅ‚a

**UtworzyÄ‡ 3 crawlery :**
- `users-crawler` â†’ `s3://bucket/raw/users/`
- `orders-crawler` â†’ `s3://bucket/raw/orders/`
- `products-crawler` â†’ `s3://bucket/raw/products/`

#### 3. Joby ETL do przeksztaÅ‚cania

**Job dla users :**
```python
# users-etl-job
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "data_lake_db",
    table_name = "users"
)

# CzyÅ›ciÄ‡ i przeksztaÅ‚caÄ‡
cleaned = Filter.apply(
    frame = datasource,
    f = lambda x: x["email"] is not None
)

glueContext.write_dynamic_frame.from_options(
    frame = cleaned,
    connection_type = "s3",
    connection_options = {"path": "s3://bucket/processed/users/"},
    format = "parquet"
)
```

#### 4. Tabele Athena do analytics

```sql
-- Tabela users
CREATE EXTERNAL TABLE users_processed (
    id INT,
    name STRING,
    email STRING,
    created_at TIMESTAMP
)
STORED AS PARQUET
LOCATION 's3://bucket/processed/users/';

-- Tabela orders
CREATE EXTERNAL TABLE orders_processed (
    id INT,
    user_id INT,
    amount DECIMAL(10,2),
    created_at TIMESTAMP
)
STORED AS PARQUET
LOCATION 's3://bucket/processed/orders/';

-- Zapytanie analityczne
SELECT 
    u.name,
    COUNT(o.id) AS order_count,
    SUM(o.amount) AS total_spent
FROM users_processed u
LEFT JOIN orders_processed o ON u.id = o.user_id
GROUP BY u.name
ORDER BY total_spent DESC;
```

#### 5. Automatyzacja z Lambda

**Lambda wyzwalana przez przesÅ‚anie S3 :**

```python
import boto3

glue = boto3.client('glue')

def lambda_handler(event, context):
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # OkreÅ›liÄ‡ ktÃ³ry job wykonaÄ‡ wedÅ‚ug prefiksu
    if 'users' in key:
        job_name = 'users-etl-job'
    elif 'orders' in key:
        job_name = 'orders-etl-job'
    else:
        job_name = 'products-etl-job'
    
    # WyzwoliÄ‡ job
    glue.start_job_run(JobName=job_name)
    
    return {'statusCode': 200}
```

### Wynik

- Funkcjonalny Data Lake
- Zautomatyzowany pipeline
- Analytics z Athena
- Kompletny projekt dla portfolio

---

## Projekt 3 : Analytics z Athena

### Cel

UtworzyÄ‡ kompletny system analytics z zapytaniami SQL na danych S3.

### Kroki

#### 1. PrzygotowaÄ‡ dane

**PrzesÅ‚aÄ‡ pliki Parquet do S3 :**
- `s3://analytics-bucket/sales/year=2024/month=01/`
- `s3://analytics-bucket/sales/year=2024/month=02/`

#### 2. UtworzyÄ‡ tabele partycjonowane

```sql
CREATE EXTERNAL TABLE sales (
    id INT,
    product_id INT,
    amount DECIMAL(10,2),
    sale_date TIMESTAMP
)
PARTITIONED BY (year INT, month INT)
STORED AS PARQUET
LOCATION 's3://analytics-bucket/sales/';

-- DodaÄ‡ partycje
ALTER TABLE sales ADD PARTITION (year=2024, month=1)
LOCATION 's3://analytics-bucket/sales/year=2024/month=01/';

ALTER TABLE sales ADD PARTITION (year=2024, month=2)
LOCATION 's3://analytics-bucket/sales/year=2024/month=02/';
```

#### 3. Zapytania analityczne

**SprzedaÅ¼ na miesiÄ…c :**
```sql
SELECT 
    year,
    month,
    SUM(amount) AS total_sales,
    COUNT(*) AS transaction_count,
    AVG(amount) AS avg_transaction
FROM sales
WHERE year = 2024
GROUP BY year, month
ORDER BY year, month;
```

**Top produkty :**
```sql
SELECT 
    product_id,
    SUM(amount) AS total_revenue,
    COUNT(*) AS sales_count
FROM sales
WHERE year = 2024
GROUP BY product_id
ORDER BY total_revenue DESC
LIMIT 10;
```

**Trendy :**
```sql
SELECT 
    DATE_TRUNC('week', sale_date) AS week,
    SUM(amount) AS weekly_sales,
    LAG(SUM(amount), 1) OVER (ORDER BY DATE_TRUNC('week', sale_date)) AS previous_week
FROM sales
WHERE year = 2024
GROUP BY DATE_TRUNC('week', sale_date)
ORDER BY week;
```

#### 4. ZapisaÄ‡ wyniki

**UtworzyÄ‡ tabelÄ™ dla wynikÃ³w :**
```sql
CREATE EXTERNAL TABLE analytics_results (
    metric_name STRING,
    metric_value DECIMAL(10,2),
    calculated_at TIMESTAMP
)
STORED AS PARQUET
LOCATION 's3://analytics-bucket/results/';
```

---

## Projekt 4 : Kompletny zautomatyzowany pipeline

### Cel

UtworzyÄ‡ kompletnie zautomatyzowany pipeline ETL z wieloma usÅ‚ugami AWS.

### Kompletna architektura

```
Plik CSV przesÅ‚any â†’ S3 (raw/)
    â†“ (Event)
Lambda (Walidacja)
    â†“
S3 (validated/)
    â†“ (Event)
Glue Job (PrzeksztaÅ‚Ä‡ CSV â†’ Parquet)
    â†“
S3 (processed/parquet/)
    â†“
Glue Crawler (Aktualizuj Catalog)
    â†“
Athena (Analytics)
    â†“
S3 (results/)
```

### Implementacja

#### 1. Lambda walidacji

```python
import boto3
import csv

s3 = boto3.client('s3')

def lambda_handler(event, context):
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # PobraÄ‡ i walidowaÄ‡
    response = s3.get_object(Bucket=bucket, Key=key)
    content = response['Body'].read().decode('utf-8')
    reader = csv.DictReader(content.splitlines())
    
    valid_rows = []
    for row in reader:
        if row.get('email') and '@' in row['email']:
            valid_rows.append(row)
    
    # PrzesÅ‚aÄ‡ zwalidowane dane
    if valid_rows:
        validated_key = key.replace('raw/', 'validated/')
        # KonwertowaÄ‡ w CSV i przesÅ‚aÄ‡
        # ...
    
    return {'statusCode': 200}
```

#### 2. Glue Job przeksztaÅ‚cania

```python
# PrzeksztaÅ‚Ä‡ zwalidowany CSV w Parquet
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "pipeline_db",
    table_name = "validated_data"
)

# PrzeksztaÅ‚ciÄ‡
transformed = Map.apply(
    frame = datasource,
    f = lambda x: {
        'id': x['id'],
        'name': x['name'].upper(),
        'email': x['email'].lower(),
        'created_at': x['created_at']
    }
)

# ZapisaÄ‡ w Parquet
glueContext.write_dynamic_frame.from_options(
    frame = transformed,
    connection_type = "s3",
    connection_options = {"path": "s3://bucket/processed/"},
    format = "parquet"
)
```

#### 3. Workflow Glue

**UtworzyÄ‡ workflow :**
1. Wyzwalacz : Nowy plik w `validated/`
2. Akcja : WykonaÄ‡ job Glue
3. NastÄ™pna akcja : AktualizowaÄ‡ crawler

### Wynik

- Kompletnie zautomatyzowany pipeline
- Automatyczna walidacja
- Automatyczne przeksztaÅ‚canie
- Analytics dostÄ™pne natychmiast

---

## Dobre praktyki dla portfolio

### Dokumentacja

**UtworzyÄ‡ README dla kaÅ¼dego projektu :**

```markdown
# Projekt : Pipeline ETL AWS

## Opis
Zautomatyzowany pipeline ETL do przeksztaÅ‚cania danych CSV w Parquet.

## Architektura
- S3 : Przechowywanie
- Glue : PrzeksztaÅ‚canie
- Athena : Analytics

## Wyniki
- Redukcja kosztÃ³w o 60%
- Czas przetwarzania zmniejszony o 80%
```

### Wizualizacje

**TworzyÄ‡ diagramy :**
- Architektura systemu
- PrzepÅ‚yw danych
- Schemat danych

**NarzÄ™dzia :**
- Draw.io
- Lucidchart
- Diagramy ASCII w README

### Metryki

**UwzglÄ™dniaÄ‡ metryki :**
- Czas wykonania przed/po
- Koszty przed/po
- Wolumen przetworzonych danych
- WydajnoÅ›Ä‡ zapytaÅ„

### Kod

**Dobre praktyki :**
- Kod skomentowany
- Zmienne Å›rodowiskowe do konfiguracji
- ObsÅ‚uga bÅ‚Ä™dÃ³w
- Logowanie

### GitHub

**UtworzyÄ‡ repozytorium :**
- README z dokumentacjÄ…
- Skrypty Lambda
- Skrypty Glue
- Konfiguracja
- Diagramy

---

## ðŸ“Š Kluczowe punkty do zapamiÄ™tania

1. **Projekty praktyczne** : NiezbÄ™dne dla portfolio
2. **Dokumentacja** : WyjaÅ›niaÄ‡ architekturÄ™ i wyniki
3. **Metryki** : PokazywaÄ‡ wpÅ‚yw (wydajnoÅ›Ä‡, koszty)
4. **Czysty kod** : Skomentowany i zorganizowany
5. **GitHub** : DzieliÄ‡ siÄ™ projektami

## ðŸ”— Zasoby

- [AWS Architecture Center](https://aws.amazon.com/architecture/)
- [AWS Solutions](https://aws.amazon.com/solutions/)
- [GitHub AWS Examples](https://github.com/aws-samples)

---

**Gratulacje !** UkoÅ„czyÅ‚eÅ› formacjÄ™ AWS dla Data Analyst. MoÅ¼esz teraz tworzyÄ‡ kompletne projekty na AWS uÅ¼ywajÄ…c wyÅ‚Ä…cznie darmowych zasobÃ³w.

