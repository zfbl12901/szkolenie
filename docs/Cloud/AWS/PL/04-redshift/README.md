# 4. Amazon Redshift - Data Warehouse

## ğŸ¯ Cele

- ZrozumieÄ‡ Amazon Redshift i jego rolÄ™
- UtworzyÄ‡ klaster Redshift (darmowy 2 miesiÄ…ce)
- ÅadowaÄ‡ dane do Redshift
- OptymalizowaÄ‡ zapytania Redshift
- IntegrowaÄ‡ z S3 i innymi usÅ‚ugami

## ğŸ“‹ Spis treÅ›ci

1. [Wprowadzenie do Redshift](#wprowadzenie-do-redshift)
2. [UtworzyÄ‡ klaster Redshift](#utworzyÄ‡-klaster-redshift)
3. [ÅadowaÄ‡ dane](#Å‚adowaÄ‡-dane)
4. [Zaawansowane zapytania SQL](#zaawansowane-zapytania-sql)
5. [Optymalizacja](#optymalizacja)
6. [Integracja z innymi usÅ‚ugami](#integracja-z-innymi-usÅ‚ugami)

---

## Wprowadzenie do Redshift

### Czym jest Amazon Redshift?

**Amazon Redshift** = ZarzÄ…dzany data warehouse w chmurze

- **OLAP** : Zoptymalizowany do analizy (nie transakcje)
- **Kolumnowy** : Przechowywanie zorientowane na kolumny
- **Masowo rÃ³wnolegÅ‚y** : Przetwarzanie rozproszone
- **Skalowalny** : Od kilku GB do kilku PB

### Przypadki uÅ¼ycia dla Data Analyst

- **Data Warehouse** : CentralizowaÄ‡ dane
- **Analytics** : ZÅ‚oÅ¼one zapytania na duÅ¼ych wolumenach
- **Business Intelligence** : Pulpity nawigacyjne i raporty
- **Data Mining** : DogÅ‚Ä™bne analizy

### Free Tier Redshift

**Darmowy 2 miesiÄ…ce :**
- 750 godzin/miesiÄ…c klastra `dc2.large`
- 32 GB przechowywania na wÄ™zeÅ‚
- Po 2 miesiÄ…cach : normalne rozliczanie

**âš ï¸ WaÅ¼ne :** ZatrzymaÄ‡ klaster gdy nieuÅ¼ywany, aby uniknÄ…Ä‡ kosztÃ³w.

---

## UtworzyÄ‡ klaster Redshift

### Krok 1 : DostÄ™p do Redshift

1. Konsola AWS â†’ SzukaÄ‡ "Redshift"
2. KliknÄ…Ä‡ "Amazon Redshift"
3. "Create cluster"

### Krok 2 : Konfiguracja klastra

**Podstawowa konfiguracja :**

1. **Cluster identifier** : `data-analyst-cluster`
2. **Node type** : `dc2.large` (darmowy 2 miesiÄ…ce)
3. **Number of nodes** : 1 (wystarczajÄ…ce do rozpoczÄ™cia)
4. **Database name** : `analytics` (domyÅ›lnie : `dev`)
5. **Database port** : 5439 (domyÅ›lnie)
6. **Master username** : `admin` (lub inny)
7. **Master password** : Silne hasÅ‚o

**Konfiguracja sieci :**

1. **VPC** : WybraÄ‡ istniejÄ…cy VPC
2. **Subnet group** : UtworzyÄ‡ lub uÅ¼yÄ‡ istniejÄ…cego
3. **Publicly accessible** : âœ… Tak (dla Å‚atwego dostÄ™pu)
4. **Availability zone** : WybraÄ‡ strefÄ™

**BezpieczeÅ„stwo :**

1. **VPC security groups** : UtworzyÄ‡ grupÄ™ bezpieczeÅ„stwa
   - ZezwoliÄ‡ port 5439 z Twojego IP
2. **Encryption** : WÅ‚Ä…czyÄ‡ (zalecane)

### Krok 3 : UtworzyÄ‡ klaster

1. KliknÄ…Ä‡ "Create cluster"
2. CzekaÄ‡ 5-10 minut (tworzenie)
3. Klaster gotowy gdy status = "Available"

**âš ï¸ WaÅ¼ne :** ZanotowaÄ‡ endpoint klastra (np. `data-analyst-cluster.xxxxx.eu-west-3.redshift.amazonaws.com:5439`)

---

## ÅadowaÄ‡ dane

### Metoda 1 : COPY z S3 (zalecane)

**Najszybsze dla duÅ¼ych iloÅ›ci :**

```sql
-- UtworzyÄ‡ tabelÄ™
CREATE TABLE users (
    id INTEGER,
    name VARCHAR(100),
    email VARCHAR(100),
    created_at TIMESTAMP
);

-- ÅadowaÄ‡ z S3
COPY users
FROM 's3://my-bucket/data/users.csv'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
CSV
IGNOREHEADER 1;
```

**Konfiguracja roli IAM :**

1. IAM â†’ "Roles" â†’ "Create role"
2. Typ : "Redshift"
3. DoÅ‚Ä…czyÄ‡ politykÄ™ : `AmazonS3ReadOnlyAccess`
4. Nazwa : `RedshiftS3Role`
5. SkopiowaÄ‡ ARN dla COPY

### Metoda 2 : INSERT (maÅ‚e iloÅ›ci)

```sql
INSERT INTO users (id, name, email, created_at)
VALUES (1, 'John Doe', 'john@example.com', '2024-01-01');
```

### Metoda 3 : INSERT z zapytania

```sql
INSERT INTO users_aggregated
SELECT 
    DATE_TRUNC('month', created_at) AS month,
    COUNT(*) AS user_count
FROM users
GROUP BY DATE_TRUNC('month', created_at);
```

### ObsÅ‚ugiwane formaty

- **CSV** : Pliki CSV
- **JSON** : Pliki JSON
- **Parquet** : Format zoptymalizowany (zalecane)
- **Avro** : Format Avro

---

## Zaawansowane zapytania SQL

### Funkcje analityczne

**Funkcje okna :**

```sql
-- ROW_NUMBER
SELECT 
    id,
    name,
    ROW_NUMBER() OVER (PARTITION BY category ORDER BY created_at) AS rank
FROM products;

-- LAG/LEAD
SELECT 
    date,
    sales,
    LAG(sales, 1) OVER (ORDER BY date) AS previous_sales,
    LEAD(sales, 1) OVER (ORDER BY date) AS next_sales
FROM daily_sales;

-- RANK
SELECT 
    user_id,
    total_spent,
    RANK() OVER (ORDER BY total_spent DESC) AS spending_rank
FROM user_totals;
```

### ZÅ‚oÅ¼one agregacje

```sql
-- GROUP BY z ROLLUP
SELECT 
    category,
    region,
    SUM(amount) AS total
FROM sales
GROUP BY ROLLUP(category, region);

-- GROUP BY z CUBE
SELECT 
    category,
    region,
    SUM(amount) AS total
FROM sales
GROUP BY CUBE(category, region);
```

### Zoptymalizowane zÅ‚Ä…czenia

```sql
-- ZÅ‚Ä…czenie z kluczem dystrybucji
SELECT 
    u.name,
    o.amount,
    o.created_at
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE u.created_at > '2024-01-01';
```

---

## Optymalizacja

### Klucze dystrybucji

**WybraÄ‡ odpowiedni klucz dystrybucji :**

```sql
-- Dystrybucja wedÅ‚ug klucza (dla zÅ‚Ä…czeÅ„)
CREATE TABLE users (
    id INTEGER DISTKEY,
    name VARCHAR(100),
    email VARCHAR(100)
);

-- Dystrybucja ALL (dla maÅ‚ych tabel)
CREATE TABLE categories (
    id INTEGER,
    name VARCHAR(100)
) DISTSTYLE ALL;

-- Dystrybucja EVEN (domyÅ›lnie)
CREATE TABLE logs (
    id INTEGER,
    message TEXT
) DISTSTYLE EVEN;
```

### Klucze sortowania

**PoprawiÄ‡ wydajnoÅ›Ä‡ zapytaÅ„ :**

```sql
-- Prosty klucz sortowania
CREATE TABLE orders (
    id INTEGER,
    user_id INTEGER,
    created_at TIMESTAMP,
    amount DECIMAL(10,2)
) SORTKEY (created_at);

-- ZÅ‚oÅ¼ony klucz sortowania
CREATE TABLE sales (
    date DATE,
    region VARCHAR(50),
    amount DECIMAL(10,2)
) SORTKEY (date, region);
```

### Kompresja

**ZmniejszyÄ‡ przestrzeÅ„ przechowywania :**

```sql
-- Automatyczna kompresja
CREATE TABLE users (
    id INTEGER,
    name VARCHAR(100) ENCODE lzo,
    email VARCHAR(100) ENCODE lzo,
    created_at TIMESTAMP ENCODE delta
);
```

### ANALYZE

**AktualizowaÄ‡ statystyki :**

```sql
-- AnalizowaÄ‡ tabelÄ™
ANALYZE users;

-- AnalizowaÄ‡ wszystkie tabele
ANALYZE;
```

---

## Integracja z innymi usÅ‚ugami

### Redshift + S3

**Unload do S3 :**

```sql
UNLOAD ('SELECT * FROM users WHERE created_at > ''2024-01-01''')
TO 's3://my-bucket/exports/users/'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
CSV
PARALLEL OFF;
```

### Redshift + Glue

**Glue moÅ¼e Å‚adowaÄ‡ do Redshift :**

```python
# W jobie Glue
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame = transformed_data,
    catalog_connection = "redshift-connection",
    connection_options = {
        "dbtable": "users",
        "database": "analytics"
    }
)
```

### Redshift + QuickSight

**PoÅ‚Ä…czyÄ‡ QuickSight z Redshift :**

1. QuickSight â†’ "Data sources"
2. "Redshift"
3. WprowadziÄ‡ informacje poÅ‚Ä…czenia
4. WybraÄ‡ tabele
5. TworzyÄ‡ wizualizacje

---

## Dobre praktyki

### WydajnoÅ›Ä‡

1. **UÅ¼ywaÄ‡ COPY** zamiast INSERT dla duÅ¼ych iloÅ›ci
2. **WybraÄ‡ odpowiednie klucze dystrybucji**
3. **UÅ¼ywaÄ‡ kluczy sortowania** dla czÄ™stych zapytaÅ„
4. **KompresowaÄ‡ kolumny** aby oszczÄ™dziÄ‡ przestrzeÅ„
5. **VACUUM regularnie** aby zoptymalizowaÄ‡

### Koszty

1. **ZatrzymaÄ‡ klaster** gdy nieuÅ¼ywany
2. **UÅ¼ywaÄ‡ odpowiedniego typu wÄ™zÅ‚a** wedÅ‚ug potrzeb
3. **MonitorowaÄ‡ uÅ¼ycie** przechowywania
4. **CzyÅ›ciÄ‡ dane** niepotrzebne

### BezpieczeÅ„stwo

1. **SzyfrowaÄ‡ dane** w tranzycie i w spoczynku
2. **UÅ¼ywaÄ‡ VPC** aby izolowaÄ‡ klaster
3. **OgraniczaÄ‡ dostÄ™p** z grupami bezpieczeÅ„stwa
4. **AudytowaÄ‡ dostÄ™p** z CloudTrail

---

## PrzykÅ‚ady praktyczne

### PrzykÅ‚ad 1 : Kompletny pipeline S3 â†’ Redshift

```sql
-- 1. UtworzyÄ‡ tabelÄ™
CREATE TABLE sales (
    id INTEGER,
    product_id INTEGER,
    amount DECIMAL(10,2),
    sale_date DATE
) DISTKEY(product_id) SORTKEY(sale_date);

-- 2. ÅadowaÄ‡ z S3
COPY sales
FROM 's3://my-bucket/data/sales/'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
CSV
IGNOREHEADER 1;

-- 3. AnalizowaÄ‡
ANALYZE sales;

-- 4. Zapytania analityczne
SELECT 
    DATE_TRUNC('month', sale_date) AS month,
    SUM(amount) AS total_sales
FROM sales
GROUP BY DATE_TRUNC('month', sale_date)
ORDER BY month;
```

### PrzykÅ‚ad 2 : Agregacje z funkcjami okna

```sql
-- Top 10 produktÃ³w na miesiÄ…c
SELECT 
    product_id,
    month,
    total_sales,
    RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) AS rank
FROM (
    SELECT 
        product_id,
        DATE_TRUNC('month', sale_date) AS month,
        SUM(amount) AS total_sales
    FROM sales
    GROUP BY product_id, DATE_TRUNC('month', sale_date)
) monthly_sales
WHERE RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) <= 10;
```

---

## ğŸ“Š Kluczowe punkty do zapamiÄ™tania

1. **Redshift = Data warehouse** dla analytics
2. **Free Tier : 2 miesiÄ…ce** darmowe (750 godzin)
3. **COPY z S3** = najszybsza metoda
4. **Klucze dystrybucji i sortowania** = klucze wydajnoÅ›ci
5. **ZatrzymaÄ‡ klaster** gdy nieuÅ¼ywany

## ğŸ”— NastÄ™pny moduÅ‚

PrzejdÅº do moduÅ‚u [5. Amazon Athena - Zapytania SQL na S3](../05-athena/README.md), aby nauczyÄ‡ siÄ™ bezpoÅ›rednio odpytywaÄ‡ pliki S3.

