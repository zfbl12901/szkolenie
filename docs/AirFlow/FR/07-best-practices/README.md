# 7. Bonnes pratiques Airflow

## üéØ Objectifs

- Structurer les DAGs efficacement
- G√©rer les erreurs
- Optimiser les performances
- Tester et d√©boguer

## üìã Table des mati√®res

1. [Structure des DAGs](#structure-des-dags)
2. [Gestion des erreurs](#gestion-des-erreurs)
3. [Performance](#performance)
4. [Tests](#tests)
5. [D√©bogage](#d√©bogage)

---

## Structure des DAGs

### Organisation des fichiers

```
airflow/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transform.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ load.py
‚îÇ   ‚îî‚îÄ‚îÄ analytics/
‚îÇ       ‚îî‚îÄ‚îÄ reports.py
‚îú‚îÄ‚îÄ plugins/
‚îÇ   ‚îî‚îÄ‚îÄ custom_operators.py
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ settings.py
```

### Bonnes pratiques de code

**1. Imports organis√©s :**

```python
# Standard library
from datetime import datetime, timedelta

# Third-party
from airflow import DAG
from airflow.operators.python import PythonOperator

# Local
from utils.helpers import process_data
```

**2. Fonctions r√©utilisables :**

```python
# utils/helpers.py
def extract_data(source):
    # Logique d'extraction
    return data

def transform_data(data):
    # Logique de transformation
    return transformed_data

# dags/etl_pipeline.py
from utils.helpers import extract_data, transform_data

extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    op_args=['source'],
    dag=dag,
)
```

**3. Configuration centralis√©e :**

```python
# config/settings.py
DEFAULT_ARGS = {
    'owner': 'data_team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# dags/my_dag.py
from config.settings import DEFAULT_ARGS

dag = DAG('my_dag', default_args=DEFAULT_ARGS)
```

---

## Gestion des erreurs

### Retry et backoff

```python
default_args = {
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(hours=1),
}
```

### Gestion d'exceptions

```python
def process_with_error_handling():
    try:
        # Code qui peut √©chouer
        result = risky_operation()
        return result
    except SpecificError as e:
        # G√©rer l'erreur sp√©cifique
        logger.error(f"Error: {e}")
        raise
    except Exception as e:
        # G√©rer les autres erreurs
        logger.error(f"Unexpected error: {e}")
        raise
```

### Callbacks

```python
def on_failure_callback(context):
    logger.error("Task failed!")
    # Envoyer une alerte
    send_alert(context)

def on_success_callback(context):
    logger.info("Task succeeded!")

task = PythonOperator(
    task_id='task',
    python_callable=my_function,
    on_failure_callback=on_failure_callback,
    on_success_callback=on_success_callback,
    dag=dag,
)
```

---

## Performance

### Optimiser les DAGs

**1. √âviter les d√©pendances inutiles :**

```python
# Mauvais : d√©pendances s√©quentielles inutiles
task1 >> task2 >> task3 >> task4

# Bon : parall√©lisme quand possible
[task1, task2] >> task3 >> task4
```

**2. Utiliser le mode reschedule pour les sensors :**

```python
sensor = FileSensor(
    task_id='wait_file',
    filepath='/path/to/file',
    mode='reschedule',  # Lib√®re le slot worker
    poke_interval=60,
    dag=dag,
)
```

**3. Limiter le parall√©lisme :**

```python
dag = DAG(
    'my_dag',
    max_active_runs=1,  # Un seul run √† la fois
    max_active_tasks=10,  # Maximum 10 t√¢ches en parall√®le
)
```

---

## Tests

### Tests unitaires

```python
# tests/test_dag.py
import pytest
from airflow.models import DagBag

def test_dag_loaded():
    dagbag = DagBag()
    dag = dagbag.get_dag(dag_id='my_dag')
    assert dag is not None
    assert len(dag.tasks) == 3

def test_dag_structure():
    dagbag = DagBag()
    dag = dagbag.get_dag(dag_id='my_dag')
    assert 'extract' in dag.task_ids
    assert 'transform' in dag.task_ids
```

### Tests d'int√©gration

```python
from airflow.operators.python import PythonOperator

def test_task_execution():
    task = PythonOperator(
        task_id='test_task',
        python_callable=lambda: "success",
    )
    result = task.execute({})
    assert result == "success"
```

---

## D√©bogage

### Logs

```python
import logging

logger = logging.getLogger(__name__)

def my_function():
    logger.info("Starting process")
    logger.debug("Debug information")
    logger.error("Error occurred")
```

### V√©rifier les logs

```bash
# Logs d'une t√¢che
airflow tasks logs my_dag my_task 2024-01-01

# Logs du scheduler
tail -f ~/airflow/logs/scheduler/*.log
```

---

## üìä Points cl√©s √† retenir

1. **Structure** : Organiser le code proprement
2. **Erreurs** : G√©rer avec retry et callbacks
3. **Performance** : Optimiser le parall√©lisme
4. **Tests** : Tester les DAGs
5. **Logs** : Utiliser le logging efficacement

## üîó Prochain module

Passer au module [8. Projets pratiques](../08-projets/README.md) pour cr√©er des projets complets.

