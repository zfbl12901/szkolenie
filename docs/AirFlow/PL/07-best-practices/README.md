# 7. Dobre praktyki Airflow

## ğŸ¯ Cele

- StrukturyzowaÄ‡ DAGi efektywnie
- ObsÅ‚ugiwaÄ‡ bÅ‚Ä™dy
- OptymalizowaÄ‡ wydajnoÅ›Ä‡
- TestowaÄ‡ i debugowaÄ‡

## ğŸ“‹ Spis treÅ›ci

1. [Struktura DAGÃ³w](#struktura-dagÃ³w)
2. [ObsÅ‚uga bÅ‚Ä™dÃ³w](#obsÅ‚uga-bÅ‚Ä™dÃ³w)
3. [WydajnoÅ›Ä‡](#wydajnoÅ›Ä‡)
4. [Testy](#testy)
5. [Debugowanie](#debugowanie)

---

## Struktura DAGÃ³w

### Organizacja plikÃ³w

```
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â””â”€â”€ load.py
â”‚   â””â”€â”€ analytics/
â”‚       â””â”€â”€ reports.py
â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ custom_operators.py
â””â”€â”€ config/
    â””â”€â”€ settings.py
```

### Dobre praktyki kodu

**1. Zorganizowane importy :**

```python
# Standardowa biblioteka
from datetime import datetime, timedelta

# ZewnÄ™trzne
from airflow import DAG
from airflow.operators.python import PythonOperator

# Lokalne
from utils.helpers import process_data
```

**2. Funkcje reuÅ¼ywalne :**

```python
# utils/helpers.py
def extract_data(source):
    # Logika ekstrakcji
    return data

def transform_data(data):
    # Logika transformacji
    return transformed_data

# dags/etl_pipeline.py
from utils.helpers import extract_data, transform_data

extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    op_args=['source'],
    dag=dag,
)
```

**3. Konfiguracja scentralizowana :**

```python
# config/settings.py
DEFAULT_ARGS = {
    'owner': 'data_team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# dags/my_dag.py
from config.settings import DEFAULT_ARGS

dag = DAG('my_dag', default_args=DEFAULT_ARGS)
```

---

## ObsÅ‚uga bÅ‚Ä™dÃ³w

### Retry i backoff

```python
default_args = {
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(hours=1),
}
```

### ObsÅ‚uga wyjÄ…tkÃ³w

```python
def process_with_error_handling():
    try:
        # Kod ktÃ³ry moÅ¼e siÄ™ nie powieÅ›Ä‡
        result = risky_operation()
        return result
    except SpecificError as e:
        # ObsÅ‚uÅ¼yÄ‡ bÅ‚Ä…d specyficzny
        logger.error(f"Error: {e}")
        raise
    except Exception as e:
        # ObsÅ‚uÅ¼yÄ‡ inne bÅ‚Ä™dy
        logger.error(f"Unexpected error: {e}")
        raise
```

### Callbacki

```python
def on_failure_callback(context):
    logger.error("Task failed!")
    # WysÅ‚aÄ‡ alert
    send_alert(context)

def on_success_callback(context):
    logger.info("Task succeeded!")

task = PythonOperator(
    task_id='task',
    python_callable=my_function,
    on_failure_callback=on_failure_callback,
    on_success_callback=on_success_callback,
    dag=dag,
)
```

---

## WydajnoÅ›Ä‡

### OptymalizowaÄ‡ DAGi

**1. UnikaÄ‡ niepotrzebnych zaleÅ¼noÅ›ci :**

```python
# ZÅ‚e : niepotrzebne zaleÅ¼noÅ›ci sekwencyjne
task1 >> task2 >> task3 >> task4

# Dobre : rÃ³wnolegÅ‚oÅ›Ä‡ gdy moÅ¼liwe
[task1, task2] >> task3 >> task4
```

**2. UÅ¼ywaÄ‡ trybu reschedule dla sensorÃ³w :**

```python
sensor = FileSensor(
    task_id='wait_file',
    filepath='/path/to/file',
    mode='reschedule',  # Uwalnia slot worker
    poke_interval=60,
    dag=dag,
)
```

**3. OgraniczaÄ‡ rÃ³wnolegÅ‚oÅ›Ä‡ :**

```python
dag = DAG(
    'my_dag',
    max_active_runs=1,  # Tylko jeden run na raz
    max_active_tasks=10,  # Maksimum 10 zadaÅ„ rÃ³wnolegle
)
```

---

## Testy

### Testy jednostkowe

```python
# tests/test_dag.py
import pytest
from airflow.models import DagBag

def test_dag_loaded():
    dagbag = DagBag()
    dag = dagbag.get_dag(dag_id='my_dag')
    assert dag is not None
    assert len(dag.tasks) == 3

def test_dag_structure():
    dagbag = DagBag()
    dag = dagbag.get_dag(dag_id='my_dag')
    assert 'extract' in dag.task_ids
    assert 'transform' in dag.task_ids
```

### Testy integracyjne

```python
from airflow.operators.python import PythonOperator

def test_task_execution():
    task = PythonOperator(
        task_id='test_task',
        python_callable=lambda: "success",
    )
    result = task.execute({})
    assert result == "success"
```

---

## Debugowanie

### Logi

```python
import logging

logger = logging.getLogger(__name__)

def my_function():
    logger.info("Starting process")
    logger.debug("Debug information")
    logger.error("Error occurred")
```

### SprawdzaÄ‡ logi

```bash
# Logi zadania
airflow tasks logs my_dag my_task 2024-01-01

# Logi scheduler
tail -f ~/airflow/logs/scheduler/*.log
```

---

## ğŸ“Š Kluczowe punkty do zapamiÄ™tania

1. **Struktura** : OrganizowaÄ‡ kod czysto
2. **BÅ‚Ä™dy** : ObsÅ‚ugiwaÄ‡ z retry i callbackami
3. **WydajnoÅ›Ä‡** : OptymalizowaÄ‡ rÃ³wnolegÅ‚oÅ›Ä‡
4. **Testy** : TestowaÄ‡ DAGi
5. **Logi** : UÅ¼ywaÄ‡ logowania efektywnie

## ğŸ”— NastÄ™pny moduÅ‚

PrzejdÅº do moduÅ‚u [8. Projekty praktyczne](../08-projets/README.md), aby tworzyÄ‡ kompletne projekty.

