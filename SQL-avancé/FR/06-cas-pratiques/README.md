# 6. Cas pratiques d'optimisation

## ðŸŽ¯ Objectifs

- Appliquer les techniques d'optimisation sur des cas rÃ©els
- Analyser les problÃ¨mes de performance
- Mesurer l'impact des optimisations
- Utiliser Dalibo pour identifier et rÃ©soudre les problÃ¨mes

## ðŸ“‹ Table des matiÃ¨res

1. [Cas 1 : RequÃªte lente avec scan sÃ©quentiel](#cas-1--requÃªte-lente-avec-scan-sÃ©quentiel)
2. [Cas 2 : Jointure lente sur grande table](#cas-2--jointure-lente-sur-grande-table)
3. [Cas 3 : AgrÃ©gation lente](#cas-3--agrÃ©gation-lente)
4. [Cas 4 : Sous-requÃªte corrÃ©lÃ©e](#cas-4--sous-requÃªte-corrÃ©lÃ©e)
5. [Cas 5 : ProblÃ¨me de cache hit ratio](#cas-5--problÃ¨me-de-cache-hit-ratio)
6. [Cas 6 : Index non utilisÃ©s](#cas-6--index-non-utilisÃ©s)

---

## Cas 1 : RequÃªte lente avec scan sÃ©quentiel

### ProblÃ¨me initial

**RequÃªte :**
```sql
SELECT * FROM users WHERE email = 'user@example.com';
```

**Plan d'exÃ©cution :**
```
Seq Scan on users  (cost=0.00..25000.00 rows=1 width=64)
  (actual time=0.123..1500.456 rows=1 loops=1)
  Filter: (email = 'user@example.com'::text)
  Rows Removed by Filter: 999999
Planning Time: 0.234 ms
Execution Time: 1500.678 ms
```

**ProblÃ¨mes identifiÃ©s :**
- ðŸ”´ Scan sÃ©quentiel sur 1 million de lignes
- ðŸ”´ Temps d'exÃ©cution : 1.5 secondes
- ðŸ”´ 999999 lignes filtrÃ©es

### Analyse avec Dalibo

```sql
-- VÃ©rifier dans pg_stat_statements
SELECT 
    query,
    calls,
    mean_exec_time,
    shared_blks_read,
    shared_blks_hit
FROM pg_stat_statements
WHERE query LIKE '%users WHERE email%'
ORDER BY mean_exec_time DESC;

-- VÃ©rifier avec pg_qualstats
SELECT 
    left_table,
    left_column,
    operator,
    execution_count
FROM pg_qualstats
WHERE left_table = 'users' AND left_column = 'email';
```

### Solution

```sql
-- CrÃ©er un index sur email
CREATE INDEX idx_users_email ON users(email);

-- VÃ©rifier le nouveau plan
EXPLAIN ANALYZE
SELECT * FROM users WHERE email = 'user@example.com';
```

**Plan optimisÃ© :**
```
Index Scan using idx_users_email on users
  (cost=0.42..8.44 rows=1 width=64)
  (actual time=0.123..0.125 rows=1 loops=1)
  Index Cond: (email = 'user@example.com'::text)
Planning Time: 0.234 ms
Execution Time: 0.125 ms
```

### RÃ©sultats

| MÃ©trique | Avant | AprÃ¨s | AmÃ©lioration |
|----------|-------|-------|--------------|
| Temps d'exÃ©cution | 1500ms | 0.125ms | **99.99%** |
| Type de scan | Seq Scan | Index Scan | âœ… |
| Lignes scannÃ©es | 1,000,000 | 1 | âœ… |

---

## Cas 2 : Jointure lente sur grande table

### ProblÃ¨me initial

**RequÃªte :**
```sql
SELECT 
    u.name,
    u.email,
    COUNT(o.id) AS order_count,
    SUM(o.amount) AS total_amount
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE u.created_at > '2024-01-01'
GROUP BY u.id, u.name, u.email;
```

**Plan d'exÃ©cution :**
```
Hash Join
  (cost=125000.00..250000.00 rows=50000 width=64)
  (actual time=5000.123..15000.456 rows=45000 loops=1)
  Hash Cond: (o.user_id = u.id)
  -> Seq Scan on orders o
      (cost=0.00..100000.00 rows=1000000 width=16)
      (actual time=0.123..5000.456 rows=1000000 loops=1)
  -> Hash
      (cost=25000.00..25000.00 rows=100000 width=48)
      (actual time=2000.123..2000.123 rows=100000 loops=1)
      Buckets: 131072  Batches: 8  Memory Usage: 5120kB
      -> Seq Scan on users u
          (cost=0.00..25000.00 rows=100000 width=48)
          (actual time=0.089..1000.567 rows=100000 loops=1)
          Filter: (created_at > '2024-01-01'::date)
Planning Time: 50.234 ms
Execution Time: 15000.678 ms
```

**ProblÃ¨mes identifiÃ©s :**
- ðŸ”´ Hash Join avec 8 batches (tri sur disque)
- ðŸ”´ Scan sÃ©quentiel sur orders (1 million de lignes)
- ðŸ”´ Temps d'exÃ©cution : 15 secondes

### Analyse avec Dalibo

```sql
-- Identifier les index manquants
SELECT 
    qs.left_table,
    qs.left_column,
    qs.operator,
    COUNT(*) AS execution_count
FROM pg_qualstats qs
WHERE qs.left_table IN ('users', 'orders')
GROUP BY qs.left_table, qs.left_column, qs.operator
ORDER BY execution_count DESC;
```

### Solution

```sql
-- CrÃ©er des index sur les clÃ©s de jointure et filtres
CREATE INDEX idx_orders_user_id ON orders(user_id);
CREATE INDEX idx_users_created_at ON users(created_at);

-- Index composite pour la requÃªte complÃ¨te
CREATE INDEX idx_orders_user_id_amount ON orders(user_id, amount);

-- Augmenter work_mem pour Ã©viter les batches
SET work_mem = '256MB';

-- VÃ©rifier le nouveau plan
EXPLAIN ANALYZE
SELECT 
    u.name,
    u.email,
    COUNT(o.id) AS order_count,
    SUM(o.amount) AS total_amount
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE u.created_at > '2024-01-01'
GROUP BY u.id, u.name, u.email;
```

**Plan optimisÃ© :**
```
Hash Join
  (cost=5000.00..15000.00 rows=50000 width=64)
  (actual time=200.123..800.456 rows=45000 loops=1)
  Hash Cond: (o.user_id = u.id)
  -> Index Scan using idx_orders_user_id on orders o
      (cost=0.42..8000.00 rows=500000 width=16)
      (actual time=0.123..300.456 rows=500000 loops=1)
  -> Hash
      (cost=2500.00..2500.00 rows=100000 width=48)
      (actual time=100.123..100.123 rows=100000 loops=1)
      Buckets: 131072  Batches: 1  Memory Usage: 5120kB
      -> Index Scan using idx_users_created_at on users u
          (cost=0.42..2500.00 rows=100000 width=48)
          (actual time=0.089..50.567 rows=100000 loops=1)
          Index Cond: (created_at > '2024-01-01'::date)
Planning Time: 5.234 ms
Execution Time: 800.678 ms
```

### RÃ©sultats

| MÃ©trique | Avant | AprÃ¨s | AmÃ©lioration |
|----------|-------|-------|--------------|
| Temps d'exÃ©cution | 15000ms | 800ms | **94.7%** |
| Batches Hash Join | 8 | 1 | âœ… |
| Type de scan orders | Seq Scan | Index Scan | âœ… |
| Lignes scannÃ©es | 1,000,000 | 500,000 | âœ… |

---

## Cas 3 : AgrÃ©gation lente

### ProblÃ¨me initial

**RequÃªte :**
```sql
SELECT 
    status,
    COUNT(*) AS count,
    AVG(amount) AS avg_amount,
    SUM(amount) AS total_amount
FROM orders
WHERE created_at BETWEEN '2024-01-01' AND '2024-12-31'
GROUP BY status
ORDER BY count DESC;
```

**Plan d'exÃ©cution :**
```
Sort
  (cost=50000.00..50000.00 rows=5 width=32)
  (actual time=10000.123..10000.234 rows=5 loops=1)
  Sort Key: (count(*)) DESC
  Sort Method: quicksort  Memory: 25kB
  -> HashAggregate
      (cost=45000.00..45000.00 rows=5 width=32)
      (actual time=8000.123..8000.456 rows=5 loops=1)
      Group Key: status
      Batches: 1  Memory Usage: 24kB
      -> Seq Scan on orders
          (cost=0.00..40000.00 rows=2000000 width=16)
          (actual time=0.123..5000.456 rows=2000000 loops=1)
          Filter: ((created_at >= '2024-01-01'::date) 
                   AND (created_at <= '2024-12-31'::date))
          Rows Removed by Filter: 0
Planning Time: 10.234 ms
Execution Time: 10000.678 ms
```

**ProblÃ¨mes identifiÃ©s :**
- ðŸ”´ Scan sÃ©quentiel sur 2 millions de lignes
- ðŸ”´ Temps d'exÃ©cution : 10 secondes
- ðŸ”´ Pas d'index sur created_at

### Solution

```sql
-- CrÃ©er un index sur created_at et status
CREATE INDEX idx_orders_created_at_status ON orders(created_at, status);

-- Alternative : Index partiel si certaines status sont rares
CREATE INDEX idx_orders_created_at_status_partial 
ON orders(created_at, status) 
WHERE status IN ('pending', 'processing');

-- VÃ©rifier le nouveau plan
EXPLAIN ANALYZE
SELECT 
    status,
    COUNT(*) AS count,
    AVG(amount) AS avg_amount,
    SUM(amount) AS total_amount
FROM orders
WHERE created_at BETWEEN '2024-01-01' AND '2024-12-31'
GROUP BY status
ORDER BY count DESC;
```

**Plan optimisÃ© :**
```
Sort
  (cost=5000.00..5000.00 rows=5 width=32)
  (actual time=500.123..500.234 rows=5 loops=1)
  Sort Key: (count(*)) DESC
  Sort Method: quicksort  Memory: 25kB
  -> HashAggregate
      (cost=4500.00..4500.00 rows=5 width=32)
      (actual time=400.123..400.456 rows=5 loops=1)
      Group Key: status
      Batches: 1  Memory Usage: 24kB
      -> Index Scan using idx_orders_created_at_status on orders
          (cost=0.42..4000.00 rows=2000000 width=16)
          (actual time=0.123..200.456 rows=2000000 loops=1)
          Index Cond: ((created_at >= '2024-01-01'::date) 
                       AND (created_at <= '2024-12-31'::date))
Planning Time: 5.234 ms
Execution Time: 500.678 ms
```

### RÃ©sultats

| MÃ©trique | Avant | AprÃ¨s | AmÃ©lioration |
|----------|-------|-------|--------------|
| Temps d'exÃ©cution | 10000ms | 500ms | **95%** |
| Type de scan | Seq Scan | Index Scan | âœ… |
| Lignes scannÃ©es | 2,000,000 | 2,000,000 | (mÃªme nombre, mais index) |

---

## Cas 4 : Sous-requÃªte corrÃ©lÃ©e

### ProblÃ¨me initial

**RequÃªte :**
```sql
SELECT 
    u.id,
    u.name,
    u.email,
    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) AS order_count,
    (SELECT MAX(created_at) FROM orders o WHERE o.user_id = u.id) AS last_order_date,
    (SELECT SUM(amount) FROM orders o WHERE o.user_id = u.id) AS total_spent
FROM users u
WHERE u.active = true;
```

**Plan d'exÃ©cution :**
```
Seq Scan on users u
  (cost=0.00..250000.00 rows=100000 width=64)
  (actual time=0.123..50000.456 rows=100000 loops=1)
  Filter: (active = true)
  SubPlan 1
    -> Aggregate
        (cost=2.50..2.50 rows=1 width=8)
        (actual time=0.100..0.100 rows=1 loops=100000)
        -> Seq Scan on orders o
            (cost=0.00..2.25 rows=10 width=0)
            (actual time=0.050..0.050 rows=5 loops=100000)
            Filter: (user_id = u.id)
  SubPlan 2
    -> Result
        (cost=2.50..2.50 rows=1 width=8)
        (actual time=0.100..0.100 rows=1 loops=100000)
        -> Aggregate
            (cost=2.50..2.50 rows=1 width=8)
            (actual time=0.100..0.100 rows=1 loops=100000)
            -> Seq Scan on orders o
                (cost=0.00..2.25 rows=10 width=0)
                (actual time=0.050..0.050 rows=5 loops=100000)
                Filter: (user_id = u.id)
  SubPlan 3
    -> Aggregate
        (cost=2.50..2.50 rows=1 width=8)
        (actual time=0.100..0.100 rows=1 loops=100000)
        -> Seq Scan on orders o
            (cost=0.00..2.25 rows=10 width=0)
            (actual time=0.050..0.050 rows=5 loops=100000)
            Filter: (user_id = u.id)
Planning Time: 5.234 ms
Execution Time: 50000.678 ms
```

**ProblÃ¨mes identifiÃ©s :**
- ðŸ”´ 3 sous-requÃªtes corrÃ©lÃ©es exÃ©cutÃ©es 100,000 fois chacune
- ðŸ”´ 300,000 scans sÃ©quentiels sur orders
- ðŸ”´ Temps d'exÃ©cution : 50 secondes

### Solution

```sql
-- Remplacer par des JOIN avec agrÃ©gation
SELECT 
    u.id,
    u.name,
    u.email,
    COALESCE(o.order_count, 0) AS order_count,
    o.last_order_date,
    COALESCE(o.total_spent, 0) AS total_spent
FROM users u
LEFT JOIN (
    SELECT 
        user_id,
        COUNT(*) AS order_count,
        MAX(created_at) AS last_order_date,
        SUM(amount) AS total_spent
    FROM orders
    GROUP BY user_id
) o ON u.id = o.user_id
WHERE u.active = true;

-- CrÃ©er un index pour accÃ©lÃ©rer la jointure
CREATE INDEX idx_orders_user_id ON orders(user_id);
```

**Plan optimisÃ© :**
```
Hash Right Join
  (cost=5000.00..15000.00 rows=100000 width=64)
  (actual time=200.123..800.456 rows=100000 loops=1)
  Hash Cond: (o.user_id = u.id)
  -> HashAggregate
      (cost=4000.00..4500.00 rows=50000 width=24)
      (actual time=150.123..200.456 rows=50000 loops=1)
      Group Key: orders.user_id
      Batches: 1  Memory Usage: 5120kB
      -> Index Scan using idx_orders_user_id on orders
          (cost=0.42..3000.00 rows=500000 width=16)
          (actual time=0.123..100.456 rows=500000 loops=1)
  -> Hash
      (cost=2000.00..2000.00 rows=100000 width=48)
      (actual time=50.123..50.123 rows=100000 loops=1)
      Buckets: 131072  Batches: 1  Memory Usage: 5120kB
      -> Seq Scan on users u
          (cost=0.00..2000.00 rows=100000 width=48)
          (actual time=0.089..25.567 rows=100000 loops=1)
          Filter: (active = true)
Planning Time: 5.234 ms
Execution Time: 800.678 ms
```

### RÃ©sultats

| MÃ©trique | Avant | AprÃ¨s | AmÃ©lioration |
|----------|-------|-------|--------------|
| Temps d'exÃ©cution | 50000ms | 800ms | **98.4%** |
| Scans sur orders | 300,000 | 1 | âœ… |
| Type d'opÃ©ration | Sous-requÃªtes | Hash Join | âœ… |

---

## Cas 5 : ProblÃ¨me de cache hit ratio

### ProblÃ¨me initial

**MÃ©triques :**
```sql
-- Cache hit ratio global
SELECT 
    ROUND(100.0 * SUM(shared_blks_hit) / 
          NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2) AS cache_hit_ratio
FROM pg_stat_statements;
-- RÃ©sultat: 75% (objectif: > 95%)
```

**RequÃªtes avec beaucoup de lectures disque :**
```sql
SELECT 
    LEFT(query, 100) AS query_preview,
    shared_blks_read,
    shared_blks_hit,
    ROUND(100.0 * shared_blks_hit / 
          NULLIF(shared_blks_hit + shared_blks_read, 0), 2) AS cache_hit_ratio,
    ROUND((shared_blks_read * 8)::numeric / 1024, 2) AS disk_read_mb
FROM pg_stat_statements
WHERE shared_blks_read > 1000
ORDER BY shared_blks_read DESC
LIMIT 10;
```

### Solution

```sql
-- 1. Augmenter shared_buffers (dans postgresql.conf)
-- shared_buffers = 4GB  (25% de RAM pour serveur dÃ©diÃ©)

-- 2. PrÃ©charger les tables frÃ©quemment utilisÃ©es
-- CrÃ©er une fonction de prÃ©chargement
CREATE OR REPLACE FUNCTION pg_prewarm_table(table_name TEXT)
RETURNS void AS $$
BEGIN
    EXECUTE format('SELECT * FROM %I LIMIT 1', table_name);
END;
$$ LANGUAGE plpgsql;

-- PrÃ©charger les tables importantes
SELECT pg_prewarm_table('users');
SELECT pg_prewarm_table('orders');
SELECT pg_prewarm_table('products');

-- 3. Utiliser pg_prewarm extension
CREATE EXTENSION IF NOT EXISTS pg_prewarm;

-- PrÃ©charger une table complÃ¨te
SELECT pg_prewarm('users');
SELECT pg_prewarm('orders');
```

**AprÃ¨s optimisation :**
```sql
-- VÃ©rifier l'amÃ©lioration
SELECT 
    ROUND(100.0 * SUM(shared_blks_hit) / 
          NULLIF(SUM(shared_blks_hit + shared_blks_read), 0), 2) AS cache_hit_ratio
FROM pg_stat_statements;
-- RÃ©sultat: 98% âœ…
```

### RÃ©sultats

| MÃ©trique | Avant | AprÃ¨s | AmÃ©lioration |
|----------|-------|-------|--------------|
| Cache hit ratio | 75% | 98% | **+23%** |
| Lectures disque | Ã‰levÃ©es | Faibles | âœ… |
| Temps de rÃ©ponse | Variable | Stable | âœ… |

---

## Cas 6 : Index non utilisÃ©s

### ProblÃ¨me initial

**Identification des index non utilisÃ©s :**
```sql
-- Index jamais utilisÃ©s
SELECT 
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,
    idx_scan AS index_scans,
    pg_relation_size(indexrelid) AS size_bytes
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND pg_relation_size(indexrelid) > 1048576  -- > 1MB
ORDER BY pg_relation_size(indexrelid) DESC;
```

**RÃ©sultat :**
```
 schemaname | tablename |      indexname       | index_size | index_scans | size_bytes
------------+-----------+----------------------+------------+-------------+------------
 public     | orders    | idx_orders_old_field | 250 MB     |           0 |  262144000
 public     | users     | idx_users_old_email  | 150 MB     |           0 |  157286400
```

**Impact :**
- ðŸ”´ 400 MB d'espace disque perdu
- ðŸ”´ Ralentissement des INSERT/UPDATE
- ðŸ”´ Maintenance inutile

### Solution

```sql
-- 1. VÃ©rifier avec HypoPG si l'index est vraiment inutile
CREATE EXTENSION IF NOT EXISTS hypopg;

-- 2. Analyser les requÃªtes qui pourraient utiliser l'index
SELECT 
    query,
    calls,
    mean_exec_time
FROM pg_stat_statements
WHERE query LIKE '%old_field%' OR query LIKE '%old_email%';

-- 3. Si vraiment inutile, supprimer l'index
DROP INDEX idx_orders_old_field;
DROP INDEX idx_users_old_email;

-- 4. VÃ©rifier l'espace libÃ©rÃ©
SELECT 
    pg_size_pretty(pg_database_size(current_database())) AS database_size;
```

### RÃ©sultats

| MÃ©trique | Avant | AprÃ¨s | AmÃ©lioration |
|----------|-------|-------|--------------|
| Espace index | 400 MB | 0 MB | **-400 MB** |
| Temps INSERT | +10% | Normal | âœ… |
| Temps UPDATE | +15% | Normal | âœ… |

---

## ðŸ“Š Points clÃ©s Ã  retenir

1. **Toujours analyser avec EXPLAIN ANALYZE** avant d'optimiser
2. **Utiliser Dalibo** pour identifier les problÃ¨mes automatiquement
3. **Mesurer l'impact** avant et aprÃ¨s chaque optimisation
4. **Index appropriÃ©s** : Solution la plus courante
5. **Ã‰viter les sous-requÃªtes corrÃ©lÃ©es** : Utiliser JOIN
6. **Surveiller rÃ©guliÃ¨rement** : Les problÃ¨mes Ã©voluent

## ðŸ”— Prochain module

Passer au module [7. Exercices](../07-exercices/README.md) pour pratiquer avec des exercices guidÃ©s.

