# 1. Rozpoczcie z Airflow

##  Cele

- Zrozumie Apache Airflow
- Zainstalowa Airflow lokalnie
- Skonfigurowa rodowisko
- Uzyska dostp do interfejsu web
- Utworzy pierwszy DAG

##  Spis treci

1. [Wprowadzenie do Airflow](#wprowadzenie-do-airflow)
2. [Instalacja](#instalacja)
3. [Podstawowa konfiguracja](#podstawowa-konfiguracja)
4. [Interfejs web](#interfejs-web)
5. [Pierwszy DAG](#pierwszy-dag)

---

## Wprowadzenie do Airflow

### Czym jest Apache Airflow?

**Apache Airflow** = Platforma open-source do orkiestracji przepyw贸w pracy

- **Workflows** : Zo偶one pipeline'y danych
- **Scheduling** : Automatyczne planowanie
- **Monitorowanie** : Monitorowanie w czasie rzeczywistym
- **Python** : Zdefiniowane w Pythonie
- **Skalowalne** : Od prostych do bardzo zo偶onych

### Dlaczego Airflow dla Data Analyst?

- **Orkiestracja ETL** : Koordynowa wiele krok贸w
- **Harmonogramowanie** : Automatyzowa zadania cykliczne
- **Monitorowanie** : Widzie status pipeline'贸w
- **Retry** : Automatyczne ponowienie przy bdzie
- **Integracja** : Z bazami danych, API, usugami chmurowymi

### Komponenty Airflow

1. **Web Server** : Interfejs web (port 8080)
2. **Scheduler** : Planuje i wykonuje DAGi
3. **Metadata Database** : Przechowuje stan i metadane
4. **Workers** : Wykonuj zadania (opcjonalne)

---

## Instalacja

### Wymagania wstpne

- **Python 3.8+** : Zainstalowany w systemie
- **pip** : Mened偶er pakiet贸w Python
- **7-8 GB RAM** : Minimum zalecane

### Instalacja z pip

**Krok 1 : Utworzy rodowisko wirtualne**

```bash
# Utworzy katalog
mkdir airflow-project
cd airflow-project

# Utworzy rodowisko wirtualne
python -m venv airflow-env

# Aktywowa rodowisko
# Windows
airflow-env\Scripts\activate
# Linux/Mac
source airflow-env/bin/activate
```

**Krok 2 : Zainstalowa Airflow**

```bash
# Zainstalowa Airflow
pip install apache-airflow

# Zainstalowa dodatkowe providers (opcjonalne)
pip install apache-airflow-providers-postgres
pip install apache-airflow-providers-http
```

**Krok 3 : Zainicjalizowa baz danych**

```bash
# Zainicjalizowa baz danych SQLite (domylnie)
airflow db init
```

**Krok 4 : Utworzy u偶ytkownika admin**

```bash
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin123
```

### Instalacja z constraints (zalecane)

**Aby unikn konflikt贸w zale偶noci :**

```bash
# Pobra constraints
AIRFLOW_VERSION=2.7.0
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"

# Zainstalowa z constraints
pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
```

---

## Podstawowa konfiguracja

### Plik airflow.cfg

**Lokalizacja :**
- Windows : `%USERPROFILE%\airflow\airflow.cfg`
- Linux/Mac : `~/airflow/airflow.cfg`

**Wa偶ne parametry :**

```ini
[core]
# Katalog DAG贸w
dags_folder = ~/airflow/dags

# Katalog log贸w
base_log_folder = ~/airflow/logs

# Strefa czasowa
default_timezone = Europe/Warsaw

[webserver]
# Port serwera web
web_server_port = 8080

# Host (0.0.0.0 dla dostpu sieciowego)
web_server_host = 0.0.0.0
```

### Zmienne rodowiskowe

**AIRFLOW_HOME :**

```bash
# Windows
set AIRFLOW_HOME=C:\airflow

# Linux/Mac
export AIRFLOW_HOME=~/airflow
```

### Struktura katalog贸w

```
airflow/
 dags/          # Twoje DAGi
 logs/          # Logi wykonania
 plugins/       # Pluginy niestandardowe
 airflow.cfg   # Konfiguracja
```

---

## Interfejs web

### Uruchomi serwer web

```bash
# Aktywowa rodowisko wirtualne
source airflow-env/bin/activate  # Linux/Mac
# lub
airflow-env\Scripts\activate  # Windows

# Uruchomi serwer web
airflow webserver --port 8080
```

### Uruchomi scheduler

**W innym terminalu :**

```bash
# Aktywowa rodowisko wirtualne
source airflow-env/bin/activate

# Uruchomi scheduler
airflow scheduler
```

### Dostp do interfejsu

1. Otworzy przegldark
2. Przej do : `http://localhost:8080`
3. Zalogowa si z :
   - **Username** : `admin`
   - **Password** : `admin123`

### Nawigacja w interfejsie

**G贸wne zakadki :**
- **DAGs** : Lista wszystkich DAG贸w
- **Graph** : Widok graficzny DAGa
- **Tree** : Widok drzewa wykonania
- **Gantt** : Diagram Gantta
- **Code** : Kod 藕r贸dowy DAGa
- **Logs** : Logi wykonania

---

## Pierwszy DAG

### Utworzy prosty DAG

**Krok 1 : Utworzy plik DAG**

```bash
# Utworzy katalog dags
mkdir -p ~/airflow/dags

# Utworzy plik DAG
nano ~/airflow/dags/my_first_dag.py
```

**Krok 2 : Kod DAGa**

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

# Zdefiniowa argumenty domylne
default_args = {
    'owner': 'data_analyst',
    'depends_on_past': False,
    'email': ['admin@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Utworzy DAG
dag = DAG(
    'my_first_dag',
    default_args=default_args,
    description='M贸j pierwszy DAG Airflow',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['tutorial'],
)

# Zadanie 1 : Wywietli dat
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

# Zadanie 2 : Wywietli wiadomo
def print_hello():
    print("Hello from Airflow!")

t2 = PythonOperator(
    task_id='print_hello',
    python_callable=print_hello,
    dag=dag,
)

# Zdefiniowa zale偶noci
t1 >> t2  # t1 wykonuje si przed t2
```

**Krok 3 : Sprawdzi DAG**

```bash
# Listowa DAGi
airflow dags list

# Sprawdzi skadni
airflow dags list-import-errors

# Testowa DAG
airflow dags test my_first_dag 2024-01-01
```

**Krok 4 : Zobaczy w interfejsie web**

1. Odwie偶y stron web
2. DAG `my_first_dag` pojawia si na licie
3. Klikn "Trigger DAG" aby go wykona

---

## Przykady praktyczne

### Przykad 1 : DAG z wieloma zadaniami

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_analyst',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'example_dag',
    default_args=default_args,
    description='Przykad DAGa z wieloma zadaniami',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
)

# Zadanie 1
extract = BashOperator(
    task_id='extract_data',
    bash_command='echo "Extracting data..."',
    dag=dag,
)

# Zadanie 2
transform = BashOperator(
    task_id='transform_data',
    bash_command='echo "Transforming data..."',
    dag=dag,
)

# Zadanie 3
load = BashOperator(
    task_id='load_data',
    bash_command='echo "Loading data..."',
    dag=dag,
)

# Zdefiniowa zale偶noci
extract >> transform >> load
```

### Przykad 2 : DAG z gaziami

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG(
    'branching_dag',
    description='DAG z gaziami',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
)

def decide_path():
    # Logika do decyzji cie偶ki
    return 'path_a'

decide = PythonOperator(
    task_id='decide',
    python_callable=decide_path,
    dag=dag,
)

path_a = BashOperator(
    task_id='path_a',
    bash_command='echo "Path A"',
    dag=dag,
)

path_b = BashOperator(
    task_id='path_b',
    bash_command='echo "Path B"',
    dag=dag,
)

# Rozgazienie warunkowe
decide >> [path_a, path_b]
```

---

## Przydatne polecenia

### Zarzdzanie DAGami

```bash
# Listowa wszystkie DAGi
airflow dags list

# Sprawdzi bdy importu
airflow dags list-import-errors

# Testowa DAG
airflow dags test my_first_dag 2024-01-01

# Wstrzyma DAG
airflow dags pause my_first_dag

# Wznowi DAG
airflow dags unpause my_first_dag

# Usun DAG
airflow dags delete my_first_dag
```

### Zarzdzanie zadaniami

```bash
# Testowa zadanie
airflow tasks test my_first_dag print_date 2024-01-01

# Wykona zadanie
airflow tasks run my_first_dag print_date 2024-01-01
```

### Zarzdzanie baz danych

```bash
# Zainicjalizowa baz
airflow db init

# Zaktualizowa baz
airflow db upgrade

# Zresetowa baz (UWAGA : usuwa wszystko)
airflow db reset
```

---

## Rozwizywanie problem贸w

### Problem : DAG niewidoczny w interfejsie

**Rozwizania :**
1. Sprawdzi 偶e plik jest w `~/airflow/dags/`
2. Sprawdzi skadni Pythona
3. Sprawdzi bdy : `airflow dags list-import-errors`
4. Uruchomi ponownie scheduler

### Problem : Bd importu

**Rozwizania :**
1. Sprawdzi 偶e wszystkie zale偶noci s zainstalowane
2. Sprawdzi importy w DAGu
3. Sprawdzi cie偶ki Pythona

### Problem : Scheduler nie uruchamia si

**Rozwizania :**
1. Sprawdzi 偶e baza danych jest zainicjalizowana
2. Sprawdzi logi : `~/airflow/logs/scheduler/`
3. Sprawdzi uprawnienia

---

##  Kluczowe punkty do zapamitania

1. **Airflow = Orkiestracja** przepyw贸w pracy Python
2. **DAGi** definiuj workflows
3. **Zadania** to pojedyncze kroki
4. **Scheduler** wykonuje DAGi wedug harmonogramu
5. **Interfejs web** umo偶liwia monitorowanie i zarzdzanie

##  Nastpny modu

Przejd藕 do moduu [2. Podstawowe koncepcje](../02-concepts/README.md), aby pogbi koncepcje Airflow.

