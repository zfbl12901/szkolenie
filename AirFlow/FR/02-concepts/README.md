# 2. Concepts fondamentaux Airflow

## üéØ Objectifs

- Comprendre les DAGs (Directed Acyclic Graphs)
- Ma√Ætriser les t√¢ches et d√©pendances
- Comprendre le scheduling
- Utiliser les variables et connexions
- G√©rer les ex√©cutions

## üìã Table des mati√®res

1. [DAGs (Directed Acyclic Graphs)](#dags-directed-acyclic-graphs)
2. [Tasks et d√©pendances](#tasks-et-d√©pendances)
3. [Scheduling](#scheduling)
4. [Variables et Connexions](#variables-et-connexions)
5. [Ex√©cutions et √©tats](#ex√©cutions-et-√©tats)

---

## DAGs (Directed Acyclic Graphs)

### Qu'est-ce qu'un DAG ?

**DAG** = Graphe orient√© acyclique

- **Oriented** : Les t√¢ches ont un sens (d√©pendances)
- **Acyclic** : Pas de boucles (pas de d√©pendances circulaires)
- **Graph** : Repr√©sentation visuelle des workflows

### Structure d'un DAG

```python
from airflow import DAG
from datetime import datetime, timedelta

# Arguments par d√©faut
default_args = {
    'owner': 'data_analyst',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Cr√©er le DAG
dag = DAG(
    'my_dag',
    default_args=default_args,
    description='Description du DAG',
    schedule_interval='@daily',  # Fr√©quence d'ex√©cution
    start_date=datetime(2024, 1, 1),
    catchup=False,  # Ne pas ex√©cuter les runs pass√©s
    tags=['example'],
)
```

### Propri√©t√©s d'un DAG

**ID unique :**
- Doit √™tre unique dans l'installation Airflow
- Utilis√© pour identifier le DAG

**Schedule interval :**
- `@daily` : Tous les jours
- `@hourly` : Toutes les heures
- `timedelta(days=1)` : Tous les jours
- `'0 2 * * *'` : Cron expression (tous les jours √† 2h)
- `None` : D√©clenchement manuel uniquement

**Start date :**
- Date de d√©but du scheduling
- Format : `datetime(ann√©e, mois, jour)`

**Catchup :**
- `True` : Ex√©cute les runs manqu√©s depuis start_date
- `False` : N'ex√©cute que les runs futurs

---

## Tasks et d√©pendances

### Qu'est-ce qu'une Task ?

**Task** = √âtape individuelle dans un DAG

- **Op√©rateur** : Type de t√¢che (Python, Bash, SQL, etc.)
- **ID unique** : Identifiant dans le DAG
- **D√©pendances** : Relations avec d'autres t√¢ches

### Types d'op√©rateurs

**BashOperator :**
```python
from airflow.operators.bash import BashOperator

task = BashOperator(
    task_id='bash_task',
    bash_command='echo "Hello"',
    dag=dag,
)
```

**PythonOperator :**
```python
from airflow.operators.python import PythonOperator

def my_function():
    print("Hello from Python")

task = PythonOperator(
    task_id='python_task',
    python_callable=my_function,
    dag=dag,
)
```

### D√©finir les d√©pendances

**M√©thode 1 : Op√©rateur >>**

```python
# t1 s'ex√©cute avant t2
t1 >> t2

# Plusieurs d√©pendances
t1 >> [t2, t3] >> t4
```

**M√©thode 2 : set_upstream / set_downstream**

```python
# t1 s'ex√©cute avant t2
t1.set_downstream(t2)
# ou
t2.set_upstream(t1)
```

**M√©thode 3 : bitshift**

```python
# t1 >> t2 √©quivaut √†
t1 >> t2
```

### Exemple de d√©pendances

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

dag = DAG('dependencies_example', start_date=datetime(2024, 1, 1))

# T√¢ches
extract = BashOperator(task_id='extract', bash_command='echo extract', dag=dag)
transform = BashOperator(task_id='transform', bash_command='echo transform', dag=dag)
load = BashOperator(task_id='load', bash_command='echo load', dag=dag)
validate = BashOperator(task_id='validate', bash_command='echo validate', dag=dag)

# D√©pendances
extract >> transform >> [load, validate]
```

---

## Scheduling

### Schedule Interval

**Expressions courantes :**

```python
# Tous les jours √† minuit
schedule_interval='@daily'
# ou
schedule_interval=timedelta(days=1)

# Toutes les heures
schedule_interval='@hourly'
# ou
schedule_interval=timedelta(hours=1)

# Toutes les semaines
schedule_interval='@weekly'

# Expression cron
schedule_interval='0 2 * * *'  # Tous les jours √† 2h
schedule_interval='0 */6 * * *'  # Toutes les 6 heures
schedule_interval='0 0 * * MON'  # Tous les lundis √† minuit
```

### Start Date et Execution Date

**Start Date :**
- Date de d√©but du scheduling
- Format : `datetime(2024, 1, 1)`

**Execution Date :**
- Date logique d'ex√©cution
- Format : `YYYY-MM-DDTHH:MM:SS`

**Exemple :**
```python
dag = DAG(
    'scheduled_dag',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
)
```

### Catchup

**Catchup = True :**
- Ex√©cute tous les runs manqu√©s depuis start_date
- Peut cr√©er beaucoup de runs

**Catchup = False :**
- N'ex√©cute que les runs futurs
- Recommand√© pour la plupart des cas

---

## Variables et Connexions

### Variables

**Variables = Configuration globale**

**Cr√©er une variable :**

```bash
# Via CLI
airflow variables set my_key "my_value"

# Via interface web
# Admin ‚Üí Variables ‚Üí Add
```

**Utiliser une variable :**

```python
from airflow.models import Variable

# R√©cup√©rer une variable
my_value = Variable.get("my_key")
my_value_default = Variable.get("my_key", default_var="default")

# Dans un template
# {{ var.value.my_key }}
```

**Exemple :**

```python
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG('variables_example', start_date=datetime(2024, 1, 1))

def use_variable():
    api_key = Variable.get("api_key")
    print(f"API Key: {api_key}")

task = PythonOperator(
    task_id='use_variable',
    python_callable=use_variable,
    dag=dag,
)
```

### Connexions

**Connexions = Informations de connexion**

**Cr√©er une connexion :**

```bash
# Via CLI
airflow connections add 'my_postgres' \
    --conn-type 'postgres' \
    --conn-host 'localhost' \
    --conn-login 'user' \
    --conn-password 'password' \
    --conn-port 5432 \
    --conn-schema 'mydb'
```

**Utiliser une connexion :**

```python
from airflow.hooks.base import BaseHook

# R√©cup√©rer une connexion
conn = BaseHook.get_connection('my_postgres')
print(f"Host: {conn.host}")
print(f"Login: {conn.login}")
print(f"Password: {conn.password}")
```

---

## Ex√©cutions et √©tats

### √âtats des t√¢ches

- **None** : Pas encore ex√©cut√©e
- **Scheduled** : Planifi√©e
- **Queued** : En attente
- **Running** : En cours d'ex√©cution
- **Success** : R√©ussie
- **Failed** : √âchou√©e
- **Skipped** : Ignor√©e
- **Retry** : Nouvelle tentative
- **Up for retry** : Pr√™te pour retry

### √âtats des DAGs

- **Running** : En cours d'ex√©cution
- **Success** : Toutes les t√¢ches r√©ussies
- **Failed** : Au moins une t√¢che √©chou√©e

### G√©rer les ex√©cutions

**Via l'interface web :**
- Voir l'√©tat des ex√©cutions
- Relancer une t√¢che
- Marquer comme succ√®s/√©chec
- Voir les logs

**Via CLI :**

```bash
# Lister les runs
airflow dags list-runs -d my_dag

# D√©clencher un DAG
airflow dags trigger my_dag

# Marquer une t√¢che comme succ√®s
airflow tasks clear my_dag task_id -s 2024-01-01
```

---

## Exemples pratiques

### Exemple 1 : DAG avec variables

```python
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG('variables_dag', start_date=datetime(2024, 1, 1))

def process_data():
    # R√©cup√©rer des variables
    input_path = Variable.get("input_path")
    output_path = Variable.get("output_path")
    
    print(f"Processing: {input_path} -> {output_path}")

task = PythonOperator(
    task_id='process',
    python_callable=process_data,
    dag=dag,
)
```

### Exemple 2 : DAG avec connexion

```python
from airflow import DAG
from airflow.hooks.postgres import PostgresHook
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG('connection_dag', start_date=datetime(2024, 1, 1))

def query_database():
    # Utiliser une connexion PostgreSQL
    hook = PostgresHook(postgres_conn_id='my_postgres')
    records = hook.get_records("SELECT * FROM users LIMIT 10")
    print(records)

task = PythonOperator(
    task_id='query',
    python_callable=query_database,
    dag=dag,
)
```

---

## üìä Points cl√©s √† retenir

1. **DAGs** d√©finissent les workflows
2. **Tasks** sont les √©tapes individuelles
3. **D√©pendances** d√©finissent l'ordre d'ex√©cution
4. **Scheduling** planifie les ex√©cutions
5. **Variables et Connexions** pour la configuration

## üîó Prochain module

Passer au module [3. Op√©rateurs](../03-operators/README.md) pour apprendre √† utiliser les diff√©rents op√©rateurs Airflow.

