# 1. Prise en main Airflow

## üéØ Objectifs

- Comprendre Apache Airflow
- Installer Airflow localement
- Configurer l'environnement
- Acc√©der √† l'interface web
- Cr√©er votre premier DAG

## üìã Table des mati√®res

1. [Introduction √† Airflow](#introduction-√†-airflow)
2. [Installation](#installation)
3. [Configuration de base](#configuration-de-base)
4. [Interface web](#interface-web)
5. [Premier DAG](#premier-dag)

---

## Introduction √† Airflow

### Qu'est-ce qu'Apache Airflow ?

**Apache Airflow** = Plateforme open-source d'orchestration de workflows

- **Workflows** : Pipelines de donn√©es complexes
- **Scheduling** : Planification automatique
- **Monitoring** : Surveillance en temps r√©el
- **Python** : D√©fini en Python
- **Scalable** : De simple √† tr√®s complexe

### Pourquoi Airflow pour Data Analyst ?

- **Orchestration ETL** : Coordonner plusieurs √©tapes
- **Scheduling** : Automatiser les t√¢ches r√©currentes
- **Monitoring** : Voir l'√©tat des pipelines
- **Retry** : Nouvelle tentative automatique en cas d'erreur
- **Int√©gration** : Avec bases de donn√©es, APIs, services cloud

### Composants Airflow

1. **Web Server** : Interface web (port 8080)
2. **Scheduler** : Planifie et ex√©cute les DAGs
3. **Metadata Database** : Stocke l'√©tat et les m√©tadonn√©es
4. **Workers** : Ex√©cutent les t√¢ches (optionnel)

---

## Installation

### Pr√©requis

- **Python 3.8+** : Install√© sur votre syst√®me
- **pip** : Gestionnaire de paquets Python
- **7-8 Go RAM** : Minimum recommand√©

### Installation avec pip

**√âtape 1 : Cr√©er un environnement virtuel**

```bash
# Cr√©er un r√©pertoire
mkdir airflow-project
cd airflow-project

# Cr√©er un environnement virtuel
python -m venv airflow-env

# Activer l'environnement
# Windows
airflow-env\Scripts\activate
# Linux/Mac
source airflow-env/bin/activate
```

**√âtape 2 : Installer Airflow**

```bash
# Installer Airflow
pip install apache-airflow

# Installer des providers suppl√©mentaires (optionnel)
pip install apache-airflow-providers-postgres
pip install apache-airflow-providers-http
```

**√âtape 3 : Initialiser la base de donn√©es**

```bash
# Initialiser la base de donn√©es SQLite (par d√©faut)
airflow db init
```

**√âtape 4 : Cr√©er un utilisateur admin**

```bash
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin123
```

### Installation avec constraints (recommand√©)

**Pour √©viter les conflits de d√©pendances :**

```bash
# T√©l√©charger les constraints
AIRFLOW_VERSION=2.7.0
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"

# Installer avec constraints
pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
```

---

## Configuration de base

### Fichier airflow.cfg

**Localisation :**
- Windows : `%USERPROFILE%\airflow\airflow.cfg`
- Linux/Mac : `~/airflow/airflow.cfg`

**Param√®tres importants :**

```ini
[core]
# R√©pertoire des DAGs
dags_folder = ~/airflow/dags

# R√©pertoire des logs
base_log_folder = ~/airflow/logs

# Fuseau horaire
default_timezone = Europe/Paris

[webserver]
# Port du serveur web
web_server_port = 8080

# Host (0.0.0.0 pour acc√®s r√©seau)
web_server_host = 0.0.0.0
```

### Variables d'environnement

**AIRFLOW_HOME :**

```bash
# Windows
set AIRFLOW_HOME=C:\airflow

# Linux/Mac
export AIRFLOW_HOME=~/airflow
```

### Structure des r√©pertoires

```
airflow/
‚îú‚îÄ‚îÄ dags/          # Vos DAGs
‚îú‚îÄ‚îÄ logs/          # Logs d'ex√©cution
‚îú‚îÄ‚îÄ plugins/       # Plugins personnalis√©s
‚îî‚îÄ‚îÄ airflow.cfg   # Configuration
```

---

## Interface web

### D√©marrer le serveur web

```bash
# Activer l'environnement virtuel
source airflow-env/bin/activate  # Linux/Mac
# ou
airflow-env\Scripts\activate  # Windows

# D√©marrer le serveur web
airflow webserver --port 8080
```

### D√©marrer le scheduler

**Dans un autre terminal :**

```bash
# Activer l'environnement virtuel
source airflow-env/bin/activate

# D√©marrer le scheduler
airflow scheduler
```

### Acc√©der √† l'interface

1. Ouvrir un navigateur
2. Aller sur : `http://localhost:8080`
3. Se connecter avec :
   - **Username** : `admin`
   - **Password** : `admin123`

### Navigation dans l'interface

**Onglets principaux :**
- **DAGs** : Liste de tous les DAGs
- **Graph** : Vue graphique d'un DAG
- **Tree** : Vue arborescente des ex√©cutions
- **Gantt** : Diagramme de Gantt
- **Code** : Code source du DAG
- **Logs** : Logs d'ex√©cution

---

## Premier DAG

### Cr√©er un DAG simple

**√âtape 1 : Cr√©er le fichier DAG**

```bash
# Cr√©er le r√©pertoire dags
mkdir -p ~/airflow/dags

# Cr√©er un fichier DAG
nano ~/airflow/dags/my_first_dag.py
```

**√âtape 2 : Code du DAG**

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

# D√©finir les arguments par d√©faut
default_args = {
    'owner': 'data_analyst',
    'depends_on_past': False,
    'email': ['admin@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Cr√©er le DAG
dag = DAG(
    'my_first_dag',
    default_args=default_args,
    description='Mon premier DAG Airflow',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['tutorial'],
)

# T√¢che 1 : Afficher la date
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

# T√¢che 2 : Afficher un message
def print_hello():
    print("Hello from Airflow!")

t2 = PythonOperator(
    task_id='print_hello',
    python_callable=print_hello,
    dag=dag,
)

# D√©finir les d√©pendances
t1 >> t2  # t1 s'ex√©cute avant t2
```

**√âtape 3 : V√©rifier le DAG**

```bash
# Lister les DAGs
airflow dags list

# V√©rifier la syntaxe
airflow dags list-import-errors

# Tester le DAG
airflow dags test my_first_dag 2024-01-01
```

**√âtape 4 : Voir dans l'interface web**

1. Rafra√Æchir la page web
2. Le DAG `my_first_dag` appara√Æt dans la liste
3. Cliquer sur "Trigger DAG" pour l'ex√©cuter

---

## Exemples pratiques

### Exemple 1 : DAG avec plusieurs t√¢ches

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_analyst',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'example_dag',
    default_args=default_args,
    description='Exemple de DAG avec plusieurs t√¢ches',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
)

# T√¢che 1
extract = BashOperator(
    task_id='extract_data',
    bash_command='echo "Extracting data..."',
    dag=dag,
)

# T√¢che 2
transform = BashOperator(
    task_id='transform_data',
    bash_command='echo "Transforming data..."',
    dag=dag,
)

# T√¢che 3
load = BashOperator(
    task_id='load_data',
    bash_command='echo "Loading data..."',
    dag=dag,
)

# D√©finir les d√©pendances
extract >> transform >> load
```

### Exemple 2 : DAG avec branches

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG(
    'branching_dag',
    description='DAG avec branches',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
)

def decide_path():
    # Logique pour d√©cider du chemin
    return 'path_a'

decide = PythonOperator(
    task_id='decide',
    python_callable=decide_path,
    dag=dag,
)

path_a = BashOperator(
    task_id='path_a',
    bash_command='echo "Path A"',
    dag=dag,
)

path_b = BashOperator(
    task_id='path_b',
    bash_command='echo "Path B"',
    dag=dag,
)

# Branchement conditionnel
decide >> [path_a, path_b]
```

---

## Commandes utiles

### Gestion des DAGs

```bash
# Lister tous les DAGs
airflow dags list

# V√©rifier les erreurs d'import
airflow dags list-import-errors

# Tester un DAG
airflow dags test my_first_dag 2024-01-01

# Pauser un DAG
airflow dags pause my_first_dag

# Reprendre un DAG
airflow dags unpause my_first_dag

# Supprimer un DAG
airflow dags delete my_first_dag
```

### Gestion des t√¢ches

```bash
# Tester une t√¢che
airflow tasks test my_first_dag print_date 2024-01-01

# Ex√©cuter une t√¢che
airflow tasks run my_first_dag print_date 2024-01-01
```

### Gestion de la base de donn√©es

```bash
# Initialiser la base
airflow db init

# Mettre √† jour la base
airflow db upgrade

# R√©initialiser la base (ATTENTION : supprime tout)
airflow db reset
```

---

## D√©pannage

### Probl√®me : DAG non visible dans l'interface

**Solutions :**
1. V√©rifier que le fichier est dans `~/airflow/dags/`
2. V√©rifier la syntaxe Python
3. V√©rifier les erreurs : `airflow dags list-import-errors`
4. Red√©marrer le scheduler

### Probl√®me : Erreur d'import

**Solutions :**
1. V√©rifier que toutes les d√©pendances sont install√©es
2. V√©rifier les imports dans le DAG
3. V√©rifier les chemins Python

### Probl√®me : Scheduler ne d√©marre pas

**Solutions :**
1. V√©rifier que la base de donn√©es est initialis√©e
2. V√©rifier les logs : `~/airflow/logs/scheduler/`
3. V√©rifier les permissions

---

## üìä Points cl√©s √† retenir

1. **Airflow = Orchestration** de workflows Python
2. **DAGs** d√©finissent les workflows
3. **Tasks** sont les √©tapes individuelles
4. **Scheduler** ex√©cute les DAGs selon le planning
5. **Interface web** permet de monitorer et g√©rer

## üîó Prochain module

Passer au module [2. Concepts fondamentaux](../02-concepts/README.md) pour approfondir les concepts d'Airflow.

