# 7. Projets pratiques Azure

## üéØ Objectifs

- Appliquer les connaissances acquises
- Cr√©er des pipelines ETL complets
- Int√©grer avec PowerBI
- Cr√©er des projets pour votre portfolio
- Utiliser plusieurs services Azure

## üìã Table des mati√®res

1. [Projet 1 : Pipeline ETL Blob ‚Üí SQL Database](#projet-1--pipeline-etl-blob---sql-database)
2. [Projet 2 : Data Lake avec Synapse](#projet-2--data-lake-avec-synapse)
3. [Projet 3 : Analytics avec PowerBI](#projet-3--analytics-avec-powerbi)
4. [Projet 4 : Pipeline automatis√© complet](#projet-4--pipeline-automatis√©-complet)
5. [Bonnes pratiques pour portfolio](#bonnes-pratiques-pour-portfolio)

---

## Projet 1 : Pipeline ETL Blob ‚Üí SQL Database

### Objectif

Cr√©er un pipeline ETL qui charge des fichiers CSV depuis Blob Storage vers SQL Database.

### Architecture

```
Blob Storage (CSV) ‚Üí Data Factory ‚Üí SQL Database ‚Üí PowerBI
```

### √âtapes

#### 1. Pr√©parer les donn√©es

**Cr√©er un container Blob Storage :**
- Nom : `raw-data`
- Uploader un fichier CSV de test

**Exemple de donn√©es CSV :**
```csv
id,name,email,created_at,status
1,John Doe,john@example.com,2024-01-01,active
2,Jane Smith,jane@example.com,2024-01-02,inactive
```

#### 2. Cr√©er une base SQL Database

1. Portail Azure ‚Üí Cr√©er SQL Database
2. Configuration :
   - Name : `analytics-db`
   - Server : Cr√©er nouveau serveur
   - Service tier : Basic (gratuit 12 mois)
3. Cr√©er la base

#### 3. Cr√©er la table dans SQL Database

```sql
CREATE TABLE users (
    id INT PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(100),
    created_at DATETIME2,
    status VARCHAR(20)
);
```

#### 4. Cr√©er un pipeline Data Factory

1. Data Factory Studio ‚Üí "Author" ‚Üí "Pipelines"
2. Cr√©er un nouveau pipeline : `LoadCSVToSQL`
3. Ajouter activit√© "Copy Data"
4. Configuration :
   - **Source** : Azure Blob Storage (CSV)
   - **Sink** : Azure SQL Database (table users)
5. Publier le pipeline

#### 5. Ex√©cuter le pipeline

1. Cliquer sur "Trigger now"
2. V√©rifier l'ex√©cution dans "Monitor"
3. V√©rifier les donn√©es dans SQL Database

### R√©sultat

- Fichiers CSV charg√©s dans SQL Database
- Pipeline ETL fonctionnel
- Pr√™t pour analytics avec PowerBI

---

## Projet 2 : Data Lake avec Synapse

### Objectif

Cr√©er un Data Lake complet avec ingestion, transformation et analytics.

### Architecture

```
Sources ‚Üí Data Lake Storage (Raw) ‚Üí Synapse (Transform) ‚Üí Data Lake (Processed) ‚Üí PowerBI
                ‚Üì
        Data Factory (Orchestration)
```

### √âtapes

#### 1. Structure Data Lake Storage

```
data-lake/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îú‚îÄ‚îÄ orders/
‚îÇ   ‚îî‚îÄ‚îÄ products/
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îú‚îÄ‚îÄ orders/
‚îÇ   ‚îî‚îÄ‚îÄ products/
‚îî‚îÄ‚îÄ analytics/
    ‚îî‚îÄ‚îÄ results/
```

#### 2. Cr√©er un workspace Synapse

1. Portail Azure ‚Üí Cr√©er Azure Synapse Analytics
2. Configuration :
   - Workspace name : `my-synapse-workspace`
   - Data Lake Storage : Cr√©er nouveau
3. Cr√©er le workspace

#### 3. Pipelines Data Factory pour transformation

**Pipeline pour users :**

1. Synapse Studio ‚Üí "Integrate" ‚Üí "Pipelines"
2. Cr√©er pipeline : `TransformUsers`
3. Activit√©s :
   - Source : Data Lake Storage (raw/users/)
   - Data Flow : Transformer (filtrer, nettoyer)
   - Sink : Data Lake Storage (processed/users/)
4. Publier

#### 4. Tables Synapse pour analytics

```sql
-- Cr√©er une table externe
CREATE EXTERNAL TABLE users_processed (
    id INT,
    name VARCHAR(100),
    email VARCHAR(100),
    created_at DATETIME2
)
WITH (
    LOCATION = 'processed/users/',
    DATA_SOURCE = DataLakeStorage,
    FILE_FORMAT = ParquetFormat
);

-- Requ√™te analytique
SELECT 
    YEAR(created_at) AS year,
    COUNT(*) AS user_count
FROM users_processed
GROUP BY YEAR(created_at);
```

#### 5. Automatisation avec Triggers

1. Pipeline ‚Üí "Add trigger" ‚Üí "New/Edit"
2. Type : Schedule
3. Recurrence : Daily
4. Start time : 02:00
5. Sauvegarder

### R√©sultat

- Data Lake fonctionnel
- Pipeline automatis√©
- Analytics avec Synapse
- Projet complet pour portfolio

---

## Projet 3 : Analytics avec PowerBI

### Objectif

Cr√©er un syst√®me d'analytics complet avec PowerBI connect√© √† Azure.

### √âtapes

#### 1. Pr√©parer les donn√©es

**Dans SQL Database ou Synapse :**
- Charger des donn√©es
- Cr√©er des vues analytiques

#### 2. Connecter PowerBI √† Azure SQL Database

1. PowerBI Desktop ‚Üí "Get Data"
2. "Azure" ‚Üí "Azure SQL Database"
3. Configuration :
   - Server : `my-sql-server.database.windows.net`
   - Database : `analytics-db`
   - Authentication : Database
4. S√©lectionner les tables ou vues
5. Cliquer sur "Load"

#### 3. Cr√©er des visualisations

**Exemple :**
1. Importer la table `users`
2. Cr√©er un graphique : Nombre d'utilisateurs par mois
3. Ajouter des filtres
4. Cr√©er un dashboard

#### 4. Publier sur PowerBI Service

1. PowerBI Desktop ‚Üí "Publish"
2. S√©lectionner l'espace de travail
3. Publier
4. Acc√©der au rapport sur powerbi.com

#### 5. Actualiser les donn√©es

1. PowerBI Service ‚Üí Dataset ‚Üí "Schedule refresh"
2. Configuration :
   - Frequency : Daily
   - Time : 03:00
3. Sauvegarder

### R√©sultat

- Analytics avec PowerBI
- Visualisations interactives
- Actualisation automatique
- Projet complet pour portfolio

---

## Projet 4 : Pipeline automatis√© complet

### Objectif

Cr√©er un pipeline ETL compl√®tement automatis√© avec plusieurs services Azure.

### Architecture compl√®te

```
Fichier CSV upload√© ‚Üí Blob Storage (raw/)
    ‚Üì (Event)
Azure Function (Validation)
    ‚Üì
Blob Storage (validated/)
    ‚Üì (Trigger)
Data Factory Pipeline (Transform CSV ‚Üí Parquet)
    ‚Üì
Data Lake Storage (processed/)
    ‚Üì
Synapse (Analytics)
    ‚Üì
SQL Database (Results)
    ‚Üì
PowerBI (Visualization)
```

### Impl√©mentation

#### 1. Azure Function de validation

```python
import azure.functions as func
import logging
import csv
from azure.storage.blob import BlobServiceClient

def main(blob: func.InputStream):
    logging.info(f'Processing blob: {blob.name}')
    
    # Lire le blob
    content = blob.read().decode('utf-8')
    reader = csv.DictReader(content.splitlines())
    
    # Valider
    valid_rows = []
    for row in reader:
        if row.get('email') and '@' in row['email']:
            valid_rows.append(row)
    
    # Uploader les donn√©es valid√©es
    if valid_rows:
        # Upload vers validated/
        # ...
    
    logging.info(f'Validated {len(valid_rows)} rows')
```

#### 2. Data Factory Pipeline de transformation

**Pipeline :**
1. Source : Blob Storage (validated/)
2. Data Flow : Transformer (nettoyer, enrichir)
3. Sink : Data Lake Storage (processed/parquet/)

#### 3. Synapse pour analytics

```sql
-- Cr√©er une vue analytique
CREATE VIEW vw_user_analytics AS
SELECT 
    u.id,
    u.name,
    COUNT(o.id) AS order_count,
    SUM(o.amount) AS total_spent
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
GROUP BY u.id, u.name;
```

#### 4. PowerBI pour visualisation

1. Connecter PowerBI √† Synapse
2. Utiliser la vue `vw_user_analytics`
3. Cr√©er des visualisations
4. Publier le rapport

### R√©sultat

- Pipeline compl√®tement automatis√©
- Validation automatique
- Transformation automatique
- Analytics disponibles imm√©diatement
- Visualisations PowerBI

---

## Bonnes pratiques pour portfolio

### Documentation

**Cr√©er un README pour chaque projet :**

```markdown
# Projet : Pipeline ETL Azure

## Description
Pipeline ETL automatis√© pour transformer des donn√©es CSV en Parquet.

## Architecture
- Blob Storage : Stockage
- Data Factory : Transformation
- SQL Database : Base de donn√©es
- PowerBI : Visualisation

## R√©sultats
- R√©duction des co√ªts de 50%
- Temps de traitement r√©duit de 70%
```

### Visualisations

**Cr√©er des diagrammes :**
- Architecture du syst√®me
- Flux de donn√©es
- Sch√©ma de donn√©es

**Outils :**
- Draw.io
- Lucidchart
- Diagrammes ASCII dans README

### M√©triques

**Inclure des m√©triques :**
- Temps d'ex√©cution avant/apr√®s
- Co√ªts avant/apr√®s
- Volume de donn√©es trait√©es
- Performance des requ√™tes

### Code

**Bonnes pratiques :**
- Code comment√©
- Variables d'environnement pour configuration
- Gestion d'erreurs
- Logging

### GitHub

**Cr√©er un repository :**
- README avec documentation
- Scripts Data Factory (JSON)
- Scripts SQL
- Configuration
- Diagrammes

---

## üìä Points cl√©s √† retenir

1. **Projets pratiques** : Essentiels pour portfolio
2. **Documentation** : Expliquer l'architecture et les r√©sultats
3. **M√©triques** : Montrer l'impact (performance, co√ªts)
4. **Code propre** : Comment√© et organis√©
5. **GitHub** : Partager vos projets

## üîó Ressources

- [Azure Architecture Center](https://learn.microsoft.com/azure/architecture/)
- [Azure Solutions](https://azure.microsoft.com/solutions/)
- [GitHub Azure Examples](https://github.com/Azure-Samples)

---

**F√©licitations !** Vous avez termin√© la formation Azure pour Data Analyst. Vous pouvez maintenant cr√©er des projets complets sur Azure en utilisant les ressources gratuites disponibles.

