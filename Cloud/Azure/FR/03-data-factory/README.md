# 3. Azure Data Factory - ETL Cloud

## üéØ Objectifs

- Comprendre Azure Data Factory et son r√¥le
- Cr√©er des pipelines ETL
- Utiliser des activit√©s de transformation
- Int√©grer avec des sources de donn√©es
- Orchestrer des workflows

## üìã Table des mati√®res

1. [Introduction √† Data Factory](#introduction-√†-data-factory)
2. [Cr√©er un Data Factory](#cr√©er-un-data-factory)
3. [Cr√©er un pipeline](#cr√©er-un-pipeline)
4. [Activit√©s de transformation](#activit√©s-de-transformation)
5. [Int√©gration avec sources de donn√©es](#int√©gration-avec-sources-de-donn√©es)
6. [Orchestration et scheduling](#orchestration-et-scheduling)

---

## Introduction √† Data Factory

### Qu'est-ce qu'Azure Data Factory ?

**Azure Data Factory** = Service ETL cloud g√©r√©

- **ETL** : Extract, Transform, Load
- **Cloud** : Pas d'infrastructure √† g√©rer
- **G√©r√©** : Microsoft g√®re l'infrastructure
- **Scalable** : S'adapte automatiquement

### Composants Data Factory

1. **Pipelines** : Flux de travail ETL
2. **Activities** : √âtapes dans un pipeline
3. **Datasets** : Repr√©sentations des donn√©es
4. **Linked Services** : Connexions aux sources
5. **Triggers** : D√©clenchement automatique

### Free Tier Data Factory

**Gratuit √† vie :**
- 5 pipelines gratuits
- Activit√©s limit√©es
- Au-del√† : facturation √† l'usage

**‚ö†Ô∏è Important :** Surveiller les co√ªts, surtout pour les activit√©s de transformation.

---

## Cr√©er un Data Factory

### √âtape 1 : Acc√©der √† Data Factory

1. Portail Azure ‚Üí Rechercher "Data Factory"
2. Cliquer sur "Data factories"
3. Cliquer sur "Create"

### √âtape 2 : Configuration de base

**Informations de base :**
- **Subscription** : Choisir votre abonnement
- **Resource group** : Cr√©er ou utiliser existant
- **Name** : `my-data-factory` (unique globalement)
- **Version** : V2 (recommand√©)
- **Region** : Choisir la r√©gion la plus proche

**Git configuration (optionnel) :**
- **Configure Git later** : Pour d√©buter rapidement
- Ou configurer Git/GitHub pour versioning

### √âtape 3 : Cr√©er le Data Factory

1. Cliquer sur "Review + create"
2. V√©rifier la configuration
3. Cliquer sur "Create"
4. Attendre la cr√©ation (2-3 minutes)

**‚ö†Ô∏è Important :** Noter le nom du Data Factory.

### √âtape 4 : Ouvrir Data Factory Studio

1. Une fois cr√©√©, cliquer sur "Open Azure Data Factory Studio"
2. Interface web pour cr√©er des pipelines

---

## Cr√©er un pipeline

### √âtape 1 : Cr√©er un Linked Service

**Linked Service = Connexion √† une source de donn√©es**

**Exemple : Azure Blob Storage**

1. Data Factory Studio ‚Üí "Manage" ‚Üí "Linked services"
2. Cliquer sur "+ New"
3. Rechercher "Azure Blob Storage"
4. Configuration :
   - **Name** : `AzureBlobStorage1`
   - **Storage account name** : S√©lectionner votre compte
   - **Authentication method** : Account key (ou autre)
5. Cliquer sur "Create"

### √âtape 2 : Cr√©er un Dataset

**Dataset = Repr√©sentation des donn√©es**

1. Data Factory Studio ‚Üí "Author" ‚Üí "Datasets"
2. Cliquer sur "+ New"
3. Choisir "Azure Blob Storage"
4. Configuration :
   - **Name** : `CSVData`
   - **Linked service** : `AzureBlobStorage1`
   - **File path** : `raw-data/`
   - **File format** : DelimitedText (CSV)
5. Cliquer sur "Create"

### √âtape 3 : Cr√©er un pipeline

1. Data Factory Studio ‚Üí "Author" ‚Üí "Pipelines"
2. Cliquer sur "+ New pipeline"
3. Nommer le pipeline : `CopyCSVToParquet`

### √âtape 4 : Ajouter une activit√©

**Exemple : Copy Data**

1. Dans le pipeline, glisser "Copy Data" depuis "Move & transform"
2. Configurer :
   - **Source** : Dataset `CSVData`
   - **Sink (Destination)** : Cr√©er un nouveau dataset Parquet
3. Cliquer sur "Publish" pour sauvegarder

---

## Activit√©s de transformation

### Copy Data

**Copier des donn√©es d'une source √† une destination**

**Configuration :**
- **Source** : Dataset source
- **Sink** : Dataset destination
- **Mapping** : Mapping des colonnes

**Exemple : CSV ‚Üí Parquet**

```json
{
  "name": "CopyCSVToParquet",
  "type": "Copy",
  "inputs": [{"referenceName": "CSVData"}],
  "outputs": [{"referenceName": "ParquetData"}],
  "typeProperties": {
    "source": {"type": "DelimitedTextSource"},
    "sink": {"type": "ParquetSink"}
  }
}
```

### Data Flow

**Transformation de donn√©es avec interface graphique**

**√âtapes :**
1. Cr√©er un Data Flow
2. Ajouter une source
3. Ajouter des transformations :
   - **Select** : S√©lectionner colonnes
   - **Filter** : Filtrer des lignes
   - **Derived Column** : Cr√©er des colonnes calcul√©es
   - **Aggregate** : Agr√©gations
   - **Join** : Joindre des donn√©es
4. Ajouter un sink

**Exemple de transformations :**

```
Source (CSV) 
  ‚Üí Select (colonnes)
  ‚Üí Filter (status = 'active')
  ‚Üí Derived Column (nouvelle colonne)
  ‚Üí Aggregate (SUM, COUNT)
  ‚Üí Sink (Parquet)
```

### Lookup

**Rechercher des valeurs dans une autre source**

**Utilisation :**
- Valider des donn√©es
- Enrichir des donn√©es
- V√©rifier des r√©f√©rences

### Stored Procedure

**Ex√©cuter une proc√©dure stock√©e SQL**

**Utilisation :**
- Traitement dans SQL Database
- Logique m√©tier complexe
- Optimisation c√¥t√© base

---

## Int√©gration avec sources de donn√©es

### Azure Blob Storage

**Source de donn√©es :**

```json
{
  "type": "AzureBlobStorage",
  "typeProperties": {
    "connectionString": "...",
    "container": "raw-data"
  }
}
```

### Azure SQL Database

**Source de donn√©es :**

```json
{
  "type": "AzureSqlDatabase",
  "typeProperties": {
    "connectionString": "...",
    "tableName": "users"
  }
}
```

### Azure Data Lake Storage Gen2

**Source de donn√©es :**

```json
{
  "type": "AzureBlobFS",
  "typeProperties": {
    "url": "https://account.dfs.core.windows.net",
    "fileSystem": "data-lake"
  }
}
```

### Fichiers locaux (via Self-hosted IR)

**Integration Runtime :**
- Self-hosted IR pour acc√®s aux fichiers locaux
- Installer sur une machine locale
- Connecter au Data Factory

---

## Orchestration et scheduling

### D√©clencher manuellement

1. Data Factory Studio ‚Üí "Monitor"
2. S√©lectionner le pipeline
3. Cliquer sur "Trigger now"
4. Voir l'ex√©cution en temps r√©el

### Planifier un pipeline (Trigger)

**Cr√©er un trigger :**

1. Pipeline ‚Üí "Add trigger" ‚Üí "New/Edit"
2. Type : "Schedule"
3. Configuration :
   - **Name** : `DailyTrigger`
   - **Type** : Schedule
   - **Recurrence** : Daily
   - **Start time** : 02:00
4. Cliquer sur "OK"

**Types de triggers :**
- **Schedule** : Planifi√© (cron)
- **Event** : D√©clench√© par √©v√©nement
- **Tumbling window** : Fen√™tre glissante

### D√©clencher par √©v√©nement

**Exemple : Nouveau fichier dans Blob Storage**

1. Cr√©er un trigger "Storage event"
2. Configurer :
   - **Storage account** : Votre compte
   - **Container** : `raw-data`
   - **Event type** : Blob created
3. Associer au pipeline

---

## Bonnes pratiques

### Performance

1. **Utiliser Data Flow** pour transformations complexes
2. **Optimiser les activit√©s** pour r√©duire le temps
3. **Utiliser le parall√©lisme** quand possible
4. **Choisir les bonnes r√©gions** pour r√©duire la latence

### Co√ªts

1. **Surveiller les ex√©cutions** dans Monitor
2. **Utiliser les 5 pipelines gratuits** intelligemment
3. **Optimiser les Data Flows** (co√ªteux)
4. **Arr√™ter les pipelines** non utilis√©s

### Organisation

1. **Nommer clairement** les pipelines et activit√©s
2. **Documenter** les transformations
3. **Versionner** avec Git
4. **Tester** avant de publier

### S√©curit√©

1. **Utiliser Key Vault** pour les secrets
2. **Limiter les permissions** des Linked Services
3. **Auditer** les ex√©cutions
4. **Chiffrer** les donn√©es en transit

---

## Exemples pratiques

### Exemple 1 : Pipeline simple CSV ‚Üí Parquet

**Pipeline :**
1. Source : Azure Blob Storage (CSV)
2. Activity : Copy Data
3. Sink : Azure Blob Storage (Parquet)

**Configuration :**
- Source : `raw-data/data.csv`
- Sink : `processed-data/data.parquet`
- Format : DelimitedText ‚Üí Parquet

### Exemple 2 : Pipeline avec transformation

**Pipeline :**
1. Source : Azure SQL Database
2. Data Flow :
   - Select colonnes
   - Filter lignes
   - Aggregate
3. Sink : Azure Blob Storage (Parquet)

### Exemple 3 : Pipeline orchestr√©

**Pipeline :**
1. Lookup : V√©rifier si nouvelles donn√©es
2. If Condition : Si nouvelles donn√©es
3. Copy Data : Copier vers staging
4. Data Flow : Transformer
5. Copy Data : Charger dans destination

---

## üìä Points cl√©s √† retenir

1. **Data Factory = ETL cloud** g√©r√© par Microsoft
2. **Free Tier : 5 pipelines** gratuits
3. **Pipelines** orchestrent les activit√©s
4. **Data Flows** pour transformations complexes
5. **Triggers** permettent l'automatisation

## üîó Prochain module

Passer au module [4. Azure SQL Database - Base de donn√©es](../04-sql-database/README.md) pour apprendre √† utiliser SQL Database sur Azure.

