# 2. Azure Storage - Przechowywanie danych

## ğŸ¯ Cele

- ZrozumieÄ‡ Azure Storage i jego uÅ¼ycie
- TworzyÄ‡ konta przechowywania
- UÅ¼ywaÄ‡ Blob Storage i Data Lake Storage
- PrzesyÅ‚aÄ‡ i zarzÄ…dzaÄ‡ plikami
- OrganizowaÄ‡ dane

## ğŸ“‹ Spis treÅ›ci

1. [Wprowadzenie do Azure Storage](#wprowadzenie-do-azure-storage)
2. [UtworzyÄ‡ konto przechowywania](#utworzyÄ‡-konto-przechowywania)
3. [Blob Storage](#blob-storage)
4. [Data Lake Storage Gen2](#data-lake-storage-gen2)
5. [PrzesyÅ‚aÄ‡ i zarzÄ…dzaÄ‡ plikami](#przesyÅ‚aÄ‡-i-zarzÄ…dzaÄ‡-plikami)
6. [Integracja z innymi usÅ‚ugami](#integracja-z-innymi-usÅ‚ugami)

---

## Wprowadzenie do Azure Storage

### Czym jest Azure Storage?

**Azure Storage** = ZarzÄ…dzana usÅ‚uga przechowywania w chmurze

- **Nieograniczone przechowywanie** : Skalowalne wedÅ‚ug potrzeb
- **Wysoka dostÄ™pnoÅ›Ä‡** : 99.99% dostÄ™pnoÅ›ci
- **Bezpieczne** : Szyfrowanie domyÅ›lnie
- **Integracja** : Ze wszystkimi usÅ‚ugami Azure

### Typy przechowywania

1. **Blob Storage** : Pliki (CSV, JSON, Parquet, itp.)
2. **Data Lake Storage Gen2** : Data Lake z hierarchicznym systemem plikÃ³w
3. **File Storage** : UdziaÅ‚y plikÃ³w
4. **Queue Storage** : Kolejki
5. **Table Storage** : Przechowywanie NoSQL

### Azure Storage Free Tier

**Darmowe 12 miesiÄ™cy :**
- 5 GB przechowywania Blob
- 5 GB przechowywania File
- 5 GB przechowywania Table
- 5 GB przechowywania Queue

**Darmowe na zawsze :**
- 200 GB transferu danych wychodzÄ…cych/miesiÄ…c

**âš ï¸ WaÅ¼ne :** Poza tymi limitami, normalne rozliczanie.

---

## UtworzyÄ‡ konto przechowywania

### Krok 1 : DostÄ™p do Azure Storage

1. Portal Azure â†’ SzukaÄ‡ "Storage accounts"
2. KliknÄ…Ä‡ "Storage accounts"
3. KliknÄ…Ä‡ "Create"

### Krok 2 : Podstawowa konfiguracja

**Podstawowe informacje :**
- **Subscription** : WybraÄ‡ subskrypcjÄ™
- **Resource group** : UtworzyÄ‡ lub uÅ¼yÄ‡ istniejÄ…cego
- **Storage account name** : Nazwa unikalna globalnie (np. `mydataanalyststorage`)
- **Region** : WybraÄ‡ najbliÅ¼szy region (np. `France Central`)

**Opcje wydajnoÅ›ci :**
- **Performance** : Standard (zalecane do rozpoczÄ™cia)
- **Redundancy** : LRS (Locally Redundant Storage) - najtaÅ„sze

### Krok 3 : Opcje zaawansowane

**BezpieczeÅ„stwo :**
- **Secure transfer required** : âœ… WÅ‚Ä…czyÄ‡ (zalecane)
- **Allow Blob public access** : âŒ WyÅ‚Ä…czyÄ‡ (bezpieczeÅ„stwo)

**Data Lake Storage Gen2 :**
- **Hierarchical namespace** : âœ… WÅ‚Ä…czyÄ‡ jeÅ›li potrzeba Data Lake

### Krok 4 : UtworzyÄ‡ konto

1. KliknÄ…Ä‡ "Review + create"
2. SprawdziÄ‡ konfiguracjÄ™
3. KliknÄ…Ä‡ "Create"
4. CzekaÄ‡ na utworzenie (1-2 minuty)

**âš ï¸ WaÅ¼ne :** ZanotowaÄ‡ nazwÄ™ konta przechowywania.

---

## Blob Storage

### Czym jest Blob Storage?

**Blob Storage** = Przechowywanie obiektÃ³w dla plikÃ³w

- **Containers** : OrganizujÄ… pliki (jak foldery)
- **Blobs** : Pojedyncze pliki
- **Typy** : Block blobs, Page blobs, Append blobs

### UtworzyÄ‡ container

**Przez portal Azure :**

1. Storage account â†’ "Containers"
2. KliknÄ…Ä‡ "+ Container"
3. Nazwa : `raw-data` (lub inna)
4. Poziom dostÄ™pu publicznego : Private (zalecane)
5. KliknÄ…Ä‡ "Create"

**Przez Azure CLI :**

```bash
az storage container create \
  --name raw-data \
  --account-name mydataanalyststorage \
  --auth-mode login
```

**Przez Python :**

```python
from azure.storage.blob import BlobServiceClient

# PoÅ‚Ä…czenie
connection_string = "DefaultEndpointsProtocol=https;AccountName=..."
blob_service_client = BlobServiceClient.from_connection_string(connection_string)

# UtworzyÄ‡ container
container_client = blob_service_client.create_container("raw-data")
```

### Typy blobÃ³w

**Block Blobs :**
- Pliki (CSV, JSON, Parquet, obrazy, itp.)
- Do 4.75 TB na blob
- Zalecane dla wiÄ™kszoÅ›ci przypadkÃ³w

**Page Blobs :**
- Dyski wirtualne
- Do 8 TB

**Append Blobs :**
- Logi
- Dane tylko do dodawania

---

## Data Lake Storage Gen2

### Czym jest Data Lake Storage Gen2?

**Data Lake Storage Gen2** = Blob Storage + hierarchiczny system plikÃ³w

- **Zgodne z Blob Storage** : UÅ¼ywa tych samych API
- **System plikÃ³w** : Organizacja hierarchiczna
- **Zoptymalizowane Big Data** : Dla analytics i ML
- **Integracja** : Z Azure Synapse, Databricks, itp.

### WÅ‚Ä…czyÄ‡ Data Lake Storage Gen2

**Podczas tworzenia konta :**
1. W "Advanced" â†’ WÅ‚Ä…czyÄ‡ "Hierarchical namespace"
2. UtworzyÄ‡ konto

**âš ï¸ WaÅ¼ne :** Nie moÅ¼na wÅ‚Ä…czyÄ‡ po utworzeniu.

### Struktura Data Lake

```
data-lake/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ 2024/
â”‚   â”‚   â”œâ”€â”€ 01/
â”‚   â”‚   â””â”€â”€ 02/
â”œâ”€â”€ processed/
â”‚   â””â”€â”€ 2024/
â””â”€â”€ analytics/
    â””â”€â”€ results/
```

### TworzyÄ‡ pliki i foldery

**Przez portal Azure :**

1. Storage account â†’ "Data Lake"
2. NawigowaÄ‡ w strukturze
3. PrzesyÅ‚aÄ‡ pliki
4. TworzyÄ‡ foldery

**Przez Python :**

```python
from azure.storage.filedatalake import DataLakeServiceClient

# PoÅ‚Ä…czenie
account_name = "mydataanalyststorage"
account_key = "..."
datalake_service_client = DataLakeServiceClient(
    account_url=f"https://{account_name}.dfs.core.windows.net",
    credential=account_key
)

# UtworzyÄ‡ system plikÃ³w
file_system_client = datalake_service_client.create_file_system("data-lake")

# UtworzyÄ‡ katalog
directory_client = file_system_client.create_directory("raw/2024")
```

---

## PrzesyÅ‚aÄ‡ i zarzÄ…dzaÄ‡ plikami

### PrzesÅ‚aÄ‡ plik

**Przez portal Azure :**

1. Container â†’ "Upload"
2. WybraÄ‡ plik
3. KliknÄ…Ä‡ "Upload"

**Przez Azure CLI :**

```bash
az storage blob upload \
  --account-name mydataanalyststorage \
  --container-name raw-data \
  --name data.csv \
  --file ./local-data.csv \
  --auth-mode login
```

**Przez Python :**

```python
from azure.storage.blob import BlobServiceClient

blob_service_client = BlobServiceClient.from_connection_string(connection_string)
container_client = blob_service_client.get_container_client("raw-data")

# PrzesÅ‚aÄ‡ plik
with open("local-data.csv", "rb") as data:
    container_client.upload_blob(name="data.csv", data=data)
```

### PobraÄ‡ plik

**Przez Python :**

```python
# PobraÄ‡ blob
blob_client = container_client.get_blob_client("data.csv")
with open("downloaded-data.csv", "wb") as download_file:
    download_file.write(blob_client.download_blob().readall())
```

### ListowaÄ‡ pliki

**Przez Python :**

```python
# ListowaÄ‡ wszystkie bloby w kontenerze
blob_list = container_client.list_blobs()
for blob in blob_list:
    print(f"Name: {blob.name}, Size: {blob.size}")
```

### UsunÄ…Ä‡ plik

**Przez Python :**

```python
# UsunÄ…Ä‡ blob
blob_client = container_client.get_blob_client("data.csv")
blob_client.delete_blob()
```

---

## Integracja z innymi usÅ‚ugami

### Azure Storage + Data Factory

**UÅ¼ycie :**
- Å¹rÃ³dÅ‚o danych dla pipeline'Ã³w ETL
- Miejsce docelowe dla przeksztaÅ‚conych danych

**PrzykÅ‚ad :**
```json
{
  "type": "AzureBlobStorage",
  "typeProperties": {
    "connectionString": "...",
    "container": "raw-data"
  }
}
```

### Azure Storage + Azure SQL Database

**UÅ¼ycie :**
- ImportowaÄ‡ dane z Blob Storage
- EksportowaÄ‡ dane do Blob Storage

**PrzykÅ‚ad SQL :**
```sql
-- ImportowaÄ‡ z Blob Storage
BULK INSERT my_table
FROM 'https://mystorageaccount.blob.core.windows.net/raw-data/data.csv'
WITH (
    FORMAT = 'CSV',
    FIRSTROW = 2
);
```

### Azure Storage + PowerBI

**UÅ¼ycie :**
- PoÅ‚Ä…czyÄ‡ PowerBI z Blob Storage
- AnalizowaÄ‡ pliki bezpoÅ›rednio

**Konfiguracja :**
1. PowerBI â†’ "Get Data"
2. "Azure Blob Storage"
3. WprowadziÄ‡ URL kontenera
4. WybraÄ‡ pliki

### Azure Storage + Azure Functions

**UÅ¼ycie :**
- WyzwalaÄ‡ Functions przy przesÅ‚aniu
- Automatycznie przetwarzaÄ‡ pliki

**Konfiguracja :**
1. Function â†’ "Add trigger"
2. "Azure Blob Storage trigger"
3. SkonfigurowaÄ‡ kontener i Å›cieÅ¼kÄ™

---

## Dobre praktyki

### Organizacja

1. **UÅ¼ywaÄ‡ kontenerÃ³w** aby organizowaÄ‡ wedÅ‚ug projektu
2. **NazywaÄ‡ jasno** pliki i kontenery
3. **OrganizowaÄ‡ wedÅ‚ug daty** : `raw/2024/01/data.csv`
4. **RozdzielaÄ‡ wedÅ‚ug typu** : `raw/`, `processed/`, `analytics/`

### WydajnoÅ›Ä‡

1. **UÅ¼ywaÄ‡ losowych nazw** dla blobÃ³w (unikaÄ‡ sekwencji)
2. **WÅ‚Ä…czyÄ‡ CDN** jeÅ›li potrzeba globalnej dystrybucji (pÅ‚atne)
3. **UÅ¼ywaÄ‡ blobÃ³w blokowych** dla wiÄ™kszoÅ›ci przypadkÃ³w
4. **PartycjonowaÄ‡ dane** aby poprawiÄ‡ wydajnoÅ›Ä‡

### Koszty

1. **MonitorowaÄ‡ uÅ¼ycie** w Azure Cost Management
2. **UsuwaÄ‡ niepotrzebne pliki**
3. **UÅ¼ywaÄ‡ odpowiednich klas** przechowywania
4. **KonfigurowaÄ‡ reguÅ‚y cyklu Å¼ycia** aby automatyzowaÄ‡

### BezpieczeÅ„stwo

1. **Nigdy nie udostÄ™pniaÄ‡ publicznie** kontenerÃ³w (oprÃ³cz konkretnej potrzeby)
2. **UÅ¼ywaÄ‡ SAS (Shared Access Signature)** dla tymczasowego dostÄ™pu
3. **WÅ‚Ä…czyÄ‡ szyfrowanie** domyÅ›lnie
4. **UÅ¼ywaÄ‡ Azure AD** do uwierzytelniania

---

## PrzykÅ‚ady praktyczne

### PrzykÅ‚ad 1 : PrzesÅ‚aÄ‡ plik CSV

```python
from azure.storage.blob import BlobServiceClient
import pandas as pd

# PoÅ‚Ä…czenie
connection_string = "DefaultEndpointsProtocol=https;AccountName=..."
blob_service_client = BlobServiceClient.from_connection_string(connection_string)
container_client = blob_service_client.get_container_client("raw-data")

# CzytaÄ‡ lokalny plik
df = pd.read_csv("local-data.csv")

# PrzesÅ‚aÄ‡ do Azure Storage
with open("local-data.csv", "rb") as data:
    container_client.upload_blob(name="2024/01/data.csv", data=data)
```

### PrzykÅ‚ad 2 : PobraÄ‡ i przetworzyÄ‡

```python
# PobraÄ‡ z Azure Storage
blob_client = container_client.get_blob_client("2024/01/data.csv")
with open("downloaded-data.csv", "wb") as download_file:
    download_file.write(blob_client.download_blob().readall())

# PrzetworzyÄ‡
df = pd.read_csv("downloaded-data.csv")
# ... przetwarzanie ...

# PrzesÅ‚aÄ‡ wynik
df.to_csv("processed-data.csv", index=False)
with open("processed-data.csv", "rb") as data:
    container_client.upload_blob(name="processed/2024/01/data.csv", data=data)
```

### PrzykÅ‚ad 3 : ListowaÄ‡ i filtrowaÄ‡

```python
# ListowaÄ‡ wszystkie pliki w prefiksie
blob_list = container_client.list_blobs(name_starts_with="2024/01/")
for blob in blob_list:
    print(f"File: {blob.name}, Size: {blob.size} bytes, Modified: {blob.last_modified}")
```

---

## ğŸ“Š Kluczowe punkty do zapamiÄ™tania

1. **Azure Storage = Nieograniczone przechowywanie** i wysoka dostÄ™pnoÅ›Ä‡
2. **Free Tier : 5 GB** przez 12 miesiÄ™cy
3. **Blob Storage** dla plikÃ³w, **Data Lake Gen2** dla Big Data
4. **OrganizowaÄ‡ z kontenerami** i prefiksami
5. **Natywna integracja** ze wszystkimi usÅ‚ugami Azure data

## ğŸ”— NastÄ™pny moduÅ‚

PrzejdÅº do moduÅ‚u [3. Azure Data Factory - ETL w chmurze](../03-data-factory/README.md), aby nauczyÄ‡ siÄ™ tworzyÄ‡ pipeline'y ETL na Azure.

