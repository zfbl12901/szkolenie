# 6. Azure Databricks - Analiza Big Data

## ğŸ¯ Cele

- ZrozumieÄ‡ Azure Databricks
- UtworzyÄ‡ obszar roboczy Databricks
- UÅ¼ywaÄ‡ notebookÃ³w Python/SQL
- PrzetwarzaÄ‡ dane ze Spark
- IntegrowaÄ‡ z innymi usÅ‚ugami Azure

## ğŸ“‹ Spis treÅ›ci

1. [Wprowadzenie do Databricks](#wprowadzenie-do-databricks)
2. [UtworzyÄ‡ obszar roboczy Databricks](#utworzyÄ‡-obszar-roboczy-databricks)
3. [UtworzyÄ‡ klaster](#utworzyÄ‡-klaster)
4. [Notebooki Python/SQL](#notebooki-pythonsql)
5. [Przetwarzanie danych ze Spark](#przetwarzanie-danych-ze-spark)
6. [Integracja z innymi usÅ‚ugami](#integracja-z-innymi-usÅ‚ugami)

---

## Wprowadzenie do Databricks

### Czym jest Azure Databricks?

**Azure Databricks** = Platforma Big Data oparta na Apache Spark

- **Apache Spark** : Silnik przetwarzania rozproszonego
- **Notebooki** : Python, SQL, Scala, R
- **ZarzÄ…dzane** : Microsoft zarzÄ…dza infrastrukturÄ…
- **Skalowalne** : Klastry auto-scaling

### Przypadki uÅ¼ycia dla Data Analyst

- **Przetwarzanie Big Data** : PrzetwarzaÄ‡ duÅ¼e iloÅ›ci
- **ETL** : ZÅ‚oÅ¼one przeksztaÅ‚cenia
- **Machine Learning** : Zintegrowany MLlib
- **Data Science** : Interaktywne notebooki

### Databricks Free Tier

**Darmowe z kredytem Azure :**
- UÅ¼ywaÄ‡ 200$ darmowego kredytu (30 dni)
- Po tym : normalne rozliczanie

**âš ï¸ WaÅ¼ne :** Databricks moÅ¼e byÄ‡ kosztowne. MonitorowaÄ‡ koszty uwaÅ¼nie.

---

## UtworzyÄ‡ obszar roboczy Databricks

### Krok 1 : DostÄ™p do Databricks

1. Portal Azure â†’ SzukaÄ‡ "Azure Databricks"
2. KliknÄ…Ä‡ "Azure Databricks"
3. KliknÄ…Ä‡ "Create"

### Krok 2 : Podstawowa konfiguracja

**Podstawowe informacje :**
- **Subscription** : WybraÄ‡ subskrypcjÄ™
- **Resource group** : UtworzyÄ‡ lub uÅ¼yÄ‡ istniejÄ…cego
- **Workspace name** : `my-databricks-workspace`
- **Region** : WybraÄ‡ region
- **Pricing tier** : Standard (lub Premium)

**Networking :**
- **Virtual network** : UtworzyÄ‡ nowy lub uÅ¼yÄ‡ istniejÄ…cego
- **Public IP** : âœ… WÅ‚Ä…czyÄ‡ (dla Å‚atwego dostÄ™pu)

### Krok 3 : UtworzyÄ‡ obszar roboczy

1. KliknÄ…Ä‡ "Review + create"
2. SprawdziÄ‡ konfiguracjÄ™
3. KliknÄ…Ä‡ "Create"
4. CzekaÄ‡ na utworzenie (5-10 minut)

**âš ï¸ WaÅ¼ne :** ZanotowaÄ‡ URL obszaru roboczego.

### Krok 4 : OtworzyÄ‡ Databricks

1. Po utworzeniu, kliknÄ…Ä‡ "Launch Workspace"
2. Interfejs web Databricks
3. ZalogowaÄ‡ siÄ™ z Azure AD

---

## UtworzyÄ‡ klaster

### Krok 1 : DostÄ™p do klastrÃ³w

1. Databricks Workspace â†’ "Compute"
2. KliknÄ…Ä‡ "Create Cluster"

### Krok 2 : Konfiguracja klastra

**Podstawowa konfiguracja :**
- **Cluster name** : `my-cluster`
- **Cluster mode** : Standard (lub Single Node dla testÃ³w)
- **Databricks runtime version** : Latest LTS (zalecane)
- **Python version** : 3.11

**Typ wÄ™zÅ‚a :**
- **Worker type** : Standard_DS3_v2 (do rozpoczÄ™cia)
- **Driver type** : Standard_DS3_v2
- **Min workers** : 0 (aby oszczÄ™dziÄ‡)
- **Max workers** : 2 (do rozpoczÄ™cia)

**âš ï¸ WaÅ¼ne :** Min workers = 0 umoÅ¼liwia auto-termination gdy nieaktywny.

### Krok 3 : Opcje zaawansowane

**Auto-termination :**
- âœ… WÅ‚Ä…czyÄ‡ (zatrzymuje klaster po nieaktywnoÅ›ci)
- **Terminate after** : 30 minut

**Tagi :**
- DodaÄ‡ tagi do organizacji

### Krok 4 : UtworzyÄ‡ klaster

1. KliknÄ…Ä‡ "Create Cluster"
2. CzekaÄ‡ na uruchomienie (3-5 minut)
3. Klaster gotowy gdy status = "Running"

**âš ï¸ WaÅ¼ne :** Klaster zuÅ¼ywa zasoby nawet nieaktywny. ZatrzymaÄ‡ gdy nieuÅ¼ywany.

---

## Notebooki Python/SQL

### UtworzyÄ‡ notebook

**Krok 1 : UtworzyÄ‡ notebook**

1. Databricks Workspace â†’ "Workspace"
2. Klik prawy â†’ "Create" â†’ "Notebook"
3. Nazwa : `data-processing`
4. JÄ™zyk : Python (lub SQL)
5. Klaster : DoÅ‚Ä…czyÄ‡ do utworzonego klastra

### Krok 2 : UÅ¼ywaÄ‡ notebook

**KomÃ³rki Python :**

```python
# KomÃ³rka 1 : ImportowaÄ‡ biblioteki
import pandas as pd
from pyspark.sql import SparkSession

# KomÃ³rka 2 : UtworzyÄ‡ sesjÄ™ Spark
spark = SparkSession.builder.appName("DataProcessing").getOrCreate()

# KomÃ³rka 3 : CzytaÄ‡ dane
df = spark.read.csv("dbfs:/FileStore/data/users.csv", header=True, inferSchema=True)

# KomÃ³rka 4 : WyÅ›wietliÄ‡ dane
df.show()

# KomÃ³rka 5 : PrzeksztaÅ‚ciÄ‡
df_filtered = df.filter(df["status"] == "active")
df_filtered.show()
```

**KomÃ³rki SQL :**

```sql
-- KomÃ³rka SQL : UtworzyÄ‡ widok tymczasowy
CREATE OR REPLACE TEMPORARY VIEW users AS
SELECT * FROM csv.`dbfs:/FileStore/data/users.csv`

-- Zapytanie SQL
SELECT 
    YEAR(created_at) AS year,
    COUNT(*) AS user_count
FROM users
GROUP BY YEAR(created_at)
ORDER BY year;
```

### WykonaÄ‡ notebook

- **Run cell** : WykonaÄ‡ komÃ³rkÄ™
- **Run all** : WykonaÄ‡ wszystkie komÃ³rki
- **Run all above** : WykonaÄ‡ wszystkie komÃ³rki powyÅ¼ej

---

## Przetwarzanie danych ze Spark

### CzytaÄ‡ dane

**Z Data Lake Storage :**

```python
# CzytaÄ‡ CSV
df = spark.read.csv(
    "abfss://container@account.dfs.core.windows.net/data/users.csv",
    header=True,
    inferSchema=True
)

# CzytaÄ‡ Parquet
df = spark.read.parquet(
    "abfss://container@account.dfs.core.windows.net/data/users.parquet"
)

# CzytaÄ‡ JSON
df = spark.read.json(
    "abfss://container@account.dfs.core.windows.net/data/users.json"
)
```

**Z Azure Blob Storage :**

```python
# SkonfigurowaÄ‡ dostÄ™p
spark.conf.set(
    "fs.azure.account.key.accountname.blob.core.windows.net",
    "your-account-key"
)

# CzytaÄ‡
df = spark.read.csv(
    "wasbs://container@accountname.blob.core.windows.net/data/users.csv",
    header=True
)
```

### PrzeksztaÅ‚caÄ‡ dane

**FiltrowaÄ‡ :**

```python
df_filtered = df.filter(df["age"] > 18)
```

**WybieraÄ‡ kolumny :**

```python
df_selected = df.select("id", "name", "email")
```

**Agregacje :**

```python
df_aggregated = df.groupBy("category").agg({
    "amount": "sum",
    "id": "count"
})
```

**ÅÄ…czyÄ‡ :**

```python
df_joined = df1.join(df2, df1.id == df2.user_id, "inner")
```

### ZapisywaÄ‡ dane

**Do Data Lake Storage :**

```python
# ZapisaÄ‡ w Parquet
df.write.mode("overwrite").parquet(
    "abfss://container@account.dfs.core.windows.net/processed/users.parquet"
)

# ZapisaÄ‡ w CSV
df.write.mode("overwrite").csv(
    "abfss://container@account.dfs.core.windows.net/processed/users.csv"
)
```

---

## Integracja z innymi usÅ‚ugami

### Databricks + Data Lake Storage

**BezpoÅ›redni dostÄ™p :**

```python
# SkonfigurowaÄ‡ dostÄ™p
spark.conf.set(
    "fs.azure.account.auth.type.account.dfs.core.windows.net",
    "OAuth"
)
spark.conf.set(
    "fs.azure.account.oauth.provider.type.account.dfs.core.windows.net",
    "org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider"
)

# CzytaÄ‡
df = spark.read.parquet(
    "abfss://container@account.dfs.core.windows.net/data/users.parquet"
)
```

### Databricks + Azure SQL Database

**CzytaÄ‡ z SQL Database :**

```python
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:sqlserver://server.database.windows.net:1433;database=db") \
    .option("dbtable", "users") \
    .option("user", "sqladmin") \
    .option("password", "password") \
    .load()
```

**ZapisaÄ‡ do SQL Database :**

```python
df.write \
    .format("jdbc") \
    .option("url", "jdbc:sqlserver://server.database.windows.net:1433;database=db") \
    .option("dbtable", "users_processed") \
    .option("user", "sqladmin") \
    .option("password", "password") \
    .mode("overwrite") \
    .save()
```

### Databricks + Data Factory

**Pipeline Data Factory :**
1. Å¹rÃ³dÅ‚o : Azure Blob Storage
2. DziaÅ‚anie : Databricks Notebook
3. Sink : Azure SQL Database

**Konfiguracja :**
- Notebook path : `/Workspace/path/to/notebook`
- Parameters : PrzekazaÄ‡ parametry

---

## Dobre praktyki

### WydajnoÅ›Ä‡

1. **UÅ¼ywaÄ‡ cache** aby ponownie uÅ¼ywaÄ‡ danych
2. **PartycjonowaÄ‡** dane aby poprawiÄ‡ wydajnoÅ›Ä‡
3. **OptymalizowaÄ‡ przeksztaÅ‚cenia** aby zmniejszyÄ‡ czas
4. **UÅ¼ywaÄ‡ odpowiedniej liczby workers**

### Koszty

1. **ZatrzymywaÄ‡ klastry** gdy nieuÅ¼ywane
2. **UÅ¼ywaÄ‡ auto-termination** aby oszczÄ™dziÄ‡
3. **MonitorowaÄ‡ koszty** w Azure Cost Management
4. **UÅ¼ywaÄ‡ mniejszych klastrÃ³w** do rozpoczÄ™cia

### Organizacja

1. **OrganizowaÄ‡ notebooki** w folderach
2. **NazywaÄ‡ jasno** notebooki i klastry
3. **DokumentowaÄ‡** kod
4. **WersjonowaÄ‡** z Git

### BezpieczeÅ„stwo

1. **UÅ¼ywaÄ‡ Azure AD** do uwierzytelniania
2. **OgraniczaÄ‡ dostÄ™p** z RBAC
3. **SzyfrowaÄ‡ dane** w tranzycie i w spoczynku
4. **AudytowaÄ‡** dostÄ™p

---

## PrzykÅ‚ady praktyczne

### PrzykÅ‚ad 1 : Kompletny pipeline ETL

**Notebook Databricks :**

```python
# 1. CzytaÄ‡ z Data Lake
df = spark.read.parquet(
    "abfss://container@account.dfs.core.windows.net/raw/users.parquet"
)

# 2. PrzeksztaÅ‚ciÄ‡
df_processed = df \
    .filter(df["status"] == "active") \
    .select("id", "name", "email", "created_at") \
    .withColumn("year", year(col("created_at")))

# 3. ZapisaÄ‡ do Data Lake
df_processed.write.mode("overwrite").parquet(
    "abfss://container@account.dfs.core.windows.net/processed/users.parquet"
)
```

### PrzykÅ‚ad 2 : Analiza ze Spark SQL

```python
# UtworzyÄ‡ widok tymczasowy
df.createOrReplaceTempView("users")

# Zapytanie SQL
result = spark.sql("""
    SELECT 
        YEAR(created_at) AS year,
        COUNT(*) AS user_count,
        COUNT(DISTINCT email) AS unique_emails
    FROM users
    GROUP BY YEAR(created_at)
    ORDER BY year
""")

result.show()
```

### PrzykÅ‚ad 3 : Integracja z Data Factory

1. UtworzyÄ‡ notebook Databricks
2. W Data Factory, dodaÄ‡ dziaÅ‚anie "Databricks Notebook"
3. SkonfigurowaÄ‡ notebook
4. WykonaÄ‡ pipeline

---

## ğŸ“Š Kluczowe punkty do zapamiÄ™tania

1. **Databricks = Big Data** z Apache Spark
2. **Notebooki** Python/SQL do rozwoju
3. **Klastry auto-scaling** dla wydajnoÅ›ci
4. **Natywna integracja** z usÅ‚ugami Azure
5. **PÅ‚atne** : MonitorowaÄ‡ koszty

## ğŸ”— NastÄ™pny moduÅ‚

PrzejdÅº do moduÅ‚u [7. Projekty praktyczne](../07-projets/README.md), aby tworzyÄ‡ kompletne projekty z Azure.

