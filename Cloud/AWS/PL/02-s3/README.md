# 2. Amazon S3 - Przechowywanie danych

## ğŸ¯ Cele

- ZrozumieÄ‡ Amazon S3 i jego uÅ¼ycie
- TworzyÄ‡ i zarzÄ…dzaÄ‡ bucketami S3
- PrzesyÅ‚aÄ‡ i organizowaÄ‡ pliki
- ZrozumieÄ‡ klasy przechowywania
- IntegrowaÄ‡ S3 z innymi usÅ‚ugami AWS

## ğŸ“‹ Spis treÅ›ci

1. [Wprowadzenie do S3](#wprowadzenie-do-s3)
2. [UtworzyÄ‡ bucket S3](#utworzyÄ‡-bucket-s3)
3. [PrzesyÅ‚aÄ‡ i zarzÄ…dzaÄ‡ plikami](#przesyÅ‚aÄ‡-i-zarzÄ…dzaÄ‡-plikami)
4. [Klasy przechowywania](#klasy-przechowywania)
5. [Organizacja danych](#organizacja-danych)
6. [Integracja z innymi usÅ‚ugami](#integracja-z-innymi-usÅ‚ugami)

---

## Wprowadzenie do S3

### Czym jest Amazon S3?

**Amazon S3** (Simple Storage Service) = UsÅ‚uga przechowywania obiektÃ³w

- Nieograniczone przechowywanie
- Wysoka dostÄ™pnoÅ›Ä‡ (99.99%)
- Bezpieczne domyÅ›lnie
- Integracja ze wszystkimi usÅ‚ugami AWS

### Przypadki uÅ¼ycia dla Data Analyst

- **Data Lake** : PrzechowywaÄ‡ surowe dane
- **Backup** : TworzyÄ‡ kopie zapasowe danych
- **ETL** : Å¹rÃ³dÅ‚o/destynacja dla pipeline'Ã³w
- **Analytics** : Dane dla Athena, Redshift
- **Archiwizacja** : Dane historyczne

### Free Tier S3

**Darmowe na zawsze :**
- 5 GB standardowego przechowywania
- 20 000 Å¼Ä…daÅ„ GET
- 2 000 Å¼Ä…daÅ„ PUT
- 15 GB transferu danych wychodzÄ…cych

**âš ï¸ WaÅ¼ne :** Poza tymi limitami, normalne rozliczanie.

---

## UtworzyÄ‡ bucket S3

### Krok 1 : DostÄ™p do S3

1. Konsola AWS â†’ SzukaÄ‡ "S3"
2. KliknÄ…Ä‡ "Amazon S3"
3. KliknÄ…Ä‡ "Create bucket"

### Krok 2 : Konfiguracja bucketa

**Podstawowe informacje :**
- **Bucket name** : Nazwa unikalna globalnie (np. `my-data-analyst-bucket`)
- **Region** : WybraÄ‡ najbliÅ¼szy region (np. `eu-west-3` ParyÅ¼)

**Opcje konfiguracji :**

1. **Object Ownership**
   - "ACLs disabled" (zalecane)
   - "Bucket owner enforced"

2. **Block Public Access**
   - âœ… **Wszystko wÅ‚Ä…czyÄ‡** (bezpieczeÅ„stwo domyÅ›lne)
   - WyÅ‚Ä…czyÄ‡ tylko w przypadku konkretnej potrzeby

3. **Versioning**
   - WyÅ‚Ä…czone domyÅ›lnie (darmowe)
   - WÅ‚Ä…czyÄ‡ jeÅ›li potrzeba wielu wersji

4. **Tags** (opcjonalne)
   - DodaÄ‡ tagi do organizacji
   - Np. `Project: Data-Analyst-Training`

5. **Default encryption**
   - âœ… WÅ‚Ä…czyÄ‡ (zalecane)
   - "Amazon S3 managed keys (SSE-S3)" (darmowe)

### Krok 3 : UtworzyÄ‡ bucket

1. KliknÄ…Ä‡ "Create bucket"
2. Bucket utworzony i widoczny na liÅ›cie
3. Gotowy do uÅ¼ycia

**âš ï¸ WaÅ¼ne :** Nazwa bucketa musi byÄ‡ unikalna globalnie w AWS.

---

## PrzesyÅ‚aÄ‡ i zarzÄ…dzaÄ‡ plikami

### PrzesÅ‚aÄ‡ plik

**Metoda 1 : Interfejs web**

1. KliknÄ…Ä‡ nazwÄ™ bucketa
2. KliknÄ…Ä‡ "Upload"
3. "Add files" lub "Add folder"
4. WybraÄ‡ pliki
5. KliknÄ…Ä‡ "Upload"

**Metoda 2 : AWS CLI**

```bash
# ZainstalowaÄ‡ AWS CLI (jeÅ›li jeszcze nie)
# Windows: https://aws.amazon.com/cli/
# Linux/Mac: pip install awscli

# SkonfigurowaÄ‡ credentials
aws configure

# PrzesÅ‚aÄ‡ plik
aws s3 cp local-file.csv s3://my-data-analyst-bucket/data/
```

**Metoda 3 : SDK Python (boto3)**

```python
import boto3

# UtworzyÄ‡ klienta S3
s3 = boto3.client('s3')

# PrzesÅ‚aÄ‡ plik
s3.upload_file('local-file.csv', 'my-data-analyst-bucket', 'data/file.csv')
```

### PobraÄ‡ plik

**Interfejs web :**
1. KliknÄ…Ä‡ plik
2. KliknÄ…Ä‡ "Download"

**AWS CLI :**
```bash
aws s3 cp s3://my-data-analyst-bucket/data/file.csv local-file.csv
```

**Python :**
```python
s3.download_file('my-data-analyst-bucket', 'data/file.csv', 'local-file.csv')
```

### ZarzÄ…dzaÄ‡ plikami

**DostÄ™pne akcje :**
- **Download** : PobieraÄ‡
- **Open** : OtwieraÄ‡ w przeglÄ…darce
- **Copy** : KopiowaÄ‡ do innej lokalizacji
- **Move** : PrzenosiÄ‡
- **Delete** : UsuwaÄ‡
- **Make public** : UczyniÄ‡ publicznym (uwaga bezpieczeÅ„stwo)

---

## Klasy przechowywania

### S3 Standard (domyÅ›lne)

**UÅ¼ycie :**
- Dane czÄ™sto dostÄ™pne
- Aplikacje produkcyjne

**Charakterystyka :**
- Szybki dostÄ™p
- 99.99% dostÄ™pnoÅ›ci
- Koszt : ~0.023$ za GB/miesiÄ…c

**Free Tier :** 5 GB darmowe

### S3 Intelligent-Tiering

**UÅ¼ycie :**
- Dane ze zmiennym dostÄ™pem
- Automatyczna optymalizacja kosztÃ³w

**Charakterystyka :**
- Automatycznie przenosi miÄ™dzy klasami
- Brak opÅ‚at za odzyskiwanie
- Koszt : ~0.023$ za GB/miesiÄ…c

### S3 Standard-IA (Infrequent Access)

**UÅ¼ycie :**
- Dane rzadko dostÄ™pne
- Backup, archiwa

**Charakterystyka :**
- Szybki dostÄ™p gdy potrzebny
- Koszt przechowywania : ~0.0125$ za GB/miesiÄ…c
- Koszt odzyskiwania : ~0.01$ za GB

### S3 One Zone-IA

**UÅ¼ycie :**
- Dane reprodukowalne
- Backup drugorzÄ™dny

**Charakterystyka :**
- Przechowywanie w jednej strefie
- Koszt : ~0.01$ za GB/miesiÄ…c
- âš ï¸ Ryzyko utraty jeÅ›li strefa ulegnie awarii

### S3 Glacier

**UÅ¼ycie :**
- Archiwizacja dÅ‚ugoterminowa
- Dane rzadko potrzebne

**Charakterystyka :**
- Odzyskiwanie : 1-5 minut do kilku godzin
- Koszt : ~0.004$ za GB/miesiÄ…c
- OpÅ‚aty za odzyskiwanie wedÅ‚ug prÄ™dkoÅ›ci

### WybraÄ‡ klasÄ™ przechowywania

**Dla Data Analyst :**
- **S3 Standard** : Dane aktywne (czÄ™ste analizy)
- **S3 Standard-IA** : Dane historyczne (okazjonalne analizy)
- **S3 Glacier** : Archiwa (rzadko uÅ¼ywane)

**Automatyczne przejÅ›cie :**
- SkonfigurowaÄ‡ reguÅ‚y przejÅ›cia
- PrzykÅ‚ad : Standard â†’ Standard-IA po 30 dniach

---

## Organizacja danych

### Zalecana struktura

**Organizacja wedÅ‚ug projektu :**
```
bucket-name/
â”œâ”€â”€ raw/              # Dane surowe
â”‚   â”œâ”€â”€ 2024/
â”‚   â”‚   â”œâ”€â”€ 01/
â”‚   â”‚   â”œâ”€â”€ 02/
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ processed/        # Dane przeksztaÅ‚cone
â”‚   â”œâ”€â”€ 2024/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ analytics/        # Dane do analizy
â”‚   â””â”€â”€ ...
â””â”€â”€ archive/          # Archiwa
    â””â”€â”€ ...
```

**Organizacja wedÅ‚ug typu :**
```
bucket-name/
â”œâ”€â”€ csv/
â”œâ”€â”€ json/
â”œâ”€â”€ parquet/
â””â”€â”€ logs/
```

### Prefiksy i foldery

**S3 nie ma "prawdziwych" folderÃ³w**, ale uÅ¼ywa prefiksÃ³w :

- `data/2024/01/file.csv` = Prefiks `data/2024/01/`
- Interfejs web symuluje foldery
- UÅ¼ywaÄ‡ `/` do organizacji

**Dobre praktyki :**
- UÅ¼ywaÄ‡ spÃ³jnych prefiksÃ³w
- UwzglÄ™dniaÄ‡ datÄ™ w Å›cieÅ¼ce
- RozdzielaÄ‡ wedÅ‚ug typu danych

---

## Integracja z innymi usÅ‚ugami

### S3 + AWS Glue

**UÅ¼ycie :**
- S3 jako ÅºrÃ³dÅ‚o danych
- Glue przeksztaÅ‚ca dane
- Wynik do S3 lub innej destynacji

**PrzykÅ‚ad :**
```python
# Job Glue czyta z S3
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "my_database",
    table_name = "s3_data"
)
```

### S3 + Amazon Athena

**UÅ¼ycie :**
- Zapytania SQL bezpoÅ›rednio na plikach S3
- Nie potrzeba Å‚adowaÄ‡ do bazy danych
- Pay-per-query

**PrzykÅ‚ad :**
```sql
-- UtworzyÄ‡ tabelÄ™ zewnÄ™trznÄ… wskazujÄ…cÄ… na S3
CREATE EXTERNAL TABLE my_table (
    id INT,
    name STRING
)
STORED AS PARQUET
LOCATION 's3://my-bucket/data/';
```

### S3 + Amazon Redshift

**UÅ¼ycie :**
- S3 jako ÅºrÃ³dÅ‚o dla COPY
- Redshift jako data warehouse
- Szybkie Å‚adowanie duÅ¼ych iloÅ›ci

**PrzykÅ‚ad :**
```sql
COPY my_table
FROM 's3://my-bucket/data/file.csv'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
CSV;
```

### S3 + AWS Lambda

**UÅ¼ycie :**
- WyzwalaÄ‡ Lambda przy przesÅ‚aniu
- Automatyczne przetwarzanie plikÃ³w
- PrzeksztaÅ‚canie, walidacja, etc.

**Konfiguracja :**
1. S3 â†’ Properties â†’ Event notifications
2. UtworzyÄ‡ powiadomienie
3. Wyzwalacz : "All object create events"
4. Destynacja : Funkcja Lambda

---

## Dobre praktyki

### BezpieczeÅ„stwo

1. **Nigdy nie czyniÄ‡ bucketÃ³w publicznymi** (chyba Å¼e konkretna potrzeba)
2. **UÅ¼ywaÄ‡ IAM** do kontroli dostÄ™pu
3. **WÅ‚Ä…czyÄ‡ szyfrowanie** domyÅ›lnie
4. **UÅ¼ywaÄ‡ bucket policies** dla szczegÃ³Å‚owych uprawnieÅ„

### WydajnoÅ›Ä‡

1. **UÅ¼ywaÄ‡ prefiksÃ³w** do rozÅ‚oÅ¼enia obciÄ…Å¼enia
2. **UnikaÄ‡ nazw sekwencyjnych** (np. file1, file2, file3)
3. **UÅ¼ywaÄ‡ Multipart Upload** dla duÅ¼ych plikÃ³w (>100MB)
4. **WÅ‚Ä…czyÄ‡ Transfer Acceleration** jeÅ›li potrzeba (pÅ‚atne)

### Koszty

1. **MonitorowaÄ‡ uÅ¼ycie** regularnie
2. **UÅ¼ywaÄ‡ odpowiednich klas** przechowywania
3. **UsuwaÄ‡ niepotrzebne pliki**
4. **KonfigurowaÄ‡ automatyczne przejÅ›cia**
5. **UÅ¼ywaÄ‡ S3 Lifecycle** do automatyzacji

### Organizacja

1. **NazywaÄ‡ buckety** spÃ³jnie
2. **UÅ¼ywaÄ‡ tagÃ³w** do organizacji
3. **DokumentowaÄ‡ strukturÄ™** danych
4. **TworzyÄ‡ konwencje** nazewnictwa

---

## PrzykÅ‚ady praktyczne

### PrzykÅ‚ad 1 : PrzesÅ‚aÄ‡ plik CSV

```python
import boto3
import pandas as pd

# UtworzyÄ‡ klienta S3
s3 = boto3.client('s3')

# CzytaÄ‡ plik lokalny
df = pd.read_csv('data.csv')

# PrzesÅ‚aÄ‡ do S3
s3.upload_file('data.csv', 'my-bucket', 'raw/2024/data.csv')
```

### PrzykÅ‚ad 2 : ListowaÄ‡ pliki z prefiksu

```python
# ListowaÄ‡ wszystkie pliki w prefiksie
response = s3.list_objects_v2(
    Bucket='my-bucket',
    Prefix='raw/2024/'
)

for obj in response.get('Contents', []):
    print(obj['Key'], obj['Size'])
```

### PrzykÅ‚ad 3 : PobraÄ‡ i przetworzyÄ‡

```python
# PobraÄ‡ z S3
s3.download_file('my-bucket', 'raw/data.csv', 'local-data.csv')

# PrzetworzyÄ‡
df = pd.read_csv('local-data.csv')
# ... przetwarzanie ...

# PrzesÅ‚aÄ‡ wynik
df.to_csv('processed-data.csv', index=False)
s3.upload_file('processed-data.csv', 'my-bucket', 'processed/data.csv')
```

---

## ğŸ“Š Kluczowe punkty do zapamiÄ™tania

1. **S3 = Nieograniczone przechowywanie** i wysoka dostÄ™pnoÅ›Ä‡
2. **Free Tier : 5 GB** zawsze darmowe
3. **OrganizowaÄ‡ z prefiksami** dla lepszej wydajnoÅ›ci
4. **WybraÄ‡ odpowiedniÄ… klasÄ™** wedÅ‚ug uÅ¼ycia
5. **S3 integruje siÄ™** ze wszystkimi usÅ‚ugami danych AWS

## ğŸ”— NastÄ™pny moduÅ‚

PrzejdÅº do moduÅ‚u [3. AWS Glue - ETL Serverless](../03-glue/README.md), aby nauczyÄ‡ siÄ™ przeksztaÅ‚caÄ‡ dane z AWS Glue.

