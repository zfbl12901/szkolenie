# 3. AWS Glue - ETL Serverless

## ğŸ¯ Cele

- ZrozumieÄ‡ AWS Glue i jego rolÄ™ w ETL
- TworzyÄ‡ crawlery do odkrywania danych
- TworzyÄ‡ joby ETL z Glue
- PrzeksztaÅ‚caÄ‡ dane z PySpark
- IntegrowaÄ‡ Glue z S3 i innymi usÅ‚ugami

## ğŸ“‹ Spis treÅ›ci

1. [Wprowadzenie do AWS Glue](#wprowadzenie-do-aws-glue)
2. [UtworzyÄ‡ Data Catalog](#utworzyÄ‡-data-catalog)
3. [Crawlery - OdkrywaÄ‡ dane](#crawlery---odkrywaÄ‡-dane)
4. [UtworzyÄ‡ job ETL](#utworzyÄ‡-job-etl)
5. [PrzeksztaÅ‚canie danych](#przeksztaÅ‚canie-danych)
6. [Orkiestracja i planowanie](#orkiestracja-i-planowanie)

---

## Wprowadzenie do AWS Glue

### Czym jest AWS Glue?

**AWS Glue** = ZarzÄ…dzana usÅ‚uga ETL serverless

- **ETL** : Extract, Transform, Load
- **Serverless** : Brak serwerÃ³w do zarzÄ…dzania
- **ZarzÄ…dzane** : AWS zarzÄ…dza infrastrukturÄ…
- **Skalowalne** : Automatycznie dostosowuje siÄ™

### Komponenty Glue

1. **Data Catalog** : Katalog metadanych
2. **Crawlery** : Automatycznie odkrywajÄ… schematy
3. **ETL Jobs** : Skrypty przeksztaÅ‚cania (Python/PySpark)
4. **Triggers** : Automatyczne wyzwalanie
5. **Workflows** : Orkiestracja wielu jobÃ³w

### Free Tier Glue

**Darmowe na zawsze :**
- 10 000 obiektÃ³w/miesiÄ…c w Data Catalog
- 1 milion zapytaÅ„/miesiÄ…c do Data Catalog
- 0.44$ za DPU-godzinÄ™ (pierwszy milion darmowy)

**âš ï¸ WaÅ¼ne :** Joby Glue zuÅ¼ywajÄ… DPU (Data Processing Units). MonitorowaÄ‡ koszty.

---

## UtworzyÄ‡ Data Catalog

### Czym jest Data Catalog?

**Data Catalog** = Scentralizowany katalog metadanych

- Schematy danych
- Lokalizacje (S3, bazy danych)
- Typy danych
- Partycje

### Struktura Data Catalog

- **Databases** : Grupy tabel
- **Tables** : Metadane danych
- **Partitions** : Organizacja danych

### UtworzyÄ‡ bazÄ™ danych

1. Konsola AWS â†’ Glue â†’ "Databases"
2. "Add database"
3. Nazwa : `data_analyst_db`
4. Opis (opcjonalne)
5. "Create"

**UÅ¼ycie :**
- OrganizowaÄ‡ tabele wedÅ‚ug projektu
- PrzykÅ‚ad : `raw_data_db`, `processed_data_db`

---

## Crawlery - OdkrywaÄ‡ dane

### Czym jest Crawler?

**Crawler** = UsÅ‚uga skanujÄ…ca dane i automatycznie tworzÄ…ca tabele

- Analizuje pliki w S3
- Automatycznie wykrywa schemat
- Tworzy tabele w Data Catalog
- ObsÅ‚uguje : CSV, JSON, Parquet, etc.

### UtworzyÄ‡ Crawler

**Krok 1 : Podstawowa konfiguracja**

1. Glue â†’ "Crawlers" â†’ "Add crawler"
2. Nazwa : `s3-csv-crawler`
3. Opis (opcjonalne)

**Krok 2 : Å¹rÃ³dÅ‚o danych**

1. "Add a data source"
2. Typ : "S3"
3. ÅšcieÅ¼ka S3 : `s3://my-bucket/raw/`
4. UwzglÄ™dniÄ‡ podfoldery (opcjonalne)

**Krok 3 : Rola IAM**

1. UtworzyÄ‡ nowÄ… rolÄ™ lub uÅ¼yÄ‡ istniejÄ…cej
2. Nazwa : `AWSGlueServiceRole-default`
3. Uprawnienia : DostÄ™p S3 i Glue

**Krok 4 : WyjÅ›cie**

1. Baza danych : `data_analyst_db`
2. Prefiks tabel (opcjonalne)

**Krok 5 : WykonaÄ‡**

1. "Run crawler now" lub zaplanowaÄ‡
2. CzekaÄ‡ na zakoÅ„czenie (kilka minut)
3. SprawdziÄ‡ utworzone tabele

### Wynik Crawlera

**Automatycznie utworzona tabela :**
- Wykryte kolumny
- Wnioskowane typy danych
- Lokalizacja S3
- Format pliku

**PrzykÅ‚ad utworzonej tabeli :**
```
Table: raw_data
Columns:
  - id (bigint)
  - name (string)
  - created_at (timestamp)
Location: s3://my-bucket/raw/
Format: csv
```

---

## UtworzyÄ‡ job ETL

### Typy jobÃ³w Glue

1. **Spark** : Joby PySpark (zalecane)
2. **Python shell** : Proste skrypty Python
3. **Ray** : Zaawansowane przetwarzanie rozproszone

### UtworzyÄ‡ job Spark

**Krok 1 : Konfiguracja**

1. Glue â†’ "ETL jobs" â†’ "Add job"
2. Nazwa : `transform-csv-job`
3. Rola IAM : `AWSGlueServiceRole-default`
4. Typ : "Spark"
5. Wersja Glue : "4.0" (zalecane)
6. DPU : 2 (minimum, regulowane)

**Krok 2 : Å¹rÃ³dÅ‚o danych**

1. "Data source" : WybraÄ‡ tabelÄ™ z Data Catalog
2. Lub : BezpoÅ›rednia Å›cieÅ¼ka S3

**Krok 3 : Destynacja**

1. "Data target" : S3
2. Format : Parquet (zalecane dla analytics)
3. ÅšcieÅ¼ka : `s3://my-bucket/processed/`

**Krok 4 : Skrypt**

1. WygenerowaÄ‡ skrypt automatyczny
2. Lub : NapisaÄ‡ skrypt niestandardowy

### Podstawowy skrypt ETL

**Automatycznie wygenerowany skrypt :**

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# CzytaÄ‡ z Data Catalog
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "raw_data"
)

# PrzeksztaÅ‚ciÄ‡ (przykÅ‚ad : filtrowaÄ‡)
filtered = Filter.apply(
    frame = datasource,
    f = lambda x: x["status"] == "active"
)

# ZapisaÄ‡ do S3
glueContext.write_dynamic_frame.from_options(
    frame = filtered,
    connection_type = "s3",
    connection_options = {"path": "s3://my-bucket/processed/"},
    format = "parquet"
)

job.commit()
```

---

## PrzeksztaÅ‚canie danych

### CzÄ™ste przeksztaÅ‚cenia

#### 1. FiltrowaÄ‡ wiersze

```python
from awsglue.transforms import Filter

filtered = Filter.apply(
    frame = datasource,
    f = lambda x: x["age"] > 18
)
```

#### 2. WybieraÄ‡ kolumny

```python
from awsglue.transforms import SelectFields

selected = SelectFields.apply(
    frame = datasource,
    paths = ["id", "name", "email"]
)
```

#### 3. ZmieniaÄ‡ nazwy kolumn

```python
from awsglue.transforms import RenameField

renamed = RenameField.apply(
    frame = datasource,
    old_name = "old_column",
    new_name = "new_column"
)
```

#### 4. ÅÄ…czyÄ‡ dane

```python
joined = Join.apply(
    frame1 = datasource1,
    frame2 = datasource2,
    keys1 = ["id"],
    keys2 = ["user_id"]
)
```

#### 5. Agregacje

```python
# KonwertowaÄ‡ na DataFrame Spark dla agregacji
df = datasource.toDF()

aggregated = df.groupBy("category").agg({
    "amount": "sum",
    "id": "count"
})

# KonwertowaÄ‡ z powrotem na DynamicFrame
from awsglue.dynamicframe import DynamicFrame
result = DynamicFrame.fromDF(aggregated, glueContext, "result")
```

### Kompletny przykÅ‚ad : PrzeksztaÅ‚cenie CSV â†’ Parquet

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# 1. CzytaÄ‡ z S3 (przez Data Catalog)
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "raw_data"
)

# 2. FiltrowaÄ‡ dane
filtered = Filter.apply(
    frame = datasource,
    f = lambda x: x["status"] == "active"
)

# 3. WybieraÄ‡ kolumny
selected = SelectFields.apply(
    frame = filtered,
    paths = ["id", "name", "email", "created_at"]
)

# 4. KonwertowaÄ‡ na DataFrame dla zaawansowanych przeksztaÅ‚ceÅ„
df = selected.toDF()

# 5. DodaÄ‡ kolumnÄ™ obliczonÄ…
from pyspark.sql.functions import col, year
df = df.withColumn("year", year(col("created_at")))

# 6. KonwertowaÄ‡ z powrotem na DynamicFrame
from awsglue.dynamicframe import DynamicFrame
result = DynamicFrame.fromDF(df, glueContext, "result")

# 7. ZapisaÄ‡ do S3 w Parquet (partycjonowane wedÅ‚ug roku)
glueContext.write_dynamic_frame.from_options(
    frame = result,
    connection_type = "s3",
    connection_options = {
        "path": "s3://my-bucket/processed/",
        "partitionKeys": ["year"]
    },
    format = "parquet"
)

job.commit()
```

---

## Orkiestracja i planowanie

### WyzwoliÄ‡ job rÄ™cznie

1. Glue â†’ "ETL jobs"
2. WybraÄ‡ job
3. "Run job"
4. ZobaczyÄ‡ logi w czasie rzeczywistym

### ZaplanowaÄ‡ job (Trigger)

**UtworzyÄ‡ trigger :**

1. Glue â†’ "Triggers" â†’ "Add trigger"
2. Nazwa : `daily-etl-trigger`
3. Typ : "Scheduled"
4. CzÄ™stotliwoÅ›Ä‡ : "Cron expression"
   - PrzykÅ‚ad : `cron(0 2 * * ? *)` = Codziennie o 2h
5. Akcje : WybraÄ‡ job do wykonania
6. "Add"

**Typy triggerÃ³w :**
- **On-demand** : RÄ™czne wyzwalanie
- **Scheduled** : Zaplanowane (cron)
- **Event-driven** : Wyzwalane przez zdarzenie (np. nowy plik S3)

### Workflows (zÅ‚oÅ¼ona orkiestracja)

**UtworzyÄ‡ workflow :**

1. Glue â†’ "Workflows" â†’ "Add workflow"
2. Nazwa : `etl-pipeline-workflow`
3. DodaÄ‡ kroki :
   - Crawler â†’ Job ETL â†’ Inny Job
4. ZdefiniowaÄ‡ zaleÅ¼noÅ›ci
5. WyzwoliÄ‡ workflow

**PrzykÅ‚ad workflow :**
```
1. Crawler S3 â†’ Odkrywa nowe pliki
2. Job ETL 1 â†’ PrzeksztaÅ‚ca surowe dane
3. Job ETL 2 â†’ Agreguje dane
4. Job ETL 3 â†’ Åaduje do Redshift
```

---

## Dobre praktyki

### WydajnoÅ›Ä‡

1. **UÅ¼ywaÄ‡ Parquet** zamiast CSV (szybsze)
2. **PartycjonowaÄ‡ dane** (poprawia wydajnoÅ›Ä‡)
3. **DostosowaÄ‡ DPU** wedÅ‚ug rozmiaru danych
4. **UÅ¼ywaÄ‡ cache Spark** do ponownego uÅ¼ycia danych

### Koszty

1. **MonitorowaÄ‡ DPU-godziny** uÅ¼ywane
2. **OptymalizowaÄ‡ skrypty** do zmniejszenia czasu wykonania
3. **UÅ¼ywaÄ‡ odpowiednich klas S3** (Standard-IA dla archiwÃ³w)
4. **ZatrzymywaÄ‡ joby** ktÃ³re szybko koÅ„czÄ… siÄ™ niepowodzeniem

### Organizacja

1. **NazywaÄ‡ joby** spÃ³jnie
2. **DokumentowaÄ‡ przeksztaÅ‚cenia**
3. **WersjonowaÄ‡ skrypty** (Git)
4. **TestowaÄ‡ lokalnie** przed wdroÅ¼eniem

---

## PrzykÅ‚ady praktyczne

### PrzykÅ‚ad 1 : PrzeksztaÅ‚ciÄ‡ CSV â†’ Parquet

```python
# CzytaÄ‡ CSV z S3
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "raw_csv_data"
)

# ZapisaÄ‡ w Parquet
glueContext.write_dynamic_frame.from_options(
    frame = datasource,
    connection_type = "s3",
    connection_options = {"path": "s3://my-bucket/parquet/"},
    format = "parquet"
)
```

### PrzykÅ‚ad 2 : CzyÅ›ciÄ‡ i walidowaÄ‡

```python
# FiltrowaÄ‡ nieprawidÅ‚owe wiersze
cleaned = Filter.apply(
    frame = datasource,
    f = lambda x: x["email"] is not None and "@" in x["email"]
)

# UsunÄ…Ä‡ duplikaty (przez DataFrame)
df = cleaned.toDF()
df = df.dropDuplicates(["id"])

result = DynamicFrame.fromDF(df, glueContext, "result")
```

### PrzykÅ‚ad 3 : ÅÄ…czyÄ‡ wiele ÅºrÃ³deÅ‚

```python
# CzytaÄ‡ dwie tabele
users = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "users"
)

orders = glueContext.create_dynamic_frame.from_catalog(
    database = "data_analyst_db",
    table_name = "orders"
)

# ÅÄ…czyÄ‡
joined = Join.apply(
    frame1 = users,
    frame2 = orders,
    keys1 = ["id"],
    keys2 = ["user_id"]
)
```

---

## ğŸ“Š Kluczowe punkty do zapamiÄ™tania

1. **Glue = ETL serverless** zarzÄ…dzane przez AWS
2. **Crawlery** automatycznie odkrywajÄ… schematy
3. **Joby ETL** uÅ¼ywajÄ… PySpark do przeksztaÅ‚ceÅ„
4. **Data Catalog** centralizuje metadane
5. **Triggers** umoÅ¼liwiajÄ… automatyzacjÄ™

## ğŸ”— NastÄ™pny moduÅ‚

PrzejdÅº do moduÅ‚u [4. Amazon Redshift - Data Warehouse](../04-redshift/README.md), aby nauczyÄ‡ siÄ™ uÅ¼ywaÄ‡ Redshift do analizy danych.

