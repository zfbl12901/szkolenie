# 7. Projets pratiques AWS

## üéØ Objectifs

- Appliquer les connaissances acquises
- Cr√©er des pipelines ETL complets
- Construire un Data Lake sur AWS
- Cr√©er des projets pour votre portfolio
- Int√©grer plusieurs services AWS

## üìã Table des mati√®res

1. [Projet 1 : Pipeline ETL S3 ‚Üí Parquet](#projet-1--pipeline-etl-s3---parquet)
2. [Projet 2 : Data Lake sur AWS](#projet-2--data-lake-sur-aws)
3. [Projet 3 : Analytics avec Athena](#projet-3--analytics-avec-athena)
4. [Projet 4 : Pipeline automatis√© complet](#projet-4--pipeline-automatis√©-complet)
5. [Bonnes pratiques pour portfolio](#bonnes-pratiques-pour-portfolio)

---

## Projet 1 : Pipeline ETL S3 ‚Üí Parquet

### Objectif

Cr√©er un pipeline ETL qui transforme des fichiers CSV depuis S3 en format Parquet optimis√©.

### Architecture

```
S3 (raw/) ‚Üí Glue Crawler ‚Üí Data Catalog ‚Üí Glue Job ‚Üí S3 (processed/parquet/)
```

### √âtapes

#### 1. Pr√©parer les donn√©es

**Cr√©er un bucket S3 :**
- Nom : `data-analyst-project-1`
- Cr√©er un dossier `raw/`
- Uploader un fichier CSV de test

**Exemple de donn√©es CSV :**
```csv
id,name,email,created_at,status
1,John Doe,john@example.com,2024-01-01,active
2,Jane Smith,jane@example.com,2024-01-02,inactive
```

#### 2. Cr√©er un Crawler Glue

1. Glue ‚Üí "Crawlers" ‚Üí "Add crawler"
2. Nom : `csv-crawler`
3. Data source : `s3://data-analyst-project-1/raw/`
4. IAM Role : Cr√©er un r√¥le avec acc√®s S3
5. Database : `project1_db`
6. Ex√©cuter le crawler

#### 3. Cr√©er un Job Glue

1. Glue ‚Üí "ETL jobs" ‚Üí "Add job"
2. Nom : `csv-to-parquet-job`
3. Type : Spark
4. Script :

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Lire depuis Data Catalog
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "project1_db",
    table_name = "raw_data"
)

# Filtrer les donn√©es actives
filtered = Filter.apply(
    frame = datasource,
    f = lambda x: x["status"] == "active"
)

# √âcrire en Parquet
glueContext.write_dynamic_frame.from_options(
    frame = filtered,
    connection_type = "s3",
    connection_options = {
        "path": "s3://data-analyst-project-1/processed/parquet/"
    },
    format = "parquet"
)

job.commit()
```

#### 4. Ex√©cuter le job

1. S√©lectionner le job
2. "Run job"
3. V√©rifier les logs
4. V√©rifier les fichiers Parquet dans S3

### R√©sultat

- Fichiers CSV transform√©s en Parquet
- Donn√©es filtr√©es (seulement actives)
- Pr√™t pour analytics avec Athena

---

## Projet 2 : Data Lake sur AWS

### Objectif

Cr√©er un Data Lake complet avec ingestion, transformation et analytics.

### Architecture

```
Sources ‚Üí S3 (Raw) ‚Üí Glue (Transform) ‚Üí S3 (Processed) ‚Üí Athena (Analytics)
                ‚Üì
            Lambda (Trigger)
```

### √âtapes

#### 1. Structure S3

```
data-lake-bucket/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îú‚îÄ‚îÄ orders/
‚îÇ   ‚îî‚îÄ‚îÄ products/
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îú‚îÄ‚îÄ orders/
‚îÇ   ‚îî‚îÄ‚îÄ products/
‚îî‚îÄ‚îÄ analytics/
    ‚îî‚îÄ‚îÄ results/
```

#### 2. Crawlers pour chaque source

**Cr√©er 3 crawlers :**
- `users-crawler` ‚Üí `s3://bucket/raw/users/`
- `orders-crawler` ‚Üí `s3://bucket/raw/orders/`
- `products-crawler` ‚Üí `s3://bucket/raw/products/`

#### 3. Jobs ETL pour transformation

**Job pour users :**
```python
# users-etl-job
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "data_lake_db",
    table_name = "users"
)

# Nettoyer et transformer
cleaned = Filter.apply(
    frame = datasource,
    f = lambda x: x["email"] is not None
)

glueContext.write_dynamic_frame.from_options(
    frame = cleaned,
    connection_type = "s3",
    connection_options = {"path": "s3://bucket/processed/users/"},
    format = "parquet"
)
```

#### 4. Tables Athena pour analytics

```sql
-- Table users
CREATE EXTERNAL TABLE users_processed (
    id INT,
    name STRING,
    email STRING,
    created_at TIMESTAMP
)
STORED AS PARQUET
LOCATION 's3://bucket/processed/users/';

-- Table orders
CREATE EXTERNAL TABLE orders_processed (
    id INT,
    user_id INT,
    amount DECIMAL(10,2),
    created_at TIMESTAMP
)
STORED AS PARQUET
LOCATION 's3://bucket/processed/orders/';

-- Requ√™te analytique
SELECT 
    u.name,
    COUNT(o.id) AS order_count,
    SUM(o.amount) AS total_spent
FROM users_processed u
LEFT JOIN orders_processed o ON u.id = o.user_id
GROUP BY u.name
ORDER BY total_spent DESC;
```

#### 5. Automatisation avec Lambda

**Lambda d√©clench√© par upload S3 :**

```python
import boto3

glue = boto3.client('glue')

def lambda_handler(event, context):
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # D√©terminer quel job ex√©cuter selon le pr√©fixe
    if 'users' in key:
        job_name = 'users-etl-job'
    elif 'orders' in key:
        job_name = 'orders-etl-job'
    else:
        job_name = 'products-etl-job'
    
    # D√©clencher le job
    glue.start_job_run(JobName=job_name)
    
    return {'statusCode': 200}
```

### R√©sultat

- Data Lake fonctionnel
- Pipeline automatis√©
- Analytics avec Athena
- Projet complet pour portfolio

---

## Projet 3 : Analytics avec Athena

### Objectif

Cr√©er un syst√®me d'analytics complet avec requ√™tes SQL sur donn√©es S3.

### √âtapes

#### 1. Pr√©parer les donn√©es

**Uploader des fichiers Parquet dans S3 :**
- `s3://analytics-bucket/sales/year=2024/month=01/`
- `s3://analytics-bucket/sales/year=2024/month=02/`

#### 2. Cr√©er des tables partitionn√©es

```sql
CREATE EXTERNAL TABLE sales (
    id INT,
    product_id INT,
    amount DECIMAL(10,2),
    sale_date TIMESTAMP
)
PARTITIONED BY (year INT, month INT)
STORED AS PARQUET
LOCATION 's3://analytics-bucket/sales/';

-- Ajouter les partitions
ALTER TABLE sales ADD PARTITION (year=2024, month=1)
LOCATION 's3://analytics-bucket/sales/year=2024/month=01/';

ALTER TABLE sales ADD PARTITION (year=2024, month=2)
LOCATION 's3://analytics-bucket/sales/year=2024/month=02/';
```

#### 3. Requ√™tes analytiques

**Ventes par mois :**
```sql
SELECT 
    year,
    month,
    SUM(amount) AS total_sales,
    COUNT(*) AS transaction_count,
    AVG(amount) AS avg_transaction
FROM sales
WHERE year = 2024
GROUP BY year, month
ORDER BY year, month;
```

**Top produits :**
```sql
SELECT 
    product_id,
    SUM(amount) AS total_revenue,
    COUNT(*) AS sales_count
FROM sales
WHERE year = 2024
GROUP BY product_id
ORDER BY total_revenue DESC
LIMIT 10;
```

**Tendances :**
```sql
SELECT 
    DATE_TRUNC('week', sale_date) AS week,
    SUM(amount) AS weekly_sales,
    LAG(SUM(amount), 1) OVER (ORDER BY DATE_TRUNC('week', sale_date)) AS previous_week
FROM sales
WHERE year = 2024
GROUP BY DATE_TRUNC('week', sale_date)
ORDER BY week;
```

#### 4. Sauvegarder les r√©sultats

**Cr√©er une table pour r√©sultats :**
```sql
CREATE EXTERNAL TABLE analytics_results (
    metric_name STRING,
    metric_value DECIMAL(10,2),
    calculated_at TIMESTAMP
)
STORED AS PARQUET
LOCATION 's3://analytics-bucket/results/';
```

---

## Projet 4 : Pipeline automatis√© complet

### Objectif

Cr√©er un pipeline ETL compl√®tement automatis√© avec plusieurs services AWS.

### Architecture compl√®te

```
Fichier CSV upload√© ‚Üí S3 (raw/)
    ‚Üì (Event)
Lambda (Validation)
    ‚Üì
S3 (validated/)
    ‚Üì (Event)
Glue Job (Transform CSV ‚Üí Parquet)
    ‚Üì
S3 (processed/parquet/)
    ‚Üì
Glue Crawler (Update Catalog)
    ‚Üì
Athena (Analytics)
    ‚Üì
S3 (results/)
```

### Impl√©mentation

#### 1. Lambda de validation

```python
import boto3
import csv

s3 = boto3.client('s3')

def lambda_handler(event, context):
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # T√©l√©charger et valider
    response = s3.get_object(Bucket=bucket, Key=key)
    content = response['Body'].read().decode('utf-8')
    reader = csv.DictReader(content.splitlines())
    
    valid_rows = []
    for row in reader:
        if row.get('email') and '@' in row['email']:
            valid_rows.append(row)
    
    # Uploader les donn√©es valid√©es
    if valid_rows:
        validated_key = key.replace('raw/', 'validated/')
        # Convertir en CSV et uploader
        # ...
    
    return {'statusCode': 200}
```

#### 2. Glue Job de transformation

```python
# Transform validated CSV to Parquet
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "pipeline_db",
    table_name = "validated_data"
)

# Transformer
transformed = Map.apply(
    frame = datasource,
    f = lambda x: {
        'id': x['id'],
        'name': x['name'].upper(),
        'email': x['email'].lower(),
        'created_at': x['created_at']
    }
)

# √âcrire en Parquet
glueContext.write_dynamic_frame.from_options(
    frame = transformed,
    connection_type = "s3",
    connection_options = {"path": "s3://bucket/processed/"},
    format = "parquet"
)
```

#### 3. Workflow Glue

**Cr√©er un workflow :**
1. Trigger : Nouveau fichier dans `validated/`
2. Action : Ex√©cuter le job Glue
3. Action suivante : Mettre √† jour le crawler

### R√©sultat

- Pipeline compl√®tement automatis√©
- Validation automatique
- Transformation automatique
- Analytics disponibles imm√©diatement

---

## Bonnes pratiques pour portfolio

### Documentation

**Cr√©er un README pour chaque projet :**

```markdown
# Projet : Pipeline ETL AWS

## Description
Pipeline ETL automatis√© pour transformer des donn√©es CSV en Parquet.

## Architecture
- S3 : Stockage
- Glue : Transformation
- Athena : Analytics

## R√©sultats
- R√©duction des co√ªts de 60%
- Temps de traitement r√©duit de 80%
```

### Visualisations

**Cr√©er des diagrammes :**
- Architecture du syst√®me
- Flux de donn√©es
- Sch√©ma de donn√©es

**Outils :**
- Draw.io
- Lucidchart
- Diagrammes ASCII dans README

### M√©triques

**Inclure des m√©triques :**
- Temps d'ex√©cution avant/apr√®s
- Co√ªts avant/apr√®s
- Volume de donn√©es trait√©es
- Performance des requ√™tes

### Code

**Bonnes pratiques :**
- Code comment√©
- Variables d'environnement pour configuration
- Gestion d'erreurs
- Logging

### GitHub

**Cr√©er un repository :**
- README avec documentation
- Scripts Lambda
- Scripts Glue
- Configuration
- Diagrammes

---

## üìä Points cl√©s √† retenir

1. **Projets pratiques** : Essentiels pour portfolio
2. **Documentation** : Expliquer l'architecture et les r√©sultats
3. **M√©triques** : Montrer l'impact (performance, co√ªts)
4. **Code propre** : Comment√© et organis√©
5. **GitHub** : Partager vos projets

## üîó Ressources

- [AWS Architecture Center](https://aws.amazon.com/architecture/)
- [AWS Solutions](https://aws.amazon.com/solutions/)
- [GitHub AWS Examples](https://github.com/aws-samples)

---

**F√©licitations !** Vous avez termin√© la formation AWS pour Data Analyst. Vous pouvez maintenant cr√©er des projets complets sur AWS en utilisant uniquement des ressources gratuites.

