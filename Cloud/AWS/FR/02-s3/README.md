# 2. Amazon S3 - Stockage de donn√©es

## üéØ Objectifs

- Comprendre Amazon S3 et son utilisation
- Cr√©er et g√©rer des buckets S3
- Uploader et organiser des fichiers
- Comprendre les classes de stockage
- Int√©grer S3 avec d'autres services AWS

## üìã Table des mati√®res

1. [Introduction √† S3](#introduction-√†-s3)
2. [Cr√©er un bucket S3](#cr√©er-un-bucket-s3)
3. [Uploader et g√©rer des fichiers](#uploader-et-g√©rer-des-fichiers)
4. [Classes de stockage](#classes-de-stockage)
5. [Organisation des donn√©es](#organisation-des-donn√©es)
6. [Int√©gration avec autres services](#int√©gration-avec-autres-services)

---

## Introduction √† S3

### Qu'est-ce qu'Amazon S3 ?

**Amazon S3** (Simple Storage Service) = Service de stockage d'objets

- Stockage illimit√©
- Haute disponibilit√© (99.99%)
- S√©curis√© par d√©faut
- Int√©gration avec tous les services AWS

### Cas d'usage pour Data Analyst

- **Data Lake** : Stocker des donn√©es brutes
- **Backup** : Sauvegarder des donn√©es
- **ETL** : Source/destination pour pipelines
- **Analytics** : Donn√©es pour Athena, Redshift
- **Archivage** : Donn√©es historiques

### Free Tier S3

**Gratuit √† vie :**
- 5 Go de stockage standard
- 20 000 requ√™tes GET
- 2 000 requ√™tes PUT
- 15 Go de transfert de donn√©es sortantes

**‚ö†Ô∏è Important :** Au-del√† de ces limites, facturation normale.

---

## Cr√©er un bucket S3

### √âtape 1 : Acc√©der √† S3

1. Console AWS ‚Üí Rechercher "S3"
2. Cliquer sur "Amazon S3"
3. Cliquer sur "Create bucket"

### √âtape 2 : Configuration du bucket

**Informations de base :**
- **Bucket name** : Nom unique globalement (ex: `my-data-analyst-bucket`)
- **Region** : Choisir la r√©gion la plus proche (ex: `eu-west-3` Paris)

**Options de configuration :**

1. **Object Ownership**
   - "ACLs disabled" (recommand√©)
   - "Bucket owner enforced"

2. **Block Public Access**
   - ‚úÖ **Tout activer** (s√©curit√© par d√©faut)
   - D√©sactiver seulement si besoin sp√©cifique

3. **Versioning**
   - D√©sactiv√© par d√©faut (gratuit)
   - Activer si besoin de versions multiples

4. **Tags** (optionnel)
   - Ajouter des tags pour organisation
   - Ex: `Project: Data-Analyst-Training`

5. **Default encryption**
   - ‚úÖ Activer (recommand√©)
   - "Amazon S3 managed keys (SSE-S3)" (gratuit)

### √âtape 3 : Cr√©er le bucket

1. Cliquer sur "Create bucket"
2. Bucket cr√©√© et visible dans la liste
3. Pr√™t √† utiliser

**‚ö†Ô∏è Important :** Le nom du bucket doit √™tre unique globalement dans AWS.

---

## Uploader et g√©rer des fichiers

### Uploader un fichier

**M√©thode 1 : Interface web**

1. Cliquer sur le nom du bucket
2. Cliquer sur "Upload"
3. "Add files" ou "Add folder"
4. S√©lectionner les fichiers
5. Cliquer sur "Upload"

**M√©thode 2 : AWS CLI**

```bash
# Installer AWS CLI (si pas d√©j√† fait)
# Windows: https://aws.amazon.com/cli/
# Linux/Mac: pip install awscli

# Configurer les credentials
aws configure

# Uploader un fichier
aws s3 cp local-file.csv s3://my-data-analyst-bucket/data/
```

**M√©thode 3 : SDK Python (boto3)**

```python
import boto3

# Cr√©er un client S3
s3 = boto3.client('s3')

# Uploader un fichier
s3.upload_file('local-file.csv', 'my-data-analyst-bucket', 'data/file.csv')
```

### T√©l√©charger un fichier

**Interface web :**
1. Cliquer sur le fichier
2. Cliquer sur "Download"

**AWS CLI :**
```bash
aws s3 cp s3://my-data-analyst-bucket/data/file.csv local-file.csv
```

**Python :**
```python
s3.download_file('my-data-analyst-bucket', 'data/file.csv', 'local-file.csv')
```

### G√©rer les fichiers

**Actions disponibles :**
- **Download** : T√©l√©charger
- **Open** : Ouvrir dans le navigateur
- **Copy** : Copier vers un autre emplacement
- **Move** : D√©placer
- **Delete** : Supprimer
- **Make public** : Rendre public (attention s√©curit√©)

---

## Classes de stockage

### S3 Standard (par d√©faut)

**Utilisation :**
- Donn√©es fr√©quemment acc√©d√©es
- Applications en production

**Caract√©ristiques :**
- Acc√®s rapide
- 99.99% de disponibilit√©
- Co√ªt : ~0.023$ par Go/mois

**Free Tier :** 5 Go gratuit

### S3 Intelligent-Tiering

**Utilisation :**
- Donn√©es avec acc√®s variable
- Optimisation automatique des co√ªts

**Caract√©ristiques :**
- D√©place automatiquement entre classes
- Pas de frais de r√©cup√©ration
- Co√ªt : ~0.023$ par Go/mois

### S3 Standard-IA (Infrequent Access)

**Utilisation :**
- Donn√©es rarement acc√©d√©es
- Backup, archives

**Caract√©ristiques :**
- Acc√®s rapide quand n√©cessaire
- Co√ªt stockage : ~0.0125$ par Go/mois
- Co√ªt r√©cup√©ration : ~0.01$ par Go

### S3 One Zone-IA

**Utilisation :**
- Donn√©es reproductibles
- Backup secondaire

**Caract√©ristiques :**
- Stockage dans une seule zone
- Co√ªt : ~0.01$ par Go/mois
- ‚ö†Ô∏è Risque de perte si zone d√©faillante

### S3 Glacier

**Utilisation :**
- Archivage long terme
- Donn√©es rarement n√©cessaires

**Caract√©ristiques :**
- R√©cup√©ration : 1-5 minutes √† plusieurs heures
- Co√ªt : ~0.004$ par Go/mois
- Frais de r√©cup√©ration selon vitesse

### Choisir la classe de stockage

**Pour Data Analyst :**
- **S3 Standard** : Donn√©es actives (analyses fr√©quentes)
- **S3 Standard-IA** : Donn√©es historiques (analyses occasionnelles)
- **S3 Glacier** : Archives (rarement utilis√©es)

**Transition automatique :**
- Configurer des r√®gles de transition
- Exemple : Standard ‚Üí Standard-IA apr√®s 30 jours

---

## Organisation des donn√©es

### Structure recommand√©e

**Organisation par projet :**
```
bucket-name/
‚îú‚îÄ‚îÄ raw/              # Donn√©es brutes
‚îÇ   ‚îú‚îÄ‚îÄ 2024/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ processed/        # Donn√©es transform√©es
‚îÇ   ‚îú‚îÄ‚îÄ 2024/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ analytics/        # Donn√©es pour analyse
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ archive/          # Archives
    ‚îî‚îÄ‚îÄ ...
```

**Organisation par type :**
```
bucket-name/
‚îú‚îÄ‚îÄ csv/
‚îú‚îÄ‚îÄ json/
‚îú‚îÄ‚îÄ parquet/
‚îî‚îÄ‚îÄ logs/
```

### Pr√©fixes et dossiers

**S3 n'a pas de "vrais" dossiers**, mais utilise des pr√©fixes :

- `data/2024/01/file.csv` = Pr√©fixe `data/2024/01/`
- Interface web simule des dossiers
- Utiliser `/` pour organiser

**Bonnes pratiques :**
- Utiliser des pr√©fixes coh√©rents
- Inclure la date dans le chemin
- S√©parer par type de donn√©es

---

## Int√©gration avec autres services

### S3 + AWS Glue

**Utilisation :**
- S3 comme source de donn√©es
- Glue transforme les donn√©es
- R√©sultat vers S3 ou autre destination

**Exemple :**
```python
# Job Glue lit depuis S3
datasource = glueContext.create_dynamic_frame.from_catalog(
    database = "my_database",
    table_name = "s3_data"
)
```

### S3 + Amazon Athena

**Utilisation :**
- Requ√™tes SQL directement sur fichiers S3
- Pas besoin de charger dans base de donn√©es
- Pay-per-query

**Exemple :**
```sql
-- Cr√©er une table externe pointant vers S3
CREATE EXTERNAL TABLE my_table (
    id INT,
    name STRING
)
STORED AS PARQUET
LOCATION 's3://my-bucket/data/';
```

### S3 + Amazon Redshift

**Utilisation :**
- S3 comme source pour COPY
- Redshift comme data warehouse
- Chargement rapide de grandes quantit√©s

**Exemple :**
```sql
COPY my_table
FROM 's3://my-bucket/data/file.csv'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
CSV;
```

### S3 + AWS Lambda

**Utilisation :**
- D√©clencher Lambda lors d'upload
- Traitement automatique des fichiers
- Transformation, validation, etc.

**Configuration :**
1. S3 ‚Üí Properties ‚Üí Event notifications
2. Cr√©er une notification
3. D√©clencher : "All object create events"
4. Destination : Lambda function

---

## Bonnes pratiques

### S√©curit√©

1. **Ne jamais rendre les buckets publics** (sauf besoin sp√©cifique)
2. **Utiliser IAM** pour contr√¥ler l'acc√®s
3. **Activer le chiffrement** par d√©faut
4. **Utiliser des bucket policies** pour permissions granulaires

### Performance

1. **Utiliser des pr√©fixes** pour distribuer la charge
2. **√âviter les noms s√©quentiels** (ex: file1, file2, file3)
3. **Utiliser Multipart Upload** pour gros fichiers (>100MB)
4. **Activer Transfer Acceleration** si besoin (payant)

### Co√ªts

1. **Surveiller l'utilisation** r√©guli√®rement
2. **Utiliser les bonnes classes** de stockage
3. **Supprimer les fichiers inutiles**
4. **Configurer des transitions** automatiques
5. **Utiliser S3 Lifecycle** pour automatiser

### Organisation

1. **Nommer les buckets** de mani√®re coh√©rente
2. **Utiliser des tags** pour organisation
3. **Documenter la structure** des donn√©es
4. **Cr√©er des conventions** de nommage

---

## Exemples pratiques

### Exemple 1 : Uploader un fichier CSV

```python
import boto3
import pandas as pd

# Cr√©er un client S3
s3 = boto3.client('s3')

# Lire un fichier local
df = pd.read_csv('data.csv')

# Uploader vers S3
s3.upload_file('data.csv', 'my-bucket', 'raw/2024/data.csv')
```

### Exemple 2 : Lister les fichiers d'un pr√©fixe

```python
# Lister tous les fichiers dans un pr√©fixe
response = s3.list_objects_v2(
    Bucket='my-bucket',
    Prefix='raw/2024/'
)

for obj in response.get('Contents', []):
    print(obj['Key'], obj['Size'])
```

### Exemple 3 : T√©l√©charger et traiter

```python
# T√©l√©charger depuis S3
s3.download_file('my-bucket', 'raw/data.csv', 'local-data.csv')

# Traiter
df = pd.read_csv('local-data.csv')
# ... traitement ...

# Uploader le r√©sultat
df.to_csv('processed-data.csv', index=False)
s3.upload_file('processed-data.csv', 'my-bucket', 'processed/data.csv')
```

---

## üìä Points cl√©s √† retenir

1. **S3 = Stockage illimit√©** et hautement disponible
2. **Free Tier : 5 Go** toujours gratuit
3. **Organiser avec pr√©fixes** pour meilleure performance
4. **Choisir la bonne classe** selon l'usage
5. **S3 s'int√®gre** avec tous les services AWS data

## üîó Prochain module

Passer au module [3. AWS Glue - ETL Serverless](../03-glue/README.md) pour apprendre √† transformer des donn√©es avec AWS Glue.

