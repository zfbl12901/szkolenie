# 4. Amazon Redshift - Data Warehouse

## üéØ Objectifs

- Comprendre Amazon Redshift et son r√¥le
- Cr√©er un cluster Redshift (gratuit 2 mois)
- Charger des donn√©es dans Redshift
- Optimiser les requ√™tes Redshift
- Int√©grer avec S3 et autres services

## üìã Table des mati√®res

1. [Introduction √† Redshift](#introduction-√†-redshift)
2. [Cr√©er un cluster Redshift](#cr√©er-un-cluster-redshift)
3. [Charger des donn√©es](#charger-des-donn√©es)
4. [Requ√™tes SQL avanc√©es](#requ√™tes-sql-avanc√©es)
5. [Optimisation](#optimisation)
6. [Int√©gration avec autres services](#int√©gration-avec-autres-services)

---

## Introduction √† Redshift

### Qu'est-ce qu'Amazon Redshift ?

**Amazon Redshift** = Data warehouse cloud g√©r√©

- **OLAP** : Optimis√© pour l'analyse (pas transactions)
- **Colonnes** : Stockage orient√© colonnes
- **Massivement parall√®le** : Traitement distribu√©
- **Scalable** : De quelques Go √† plusieurs Po

### Cas d'usage pour Data Analyst

- **Data Warehouse** : Centraliser les donn√©es
- **Analytics** : Requ√™tes complexes sur grandes volumes
- **Business Intelligence** : Dashboards et rapports
- **Data Mining** : Analyses approfondies

### Free Tier Redshift

**Gratuit 2 mois :**
- 750 heures/mois de cluster `dc2.large`
- 32 Go de stockage par n≈ìud
- Apr√®s 2 mois : facturation normale

**‚ö†Ô∏è Important :** Arr√™ter le cluster quand non utilis√© pour √©viter les co√ªts.

---

## Cr√©er un cluster Redshift

### √âtape 1 : Acc√©der √† Redshift

1. Console AWS ‚Üí Rechercher "Redshift"
2. Cliquer sur "Amazon Redshift"
3. "Create cluster"

### √âtape 2 : Configuration du cluster

**Configuration de base :**

1. **Cluster identifier** : `data-analyst-cluster`
2. **Node type** : `dc2.large` (gratuit 2 mois)
3. **Number of nodes** : 1 (suffisant pour d√©buter)
4. **Database name** : `analytics` (par d√©faut : `dev`)
5. **Database port** : 5439 (par d√©faut)
6. **Master username** : `admin` (ou autre)
7. **Master password** : Mot de passe fort

**Configuration r√©seau :**

1. **VPC** : Choisir un VPC existant
2. **Subnet group** : Cr√©er ou utiliser existant
3. **Publicly accessible** : ‚úÖ Oui (pour acc√®s facile)
4. **Availability zone** : Choisir une zone

**S√©curit√© :**

1. **VPC security groups** : Cr√©er un groupe de s√©curit√©
   - Autoriser le port 5439 depuis votre IP
2. **Encryption** : Activer (recommand√©)

### √âtape 3 : Cr√©er le cluster

1. Cliquer sur "Create cluster"
2. Attendre 5-10 minutes (cr√©ation)
3. Cluster pr√™t quand status = "Available"

**‚ö†Ô∏è Important :** Noter l'endpoint du cluster (ex: `data-analyst-cluster.xxxxx.eu-west-3.redshift.amazonaws.com:5439`)

---

## Charger des donn√©es

### M√©thode 1 : COPY depuis S3 (recommand√©)

**Le plus rapide pour grandes quantit√©s :**

```sql
-- Cr√©er une table
CREATE TABLE users (
    id INTEGER,
    name VARCHAR(100),
    email VARCHAR(100),
    created_at TIMESTAMP
);

-- Charger depuis S3
COPY users
FROM 's3://my-bucket/data/users.csv'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
CSV
IGNOREHEADER 1;
```

**Configuration IAM Role :**

1. IAM ‚Üí "Roles" ‚Üí "Create role"
2. Type : "Redshift"
3. Attacher politique : `AmazonS3ReadOnlyAccess`
4. Nom : `RedshiftS3Role`
5. Copier l'ARN pour COPY

### M√©thode 2 : INSERT (petites quantit√©s)

```sql
INSERT INTO users (id, name, email, created_at)
VALUES (1, 'John Doe', 'john@example.com', '2024-01-01');
```

### M√©thode 3 : INSERT depuis requ√™te

```sql
INSERT INTO users_aggregated
SELECT 
    DATE_TRUNC('month', created_at) AS month,
    COUNT(*) AS user_count
FROM users
GROUP BY DATE_TRUNC('month', created_at);
```

### Formats support√©s

- **CSV** : Fichiers CSV
- **JSON** : Fichiers JSON
- **Parquet** : Format optimis√© (recommand√©)
- **Avro** : Format Avro

---

## Requ√™tes SQL avanc√©es

### Fonctions analytiques

**Window functions :**

```sql
-- ROW_NUMBER
SELECT 
    id,
    name,
    ROW_NUMBER() OVER (PARTITION BY category ORDER BY created_at) AS rank
FROM products;

-- LAG/LEAD
SELECT 
    date,
    sales,
    LAG(sales, 1) OVER (ORDER BY date) AS previous_sales,
    LEAD(sales, 1) OVER (ORDER BY date) AS next_sales
FROM daily_sales;

-- RANK
SELECT 
    user_id,
    total_spent,
    RANK() OVER (ORDER BY total_spent DESC) AS spending_rank
FROM user_totals;
```

### Agr√©gations complexes

```sql
-- GROUP BY avec ROLLUP
SELECT 
    category,
    region,
    SUM(amount) AS total
FROM sales
GROUP BY ROLLUP(category, region);

-- GROUP BY avec CUBE
SELECT 
    category,
    region,
    SUM(amount) AS total
FROM sales
GROUP BY CUBE(category, region);
```

### Jointures optimis√©es

```sql
-- Jointure avec distribution key
SELECT 
    u.name,
    o.amount,
    o.created_at
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE u.created_at > '2024-01-01';
```

---

## Optimisation

### Distribution keys

**Choisir la bonne distribution key :**

```sql
-- Distribution par cl√© (pour jointures)
CREATE TABLE users (
    id INTEGER DISTKEY,
    name VARCHAR(100),
    email VARCHAR(100)
);

-- Distribution ALL (pour petites tables)
CREATE TABLE categories (
    id INTEGER,
    name VARCHAR(100)
) DISTSTYLE ALL;

-- Distribution EVEN (par d√©faut)
CREATE TABLE logs (
    id INTEGER,
    message TEXT
) DISTSTYLE EVEN;
```

### Sort keys

**Am√©liorer les performances de requ√™tes :**

```sql
-- Sort key simple
CREATE TABLE orders (
    id INTEGER,
    user_id INTEGER,
    created_at TIMESTAMP,
    amount DECIMAL(10,2)
) SORTKEY (created_at);

-- Sort key composite
CREATE TABLE sales (
    date DATE,
    region VARCHAR(50),
    amount DECIMAL(10,2)
) SORTKEY (date, region);
```

### Compression

**R√©duire l'espace de stockage :**

```sql
-- Compression automatique
CREATE TABLE users (
    id INTEGER,
    name VARCHAR(100) ENCODE lzo,
    email VARCHAR(100) ENCODE lzo,
    created_at TIMESTAMP ENCODE delta
);
```

### ANALYZE

**Mettre √† jour les statistiques :**

```sql
-- Analyser une table
ANALYZE users;

-- Analyser toutes les tables
ANALYZE;
```

---

## Int√©gration avec autres services

### Redshift + S3

**Unload vers S3 :**

```sql
UNLOAD ('SELECT * FROM users WHERE created_at > ''2024-01-01''')
TO 's3://my-bucket/exports/users/'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
CSV
PARALLEL OFF;
```

### Redshift + Glue

**Glue peut charger dans Redshift :**

```python
# Dans un job Glue
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame = transformed_data,
    catalog_connection = "redshift-connection",
    connection_options = {
        "dbtable": "users",
        "database": "analytics"
    }
)
```

### Redshift + QuickSight

**Connecter QuickSight √† Redshift :**

1. QuickSight ‚Üí "Data sources"
2. "Redshift"
3. Entrer les informations de connexion
4. S√©lectionner les tables
5. Cr√©er des visualisations

---

## Bonnes pratiques

### Performance

1. **Utiliser COPY** au lieu de INSERT pour grandes quantit√©s
2. **Choisir les bonnes distribution keys**
3. **Utiliser des sort keys** pour requ√™tes fr√©quentes
4. **Compresser les colonnes** pour √©conomiser l'espace
5. **VACUUM r√©guli√®rement** pour optimiser

### Co√ªts

1. **Arr√™ter le cluster** quand non utilis√©
2. **Utiliser le bon type de n≈ìud** selon les besoins
3. **Surveiller l'utilisation** du stockage
4. **Nettoyer les donn√©es** inutiles

### S√©curit√©

1. **Chiffrer les donn√©es** en transit et au repos
2. **Utiliser VPC** pour isoler le cluster
3. **Limiter l'acc√®s** avec security groups
4. **Auditer les acc√®s** avec CloudTrail

---

## Exemples pratiques

### Exemple 1 : Pipeline complet S3 ‚Üí Redshift

```sql
-- 1. Cr√©er la table
CREATE TABLE sales (
    id INTEGER,
    product_id INTEGER,
    amount DECIMAL(10,2),
    sale_date DATE
) DISTKEY(product_id) SORTKEY(sale_date);

-- 2. Charger depuis S3
COPY sales
FROM 's3://my-bucket/data/sales/'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
CSV
IGNOREHEADER 1;

-- 3. Analyser
ANALYZE sales;

-- 4. Requ√™tes analytiques
SELECT 
    DATE_TRUNC('month', sale_date) AS month,
    SUM(amount) AS total_sales
FROM sales
GROUP BY DATE_TRUNC('month', sale_date)
ORDER BY month;
```

### Exemple 2 : Agr√©gations avec window functions

```sql
-- Top 10 produits par mois
SELECT 
    product_id,
    month,
    total_sales,
    RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) AS rank
FROM (
    SELECT 
        product_id,
        DATE_TRUNC('month', sale_date) AS month,
        SUM(amount) AS total_sales
    FROM sales
    GROUP BY product_id, DATE_TRUNC('month', sale_date)
) monthly_sales
WHERE RANK() OVER (PARTITION BY month ORDER BY total_sales DESC) <= 10;
```

---

## üìä Points cl√©s √† retenir

1. **Redshift = Data warehouse** pour analytics
2. **Free Tier : 2 mois** gratuit (750 heures)
3. **COPY depuis S3** = m√©thode la plus rapide
4. **Distribution et sort keys** = cl√©s de performance
5. **Arr√™ter le cluster** quand non utilis√©

## üîó Prochain module

Passer au module [5. Amazon Athena - Requ√™tes SQL sur S3](../05-athena/README.md) pour apprendre √† interroger directement les fichiers S3.

