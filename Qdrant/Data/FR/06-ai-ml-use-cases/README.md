# 6. Cas d'usage IA/ML

## üéØ Objectifs

- Comprendre les cas d'usage typiques
- Impl√©menter la recherche s√©mantique
- Cr√©er des syst√®mes de recommandation
- Utiliser avec RAG (Retrieval-Augmented Generation)
- D√©tecter les doublons
- Clustering et classification

## üìã Table des mati√®res

1. [Recherche s√©mantique](#recherche-s√©mantique)
2. [Syst√®mes de recommandation](#syst√®mes-de-recommandation)
3. [RAG (Retrieval-Augmented Generation)](#rag-retrieval-augmented-generation)
4. [D√©duplication](#d√©duplication)
5. [Clustering](#clustering)
6. [Classification](#classification)
7. [Anomaly Detection](#anomaly-detection)

---

## Recherche s√©mantique

### Qu'est-ce que la recherche s√©mantique ?

La **recherche s√©mantique** trouve des r√©sultats bas√©s sur le **sens** plut√¥t que sur des mots-cl√©s exacts.

**Recherche classique (mots-cl√©s) :**
- Requ√™te : "laptop"
- R√©sultats : Documents contenant le mot "laptop"

**Recherche s√©mantique :**
- Requ√™te : "computer for work"
- R√©sultats : Documents sur "laptop", "workstation", "desktop", etc.

### Impl√©mentation compl√®te

```python
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

class SemanticSearchEngine:
    def __init__(self, collection_name="documents"):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.client = QdrantClient(host="localhost", port=6333)
        self.collection_name = collection_name
        self._ensure_collection()
    
    def _ensure_collection(self):
        """Cr√©er la collection si n√©cessaire"""
        collections = self.client.get_collections()
        if self.collection_name not in [c.name for c in collections.collections]:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=384, distance=Distance.COSINE)
            )
    
    def index_documents(self, documents):
        """Indexer des documents"""
        texts = [doc["text"] for doc in documents]
        embeddings = self.model.encode(texts)
        
        points = [
            PointStruct(
                id=doc["id"],
                vector=emb.tolist(),
                payload={"text": doc["text"], **doc.get("metadata", {})}
            )
            for doc, emb in zip(documents, embeddings)
        ]
        
        self.client.upsert(collection_name=self.collection_name, points=points)
    
    def search(self, query, limit=10, filter=None):
        """Rechercher des documents similaires"""
        query_embedding = self.model.encode([query])[0]
        
        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_embedding.tolist(),
            query_filter=filter,
            limit=limit
        )
        
        return [
            {
                "id": r.id,
                "score": r.score,
                "text": r.payload["text"],
                "metadata": {k: v for k, v in r.payload.items() if k != "text"}
            }
            for r in results
        ]

# Utilisation
engine = SemanticSearchEngine()

# Indexer
documents = [
    {"id": 1, "text": "Laptop computer for professional work"},
    {"id": 2, "text": "Gaming laptop with high-end graphics"},
    {"id": 3, "text": "Business workstation for office use"}
]
engine.index_documents(documents)

# Rechercher
results = engine.search("computer for work", limit=5)
for result in results:
    print(f"Score: {result['score']:.4f} - {result['text']}")
```

---

## Syst√®mes de recommandation

### Recommandation basique (similarit√©)

```python
def recommend_similar_products(product_id, limit=10):
    """Recommander des produits similaires"""
    
    client = QdrantClient(host="localhost", port=6333)
    
    # 1. R√©cup√©rer le vecteur du produit
    points = client.retrieve(
        collection_name="products",
        ids=[product_id],
        with_vectors=True
    )
    
    if not points:
        return []
    
    product_vector = points[0].vector
    
    # 2. Rechercher des produits similaires (exclure l'original)
    results = client.search(
        collection_name="products",
        query_vector=product_vector,
        query_filter=Filter(
            must_not=[
                FieldCondition(key="id", match=MatchValue(value=product_id))
            ]
        ),
        limit=limit
    )
    
    return results
```

### Recommandation hybride (similarit√© + pr√©f√©rences)

```python
def hybrid_recommendation(product_id, user_preferences, limit=10):
    """Recommandation combinant similarit√© et pr√©f√©rences"""
    
    client = QdrantClient(host="localhost", port=6333)
    
    # Vecteur du produit
    points = client.retrieve(
        collection_name="products",
        ids=[product_id],
        with_vectors=True
    )
    product_vector = points[0].vector
    
    # Recherche avec filtres de pr√©f√©rences
    results = client.search(
        collection_name="products",
        query_vector=product_vector,
        query_filter=Filter(
            must=[
                FieldCondition(
                    key="category",
                    match=MatchValue(value=user_preferences["preferred_category"])
                ),
                FieldCondition(
                    key="price",
                    range=Range(
                        gte=user_preferences["min_price"],
                        lte=user_preferences["max_price"]
                    )
                )
            ],
            must_not=[
                FieldCondition(key="id", match=MatchValue(value=product_id))
            ]
        ),
        limit=limit,
        score_threshold=0.7
    )
    
    return results
```

### Recommandation bas√©e sur l'historique utilisateur

```python
def user_based_recommendation(user_id, limit=10):
    """Recommandation bas√©e sur l'historique utilisateur"""
    
    client = QdrantClient(host="localhost", port=6333)
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # 1. R√©cup√©rer l'historique utilisateur
    user_history = get_user_purchase_history(user_id)  # Fonction externe
    
    # 2. G√©n√©rer embeddings des produits achet√©s
    product_texts = [item["description"] for item in user_history]
    embeddings = model.encode(product_texts)
    
    # 3. Calculer le vecteur moyen (profil utilisateur)
    user_profile_vector = embeddings.mean(axis=0).tolist()
    
    # 4. Recommander des produits similaires
    results = client.search(
        collection_name="products",
        query_vector=user_profile_vector,
        query_filter=Filter(
            must_not=[
                FieldCondition(
                    key="id",
                    match=MatchAny(any=[item["product_id"] for item in user_history])
                )
            ]
        ),
        limit=limit
    )
    
    return results
```

---

## RAG (Retrieval-Augmented Generation)

### Qu'est-ce que RAG ?

**RAG** combine :
- **Retrieval** : Recherche de documents pertinents (Qdrant)
- **Augmented Generation** : G√©n√©ration de texte avec LLM (GPT, etc.)

### Pipeline RAG simple

```python
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from openai import OpenAI

class RAGSystem:
    def __init__(self):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.client = QdrantClient(host="localhost", port=6333)
        self.llm = OpenAI()  # Ou autre LLM
    
    def retrieve_context(self, query, limit=5):
        """R√©cup√©rer le contexte pertinent"""
        query_embedding = self.embedding_model.encode([query])[0]
        
        results = self.client.search(
            collection_name="knowledge_base",
            query_vector=query_embedding.tolist(),
            limit=limit
        )
        
        # Extraire le texte
        context = "\n\n".join([
            result.payload["text"] for result in results
        ])
        
        return context
    
    def generate_answer(self, query, context):
        """G√©n√©rer une r√©ponse avec le contexte"""
        
        prompt = f"""Contexte:
{context}

Question: {query}

R√©ponds √† la question en utilisant le contexte fourni."""

        response = self.llm.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un assistant utile."},
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.choices[0].message.content
    
    def answer(self, query, limit=5):
        """Pipeline RAG complet"""
        # 1. Retrieval
        context = self.retrieve_context(query, limit)
        
        # 2. Generation
        answer = self.generate_answer(query, context)
        
        return {
            "answer": answer,
            "context": context
        }

# Utilisation
rag = RAGSystem()
result = rag.answer("Quels sont les avantages des laptops gaming?")
print(result["answer"])
```

### RAG avec sources

```python
def rag_with_sources(query, limit=5):
    """RAG qui retourne aussi les sources"""
    
    query_embedding = model.encode([query])[0]
    
    results = client.search(
        collection_name="knowledge_base",
        query_vector=query_embedding.tolist(),
        limit=limit
    )
    
    # Construire le contexte avec sources
    context_parts = []
    sources = []
    
    for i, result in enumerate(results):
        text = result.payload["text"]
        source = result.payload.get("source", f"Document {result.id}")
        
        context_parts.append(f"[Source {i+1}]: {text}")
        sources.append({
            "id": result.id,
            "source": source,
            "score": result.score
        })
    
    context = "\n\n".join(context_parts)
    
    # G√©n√©rer r√©ponse
    answer = generate_answer(query, context)
    
    return {
        "answer": answer,
        "sources": sources
    }
```

---

## D√©duplication

### D√©tecter les doublons

```python
def find_duplicates(threshold=0.95):
    """Trouver les documents dupliqu√©s"""
    
    client = QdrantClient(host="localhost", port=6333)
    
    # Parcourir tous les points
    all_points = []
    scroll_result = client.scroll(
        collection_name="documents",
        limit=100,
        with_vectors=True,
        with_payload=True
    )
    
    points, next_offset = scroll_result
    all_points.extend(points)
    
    # Continuer le scroll
    while next_offset is not None:
        scroll_result = client.scroll(
            collection_name="documents",
            limit=100,
            offset=next_offset,
            with_vectors=True,
            with_payload=True
        )
        points, next_offset = scroll_result
        all_points.extend(points)
    
    # Trouver les doublons
    duplicates = []
    processed = set()
    
    for i, point1 in enumerate(all_points):
        if point1.id in processed:
            continue
        
        similar = client.search(
            collection_name="documents",
            query_vector=point1.vector,
            limit=10,
            score_threshold=threshold
        )
        
        # Filtrer l'√©l√©ment lui-m√™me
        similar = [s for s in similar if s.id != point1.id]
        
        if similar:
            duplicates.append({
                "original": point1.id,
                "duplicates": [s.id for s in similar]
            })
            processed.add(point1.id)
            processed.update([s.id for s in similar])
    
    return duplicates
```

---

## Clustering

### Clustering simple

```python
from sklearn.cluster import KMeans
import numpy as np

def cluster_documents(n_clusters=5):
    """Grouper des documents similaires"""
    
    client = QdrantClient(host="localhost", port=6333)
    
    # R√©cup√©rer tous les vecteurs
    all_points = []
    scroll_result = client.scroll(
        collection_name="documents",
        limit=1000,
        with_vectors=True,
        with_payload=True
    )
    points, _ = scroll_result
    all_points.extend(points)
    
    # Extraire les vecteurs
    vectors = np.array([point.vector for point in all_points])
    
    # Clustering K-means
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(vectors)
    
    # Mettre √† jour le payload avec les clusters
    for point, cluster_id in zip(all_points, clusters):
        client.set_payload(
            collection_name="documents",
            payload={"cluster": int(cluster_id)},
            points=[point.id]
        )
    
    return clusters
```

---

## Classification

### Classification par similarit√©

```python
def classify_document(query_vector, categories):
    """Classer un document dans une cat√©gorie"""
    
    client = QdrantClient(host="localhost", port=6333)
    
    # Pour chaque cat√©gorie, trouver le document le plus similaire
    category_scores = {}
    
    for category in categories:
        results = client.search(
            collection_name="documents",
            query_vector=query_vector,
            query_filter=Filter(
                must=[
                    FieldCondition(key="category", match=MatchValue(value=category))
                ]
            ),
            limit=1
        )
        
        if results:
            category_scores[category] = results[0].score
    
    # Retourner la cat√©gorie avec le meilleur score
    if category_scores:
        best_category = max(category_scores, key=category_scores.get)
        confidence = category_scores[best_category]
        return best_category, confidence
    
    return None, 0.0
```

---

## Anomaly Detection

### D√©tecter les anomalies

```python
def detect_anomalies(threshold=0.3):
    """D√©tecter des documents anormaux (peu similaires aux autres)"""
    
    client = QdrantClient(host="localhost", port=6333)
    
    # Parcourir tous les points
    all_points = []
    scroll_result = client.scroll(
        collection_name="documents",
        limit=1000,
        with_vectors=True
    )
    points, _ = scroll_result
    all_points.extend(points)
    
    anomalies = []
    
    for point in all_points:
        # Chercher les documents similaires
        results = client.search(
            collection_name="documents",
            query_vector=point.vector,
            limit=10,
            score_threshold=threshold
        )
        
        # Si peu de documents similaires, c'est une anomalie
        if len(results) < 3:  # Moins de 3 documents similaires
            anomalies.append(point.id)
    
    return anomalies
```

---

## Exercices pratiques

### Exercice 1 : Syst√®me de recommandation complet

Cr√©er un syst√®me qui recommande des produits bas√© sur :
- Similarit√© vectorielle
- Cat√©gorie pr√©f√©r√©e de l'utilisateur
- Budget de l'utilisateur

**Solution :**

```python
def recommend_products(user_id, limit=10):
    # R√©cup√©rer pr√©f√©rences utilisateur
    user_prefs = get_user_preferences(user_id)
    
    # Historique utilisateur
    history = get_user_history(user_id)
    
    # Profil utilisateur (vecteur moyen)
    if history:
        vectors = [get_product_vector(pid) for pid in history]
        user_vector = np.mean(vectors, axis=0).tolist()
    else:
        # Pas d'historique : utiliser cat√©gorie pr√©f√©r√©e
        category_products = get_category_products(user_prefs["category"])
        vectors = [p.vector for p in category_products]
        user_vector = np.mean(vectors, axis=0).tolist()
    
    # Recherche avec filtres
    results = client.search(
        collection_name="products",
        query_vector=user_vector,
        query_filter=Filter(
            must=[
                FieldCondition(
                    key="price",
                    range=Range(
                        gte=user_prefs["min_price"],
                        lte=user_prefs["max_price"]
                    )
                )
            ],
            must_not=[
                FieldCondition(
                    key="id",
                    match=MatchAny(any=history)
                )
            ]
        ),
        limit=limit
    )
    
    return results
```

---

## üéØ Points cl√©s √† retenir

‚úÖ La recherche s√©mantique trouve par sens, pas par mots-cl√©s  
‚úÖ Les recommandations combinent similarit√© et pr√©f√©rences  
‚úÖ RAG combine retrieval (Qdrant) et generation (LLM)  
‚úÖ La d√©duplication utilise des seuils de similarit√© √©lev√©s  
‚úÖ Le clustering groupe des documents similaires  

---

**Prochaine √©tape :** [Bonnes Pratiques](./07-best-practices/README.md)
