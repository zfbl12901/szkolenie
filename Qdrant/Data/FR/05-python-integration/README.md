# 5. Int√©gration Python

## üéØ Objectifs

- Int√©grer avec des mod√®les d'embeddings
- Utiliser sentence-transformers
- Int√©grer avec OpenAI, Cohere, etc.
- Cr√©er des pipelines de donn√©es
- Optimiser les performances
- G√©rer les batchs d'embeddings

## üìã Table des mati√®res

1. [Mod√®les d'embeddings](#mod√®les-dembeddings)
2. [sentence-transformers](#sentence-transformers)
3. [OpenAI Embeddings](#openai-embeddings)
4. [Cohere Embeddings](#cohere-embeddings)
5. [Pipelines de donn√©es](#pipelines-de-donn√©es)
6. [Optimisation batch](#optimisation-batch)
7. [Gestion des erreurs](#gestion-des-erreurs)

---

## Mod√®les d'embeddings

### Qu'est-ce qu'un embedding ?

Un **embedding** est une repr√©sentation vectorielle d'un objet :
- **Texte** ‚Üí Vecteur de nombres
- **Image** ‚Üí Vecteur de nombres
- **Audio** ‚Üí Vecteur de nombres

### Types de mod√®les

- **Text embeddings** : Pour textes (sentence-transformers, OpenAI)
- **Image embeddings** : Pour images (CLIP, ResNet)
- **Multimodal** : Pour texte + image (CLIP)

---

## sentence-transformers

### Installation

```bash
pip install sentence-transformers
```

### Mod√®le de base

```python
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct

# Charger un mod√®le pr√©-entra√Æn√©
model = SentenceTransformer('all-MiniLM-L6-v2')  # Dimension 384

# G√©n√©rer des embeddings
texts = [
    "Laptop computer for work",
    "Gaming laptop with high performance",
    "Professional workstation"
]

embeddings = model.encode(texts)
print(f"Shape: {embeddings.shape}")  # (3, 384)
```

### Mod√®les recommand√©s

```python
# Mod√®les populaires
models = {
    # L√©ger et rapide
    "all-MiniLM-L6-v2": 384,  # 22MB, rapide
    
    # √âquilibr√©
    "all-mpnet-base-v2": 768,  # 420MB, meilleure qualit√©
    
    # Multilingue
    "paraphrase-multilingual-MiniLM-L12-v2": 384,  # Support multilingue
    
    # Sp√©cialis√©
    "ms-marco-MiniLM-L-6-v2": 384,  # Optimis√© pour recherche
}
```

### Indexer des documents

```python
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# Initialisation
model = SentenceTransformer('all-MiniLM-L6-v2')
client = QdrantClient(host="localhost", port=6333)

# Cr√©er la collection
client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(
        size=384,  # Dimension du mod√®le
        distance=Distance.COSINE
    )
)

# Documents √† indexer
documents = [
    {"id": 1, "text": "Laptop computer for professional work"},
    {"id": 2, "text": "Gaming laptop with high-end graphics"},
    {"id": 3, "text": "Business workstation for office use"}
]

# G√©n√©rer les embeddings
texts = [doc["text"] for doc in documents]
embeddings = model.encode(texts)

# Cr√©er les points
points = [
    PointStruct(
        id=doc["id"],
        vector=embedding.tolist(),
        payload={"text": doc["text"]}
    )
    for doc, embedding in zip(documents, embeddings)
]

# Ins√©rer dans Qdrant
client.upsert(collection_name="documents", points=points)
```

### Recherche s√©mantique

```python
# Requ√™te utilisateur
query = "computer for work"

# G√©n√©rer l'embedding de la requ√™te
query_embedding = model.encode([query])[0]

# Rechercher dans Qdrant
results = client.search(
    collection_name="documents",
    query_vector=query_embedding.tolist(),
    limit=5
)

# Afficher les r√©sultats
for result in results:
    print(f"Score: {result.score:.4f}")
    print(f"Text: {result.payload['text']}")
    print("---")
```

### Batch processing

```python
def index_documents_batch(documents, batch_size=100):
    """Indexer des documents par batches"""
    
    model = SentenceTransformer('all-MiniLM-L6-v2')
    client = QdrantClient(host="localhost", port=6333)
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        
        # G√©n√©rer embeddings
        texts = [doc["text"] for doc in batch]
        embeddings = model.encode(texts, show_progress_bar=True)
        
        # Cr√©er points
        points = [
            PointStruct(
                id=doc["id"],
                vector=emb.tolist(),
                payload={"text": doc["text"]}
            )
            for doc, emb in zip(batch, embeddings)
        ]
        
        # Ins√©rer
        client.upsert(collection_name="documents", points=points)
        print(f"Indexed batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}")
```

---

## OpenAI Embeddings

### Installation

```bash
pip install openai
```

### Configuration

```python
import os
from openai import OpenAI

# Configuration
os.environ["OPENAI_API_KEY"] = "your-api-key"
client_openai = OpenAI()
```

### G√©n√©rer des embeddings

```python
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct

client_openai = OpenAI()
client_qdrant = QdrantClient(host="localhost", port=6333)

# Documents
texts = [
    "Laptop computer for work",
    "Gaming laptop with high performance"
]

# G√©n√©rer embeddings avec OpenAI
response = client_openai.embeddings.create(
    model="text-embedding-ada-002",  # Dimension 1536
    input=texts
)

# Extraire les embeddings
embeddings = [item.embedding for item in response.data]

# Cr√©er les points
points = [
    PointStruct(
        id=i,
        vector=embedding,
        payload={"text": text}
    )
    for i, (text, embedding) in enumerate(zip(texts, embeddings))
]

# Ins√©rer dans Qdrant
client_qdrant.upsert(
    collection_name="documents_openai",
    points=points
)
```

### Recherche avec OpenAI

```python
# Requ√™te
query = "computer for professional use"

# Embedding de la requ√™te
response = client_openai.embeddings.create(
    model="text-embedding-ada-002",
    input=[query]
)
query_embedding = response.data[0].embedding

# Recherche
results = client_qdrant.search(
    collection_name="documents_openai",
    query_vector=query_embedding,
    limit=5
)
```

---

## Cohere Embeddings

### Installation

```bash
pip install cohere
```

### Utilisation

```python
import cohere
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct

# Initialisation
co = cohere.Client("your-api-key")
client = QdrantClient(host="localhost", port=6333)

# G√©n√©rer embeddings
texts = ["Laptop computer", "Gaming laptop"]
response = co.embed(
    texts=texts,
    model="embed-english-v2.0"  # Dimension 4096
)

embeddings = response.embeddings

# Ins√©rer dans Qdrant
points = [
    PointStruct(
        id=i,
        vector=embedding,
        payload={"text": text}
    )
    for i, (text, embedding) in enumerate(zip(texts, embeddings))
]

client.upsert(collection_name="documents_cohere", points=points)
```

---

## Pipelines de donn√©es

### Pipeline complet

```python
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import pandas as pd

class DocumentIndexer:
    def __init__(self, collection_name="documents"):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.client = QdrantClient(host="localhost", port=6333)
        self.collection_name = collection_name
        self._ensure_collection()
    
    def _ensure_collection(self):
        """Cr√©er la collection si elle n'existe pas"""
        collections = self.client.get_collections()
        if self.collection_name not in [c.name for c in collections.collections]:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=384,
                    distance=Distance.COSINE
                )
            )
    
    def index_dataframe(self, df, text_column, id_column=None, batch_size=100):
        """Indexer un DataFrame"""
        
        if id_column is None:
            df['_id'] = range(len(df))
            id_column = '_id'
        
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size]
            
            # G√©n√©rer embeddings
            texts = batch[text_column].tolist()
            embeddings = self.model.encode(texts)
            
            # Cr√©er points
            points = [
                PointStruct(
                    id=int(row[id_column]),
                    vector=emb.tolist(),
                    payload=row.to_dict()
                )
                for row, emb in zip(batch.itertuples(), embeddings)
            ]
            
            # Ins√©rer
            self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            
            print(f"Indexed {min(i+batch_size, len(df))}/{len(df)}")
    
    def search(self, query, limit=10, filter=None):
        """Rechercher dans la collection"""
        
        query_embedding = self.model.encode([query])[0]
        
        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_embedding.tolist(),
            query_filter=filter,
            limit=limit
        )
        
        return results

# Utilisation
indexer = DocumentIndexer()

# Charger des donn√©es
df = pd.read_csv("products.csv")

# Indexer
indexer.index_dataframe(df, text_column="description", id_column="product_id")

# Rechercher
results = indexer.search("laptop computer", limit=5)
```

---

## Optimisation batch

### Batch encoding

```python
# Plus efficace : encoder par batches
texts = ["text1", "text2", ..., "text1000"]

# ‚úÖ Bon : batch encoding
embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)

# ‚ùå Moins bon : un par un
embeddings = [model.encode([text])[0] for text in texts]
```

### Batch insertion

```python
def efficient_indexing(documents, batch_size=100):
    """Indexation efficace par batches"""
    
    model = SentenceTransformer('all-MiniLM-L6-v2')
    client = QdrantClient(host="localhost", port=6333)
    
    # Encoder tous les documents
    texts = [doc["text"] for doc in documents]
    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)
    
    # Ins√©rer par batches dans Qdrant
    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i+batch_size]
        batch_embeddings = embeddings[i:i+batch_size]
        
        points = [
            PointStruct(
                id=doc["id"],
                vector=emb.tolist(),
                payload={"text": doc["text"]}
            )
            for doc, emb in zip(batch_docs, batch_embeddings)
        ]
        
        client.upsert(collection_name="documents", points=points)
```

### Utilisation du GPU

```python
import torch

# V√©rifier si GPU disponible
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Charger le mod√®le sur GPU
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# Encoder (plus rapide sur GPU)
embeddings = model.encode(texts, batch_size=64)  # Batch plus grand sur GPU
```

---

## Gestion des erreurs

### Gestion robuste

```python
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
import time

def robust_indexing(documents, max_retries=3):
    """Indexation avec gestion d'erreurs"""
    
    model = SentenceTransformer('all-MiniLM-L6-v2')
    client = QdrantClient(host="localhost", port=6333)
    
    for doc in documents:
        retries = 0
        while retries < max_retries:
            try:
                # G√©n√©rer embedding
                embedding = model.encode([doc["text"]])[0]
                
                # Ins√©rer
                point = PointStruct(
                    id=doc["id"],
                    vector=embedding.tolist(),
                    payload={"text": doc["text"]}
                )
                
                client.upsert(collection_name="documents", points=[point])
                break  # Succ√®s
                
            except Exception as e:
                retries += 1
                print(f"Error indexing doc {doc['id']}: {e}")
                if retries < max_retries:
                    time.sleep(2 ** retries)  # Backoff exponentiel
                else:
                    print(f"Failed to index doc {doc['id']} after {max_retries} retries")
```

---

## Exercices pratiques

### Exercice 1 : Pipeline d'indexation

Cr√©er un pipeline qui :
1. Lit un fichier CSV
2. G√©n√®re des embeddings
3. Indexe dans Qdrant
4. Affiche la progression

**Solution :**

```python
import pandas as pd
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct

def index_csv(csv_path, text_column, id_column):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    client = QdrantClient(host="localhost", port=6333)
    
    df = pd.read_csv(csv_path)
    
    # Encoder tous les textes
    texts = df[text_column].tolist()
    embeddings = model.encode(texts, show_progress_bar=True)
    
    # Cr√©er et ins√©rer les points
    points = [
        PointStruct(
            id=int(row[id_column]),
            vector=emb.tolist(),
            payload=row.to_dict()
        )
        for row, emb in zip(df.itertuples(), embeddings)
    ]
    
    client.upsert(collection_name="documents", points=points)
    print(f"Indexed {len(points)} documents")
```

---

## üéØ Points cl√©s √† retenir

‚úÖ sentence-transformers est id√©al pour les embeddings de texte  
‚úÖ Utiliser batch encoding pour de meilleures performances  
‚úÖ GPU acc√©l√®re significativement l'encodage  
‚úÖ G√©rer les erreurs avec retry logic  
‚úÖ Cr√©er des pipelines r√©utilisables pour l'indexation  

---

**Prochaine √©tape :** [Cas d'usage IA/ML](./06-ai-ml-use-cases/README.md)
