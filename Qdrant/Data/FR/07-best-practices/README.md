# 7. Bonnes Pratiques

## üéØ Objectifs

- Optimiser les performances
- G√©rer la dimension des vecteurs
- Choisir la bonne distance
- Structurer les m√©tadonn√©es
- G√©rer la m√©moire
- Monitoring et maintenance

## üìã Table des mati√®res

1. [Dimension des vecteurs](#dimension-des-vecteurs)
2. [Choix de la distance](#choix-de-la-distance)
3. [Configuration HNSW](#configuration-hnsw)
4. [Structure des m√©tadonn√©es](#structure-des-m√©tadonn√©es)
5. [Gestion de la m√©moire](#gestion-de-la-m√©moire)
6. [Performance](#performance)
7. [S√©curit√©](#s√©curit√©)
8. [Monitoring](#monitoring)

---

## Dimension des vecteurs

### Choisir la bonne dimension

La dimension affecte :
- **Performance** : Plus grande = plus lent
- **Qualit√©** : Plus grande = g√©n√©ralement meilleure
- **M√©moire** : Plus grande = plus de m√©moire

### Recommandations par cas d'usage

| Cas d'usage | Dimension | Mod√®le exemple |
|-------------|-----------|----------------|
| **Textes courts** | 128-256 | MiniLM |
| **Textes moyens** | 384-512 | sentence-transformers |
| **Textes longs** | 768 | BERT base |
| **Textes avanc√©s** | 1536 | OpenAI ada-002 |
| **Images** | 512-768 | CLIP, ResNet |
| **Multimodal** | 512-768 | CLIP |

### Impact sur les performances

```python
# Dimension 128 : Rapide, moins pr√©cis
VectorParams(size=128, distance=Distance.COSINE)

# Dimension 384 : √âquilibr√© (recommand√©)
VectorParams(size=384, distance=Distance.COSINE)

# Dimension 1536 : Lent, tr√®s pr√©cis
VectorParams(size=1536, distance=Distance.COSINE)
```

### R√®gle g√©n√©rale

- **< 1M points** : 384-512 dimensions
- **1M - 10M points** : 256-384 dimensions
- **> 10M points** : 128-256 dimensions

---

## Choix de la distance

### Distance COSINE (recommand√© pour textes)

**Utilisation :**
- Embeddings normalis√©s
- Textes et recherche s√©mantique
- Recommandations

**Avantages :**
- Insensible √† la magnitude
- Score entre 0 et 1 (facile √† interpr√©ter)
- Id√©al pour embeddings normalis√©s

```python
VectorParams(size=384, distance=Distance.COSINE)
```

### Distance EUCLID

**Utilisation :**
- Coordonn√©es g√©ographiques
- Features num√©riques brutes
- Classification d'images

**Avantages :**
- Distance g√©om√©trique intuitive
- Bon pour donn√©es num√©riques

```python
VectorParams(size=128, distance=Distance.EUCLID)
```

### Distance DOT

**Utilisation :**
- Vecteurs non normalis√©s
- Scores pond√©r√©s
- Recommandations avec poids

**Avantages :**
- Prend en compte la magnitude
- Bon pour scores pond√©r√©s

```python
VectorParams(size=128, distance=Distance.DOT)
```

### Tableau de d√©cision

| Type de donn√©es | Distance recommand√©e |
|-----------------|---------------------|
| Textes (embeddings normalis√©s) | **COSINE** |
| Textes (embeddings non normalis√©s) | **DOT** |
| Coordonn√©es GPS | **EUCLID** |
| Features num√©riques | **EUCLID** |
| Images | **COSINE** ou **EUCLID** |
| Recommandations | **COSINE** |

---

## Configuration HNSW

### Param√®tres HNSW

```python
from qdrant_client.models import HnswConfigDiff

hnsw_config = HnswConfigDiff(
    m=16,  # Nombre de connexions (d√©faut: 16)
    ef_construct=100,  # Pr√©cision de construction (d√©faut: 100)
    full_scan_threshold=10000  # Seuil pour scan complet
)
```

### Param√®tre m (connexions)

- **m=8-16** : Rapide, moins pr√©cis (petites collections)
- **m=16-32** : √âquilibr√© (recommand√©)
- **m=32-64** : Lent, tr√®s pr√©cis (grandes collections)

```python
# Collection rapide (petite)
HnswConfigDiff(m=8, ef_construct=50)

# Collection √©quilibr√©e (moyenne)
HnswConfigDiff(m=16, ef_construct=100)

# Collection pr√©cise (grande)
HnswConfigDiff(m=32, ef_construct=200)
```

### Param√®tre ef_construct

- **ef_construct=50-100** : Construction rapide
- **ef_construct=100-200** : √âquilibr√©
- **ef_construct=200+** : Construction lente, meilleure qualit√©

### Param√®tre ef (recherche)

```python
# Recherche rapide (moins pr√©cis)
results = client.search(
    collection_name="products",
    query_vector=vector,
    limit=10,
    ef=32  # Plus petit = plus rapide
)

# Recherche pr√©cise (plus lent)
results = client.search(
    collection_name="products",
    query_vector=vector,
    limit=10,
    ef=128  # Plus grand = plus pr√©cis
)
```

---

## Structure des m√©tadonn√©es

### Bonnes pratiques pour le payload

```python
# ‚úÖ Bon : Structure claire et typ√©e
payload = {
    "title": "Product Name",  # String
    "category": "electronics",  # String (indexable)
    "price": 99.99,  # Float (indexable)
    "quantity": 10,  # Integer
    "in_stock": True,  # Boolean
    "tags": ["laptop", "gaming"],  # Array
    "created_at": "2024-01-15T10:00:00Z",  # ISO date string
    "metadata": {  # Objet imbriqu√©
        "brand": "BrandX",
        "model": "Model123"
    }
}

# ‚ùå Moins bon : Structure incoh√©rente
payload = {
    "title": "Product Name",
    "Category": "electronics",  # Incoh√©rence de casse
    "price": "99.99",  # String au lieu de number
    "tags": "laptop,gaming",  # String au lieu d'array
    "created": "15/01/2024"  # Format date non standard
}
```

### Indexer les champs fr√©quemment filtr√©s

```python
# Indexer les champs utilis√©s dans les filtres
client.create_payload_index(
    collection_name="products",
    field_name="category",
    field_schema=PayloadSchemaType.KEYWORD
)

client.create_payload_index(
    collection_name="products",
    field_name="price",
    field_schema=PayloadSchemaType.FLOAT
)

client.create_payload_index(
    collection_name="products",
    field_name="created_at",
    field_schema=PayloadSchemaType.KEYWORD  # Pour dates
)
```

### √âviter les payloads trop volumineux

```python
# ‚úÖ Bon : Payload concis
payload = {
    "id": 123,
    "title": "Product",
    "category": "electronics"
}

# ‚ùå Moins bon : Payload trop volumineux
payload = {
    "id": 123,
    "title": "Product",
    "full_description": "..." * 1000,  # Texte tr√®s long
    "high_res_image": base64_image,  # Image encod√©e
    "full_specs": {...}  # Objet tr√®s volumineux
}
```

---

## Gestion de la m√©moire

### Quantisation

La **quantisation** r√©duit la m√©moire utilis√©e :

```python
from qdrant_client.models import QuantizationConfig, ScalarQuantization, ScalarType

# Quantisation INT8 (r√©duction 4x)
quantization_config = QuantizationConfig(
    scalar=ScalarQuantization(
        type=ScalarType.INT8,
        quantile=0.99,
        always_ram=True
    )
)

client.create_collection(
    collection_name="products",
    vectors_config=VectorParams(size=384, distance=Distance.COSINE),
    quantization_config=quantization_config
)
```

**Avantages :**
- R√©duction m√©moire : 4x (float32 ‚Üí int8)
- Recherche plus rapide
- L√©g√®re perte de pr√©cision

### Memmap (m√©moire mapp√©e)

Pour tr√®s grandes collections :

```python
from qdrant_client.models import OptimizersConfigDiff

optimizers_config = OptimizersConfigDiff(
    memmap_threshold=50000  # Utiliser memmap si > 50000 points
)

client.create_collection(
    collection_name="large_collection",
    vectors_config=VectorParams(size=384, distance=Distance.COSINE),
    optimizers_config=optimizers_config
)
```

---

## Performance

### Batch operations

```python
# ‚úÖ Bon : Insertion par batches
points = [PointStruct(...) for _ in range(1000)]
client.upsert(collection_name="products", points=points)

# ‚ùå Moins bon : Insertion point par point
for point in points:
    client.upsert(collection_name="products", points=[point])
```

### Pr√©-filtrage vs Post-filtrage

```python
# Pr√©-filtrage (recommand√© si peu de r√©sultats apr√®s filtrage)
results = client.search(
    collection_name="products",
    query_vector=vector,
    query_filter=filter,  # Filtre d'abord
    limit=10
)

# Post-filtrage (si beaucoup de r√©sultats apr√®s filtrage)
results = client.search(
    collection_name="products",
    query_vector=vector,
    limit=100  # Plus de r√©sultats
)
# Filtrer manuellement ensuite
```

### Optimiser les embeddings

```python
# ‚úÖ Bon : Batch encoding
embeddings = model.encode(texts, batch_size=32)

# ‚ùå Moins bon : Un par un
embeddings = [model.encode([text])[0] for text in texts]
```

### Utiliser le GPU

```python
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# Batch plus grand sur GPU
embeddings = model.encode(texts, batch_size=64)
```

---

## S√©curit√©

### Authentification

```python
# Connexion avec API key
client = QdrantClient(
    url="https://your-cluster.qdrant.io",
    api_key="your-api-key"
)
```

### Permissions

- Utiliser des API keys diff√©rentes par environnement
- Limiter les permissions par collection
- Ne pas exposer les API keys dans le code

### Donn√©es sensibles

```python
# ‚ùå Ne pas stocker de donn√©es sensibles dans le payload
payload = {
    "name": "Product",
    "password": "secret123"  # ‚ùå Ne jamais faire √ßa
}

# ‚úÖ Stocker seulement les donn√©es n√©cessaires
payload = {
    "name": "Product",
    "category": "electronics"
}
```

---

## Monitoring

### V√©rifier la sant√©

```python
# Informations de la collection
collection_info = client.get_collection("products")

print(f"Points: {collection_info.points_count}")
print(f"Indexed: {collection_info.indexed_vectors_count}")
print(f"Status: {collection_info.status}")
```

### Statistiques

```python
# Statistiques de la collection
stats = client.get_collection("products")

print(f"Vectors count: {stats.vectors_count}")
print(f"Indexed vectors: {stats.indexed_vectors_count}")
print(f"Points count: {stats.points_count}")
```

### Monitoring des performances

```python
import time

# Mesurer le temps de recherche
start = time.time()
results = client.search(
    collection_name="products",
    query_vector=vector,
    limit=10
)
duration = time.time() - start

print(f"Search took {duration:.3f}s")
print(f"Results: {len(results)}")
```

---

## Checklist de bonnes pratiques

### Configuration

- [ ] Dimension appropri√©e pour le cas d'usage
- [ ] Distance correcte (COSINE pour textes)
- [ ] HNSW configur√© selon la taille de la collection
- [ ] Quantisation activ√©e si n√©cessaire

### Donn√©es

- [ ] Payload structur√© et coh√©rent
- [ ] Champs fr√©quemment filtr√©s index√©s
- [ ] Pas de donn√©es sensibles dans le payload
- [ ] Payload pas trop volumineux

### Performance

- [ ] Insertion par batches
- [ ] Pr√©-filtrage utilis√© quand appropri√©
- [ ] Batch encoding pour les embeddings
- [ ] GPU utilis√© si disponible

### Maintenance

- [ ] Monitoring r√©gulier
- [ ] V√©rification de la sant√© des collections
- [ ] Backup r√©gulier
- [ ] Documentation √† jour

---

## üéØ Points cl√©s √† retenir

‚úÖ Dimension 384-512 pour textes, 128-256 pour grandes collections  
‚úÖ COSINE pour textes, EUCLID pour coordonn√©es  
‚úÖ HNSW m=16-32 pour √©quilibre performance/pr√©cision  
‚úÖ Indexer les champs fr√©quemment filtr√©s  
‚úÖ Utiliser batch operations pour meilleures performances  
‚úÖ Quantisation pour r√©duire la m√©moire  
‚úÖ Monitoring r√©gulier pour d√©tecter les probl√®mes  

---

**Prochaine √©tape :** [Projets Pratiques](./08-projets/README.md)
